2013.iwslt-papers.9,W06-1645,0,0.174384,"tion errors and language model perplexities, as well as the reduction in data size. Related research has been carried out earlier in the context of adapting an out-of-domain language model with in-domain data. A popular approach has been to train an in-domain language model and select text segments with low perplexity [3]. Klakow trained language models from out-of-domain data, computing the change in in-domain perplexity, when a text segment is removed from the training data [4]. Sethy et al. used relative entropy to match the distribution of the filtered data with the in-domain distribution [5]. Instead of scoring and filtering each text segment individually, they select text segments sequentally, adding a new segment to the selection if it reduces relative entropy with respect to the in-domain data. The algorithm was later revised to use a smoothed version of the Kullback-Leibler distance that uses a tunable smoothing parameter [6], with improved results. Moore and Lewis used formal reasoning to show that if the selection method is based on the probability (in terms of cross-entropy or perplexity) given by an in-domain language model to the training text segment, one should compare"
2013.iwslt-papers.9,P10-2041,0,0.0360184,"t text segments sequentally, adding a new segment to the selection if it reduces relative entropy with respect to the in-domain data. The algorithm was later revised to use a smoothed version of the Kullback-Leibler distance that uses a tunable smoothing parameter [6], with improved results. Moore and Lewis used formal reasoning to show that if the selection method is based on the probability (in terms of cross-entropy or perplexity) given by an in-domain language model to the training text segment, one should compare the probability to the probability given by an out-of-domain language model [7]. They computed the cross-entropy of each text segment according to an in-domain language model and an out-of-domain language model, and used the difference between the two cross-entropies as the selection criterion. From the above approaches the one proposed by Klakow requires the least amount of in-domain development data, since models are estimated only from the out-of-domain data. At the time we had very little in-domain development data of conversational Finnish (we used a set of 1047 utterances in these experiments), so this was the only applicable approach. The method may become computa"
2013.iwslt-papers.9,N07-1048,1,0.846782,"onal Finnish (we used a set of 1047 utterances in these experiments), so this was the only applicable approach. The method may become computationally demanding since it 1 The most notable difference between transcribed speech and written conversation is that disfluencies are usually omitted in writing. 2. Vocabulary in conversational Finnish speech recognition and perplexity computation The highly agglutinative nature of Finnish language makes it difficult to create an exhaustive vocabulary for speech recognition. Creutz et al. show a comparison of vocabulary growth across different languages [9]. While conversational speech is generally thought to be less diverse than planned speech, there are no less word forms used in conversational Finnish text than in a similar amount of literary Finnish. The reason is that the phonetic variation in conversational Finnish is translated into new vocabulary. Finnish orthography is very close to phonemic, meaning that written letters generally correspond to spoken phonemes. In informal conversations, phonetic variation is also often reflected in writing, even to the extent that sandhi is expressed in written form. For example, “en minä tiedä” is lit"
2013.iwslt-papers.9,iskra-etal-2002-speecon,0,0.0209559,"6 Size after filtering [millions of text segments] Figure 3: Perplexity and the number of OOV morphs on a held-out data set, with a growing amount of training data included in the order of devel-lp-1gram score data. Figure 3 shows how perplexity and OOV rate behave as a function of included training data, with a fixed morph segmentation. 5. Experimental setup for language model evaluation 5.1. Speech recognizer The speech recognition experiments were carried out using Aalto ASR system [15]. Our baseline model for recognizing standard Finnish has been trained on planned speech from the SPEECON [11] corpus. The model used in these experiments was trained on the SPEECON data, augmented with 176 minutes of our new development data, and 622 minutes of audio from FinDialogue, the conversational part of the FinINTAS corpus [16]. 5.2. Error measure Phonetic variation also creates challenges when measuring recognition accuracy. As most of the words can be pronounced in several slightly different ways, and the words are written out as they are pronounced, it would be harsh to compare recognition against the verbatim phonetic transcription. Thus word forms that are simply phonetic variation were"
2013.iwslt-papers.9,W02-0603,0,0.240174,"ection of text segments based on perplexity of such a model would prefer segments with high OOV rate. 3. Transcribed Finnish conversations 100 120 140 Corpus size [millions of words] Figure 2: Development of OOV rate, when all the encountered words are added to the vocabulary, on newspaper-style formal text and Internet conversations The standard approach for unlimited vocabulary Finnish language speech recognition has been to use statistical morphs as the basic language modeling unit, instead of words [12]. It seems that statistical morphs obtained by direct application of Morfessor Baseline [13] to the word list do not model conversational Finnish well. The reason may be insufficient quality or quantity of training data, or the pronunciation variation behind new word forms. Factored language models [14] is one way to alleviate the vocabulary size issue, but at the moment there are no tools for extracting meaningful factors from colloquial Finnish word forms. Development of such tools would be extremely difficult because of the numerous ways in which phonetic variation can alter the words. We tried conversational speech recognition with morphbased models, but so far there was no impro"
2013.iwslt-papers.9,N03-2002,0,0.0447915,"rate, when all the encountered words are added to the vocabulary, on newspaper-style formal text and Internet conversations The standard approach for unlimited vocabulary Finnish language speech recognition has been to use statistical morphs as the basic language modeling unit, instead of words [12]. It seems that statistical morphs obtained by direct application of Morfessor Baseline [13] to the word list do not model conversational Finnish well. The reason may be insufficient quality or quantity of training data, or the pronunciation variation behind new word forms. Factored language models [14] is one way to alleviate the vocabulary size issue, but at the moment there are no tools for extracting meaningful factors from colloquial Finnish word forms. Development of such tools would be extremely difficult because of the numerous ways in which phonetic variation can alter the words. We tried conversational speech recognition with morphbased models, but so far there was no improvement over word models in terms of word error rate. However, the perplexity computations in the text selection algorithm have been performed using morph models. The reason is that there are so many OOV words tha"
2020.lrec-1.486,K17-2001,0,0.060676,"Missing"
2020.lrec-1.486,W02-0603,0,0.267554,"example, the simple substitution dictionary based Byte Pair Encoding segmentation algorithm (Gage, 1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training a single language-independent subword segmentation method is preferable to linguistic segmentation (Arivazhagan et al., 2019). In this study, we compare three existing and one novel subword segmentation method, all sharing the use of a unigram language model in a generative modeling framework. The previously published methods are Morfessor Baseline (Creutz and Lagus, 2002), Greedy Unigram Likelihood (Varjokallio et al., 2013), and SentencePiece (Kudo, 2018). The new Morfessor variant proposed in this work is called Morfessor EM+Prune. The contributions of this article are (i) a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish; (ii) comparing four similar segmentation methods, including a close look at the SentencePiece reference implementation, highlighting details omitted from the original article (Kudo, 2018); (iii) and showing that the proposed"
2020.lrec-1.486,J11-2002,0,0.034351,"uited for languages with agglutinative morphology. While rule-based morphological segmentation systems can achieve high quality, the large amount of human effort needed makes the approach problematic, particularly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010’s; for a survey on the methods, see Hammarström and Borin (2011). Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see Ruokolainen et al. (2016) for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks (Cotterell et al., 2017), the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmentations for particular applications. For example, the simple substitution dictiona"
2020.lrec-1.486,W10-2210,1,0.755156,"Missing"
2020.lrec-1.486,D18-2012,0,0.03975,"hard-EM. During the hard-EM, frequency based pruning of subwords begins. In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold. 2.3. SentencePiece SentencePiece (Kudo and Richardson, 2018; Kudo, 2018) is a subword segmentation method aimed for use in any NLP system, particularly NMT. One of its design goals is use in multilingual systems. Although (Kudo, 2018) implies a use of maximum likelihood estimation, the reference implementation2 uses the implicit Dirichlet Process prior called Bayesian EM (Liang and Klein, 2007). In the M-step, the count normalization is modified to P (z) = exp(Ψ(Cz )) ∑ exp(Ψ( z′ Cz′ )) (6) where Ψ is the digamma function. The seed lexicon is simply the e.g. one million most frequent substrings. SentencePiece uses an EM+Prune training algorithm. Each"
2020.lrec-1.486,P18-1007,0,0.448825,"1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training a single language-independent subword segmentation method is preferable to linguistic segmentation (Arivazhagan et al., 2019). In this study, we compare three existing and one novel subword segmentation method, all sharing the use of a unigram language model in a generative modeling framework. The previously published methods are Morfessor Baseline (Creutz and Lagus, 2002), Greedy Unigram Likelihood (Varjokallio et al., 2013), and SentencePiece (Kudo, 2018). The new Morfessor variant proposed in this work is called Morfessor EM+Prune. The contributions of this article are (i) a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish; (ii) comparing four similar segmentation methods, including a close look at the SentencePiece reference implementation, highlighting details omitted from the original article (Kudo, 2018); (iii) and showing that the proposed Morfessor EM+Prune with particular hyper-parameters yields SentencePiece. 1.1. Morpho"
2020.lrec-1.486,W10-2211,1,0.648757,"ing only the likelihood ˆ = arg min{− log P (D |θ)} θ α 0.3 0.4 0.3 0.2 0.2 – (7) θ As the tuning parameter α is no longer needed when the prior is omitted, the pruning criterion can be set to a predetermined lexicon size, without automatic tuning of α. Morfessor by default uses type-based training; to use frequency information, count dampening should be turned off. The seed lexicon should be constructed without using forced splitting. The EM+Viterbi-prune training scheme should be used, with Bayesian EM turned on. English, Finnish and Turkish data are from the Morpho Challenge 2010 data set (Kurimo et al., 2010a; Kurimo et al., 2010b). The training sets contain ca 878k, 2.9M and 617k word types, respectively. As test sets we use the union of the 10 official test set samples. For North Sámi, we use a list of ca 691k word types extracted from Den samiske tekstbanken corpus (Sametinget, 2004). and the 796 word type test set from version 2 of the data set collected by (Grönroos et al., 2015; Grönroos et al., 2016). In most experiments we use a grid search with a development set to find a suitable value for α. The exception is experiments using autotuning or lexicon size criterion, and experiments using"
2020.lrec-1.486,J16-1003,1,0.936942,"arly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010’s; for a survey on the methods, see Hammarström and Borin (2011). Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see Ruokolainen et al. (2016) for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks (Cotterell et al., 2017), the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmentations for particular applications. For example, the simple substitution dictionary based Byte Pair Encoding segmentation algorithm (Gage, 1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training"
2020.lrec-1.486,D11-1117,0,0.0357899,"steriori (MAP) estimates for parameters in models with latent variables. The EM algorithm consists of two steps. In the E-step (2), the expected value of the complete data likelihood including the latent variable is taken, and in the M-step (3), the parameters are updated to maximize the expected value of the E-step: ∫ Q(θ, θ (i−1) ) = log P (D, y |θ)P (y |D, θ (i−1) )dy y (2) i θ = arg max Q(θ, θ (i−1) ). (3) θ When applied to a (hidden) Markov model, EM is called the forward-backward algorithm. Using instead the related Viterbi algorithm (Viterbi, 1967) is sometimes referred to as hard-EM.1 Spitkovsky et al. (2011) present lateen-EM, a hybrid variant in which EM and Viterbi optimization are alternated. Virpioja (2012, Section 6.4.1.3) discusses the challenges of applying EM to learning of generative morphology. Jointly optimizing both the morph lexicon and the parameters for the morphs is intractable. If, like in Morfessor Baseline, the cost function is discontinuous when morphs are added or removed from the lexicon, there is no closed form solution to the M-step. With ML estimates for morph probabilities, EM can neither add nor remove morphs from the lexicon, because it can neither change a zero probab"
2020.sltu-1.6,P19-1120,0,0.0244691,"orms adversely. We also study the impact on cross-lingual transfer due to the target data size and number of source model layers transferred. Relatively, smaller amounts of target language data than the source language data leads to more considerable ASR performance improvements. Moreover, we find that pretrained NNLMs perform best when we transfer only the parameters of the lowest layer of the source model. Multilingual training of language models has successfully leveraged datasets from other languages to improve Neural Network Language Modeling (NNLM) performance in low-resource scenarios (Kim et al., 2019; Conneau and Lample, 2019; Conneau et al., 2019; Aharoni et al., 2019). One such method for training NNLM is the multi-taskbased approach, where multiple language corpora train the model simultaneously (Aharoni et al., 2019). Another approach is cross-lingual pretraining, where the NNLM is trained on a set of source languages followed by fine-tuning on the target language (Kim et al., 2019; Conneau and Lample, 2019; Conneau et al., 2019). The second approach, explored in this work, is favorable when re-training with the large source data is time-consuming as an existing trained source model’s"
2020.sltu-1.6,P19-4007,0,0.031474,"Missing"
2020.textgraphs-1.8,D14-1162,0,0.0996457,"also capture contextual information. We also propose a method to train the embeddings on multiple constituency parse trees to ensure the encoding of global syntactic representation. Quantitative evaluation of the embeddings shows competitive performance on POS tagging task when compared to other types of embeddings, and qualitative evaluation reveals interesting facts about the syntactic typology learned by these embeddings. 1 Introduction Distributional similarity methods have been the standard learning representation in NLP. Word representations methods such as Word2vec, GloVe, and FastText [1, 2, 3] aim to create vector representation to words from other words or characters that mutually appear in the same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown"
2020.textgraphs-1.8,Q17-1010,0,0.0490375,"also capture contextual information. We also propose a method to train the embeddings on multiple constituency parse trees to ensure the encoding of global syntactic representation. Quantitative evaluation of the embeddings shows competitive performance on POS tagging task when compared to other types of embeddings, and qualitative evaluation reveals interesting facts about the syntactic typology learned by these embeddings. 1 Introduction Distributional similarity methods have been the standard learning representation in NLP. Word representations methods such as Word2vec, GloVe, and FastText [1, 2, 3] aim to create vector representation to words from other words or characters that mutually appear in the same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown"
2020.textgraphs-1.8,P14-2050,0,0.0578677,"ods such as Word2vec, GloVe, and FastText [1, 2, 3] aim to create vector representation to words from other words or characters that mutually appear in the same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown to be advantageous in specific NLP tasks such as question type classification[7], semantic role labeling[8], part-of-speech tagging[6], biomedical event trigger identification[9], and predicting brain activation patterns [10]. One limitation of these methods is that they do not encode the hierarchical syntactic structure of which a word is a part due to its reliance on nonconstituency parsing such as dependency parsing. While the latter analyzes the grammatical structure of a sentence by establishing a directed binary head-dependent relation amon"
2020.textgraphs-1.8,N15-1142,0,0.203048,"such as Word2vec, GloVe, and FastText [1, 2, 3] aim to create vector representation to words from other words or characters that mutually appear in the same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown to be advantageous in specific NLP tasks such as question type classification[7], semantic role labeling[8], part-of-speech tagging[6], biomedical event trigger identification[9], and predicting brain activation patterns [10]. One limitation of these methods is that they do not encode the hierarchical syntactic structure of which a word is a part due to its reliance on nonconstituency parsing such as dependency parsing. While the latter analyzes the grammatical structure of a sentence by establishing a directed binary head-dependent relation among it"
2020.textgraphs-1.8,N16-1175,0,0.021886,"that mutually appear in the same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown to be advantageous in specific NLP tasks such as question type classification[7], semantic role labeling[8], part-of-speech tagging[6], biomedical event trigger identification[9], and predicting brain activation patterns [10]. One limitation of these methods is that they do not encode the hierarchical syntactic structure of which a word is a part due to its reliance on nonconstituency parsing such as dependency parsing. While the latter analyzes the grammatical structure of a sentence by establishing a directed binary head-dependent relation among its words, constituency parsing analyzes the syntactic structure of a sentence according to a phrase structure grammar. Syntac"
2020.textgraphs-1.8,P16-1113,0,0.0281538,"same context. The underlying premise is that ”a word can be defined by its company” [4]. For example, in the sentences, ”I eat an apple every day” and ”I eat an orange every day”, the words ’orange’ and ’apple’ are similar as they share similar contexts. Recent approaches have proposed a syntax-based extension to distributional word embeddings to include functional similarity in the word vectors by leveraging the power of dependency parsing[5] [6]. Syntactic word embeddings have been shown to be advantageous in specific NLP tasks such as question type classification[7], semantic role labeling[8], part-of-speech tagging[6], biomedical event trigger identification[9], and predicting brain activation patterns [10]. One limitation of these methods is that they do not encode the hierarchical syntactic structure of which a word is a part due to its reliance on nonconstituency parsing such as dependency parsing. While the latter analyzes the grammatical structure of a sentence by establishing a directed binary head-dependent relation among its words, constituency parsing analyzes the syntactic structure of a sentence according to a phrase structure grammar. Syntactic hierarchy has advantage"
2020.textgraphs-1.8,D13-1170,0,0.00638576,"on nonconstituency parsing such as dependency parsing. While the latter analyzes the grammatical structure of a sentence by establishing a directed binary head-dependent relation among its words, constituency parsing analyzes the syntactic structure of a sentence according to a phrase structure grammar. Syntactic hierarchy has advantages in tasks such as grammar checking, question answering, and information extraction [11]. It has also been encoded in neural models such as Recursive Neural Tensor Network and has proved it can predict the compositional semantic effects of sentiment in language [12]. Moreover, it can uniquely disambiguate the functional role of some words and therefore the overall semantic meaning. Figure 1 shows the modal verb should in the following sentences: (1) Let me know should you have any question. and (2) I should study harder for the next exam. Even though the word should is a modal verb (MD) in both sentences, it exhibits two different grammatical functions: conditionality and necessity respectively. Similarly, the word is in (3) The king is at home. and (4) Is the king at home? has a similar semantic meaning in both sentences, yet it exhibits two different s"
2020.textgraphs-1.8,D15-1242,0,0.0190226,"N at VP NP PRP VBP you have NP DT NP NP NN home DT NN VBZ The king is question . PP IN at NN any . VP ? NP DT NP NN home (d) (b) . NP Is SBAR VP any VBZ VP (e) (c) Figure 1: Trees (a), (b), (d) and (e) show different positions of the words should and is respectively indicating differences in syntactic functions. Tree (c) is analogous to (a) suggesting similarity in syntactic function between should and if. 2 Related Work The NLP literature is rich with studies suggesting an improvement to original word embeddings models by incorporating external semantic resources like lexicons and ontologies [14, 15, 16, 17, 18]. However, very few studies were dedicated to syntactic embeddings. One of the earliest methods was dependency-based word embeddings [5], which generalizes the Skip-gram algorithm to include arbitrary word context. Instead of using bag-of-word context, they use context derived automatically from dependency parse trees. Specifically, for a word w with modifiers m1 , ..., mk and head h, the contexts (m1, lbl1 ), ..., (mk , lblk ), (h, lblh1 ), where lbl is a type of dependency relation between the head. and the modifier (e.g. nsub, dobj, etc). For example, the context for the word scientist in “"
2020.textgraphs-1.8,D17-1024,0,0.0176725,"N at VP NP PRP VBP you have NP DT NP NP NN home DT NN VBZ The king is question . PP IN at NN any . VP ? NP DT NP NN home (d) (b) . NP Is SBAR VP any VBZ VP (e) (c) Figure 1: Trees (a), (b), (d) and (e) show different positions of the words should and is respectively indicating differences in syntactic functions. Tree (c) is analogous to (a) suggesting similarity in syntactic function between should and if. 2 Related Work The NLP literature is rich with studies suggesting an improvement to original word embeddings models by incorporating external semantic resources like lexicons and ontologies [14, 15, 16, 17, 18]. However, very few studies were dedicated to syntactic embeddings. One of the earliest methods was dependency-based word embeddings [5], which generalizes the Skip-gram algorithm to include arbitrary word context. Instead of using bag-of-word context, they use context derived automatically from dependency parse trees. Specifically, for a word w with modifiers m1 , ..., mk and head h, the contexts (m1, lbl1 ), ..., (mk , lblk ), (h, lblh1 ), where lbl is a type of dependency relation between the head. and the modifier (e.g. nsub, dobj, etc). For example, the context for the word scientist in “"
2020.textgraphs-1.8,D18-1181,0,0.0181347,"N at VP NP PRP VBP you have NP DT NP NP NN home DT NN VBZ The king is question . PP IN at NN any . VP ? NP DT NP NN home (d) (b) . NP Is SBAR VP any VBZ VP (e) (c) Figure 1: Trees (a), (b), (d) and (e) show different positions of the words should and is respectively indicating differences in syntactic functions. Tree (c) is analogous to (a) suggesting similarity in syntactic function between should and if. 2 Related Work The NLP literature is rich with studies suggesting an improvement to original word embeddings models by incorporating external semantic resources like lexicons and ontologies [14, 15, 16, 17, 18]. However, very few studies were dedicated to syntactic embeddings. One of the earliest methods was dependency-based word embeddings [5], which generalizes the Skip-gram algorithm to include arbitrary word context. Instead of using bag-of-word context, they use context derived automatically from dependency parse trees. Specifically, for a word w with modifiers m1 , ..., mk and head h, the contexts (m1, lbl1 ), ..., (mk , lblk ), (h, lblh1 ), where lbl is a type of dependency relation between the head. and the modifier (e.g. nsub, dobj, etc). For example, the context for the word scientist in “"
2020.textgraphs-1.8,N18-1202,0,0.0393129,"alue. Results on syntax-based tasks such as POS tagging and parsing show an improvement over classic word2vec embeddings. More recently, a new approach, named SynGCN, for learning dependency-based syntactic embeddings is introduced by [19]. SynGCN builds syntactic word representation by using Graph Convolution Network (GCN). Using GCN allows SynGCN to capture global information from the graph on which it was trained while remaining efficient at training due to parallelization. Experiments show that SynGCN obtains improvement over state-of-the-art approaches when used with methods such as ELMo [20]. Most syntactic word embeddings methods rely on dependency parsing, and to the best of our knowledge that our work is the first utilizing constituency parsing to build syntactic representation. 73 3 Method Our goal is to learn word embeddings that not only capture the sentence-level syntactic hierarchy encoded by the constituency parse tree, but also capture a global (suprasentential) syntactic representation, and because the constituency parse tree only provides sentence-level syntactic representations, we need a method to combine multiple constituency parse trees. We also need a flexible al"
2020.wnut-1.16,D18-1547,0,0.0518851,"Missing"
2020.wnut-1.16,P19-1360,0,0.0136963,"d similarly. Prior work (Walker et al., 2018; Budzianowski et al., 2018) has been concerned about the unnatural process of dialogue generation in the M2M approach. In our perspective, this issue affects scenarios where a simulated user cannot model the ambiguities of a real user, but for a simplistic SRT 119 4 https://github.com/Molteh/M2M use case, we disregard this issue. For creating the NLG module, we focus on the generation of surface expression based on sequences of dialogue acts. Similarly, quite a few prior work (Stent, 2001; Wen et al., 2015; Liu and Liu, 2019; Varshney et al., 2020; Chen et al., 2019; Nayak et al., 2017) have employed semantic structures to generate dialogue utterances. Stent (2001) leveraged custom dialogue acts to implement a rule-based utterance generator as part of a bigger modular conversational system. Recently, LSTMbased machine translation models (Wen et al., 2015; Nayak et al., 2017) and Transformers (Liu and Liu, 2019; Varshney et al., 2020; Chen et al., 2019) have also been successfully explored in NLG tasks for open-domain and task-specific dialogue systems. For both open-domain and task-specific modules, large corpora of annotations are required for training"
2020.wnut-1.16,N07-2038,0,0.0616888,"setup the annotated data collection. Conceived as being domain-independent, M2M generates dialogues centered on completing a specific task. The M2M consists of four major steps. 1), the developer provides the task-specific knowledge used by the system. It can be seen as a collection of all the units of information exchanged during the dialogue. 2) Given a task specification, a simulated interaction of a user and the system generates sequences of dialogue acts exhaustively. The output sequences enclose the semantic content of the dialogue. The user is modeled as an agenda-based user simulator (Schatzmann et al., 2007) while the system is designed as a Mealy machine. This process is also called self-play, where a simulated user interacts with the system. A generated example is shown in the first row of Table 1. 3) Using the semantic parses, we can then build dialogue templates using a simple domain grammar. The templates are slightly unnatural computer-generated dialogue utterances paired with their semantic representation in the form of dialogue acts (second row of Table 1). 4) Finally, the dialogue templates enter a paraphrasing phase where crowdsource workers provide natural and contextual rewrites of th"
2020.wnut-1.16,N18-3006,0,0.0402382,"Missing"
2020.wnut-1.16,L18-1628,0,0.0169387,"wo types of crowdsourcing methods. Earlier work (Kittur et al., 2008) has shown that AMT workers achieve significantly lower performances when the degree of experience and contextual knowledge is important. However, their performance improves with a more guided task structure. In our experiment, the service’s users already had the background knowledge necessary for the task. Moreover, considering the generated dialogue’s lexical richness and diversity, their paraphrases were ranked higher than AMT workers. However, at a qualitative level, both types of paraphrase ranked similarly. Prior work (Walker et al., 2018; Budzianowski et al., 2018) has been concerned about the unnatural process of dialogue generation in the M2M approach. In our perspective, this issue affects scenarios where a simulated user cannot model the ambiguities of a real user, but for a simplistic SRT 119 4 https://github.com/Molteh/M2M use case, we disregard this issue. For creating the NLG module, we focus on the generation of surface expression based on sequences of dialogue acts. Similarly, quite a few prior work (Stent, 2001; Wen et al., 2015; Liu and Liu, 2019; Varshney et al., 2020; Chen et al., 2019; Nayak et al., 2017) have"
2020.wnut-1.16,D15-1199,0,0.0409215,"Missing"
2021.nodalida-main.36,W17-0122,0,0.0492109,"MAUS) is a popular aligner based on its own speech recognition framework, utilizing a statistical expert system of pronunciation. 2.2 Cross-language forced alignment Forced alignment has also been successfully used across languages, e.g., when the target language does not have enough transcribed data. This task is called cross-language or cross-linguistic forced alignment (CLFA), sometimes untrained forced alignment. Kempton et al. (2011) used their own phonetic distance metric to evaluate the accuracy of three phoneme recognizers on isolated words from under-resourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019"
2021.nodalida-main.36,kurtic-etal-2012-corpus,0,0.036464,"s languages, e.g., when the target language does not have enough transcribed data. This task is called cross-language or cross-linguistic forced alignment (CLFA), sometimes untrained forced alignment. Kempton et al. (2011) used their own phonetic distance metric to evaluate the accuracy of three phoneme recognizers on isolated words from under-resourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019) compared MAUS’ language-independent and Italian versions for conversational speech in 1 https://github.com/lowerquality/gentle Kriol, finding that the Italian version surpassed the language-independent one. A unifying method"
2021.nodalida-main.36,strunk-etal-2014-untrained,0,0.0160417,"sourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019) compared MAUS’ language-independent and Italian versions for conversational speech in 1 https://github.com/lowerquality/gentle Kriol, finding that the Italian version surpassed the language-independent one. A unifying method was presented by Tang and Bennett (2019), who combined a larger source language and the target language with MFA to train the aligner. Finally Johnson et al. (2018) reviewed previous CLFA research and experimented on the minimum amount of data necessary for language dependent forced alignment, achieving good results with an hour of transcribed s"
C14-1111,I13-1152,0,0.0206284,"+FlatCat CatMAP CatMAP CRF+FlatCat FlatCat (First 5) Baseline CRF Baseline Baseline FlatCat FlatCat (Words) No Yes Yes – Yes Yes No No No – – – – – Yes No – 0.5057 0.5029 0.4987 0.4973 0.4912 0.4884 0.4865 0.4826 0.4821 0.4757 0.4722 0.4660 0.4582 0.4378 0.4349 0.4334 0.3483 Table 5: Information Retrieval results. Results of the method presented in this paper are hilighted using boldface. Mean Average Precision is abbreviated as MAP. Short affix removal is abbreviated as SAR. help disambiguate inflections of different lexemes that have the same surface form but should be analyzed differently (Can and Manandhar, 2013). The second direction is removal of the assumption that a morphology consists only of concatenative processes. Introducing transformations to model allomorphy in a similar manner as Kohonen et al. (2009) would allow finding the shared abstract morphemes underlying different allomorphs. This could be especially beneficial in information retrieval and machine translation applications. Acknowledgments This research has been supported by European Community’s Seventh Framework Programme (FP7/2007–2013) under grant agreement n°287678 and the Academy of Finland under the Finnish Centre of Excellence"
C14-1111,W02-0603,0,0.767845,"in, a small amount of linguistic expertise is more easily available. A well-informed native speaker of a language can often identify the different prefixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference. One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually segmented words provide significant improvements to the quality of the output when comparing to a linguistic gold standard. The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the simplest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor Baseline is simply a categorical distribution of morphs—a unigram model in the terms of statistical language modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semisupervised methods evaluated in the Morpho Challenge competitions (Kurimo et al., 2010a) so far, the next question is how the approach works for more complex models. Another popular variant of Morfessor, Categories-MAP (CatMAP) (Creutz and Lagus, 2005), models word formation using a hidden Markov model (HMM). The context-sensitivity of the model improves"
C14-1111,W04-0106,0,0.384366,"number of bits needed to encode both the model parameters and the training data. Equivalently, the cost function L can be derived from the Maximum a Posteriori (MAP) estimate: ( ) θˆ = arg max P(θ |D) = arg min − log P(θ) − log P(D |θ) = arg min L(θ, D), (1) θ θ θ where θ are the model parameters, D is the training corpus, P(θ) is the prior of the parameters and P(D |θ) is the data likelihood. In context-independent models such as Morfessor Baseline, the parameters include only the forms and probabilities of the morphs in the lexicon of the model. Morfessor Baseline and Categories-ML (CatML) (Creutz and Lagus, 2004) use a flat lexicon, in which the forms of the morphs are encoded directly as strings: each letter requires a certain number of bits to encode. Thus longer morphs are more expensive. Encoding a long morph is worthwhile only if the morph is referred to frequently enough from the words in the training data. If a certain string, let us say segmentation, is common enough in the training data, it is cost-effective to have it as a whole in the lexicon. Splitting it into two items, segment and ation, would double the number of pointers from the data, even if those morphs were already in the lexicon."
C14-1111,J11-2002,0,0.15398,"Missing"
C14-1111,W10-2210,1,0.474499,"al segmentation, i.e., finding morphs, the surface forms of the morphemes. For language processing applications, unsupervised learning of morphology can provide decentquality analyses without resources produced by human experts. However, while morphological analyzers and large annotated corpora may be expensive to obtain, a small amount of linguistic expertise is more easily available. A well-informed native speaker of a language can often identify the different prefixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference. One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually segmented words provide significant improvements to the quality of the output when comparing to a linguistic gold standard. The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the simplest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor Baseline is simply a categorical distribution of morphs—a unigram model in the terms of statistical language modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semisupervised methods evaluated in the"
C14-1111,W10-2211,1,0.927355,"Missing"
C14-1111,W13-3504,1,0.683156,"annotations from the lexicon cannot be selected, as such an operation would have infinite cost. 3 Experiments We compare Morfessor FlatCat1 to two previous Morfessor methods and a fully supervised discriminative segmentation method. The Morfessor methods used as references are the CatMAP2 and Baseline3 implementations by Creutz and Lagus (2005) and Virpioja et al. (2013), respectively. Virpioja et al. (2013) implements the semi-supervised method described by Kohonen et al. (2010). For a supervised discriminative model, we use a character-level conditional random field (CRF) implementation by Ruokolainen et al. (2013)4 . We use the English, Finnish and Turkish data sets from Morpho Challenge 2010 (Kurimo et al., 2010b). They include large unannotated word lists, one thousand annotated words for training, 700– 800 annotated words for parameter tuning, and 10 × 1000 annotated words for testing. For evalution, we use the BPR score by Virpioja et al. (2011). The score calculates the precision (Pre), recall (Rec), and F1 -score (F) of the predicted morph boundaries compared to a linguistic gold standard. In the presence of alternative gold standard analyses, we weight each alternative equally. We also report th"
E14-2006,W10-2210,1,0.551193,"for Computational Linguistics An extension of the Viterbi algorithm is used for decoding, that is, finding the optimal segmentations for new compound forms without changing the model parameters. given the observed training data D W : θ MAP = arg max p(θ)p(D W |θ) (1) θ Thus we are maximizing the product of the model prior p(θ) and the data likelihood p(D W |θ). As usual, the cost function to minimize is set as the minus logarithm of the product: 3 3.1 Semi-supervised extensions One important feature that has been implemented in Morfessor 2.0 are the semi-supervised extensions as introduced by Kohonen et al. (2010) Morfessor Baseline tends to undersegment when the model is trained for morphological segmentation using a large corpus (Creutz and Lagus, 2005b). Oversegmentation or undersegmentation of the method are easy to control heuristically by including a weight parameter α for the likelihood in the cost function. A low α increases the priors influence, favoring small construction lexicons, while a high value increases the data likelihood influence, favoring longer constructions. In semi-supervised Morfessor, the likelihood of an annotated data set is added to the cost function. As the amount of annot"
E14-2006,2007.mtsummit-papers.65,1,0.293402,"Missing"
E14-2006,W02-0603,0,0.0428573,"d. In addition to morphological segmentation, it can handle, for example, sentence chunking. To reflect this we use the following generic terms: The smallest unit that can be split will be an atom (letter). A compound (word) is a sequence of atoms. A construction (morph) is a sequence of atoms contained inside a compound. In the morphological segmentation task, the goal is to segment words into morphemes, the smallest meaning-carrying units. Morfessor is a family of methods for unsupervised morphological segmentation. The first version of Morfessor, called Morfessor Baseline, was developed by Creutz and Lagus (2002) its software implementation, Morfessor 1.0, released by Creutz and Lagus (2005b). A number of Morfessor variants have been developed later, including Morfessor Categories-MAP (Creutz and Lagus, 2005a) and Allomorfessor (Virpioja et al., 2010). Even though these algorithms improve Morfessor Baseline in some areas, the Baseline version has stayed popular as a generally applicable morphological analyzer (Spiegler et al., 2008; Monson et al., 2010). Over the past years, Morfessor has been used for a wide range of languages and applications. The applications include large vocabulary continuous spe"
E14-4015,D10-1095,0,0.0216271,"mber of tags in the data set and number of tags in the training set, respectively. POS tagging. Our preliminary experiments using the latest violation updates supported this. Consequently, we employ the early updates. We also provide results using the CRFsuite toolkit (Okazaki, 2007), which implements a 1storder CRF model. To best of our knowledge, CRFsuite is currently the fastest freely available CRF implementation.3 In addition to the averaged perceptron algorithm (Collins, 2002), the toolkit implements several training procedures (Nocedal, 1980; Crammer et al., 2006; Andrew and Gao, 2007; Mejer and Crammer, 2010; Shalev-Shwartz et al., 2011). We run CRFsuite using these algorithms employing their default parameters and the feature extraction scheme and stopping criterion described in Section 3.3. We then report results provided by the most accurate algorithm on each language. Experimental Setup Data For a quick overview of the data sets, see Table 1. Penn Treebank. The first data set we consider is the classic Penn Treebank. The complete treebank is divided into 25 sections of newswire text extracted from the Wall Street Journal. We split the data into training, development, and test sets using the s"
E14-4015,P04-1015,0,0.060625,"ity, firstname.lastname@aalto.fi b Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract quired by the stochastic gradient descent algorithm employed in ML estimation (Vishwanathan et al., 2006). Additionally, while ML and perceptron training share an identical time complexity, the perceptron is in practice faster due to sparser parameter updates. Despite its simplicity, running the perceptron algorithm can be tedious in case the data contains a large number of labels. Previously, this problem has been addressed using, for example, k-best beam search (Collins and Roark, 2004; Zhang and Clark, 2011; Huang et al., 2012) and parallelization (McDonald et al., 2010). In this work, we explore an alternative strategy, in which we modify the perceptron algorithm in spirit of the classic pseudo-likelihood approximation for ML estimation (Besag, 1975). The resulting novel algorithm has linear complexity w.r.t. the label set size and contains only a single hyper-parameter, namely, the number of passes taken over the training data set. We evaluate the algorithm, referred to as the pseudo-perceptron, empirically in POS tagging on five languages. The results suggest that the a"
E14-4015,W02-1001,0,0.871589,"l sequence labeling tasks in natural language processing, including part-of-speech (POS) tagging. In this work, we discuss accelerating the CRF model estimation in presence of a large number of labels, say, hundreds or thousands. Large label sets occur in POS tagging of morphologically rich languages (Erjavec, 2010; Haverinen et al., 2013). CRF training is most commonly associated with the (conditional) maximum likelihood (ML) criterion employed in the original work of Lafferty et al. (2001). In this work, we focus on an alternative training approach using the averaged perceptron algorithm of Collins (2002). While yielding competitive accuracy (Collins, 2002; Zhang and Clark, 2011), the perceptron algorithm avoids extensive tuning of hyper-parameters and regularization re2 2.1 Methods Pseudo-Perceptron Algorithm The (unnormalized) CRF model for input and output sequences x = (x1 , x2 , . . . , x|x |) and 74 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74–78, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics are decoded in a standard manner using the Viterbi search. The appeal of PP is that"
E14-4015,W96-0213,0,0.394504,"modified using the early update rule (Huang et al., 2012). While Huang et al. (2012) experimented with several violation-fixing methods (early, latest, maximum, hybrid), they appeared to reach termination at the same rate in 3 See benchmark results at http://www.chokkan. org/software/crfsuite/benchmark.html 76 has not increased during last three iterations. After termination, we apply the averaged parameters yielding highest performance on the development set to test instances. Test and development instances are decoded using a combination of Viterbi search and the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 method English PP PW-PP 1-best beam Pas.-Agg. Romanian PP PW-PP 1-best beam Pas.-Agg. Estonian PP PW-PP 1-best beam Pas.-Agg. Czech PP PW-PP 1-best beam Pegasos Finnish PP PW-PP 1-best beam Pas.-Agg. Software and Hardware The experiments are run on a standard desktop computer. We use our own C++-based implementation of the methods discussed in Section 2. 4 Results The obtained training times and test"
E14-4015,erjavec-2010-multext,0,0.111636,". We present experiments on five languages. Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods. 1 Introduction The conditional random field (CRF) model (Lafferty et al., 2001) has been successfully applied to several sequence labeling tasks in natural language processing, including part-of-speech (POS) tagging. In this work, we discuss accelerating the CRF model estimation in presence of a large number of labels, say, hundreds or thousands. Large label sets occur in POS tagging of morphologically rich languages (Erjavec, 2010; Haverinen et al., 2013). CRF training is most commonly associated with the (conditional) maximum likelihood (ML) criterion employed in the original work of Lafferty et al. (2001). In this work, we focus on an alternative training approach using the averaged perceptron algorithm of Collins (2002). While yielding competitive accuracy (Collins, 2002; Zhang and Clark, 2011), the perceptron algorithm avoids extensive tuning of hyper-parameters and regularization re2 2.1 Methods Pseudo-Perceptron Algorithm The (unnormalized) CRF model for input and output sequences x = (x1 , x2 , . . . , x|x |) an"
E14-4015,N12-1015,0,0.0303934,"Missing"
E14-4015,J11-1005,0,0.28515,"alto.fi b Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract quired by the stochastic gradient descent algorithm employed in ML estimation (Vishwanathan et al., 2006). Additionally, while ML and perceptron training share an identical time complexity, the perceptron is in practice faster due to sparser parameter updates. Despite its simplicity, running the perceptron algorithm can be tedious in case the data contains a large number of labels. Previously, this problem has been addressed using, for example, k-best beam search (Collins and Roark, 2004; Zhang and Clark, 2011; Huang et al., 2012) and parallelization (McDonald et al., 2010). In this work, we explore an alternative strategy, in which we modify the perceptron algorithm in spirit of the classic pseudo-likelihood approximation for ML estimation (Besag, 1975). The resulting novel algorithm has linear complexity w.r.t. the label set size and contains only a single hyper-parameter, namely, the number of passes taken over the training data set. We evaluate the algorithm, referred to as the pseudo-perceptron, empirically in POS tagging on five languages. The results suggest that the approach can yield compe"
E14-4015,N10-1069,0,\N,Missing
E14-4017,W02-1001,0,0.687613,"CRFs (Lafferty et al., 2001). Formally, the linear-chain CRF model distribution for label sequence y = (y1 , y2 , . . . , yT ) and a word form x = (x1 , x2 , . . . , xT ) is written as a conditional probability p (y |x; w) ∝ T Y Leveraging Unannotated Data   exp w · φ(yt−1 , yt , x, t) , t=2 (1) where t indexes the character positions, w denotes the model parameter vector, and φ the vectorvalued feature extracting function. The model parameters w are estimated discrimatively based on a training set of exemplar input-output pairs (x, y) using, for example, the averaged perceptron algorithm (Collins, 2002). Subsequent to estimation, the CRF model segments test word forms using the Viterbi algorithm (Lafferty et al., 2001). We next describe the feature set |φ| {φi (yt−1 , yt , x, t)}i=1 by defining emission and transition features. Denoting the label set {B, M, S} as Y, the emission feature set is defined as t xt υ(t) 1 d 1 2 r 0 3 i 0 4 v 0 5 e 1 6 r 0 7 s 0 Now, given a set of U functions {υu (t)}U u=1 , we define variants of the emission features in (2) as {υu (x, t)χm (x, t)1(yt = yt0 ) | ∀u ∈ 1..U , ∀m ∈ 1..M , ∀yt0 ∈ Y} . {χm (x, t)1(yt = yt0 ) |m ∈ 1..M , ∀yt0 ∈ Y} , (2) where the indicat"
E14-4017,W10-2210,1,0.925267,"d the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we extend the CRF-based approach to leverage unannotated data in a straightforward and computationally efficient manner via feature set augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu,"
E14-4017,W02-0603,0,0.834941,"t setup, including data partitions and evaluation metrics, described by Ruokolainen et al. (2013). Table 1 shows the total number of instances available for model estimation and testing. 3.2 CRF Feature Extraction and Training The substring features included in the CRF model are described in Section 2.1. We include all substrings which occur in the training data. The Morfessor and Harris (successor and predecessor variety) features employed by the semi-supervised extension are described in Section 2.2. We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. The Baseline models were trained on word types and the perplexity thresholds of the CatMAP models were set equivalently to the reference runs in Morpho Challenge 2010 (English: 450, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length"
E14-4017,W13-3512,0,0.0407979,"field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner. 1 Introduction We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally 84 Proceedings of the 14th Conference"
E14-4017,P08-1099,0,0.0213946,"augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage"
E14-4017,W13-3505,0,0.0213219,"mentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our ex"
E14-4017,N09-1024,0,0.0840226,"Missing"
E14-4017,W13-3504,1,0.903186,"(Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learning by exploiting available unsupervised segmentation techniques. We integrate the unsupervised techniques into the conditional random field model vi"
E14-4017,D11-1090,0,0.0357625,"t al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely"
E14-4017,P10-1040,0,0.0563713,"ture set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learn"
E14-4017,I11-1035,0,\N,Missing
E14-4017,P06-1027,0,\N,Missing
I08-3020,P95-1034,0,0.0122631,"ntic representations, often in a tree that could be generated by a context-free grammar. The resulting semantic representation can then be used as the source of a target-language generation process. The algorithm that generates such a representation from raw input could be trained on a treebank, and an annotated form of the same corpus (where the derivations in the generation space are associated with counts for each decision made) can be used to train the output component to generate language. (Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). This helps address many of the knowledge gap problems introduced by linguistic differences or in a component of the system - the analyzer or generator. This approach does have significant advantages, particularly in that it is more focused on semantics (as opposed to statistical cooccurrence), so it may be less likely to distort meaning. On the other hand, it could misinterpret or miscommunicate (or both), just like a human translator. Perhaps the crucial difference is that, while machine learning often has li"
I08-3020,P07-2045,0,0.0040378,"World English Bible (1997) and Finnish Bible (Raamattu) readings (recorded at TKK 2004). Our approach was to use existing components, and try weaving them together in an optimal way. First, there is the open vocabulary automatic speech recognition (ASR) task, where the goal is to detect phonemes in an acoustic signal and map them to words. Here, we use an “unlimited vocabulary” continuous speech recognizer (Hirsim¨aki et al., 2006), trained on a multi-speaker Finnish acoustic model with a varigram (Siivola et al., 2007) language model that includes Bible n-grams. Then, for translation, Moses (Koehn et al., 2007) is trained on words and morphemes (derived from Morfessor Baseline (Creutz and Lagus, 2005)). For speech synthesis, we used Festival (Taylor, 1999), including the built-in English voice and a Finnish voice developed at Helsinki University. 5.3 Results The following is an example fragment, taken from the test corpus. Niin Daavid meni lepoon isiens¨a luo, ja h¨anethaudattiin Daavidin kaupunkiin. Nelj¨akymment¨a vuotta h¨an oli ollut Israelin kuninkaana. Hebronissa h¨an hallitsi seitsem¨an vuotta, Jerusalemissa kolmenkymmenenkolmen vuoden ajan. Salomo nousi is¨ans¨a Daavidin valtaistuimelle,ja h"
I08-3020,W05-1601,0,0.0206321,"nerally depends on a probabilistic model trained on sentences aligned with their syntactic and semantic representations, often in a tree that could be generated by a context-free grammar. The resulting semantic representation can then be used as the source of a target-language generation process. The algorithm that generates such a representation from raw input could be trained on a treebank, and an annotated form of the same corpus (where the derivations in the generation space are associated with counts for each decision made) can be used to train the output component to generate language. (Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). This helps address many of the knowledge gap problems introduced by linguistic differences or in a component of the system - the analyzer or generator. This approach does have significant advantages, particularly in that it is more focused on semantics (as opposed to statistical cooccurrence), so it may be less likely to distort meaning. On the other hand, it could misinterpret or miscommun"
I08-3020,N07-1048,1,0.811574,"Efficiency is crucial in online translation of conversation, so a word alignment model with collapsed Gibbs sampling, rather than EM, at its core is worth experimenting with. We have written up a barebones IBM Model 1 in both C++ and Python, using the standard EM approach and a Gibbs sampling one. The latter allows for optimizations using linear algebra, and although it does not quite match the perplexity or log-likelihood achieved by EM, it is significantly faster, particularly on longer sentences. Since morpheme segmentation is at least somewhat helpful in speech recognition (Creutz, 2006; Creutz et al., 2007), it should still be considered a potential component in speech-to-speech translation. In terms of incorporating the knowledge-based approach into such a system, we think it may yet be too early, but if existing understanding-and-generation frameworks for machine translation could be adapted to this use, it could be very fruitful, in particular since spoken language generation might be more effective from a knowledge base, since it would know what it was trying to say, instead of relying on statistics alone, hoping the phonemes end up in a meaningful order. The critical step of SST is, of cour"
I08-3020,2007.mtsummit-papers.65,1,0.892184,"Missing"
J16-1003,chrupala-etal-2008-learning,0,0.0617205,"Missing"
J16-1003,W02-1001,0,0.180815,"ly sparser statistics. Subsequent to defining the label set, one can learn a segmentation model using general sequence labeling methods, such as the well-known conditional random field (CRF) framework (Lafferty, McCallum, and Pereira 2001). Denoting the word form and the corresponding label sequence as x and y, respectively, the CRFs directly model the conditional probability of the segmentation given the word form, that is, p(y |x; w). The model parameters w are estimated discriminatively from the annotated data set D using iterative learning algorithms (Lafferty, McCallum, and Pereira 2001; Collins 2002). Subsequent to estimation, the CRF model segments word forms x by using maximum a posteriori (MAP) graph inference, that is, solving an optimization problem z = arg max p (u |x; w) (3) u using the standard Viterbi search (Lafferty, McCallum, and Pereira 2001). As it turns out, the CRF model can learn to segment words with a surprisingly high accuracy from a relatively small D, that is, without utilizing any of the available unannotated word forms U. Particularly, Ruokolainen et al. (2013) showed that it is sufficient to use simple left and right substring context features that are naturally a"
J16-1003,W02-0603,0,0.294185,"Missing"
J16-1003,N09-2019,1,0.529056,"g the LSV/LPV scores from unannotated data and, subsequently, tuning the necessary threshold values on the annotated data (Çöltekin 2010). On the other hand, one could also use the LSV/LPV values as features for a classification model, in which case the threshold values can be learned discriminatively based on the available annotated data. The latter approach is essentially realized in the event the LSV/PSV scores are provided for the CRF model discussed earlier (Ruokolainen et al. 2014). As for more recent work, we first refer to the generative log-linear model of Poon, Cherry, and Toutanova (2009). Similarly to the Morfessor model family, this approach is based on defining a joint probability distribution over the unannotated word forms U and the corresponding segmentations S. The distribution is log-linear in form and is denoted as p(U, S; θ ), where θ is the model parameter vector. Again, similarly to the Morfessor framework, Poon, Cherry, and Toutanova (2009) learn a morph lexicon that is subsequently used to generate segmentations for new word forms. The learning is controlled using prior distributions on both corpus and lexicon, which penalize exceedingly complex morph lexicon (si"
J16-1003,P12-1016,0,0.064292,"e suffix -s marks the plural number. Although this is a major simplification of the diverse morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motiv"
J16-1003,C14-1111,1,0.0715702,"the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting with such a small amount of supervision as minimally supervised learning. In consequence, similar to the unsupervised methods, the minimally supervised techniques can be seen as a means of acquiring a type of morphological analysis for under-resourced langua"
J16-1003,J11-2002,0,0.106899,"the conditional random field method (Ruokolainen et al. 2013, 2014). We hope the presented discussion and empirical evaluation will be of help for future research on the considered task. The rest of the article is organized as follows. In Section 2, we provide an overview of related studies. We then provide a literature survey of published morphological segmentation methodology in Section 3. Experimental work is presented in Section 4. Finally, we provide a discussion on potential directions for future work and conclusions on the current work in Sections 5 and 6, respectively. 2. Related Work Hammarström and Borin (2011) presented a literature survey on unsupervised learning of morphology, including methods for learning morphological segmentation. Whereas the discussion provided by Hammarström and Borin focuses mainly on linguistic aspects of morphology learning, our work is strongly rooted in machine learning methodology and empirical evaluation. In addition, whereas Hammarström and Borin focus entirely on unsupervised learning, our work considers a broader range of learning paradigms. Therefore, although related, Hammarström and Borin and our current presentation are complementary in that they have differen"
J16-1003,P06-1027,0,0.250174,"itting some of the training data can improve segmentation accuracy (Virpioja, Kohonen, and Lagus 2011; Sirts and Goldwater 2013). For discriminative models, the possibly most straightforward semi-supervised learning technique is adding features derived from the unlabeled data, as exemplified by the CRF approach of Ruokolainen et al. (2014). However, discriminative, semi-supervised learning is in general a much researched field with numerous diverse techniques (Zhu and Goldberg 2009). For example, merely for the CRF model alone, there exist several proposed semi-supervised learning approaches (Jiao et al. 2006; Mann and McCallum 2008; Wang et al. 2009). On Local Search. In what follows, we will discuss a potential pitfall of some algorithms that utilize local search procedures in the parameter estimation process, as exemplified by the Morfessor model family (Creutz et al. 2007). As discussed in Section 3.3.1, the Morfessor algorithm finds a local optimum of the objective function using a local search procedure. This complicates model development because if two model variants perform differently empirically, it is uncertain whether it is because of a truly better model or merely better fit with the"
J16-1003,N03-2015,0,0.135092,"Missing"
J16-1003,W08-0704,0,0.0342654,"xes, a stem, and zero or more suffixes. The actual forms of the morphs are learned from the data and, subsequent to learning, used to generate segmentations for new word forms. In this general approach, AGs are similar to the Morfessor family (Creutz and Lagus 2007). A major difference, however, is that the morphological grammar is not hard-coded but instead specified as an input to the algorithm. This allows different grammars to be explored in a flexible manner. Prior to the work by Sirts and Goldwater, the AGs were successfully applied in a related task of segmenting utterances into words (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010). The second major difference between the Morfessor family and the AG framework is the contrast between the MAP and fully Bayesian estimation approaches. Whereas the search procedure of the Morfessor method discussed earlier returns a single model corresponding to the MAP point-estimate, AGs instead operate with full posterior distributions over all possible models. Because acquiring the posteriors analytically is intractable, inference is performed utilizing Markov chain Monte Carlo algorithms to obtain samples from the posterior distribut"
J16-1003,C10-1060,0,0.021178,"e actual forms of the morphs are learned from the data and, subsequent to learning, used to generate segmentations for new word forms. In this general approach, AGs are similar to the Morfessor family (Creutz and Lagus 2007). A major difference, however, is that the morphological grammar is not hard-coded but instead specified as an input to the algorithm. This allows different grammars to be explored in a flexible manner. Prior to the work by Sirts and Goldwater, the AGs were successfully applied in a related task of segmenting utterances into words (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010). The second major difference between the Morfessor family and the AG framework is the contrast between the MAP and fully Bayesian estimation approaches. Whereas the search procedure of the Morfessor method discussed earlier returns a single model corresponding to the MAP point-estimate, AGs instead operate with full posterior distributions over all possible models. Because acquiring the posteriors analytically is intractable, inference is performed utilizing Markov chain Monte Carlo algorithms to obtain samples from the posterior distributions of interest (Johnson 2008; Johnson and Goldwater"
J16-1003,N09-1036,0,0.145586,"g the LSV/LPV scores from unannotated data and, subsequently, tuning the necessary threshold values on the annotated data (Çöltekin 2010). On the other hand, one could also use the LSV/LPV values as features for a classification model, in which case the threshold values can be learned discriminatively based on the available annotated data. The latter approach is essentially realized in the event the LSV/PSV scores are provided for the CRF model discussed earlier (Ruokolainen et al. 2014). As for more recent work, we first refer to the generative log-linear model of Poon, Cherry, and Toutanova (2009). Similarly to the Morfessor model family, this approach is based on defining a joint probability distribution over the unannotated word forms U and the corresponding segmentations S. The distribution is log-linear in form and is denoted as p(U, S; θ ), where θ is the model parameter vector. Again, similarly to the Morfessor framework, Poon, Cherry, and Toutanova (2009) learn a morph lexicon that is subsequently used to generate segmentations for new word forms. The learning is controlled using prior distributions on both corpus and lexicon, which penalize exceedingly complex morph lexicon (si"
J16-1003,N07-1018,0,0.0868534,"Missing"
J16-1003,W10-2210,1,0.938439,"Missing"
J16-1003,W11-0301,0,0.0579217,"Missing"
J16-1003,W13-3512,0,0.107681,"Missing"
J16-1003,P08-1099,0,0.0199259,"full analysis consists of word lemma (basic form), part-of-speech, and fine-grained labels. word form auto (car) autossa (in car) autoilta (from cars) autoilta (car evening) maantie (highway) sähköauto (electric car) full analysis segmentation auto+N+Sg+Nom auto+N+Sg+Ine auto+N+Pl+Abl auto+N+Sg+Nom+# ilta+N+Sg+Nom maantie+N+Sg+Nom maa+N+Sg+Gen+# tie+N+Sg+Nom sähköauto+N+Sg+Nom sähkö+N+Sg+Nom+# auto+N+Sg+Nom auto auto+ssa auto+i+lta auto+ilta maantie maa+n+tie sähköauto sähkö+auto word forms in Table 1, where the full analyses are provided by the rule-based OMorFi analyzer developed by Pirinen (2008). Note that it is typical for word forms to have alternative analyses and/or meanings that cannot be disambiguated without sentential context. Evidently, the level of detail in the full analysis is substantially higher compared with the segmentation, as it contains lemmatization as well as morphological tagging, whereas the segmentation consists of only segment boundary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on le"
J16-1003,D14-1095,0,0.116197,"lish word houses with a corresponding segmentation house+s, where the segment house corresponds to the word stem and the suffix -s marks the plural number. Although this is a major simplification of the diverse morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such"
J16-1003,W02-0604,0,0.0657041,"ng, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards"
J16-1003,N09-1024,0,0.0739183,"Missing"
J16-1003,C14-1015,0,0.128294,"morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning"
J16-1003,W13-3504,1,0.0750803,"imally Supervised Morphological Segmentation unannotated data, supervised methods that utilize solely annotated data, and semisupervised approaches that utilize both unannotated and annotated data. Second, we perform an extensive empirical evaluation of three diverse method families, including a detailed error analysis. The approaches considered in this comparison are variants of the Morfessor algorithm (Creutz and Lagus 2002, 2005, 2007; Kohonen, Virpioja, and Lagus 2010; Grönroos et al. 2014), the adaptor grammar framework (Sirts and Goldwater 2013), and the conditional random field method (Ruokolainen et al. 2013, 2014). We hope the presented discussion and empirical evaluation will be of help for future research on the considered task. The rest of the article is organized as follows. In Section 2, we provide an overview of related studies. We then provide a literature survey of published morphological segmentation methodology in Section 3. Experimental work is presented in Section 4. Finally, we provide a discussion on potential directions for future work and conclusions on the current work in Sections 5 and 6, respectively. 2. Related Work Hammarström and Borin (2011) presented a literature survey o"
J16-1003,E14-4017,1,0.845779,"Missing"
J16-1003,N01-1024,0,0.0829055,"undary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on learning of full morphological analysis has used supervised methodology (Chrupala, Dinu, and van Genabith 2008). Lastly, there have been numerous studies on statistical learning of intermediate forms of segmentation and full analysis (Lignos 2010; Virpioja, Kohonen, and Lagus 2010) as well as alternative morphological representations (Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001; Neuvel and Fulop 2002; Johnson and Martin 2003). As for language processing, learning segmentation can be advantageous compared with learning full analyses. In particular, learning full analysis in a supervised manner typically requires up to tens of thousands of manually annotated sentences. A low-cost alternative, therefore, could be to learn morphological segmentation from unannotated word lists and a handful of annotated examples. Importantly, segmentation analysis has been found useful in a range of applications, such as speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014)"
J16-1003,Q13-1021,1,0.0498248,"for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting with such a small amount of supervision as minimally supervise"
J16-1003,E14-2006,1,0.895787,"Missing"
J16-1003,P10-1039,0,0.125197,"pensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting w"
J16-1003,P12-2063,0,0.0178246,"ally Supervised Morphological Segmentation the output of the supervised CRF model, which in some cases resulted in improved accuracy over the random initialization. We searched the optimal values for each experiment for the upweighting factor, cached versus non-cached root non-terminal, and random versus CRF initialization on the development set. An AG model is stochastic and each segmentation result is just a single sample from the posterior. A common approach in such a case is to take several samples and report the average result. Maximum marginal decoding (MMD) (Johnson and Goldwater 2009; Stallard et al. 2012) that constructs a marginal distribution from several independent samples and returns their mean value has been shown to improve the sampling-based models’ results about 1–2 percentage points. Although the AG model uses sampling for training, the MMD is not applicable here because during test time the segmentations are obtained using parsing. However, we propose another way of achieving the gain in a similar range to the MMD. We train five different models and concatenate their posterior grammars into a single joint grammar, which is then used as the final model to decode the test data. Our ex"
J16-1003,D11-1090,0,0.0618176,"Missing"
J16-1003,P10-1040,0,0.0598779,"Missing"
J16-1003,W11-4632,1,0.852352,"nce segmentations using boundary precision, boundary recall, and boundary F1-score. The boundary F1-score, or F1-score for short, equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect 2 Implementation is available at http://people.csail.mit.edu/yklee/code.html. 104 Ruokolainen et al. Minimally Supervised Morphological Segmentation to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries): Precision = C(correct) , C(proposed) (4) Recall = C(correct) . C(reference) (5) We follow Virpioja et al. (2011) and use type-based macro-averages. However, we handle word forms with alternative analyses in a different fashion. Instead of penalizing algorithms that propose an incorrect number of alternative analyses, we take the best match over the alternative reference analyses (separately for precision and recall). This is because all the methods considered in the experiments provide a single segmentation per word form. Throughout the experiments, we establish statistical significance with confidence level 0.95, according to the standard one-sided Wilcoxon signed-rank test performed on 10 random subse"
J16-1003,P00-1027,0,0.14363,"ion consists of only segment boundary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on learning of full morphological analysis has used supervised methodology (Chrupala, Dinu, and van Genabith 2008). Lastly, there have been numerous studies on statistical learning of intermediate forms of segmentation and full analysis (Lignos 2010; Virpioja, Kohonen, and Lagus 2010) as well as alternative morphological representations (Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001; Neuvel and Fulop 2002; Johnson and Martin 2003). As for language processing, learning segmentation can be advantageous compared with learning full analyses. In particular, learning full analysis in a supervised manner typically requires up to tens of thousands of manually annotated sentences. A low-cost alternative, therefore, could be to learn morphological segmentation from unannotated word lists and a handful of annotated examples. Importantly, segmentation analysis has been found useful in a range of applications, such as speech recognition (Hirsimäki et al. 200"
J16-1003,I11-1035,0,\N,Missing
N06-1062,W02-0603,0,0.522282,"le size by clustering and focusing. In (Szarvas and Furui, 2003; Alumäe, 2005; Hacioglu et al., 2003) the words are split into morphemes by languagedependent hand-crafted morphological rules. In (Kneissler and Klakow, 2001; Arisoy and Arslan, 2005) different combinations of words, grammatical morphemes and endings are utilized to decrease the OOV rate and optimize the speech recognition accuracy. However, constant large improvements over the conventional word-based language models in LVCSR have been rare. The approach presented in this paper relies on a data-driven algorithm called Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005) which is a language independent unsupervised machine learning method to find morpheme-like units (called statistical morphs) from a large text corpus. This method has several advantages over the rule-based grammatical morphemes, e.g. that no hand-crafted rules are needed and all words can be processed, even the foreign ones. Even if good grammatical morphemes are available, the language modeling results by the statistical morphs seem to be at least as good, if not better (Hirsimäki et al., 2005). In this paper we evaluate the statistical morphs for three agglutinative"
N07-1048,W02-0603,1,0.866248,"its. The general idea is to discover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called M"
N07-1048,J01-2001,0,0.0979396,"o morpheme-like units. The general idea is to discover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is"
N07-1048,P06-1085,0,0.0221068,"ough maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called Morfessor Baseline algorithm, which is publicly available for download.1 . The mathematics of the Morfessor Baseline model is briefly outlined in the following; consult Creutz (2006) for details. 1 http://www.cis.hut.fi/projects/morpho/ 381 2.1 MAP Optimization Criterion In slightly simplified form, the optimization criter"
N07-1048,N06-1062,1,0.877513,"Missing"
N07-1048,W06-1646,0,0.0649544,"Missing"
N09-2019,P08-1115,0,0.00950665,"tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) Syst"
N09-2019,W05-0826,0,0.144023,"Missing"
N09-2019,J01-2001,0,0.0126074,"d words, sometimes consisting of several parts, such as ”ulko+maa+n+kauppa+politiikka” (foreign trade policy). Due to these properties, the number of different word forms that can be observed is enormous. Morfessor (Creutz and Lagus, 2007) is a method for modeling concatenative morphology in an unsupervised manner. It tries to find morpheme-like units, morphs, that are segments of the words. Inspired by the minimum description length principle, Morfessor tries to find a concise lexicon of morphs that can effectively code the words in the training data. Unlike other unsupervised methods (e.g., Goldsmith (2001)), there is no restrictions on how many morphs a word can have. After training the model, the most likely segmentation of new words to morphs can be found using the Viterbi algorithm. There exist a few different versions of Morfessor. The baseline algorithm has been found to be very useful in automatic speech recognition of agglutinative languages (Kurimo et al., 2006). However, it 2 Full MT08 results are available at http://www.nist.gov/ speech/tests/mt/2008/doc/mt08 official results v0.html 75 often oversegments morphemes that are rare or not seen at all in the training data. Following the a"
N09-2019,H05-1085,0,0.0116237,"ions to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming worksho"
N09-2019,P05-1071,0,0.0578425,"m multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other74 wise remain attached in the MADA D2 scheme (Table 1). Translation experiments are based on the NIST MT08 Arabic-to-English translation task, including all allowed parallel data as training material (∼150M English words, and 153M or 178M Arabic words"
N09-2019,N09-1049,1,0.3452,"Missing"
N09-2019,D07-1091,0,0.00845091,", June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn w+ qrrt &gt;n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a s"
N09-2019,N04-1022,1,0.409778,"erged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an altern"
N09-2019,N06-1062,1,0.865461,"Missing"
N09-2019,J04-2003,0,0.022079,"(Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 7"
N09-2019,popovic-ney-2004-towards,0,0.0232948,"etitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association f"
N09-2019,P07-1040,0,0.0246309,"is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other7"
N09-2019,P06-1122,0,0.013867,"t we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL"
N09-2019,E03-1007,0,0.0278612,"yzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp"
N09-2019,2007.mtsummit-papers.65,1,0.720881,"Missing"
N09-2019,N06-2051,0,0.0194137,"evaluated. We focus on applications to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the eval"
N09-2019,N04-4015,0,\N,Missing
N09-2019,N06-2013,0,\N,Missing
N09-5004,N06-1062,1,\N,Missing
N09-5004,W02-0603,0,\N,Missing
N09-5004,N09-2019,1,\N,Missing
N09-5004,2007.mtsummit-papers.65,1,\N,Missing
N09-5004,2005.mtsummit-papers.11,0,\N,Missing
P07-1012,N06-1062,1,\N,Missing
P10-2056,N09-1051,0,0.0203758,"1 − F X λ2∗,i i=1 2σ∗2 (4) where Xd is data for domain d, λ∗,i the global parameters, λd,i the domain-specific parameters, σ∗2 the global variance and σd2 the domain-specific variances. The global and domain-specific variances are optimized on the heldout data. Usually, larger values are used for global parameters and for domains with more data, while for domains with less data, the variance is typically set to be smaller, encouraging the domain-specific parameters to be closer to global values. This adaptation scheme is very similar to the approaches proposed by (Chelba and Acero, 2006) and (Chen, 2009b): both use a model estimated from background data as a prior when learning a model from in-domain data. The main difference is the fact that in this method, the models are estimated jointly while in the other works, backDomain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (Daume III, 2007; Finkel and Manning, 2009). In NLP problems, data often comes from different sources (e.g., newspapers, web, textbooks, speech transcriptions). There are t"
P10-2056,N09-1053,0,0.0141135,"1 − F X λ2∗,i i=1 2σ∗2 (4) where Xd is data for domain d, λ∗,i the global parameters, λd,i the domain-specific parameters, σ∗2 the global variance and σd2 the domain-specific variances. The global and domain-specific variances are optimized on the heldout data. Usually, larger values are used for global parameters and for domains with more data, while for domains with less data, the variance is typically set to be smaller, encouraging the domain-specific parameters to be closer to global values. This adaptation scheme is very similar to the approaches proposed by (Chelba and Acero, 2006) and (Chen, 2009b): both use a model estimated from background data as a prior when learning a model from in-domain data. The main difference is the fact that in this method, the models are estimated jointly while in the other works, backDomain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (Daume III, 2007; Finkel and Manning, 2009). In NLP problems, data often comes from different sources (e.g., newspapers, web, textbooks, speech transcriptions). There are t"
P10-2056,P07-1033,0,0.134581,"Missing"
P10-2056,N09-1068,0,0.252894,"he mismatch between training and test data, often a small amount of speech data is human-transcribed. A LM is then built by interpolating the models estimated from large corpus of written language and the small corpus of transcribed data. However, in practice, different models might be of different importance depending on the word context. Global interpolation doesn’t take such variability into account and all predictions are weighted across models identically, regardless of the context. In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. The main contribution of this ∗ Review of Conditional Maximum Entropy Language Models Maximum entropy (ME) modeling is a framework that has been used in a wide area of natural language processing (NLP) tasks. A conditional ME model has the following form: P P (x|h) = P e x0 i λi fi (x,h) P 0 j λj fj (x ,h) e (1) where x is an outcome (in case of a LM, a word), h is a context (the word history), and x0 a set of all possi"
P10-4009,iskra-etal-2002-speecon,0,0.0476344,"Missing"
P10-4009,N09-2019,1,0.883175,"Missing"
P14-2043,H05-1060,0,0.0437204,"considering b = 1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 4 In this section, we compare the approach presented in Section 2 to two prior systems which attempt to utilize sub-label dependencies in a similar manner. Smith et al. (2005) use a CRF-based system for tagging Czech, in which they utilize expanded emission features similar to our (5). However, they do not utilize the full expanded transition features (6). More specifically, instead of utilizing a single chain as in our approach, Smith et al. employ five parallel structured chains. One of the chains models the sequence of word-class labels such as noun and adjective. The other four chains model gender, number, case, and lemma sequences, respectively. Therefore, in contrast to our approach, their system does not capture cross-dependencies between inflectional catego"
P14-2043,A00-1031,0,0.0409617,"uded in the CRF model using a relatively straightforward feature expansion scheme. Experiments on five languages showed that the approach can yield significant improvement in tagging accuracy given sufficiently fine-grained label sets. In future work, we aim to perform a more fine-grained error analysis to gain a better understanding where the improvement in accuracy takes place. One could also attempt to optimize the compound label splits to maximize prediction accuracy instead of applying a priori partitions. Table 2: Results. proved open-source implementation of the wellknown TnT tagger of Brants (2000). The obtained HunPos results are presented in Table 3. HunPos Eng 96.58 Rom 96.96 Est 92.76 Cze 89.57 Conclusions Fin 85.77 Table 3: Results using a generative HMM-based HunPos tagger of Halacsy et al. (2007). Acknowledgements This work was financially supported by Langnet (Finnish doctoral programme in language studies) and the Academy of Finland under the grant no 251170 (Finnish Centre of Excellence Program (2012-2017)). We would like to thank the anonymous reviewers for their useful comments. Ceau¸su (2006) uses a maximum entropy Markov model (MEMM) based system for tagging Romanian which"
P14-2043,W02-1001,0,0.622696,"er, hyphen, dash, or digit. Binary functions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features 0 ) . . . 1(y = y 0 ) | {1(yi−k = yi−k i i 0 , . . . , y 0 ∈ Y , ∀k ∈ 1 . . . n} yi−k i (6) (4) capture dependencies between adjacent labels irrespective of the input x. 1. A standard CRF model incorporating (2) and (4) is denoted as CRF(n,-). 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi ) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG)"
P14-2043,J11-1005,0,0.0130834,"not report results using CRF(2,2) since, based on preliminary experiments, this model overfits on all languages. The CRF model parameters are estimated using the averaged perceptron algorithm (Collins, 2002). The model parameters are initialized with a zero vector. We evaluate the latest averaged parameters on the held-out development set after each pass over the training data and terminate training if no improvement in accuracy is obtained during three last passes. The best-performing parameters are then applied on the test instances. We accelerate the perceptron learning using beam search (Zhang and Clark, 2011). The beam width, b, is optimized separately for each language on the development sets by considering b = 1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 4 In this section, we compare the approach presented in Section 2"
P14-2043,erjavec-2010-multext,0,0.159768,"I Mikko Kurimob Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract 1 Krister Lindéna V+NON3SG+PRES like 2 2.1 Methods Conditional Random Fields The (unnormalized) CRF model (Lafferty et al., 2001) for a sentence x = (x1 , . . . , x|x |) and a POS sequence y = (y1 , . . . , y|x |) is defined as N+SG , ham where the compound labels PRON+1SG, V+NON3SG+PRES, and N+SG stand for pronoun first person singular, verb non-third singular present tense, and noun singular, respectively. Fine-grained labels occur frequently in morphologically complex languages (Erjavec, 2010; Haverinen et al., 2013). We propose improving tagging accuracy by utilizing dependencies within the sub-labels (PRON, 1SG, V, NON3SG, N, and SG in the above example) of the compound labels. From a technical perspective, we accomplish this by making use of the fundamental ability of the CRFs to incorporate arbitrarily defined feature functions. The newlydefined features are expected to alleviate data sparp (y |x; w) ∝ |x| Y   exp w·φ(yi−n , . . . , yi , x, i) , i=n (1) where n denotes the model order, w the model parameter vector, and φ the feature extraction function. We denote the tag set"
P14-2043,P01-1035,0,0.0792301,"Missing"
P14-2043,P07-2053,0,0.0912677,"et al. employ five parallel structured chains. One of the chains models the sequence of word-class labels such as noun and adjective. The other four chains model gender, number, case, and lemma sequences, respectively. Therefore, in contrast to our approach, their system does not capture cross-dependencies between inflectional categories, such as the dependence between the word-class and case of adjacent words. Unsurprisingly, Smith et al. fail to achieve improvement over a generative HMMbased POS tagger of Hajiˇc (2001). Meanwhile, our system outperforms the generative trigram tagger HunPos (Halácsy et al., 2007) which is an imSoftware and Hardware The experiments are run on a standard desktop computer (Intel Xeon E5450 with 3.00 GHz and 64 GB of memory). The methods discussed in Section 2 are implemented in C++. 3.5 Related Work Results The obtained tagging accuracies and training times are presented in Table 2. The times include running the averaged perceptron algorithm and evaluation of the development sets. The column labeled it. corresponds to the number of passes over the training data made by the perceptron algorithm before termination. We summarize the results as follows. First, compared to st"
P14-2043,J93-2004,0,0.0496644,"nctions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features 0 ) . . . 1(y = y 0 ) | {1(yi−k = yi−k i i 0 , . . . , y 0 ∈ Y , ∀k ∈ 1 . . . n} yi−k i (6) (4) capture dependencies between adjacent labels irrespective of the input x. 1. A standard CRF model incorporating (2) and (4) is denoted as CRF(n,-). 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi ) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG) = 2. A CRF model incorporating (2), (4), a"
P14-2043,W96-0213,0,0.926591,"ure functions X with all sub-labels s ∈ S by defining the corresponding label as {χj (x, i)1(yi = yi0 ) |j ∈ 1 . . . |X |, ∀yi0 ∈ Y} , (2) where the function 1(q) returns one if and only if the proposition q is true and zero otherwise, that is  1 if yi = yi0 0 1(yi = yi ) = , (3) 0 otherwise {χj (x, i)1(s ∈ P(yi )) |∀j ∈ 1 . . . |X |, ∀s ∈ S} , (5) where 1(s ∈ P(yi )) returns one in case s is in P(yi ) and zero otherwise. Second, we exploit sublabel transitions using features |X | and X = {χj (x, i)}j=1 is the set of functions characterizing the word position i. Following the classic work of Ratnaparkhi (1996), our X comprises simple binary functions: {1(si−k ∈ P(yi−k )) . . . 1(si ∈ P(yi )) | ∀si−k , . . . , si ∈ S , ∀k ∈ 1 . . . m} . 1. Bias (always active irrespective of input). Note that we define the sub-label transitions up to order m, 1 ≤ m ≤ n, that is, an nth-order CRF model is not obliged to utilize sub-label transitions all the way up to order n. This is because employing high-order sub-label transitions may potentially cause overfitting to training data due to substantially increased number of features (equivalent to the number of model parameters, |w |= |φ|). For example, in a second-o"
varjokallio-kurimo-2014-toolkit,W02-0603,0,\N,Missing
varjokallio-kurimo-2014-toolkit,P06-1085,0,\N,Missing
varjokallio-kurimo-2014-toolkit,P98-1047,0,\N,Missing
varjokallio-kurimo-2014-toolkit,C98-1047,0,\N,Missing
W10-1729,W02-0603,0,0.0427161,"stical natural language processing, especially with English, but morphologically rich languages can benefit from more fine-grained information. For instance, statistical morphs discovered with unsupervised methods result in better performance in automatic speech recognition for highly-inflecting and agglutinative languages (Hirsim¨aki et al., 2006; Kurimo et al., 2006). Virpioja et al. (2007) applied morph-based models in statistical machine translation (SMT) between several language pairs without gaining improvement in BLEU score, but obtaining re2.1 Morphological models for words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfes"
W10-1729,2007.mtsummit-papers.65,1,0.906619,"Missing"
W10-1729,N09-2019,1,0.886268,"Missing"
W10-1729,D07-1091,0,0.138279,"Missing"
W10-1729,P07-2045,0,0.0141692,"rfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the Germanto-English translation. However, we did not see improvements for the Czech-toEnglish translations. 1 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. Introduction The effect of morphological variation in languages can be alleviated by using word analysis schemes, which may include morpheme discovery, part-ofspeech tagging, or other linguistic information. Words are very convenient and even efficient representation in statistical natural language processing, especially with English, but morphologically rich languages can"
W10-1729,N04-1022,0,0.0428466,"h the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 2.3 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination (Sim et al., 2007). N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). In this work, the focus of the system combination is not to combine different translation systems (e.g., Moses and Systran), but to combine systems trained with the same translation algorithm using the same source language data with with different morphological decompositions. 3 Experiments The German-to-English and Czech-to-English parts of the ACL WMT10 shared task data were investigated. Vanilla SMT models were trained with Moses using word tokens for MBR combination and comparison purposes. Several different morphological segmentation models for German and Czech were trained with Morfess"
W10-1729,N06-1062,1,\N,Missing
W10-2211,W02-0603,1,0.860784,"ervised algorithms are almost at par and the differences are not significant. For German and Finnish, the best unsupervised methods can also beat in a statistically significant way the baseline of not doing any segmentation or stemming. The best algorithms that performed well across languages are ParaMor (Monson et al., 2008), Bernhard (Bernhard, 2006), Morfessor Baseline, and McNamee (McNamee, 2008). 3.2 Evaluated algorithms This section attempts to describe very briefly some of the individual morpheme analysis algorithms that have been most successful in the evaluations. Morfessor Baseline (Creutz and Lagus, 2002): This is a public baseline algorithm based on jointly minimizing the size of the morph codebook and the encoded size of the all the word forms using the minimum description length MDL cost function. The performance is above average for all evaluated tasks in most languages. Allomorfessor (Kohonen et al., 2009; Virpioja and Kohonen, 2009): The development of this method was based on the observation that the Comparing the results to the linguistic evaluation (section 3.1.1), it seems that methods that perform well at the IR task tend to have good precision in the linguistic task, with exception"
W10-2211,N09-2019,1,0.88494,"Missing"
W10-2211,P06-1027,0,0.0127801,"parisons to linguistic gold standard between various inflected word forms. Until 2010 the Morpho Challenge has been defined only as an unsupervised learning task. However, since small samples of morphologically labeled data can be provided already for quite many languages, also the semi-supervised learning task has become of interest. Moreover, while there exists a fair amount of research and now even books on semi-supervised learning (Zhu, 2005; Abney, 2007; Zhu, 2010), it has not been as widely studied for structured classification problems like sequence segmentation and labeling (cf. e.g. (Jiao et al., 2006)). The semi-supervised learning challenge introduced for Morpho Challenge 2010 can thus be viewed as an opportunity to strengthen research in both morphology modeling as well as in semi-supervised learning for sequence segmentation and labeling in general. 3 The first Morpho Challenge in 2005 (Kurimo et al., 2006) considered unsupervised segmentation of words into morphemes. The evaluation was based on comparing the segmentation boundaries given by the competitor’s algorithm to the boundaries obtained from a gold standard analysis. From 2007 onwards, the task was changed to full morpheme analy"
W13-3504,N07-1048,1,0.774648,"Missing"
W13-3504,P12-1016,0,0.0627695,"egmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting several times larger training set (588,244 word tokens). Second, we present empirical comparison between the CRF approach and two state-of-art methods (Poon et al.,"
W13-3504,J11-2002,0,0.0292684,"based segmentation approach with two stateof-art methods, the log-linear modeling approach presented by Poon et al. (2009) and the semisupervised Morfessor algorithm (Kohonen et al., 2010). As stated previously, the CRF-based segmentation approach differs from these methods in that it learns to predict morph boundaries from a small amount of annotated data, in contrast to learning morph lexicons from both annotated and large amounts of unannotated data. Lastly, there exists ample work on varying unsupervised (and semi-supervised) morphological segmentation methods. A useful review is given by Hammarström and Borin (2011). The fundamental difference between our approach and these techniques is that our method necessarily requires manually annotated training data. Turkish using the Morpho Challenge 2009/2010 data sets (Kurimo et al., 2009; Kurimo et al., 2010). The results are compared against two stateof-art techniques, namely the log-linear modeling approach presented by Poon et al. (2009) and the semi-supervised Morfessor algorithm (Kohonen et al., 2010). We show that when employing the same small amount of annotated training data, the CRF-based boundary prediction approach outperforms these reference method"
W13-3504,W10-2210,1,0.64668,"rface forms morphs. Thus, morphs are natural targets for the segmentation. For most languages, existing resources contain large amounts of raw unannotated text data, only small amounts of manually prepared annotated training data, and no freely available rule-based morphological analyzers. The focus of our work is on performing morphological segmentation in this low-resource scenario. Given this setting, the current state-of-art methods approach the problem by learning morph lexicons from both annotated and unannotated data using semi-supervised machine learning techniques (Poon et al., 2009; Kohonen et al., 2010). Subsequent to model training, the methods uncover morph boundaries for new word forms by generating their most likely morph sequences according to the morph lexicons. In contrast to learning morph lexicons (Poon et al., 2009; Kohonen et al., 2010), we study morphological segmentation by learning to directly predict morph boundaries based on their local substring contexts. Specifically, we apply the linearchain conditional random field model, a popular discriminative log-linear model for segmentation presented originally by Lafferty et al. (2001). Importantly, we learn the segmentation model"
W13-3504,N03-1028,0,0.0276986,"ds In this section, we describe in detail the CRFbased approach for supervised morphological segmentation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Tr"
W13-3504,W06-0127,0,0.0843718,"entation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting several times larger training set (588,244 word t"
W13-3504,P08-1099,0,0.0754272,"Missing"
W13-3504,W03-0430,0,0.0333757,"ased approach for supervised morphological segmentation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting severa"
W13-3504,C04-1081,0,0.0231624,"unction f . We will next describe and motivate the feature set used in the experiments. Our feature set consists of binary indicator functions describing the position t of word x using all left and right substrings up to a maximum length δ. For example, consider the problem of deciding if the letter e in the word drivers is preceded by a morph boundary. This decision is now based on the overlapping substrings 3.4 Parameter estimation The CRF model parameters w are estimated based on an annotated training data set. Common training criteria include the maximum likelihood (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006), averaged structured perceptron (Collins, 2002), and max-margin (Szummer et al., 2008). In this work, we estimate the parameters using the perceptron algorithm (Collins, 2002). 31 In perceptron training, the required graph inference can be efficiently performed using the standard Viterbi algorithm. Subsequent to training, the segmentations for test instances are acquired again using Viterbi search. Compared to other training criteria, the structured perceptron has the advantage of employing only a single hyperparameter, namely the number of passes over training data, makin"
W13-3504,N09-1024,0,0.52038,"phemes and their surface forms morphs. Thus, morphs are natural targets for the segmentation. For most languages, existing resources contain large amounts of raw unannotated text data, only small amounts of manually prepared annotated training data, and no freely available rule-based morphological analyzers. The focus of our work is on performing morphological segmentation in this low-resource scenario. Given this setting, the current state-of-art methods approach the problem by learning morph lexicons from both annotated and unannotated data using semi-supervised machine learning techniques (Poon et al., 2009; Kohonen et al., 2010). Subsequent to model training, the methods uncover morph boundaries for new word forms by generating their most likely morph sequences according to the morph lexicons. In contrast to learning morph lexicons (Poon et al., 2009; Kohonen et al., 2010), we study morphological segmentation by learning to directly predict morph boundaries based on their local substring contexts. Specifically, we apply the linearchain conditional random field model, a popular discriminative log-linear model for segmentation presented originally by Lafferty et al. (2001). Importantly, we learn"
W13-3504,W02-1001,0,\N,Missing
W15-3010,D09-1075,0,0.309766,"Missing"
W15-3010,P11-1004,0,0.114421,"Missing"
W15-3010,N09-2019,1,0.917433,"Missing"
W15-3010,fishel-kirik-2010-linguistically,0,0.559354,"Missing"
W15-3010,C14-1111,1,0.868026,"Missing"
W15-3010,N06-2013,0,0.176461,"Missing"
W15-3010,P07-2045,0,0.017174,"Missing"
W15-3010,2005.mtsummit-papers.11,0,0.103752,"Missing"
W15-3010,N04-4015,0,0.086757,"Missing"
W15-3010,W15-1011,0,0.0384613,"Missing"
W15-3010,P08-1084,0,0.0455317,"Missing"
W15-3010,W11-2129,0,0.503578,"Missing"
W15-3010,2007.mtsummit-papers.65,1,0.946664,"Missing"
W15-3010,J04-2003,0,0.0761711,"Missing"
W15-3010,P03-1021,0,0.184737,"Missing"
W15-3010,P02-1040,0,0.0977139,"Missing"
W16-2312,W15-3010,1,0.811033,"anguage model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based on symmetry between the languages in a parallel corpus (Grönroos et al., 2015). To combine the advantages of linguistic segmentation and data-driven segmentation, we propose a hybrid approach for morphological segmentation. We optimize the segmentation in a datadriven manner, aiming for a similar granularity as the second language of the language pair, but restricting the possible set of segmentation boundaries to those between linguistic morphs. That is, the segmentation method may decide to join any of the linguistic morphs, but it cannot add new segmentation boundaries to known linguistic morphs. We show that it is possible to improve on the linguistically accurate s"
W16-2312,E14-1061,0,0.0171357,"ja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder."
W16-2312,N06-2013,0,0.110633,"Missing"
W16-2312,P07-2045,0,0.00885311,"ompounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder. The standard pre-processing steps not specified in Figure 1 consist of normalization of punctuation, tokenization, and statistical truecasing. All of these were performed with the tools included in Moses. The pre-processing steps are followed by morphological segmentation. In addition, the parallel data was cleaned and duplicate sentences were removed. Cleaning was performed after morphological segmentation, as the segmentation can increase the length in tokens of a sentence."
W16-2312,D09-1075,0,0.0206514,"Segmentation for Phrase-Based Machine Translation Stig-Arne Grönroos Sami Virpioja Department of Signal Processing and Acoustics Department of Computer Science Aalto University, Finland Aalto University, Finland stig-arne.gronroos@aalto.fi sami.virpioja@aalto.fi Mikko Kurimo Department of Signal Processing and Acoustics Aalto University, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation gra"
W16-2312,D10-1015,0,0.0204143,"preprocess postprocess Designer objects Design-esineitä predict boundaries 3 1 me +i +llä on ol +ta +va . design-esineitä tune ORM rejoin with Morfessor mei +llä on oltava . train Moses parallel dev set train n-gram LMs resegmentation model train TheanoLM 2 TheanoLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect"
W16-2312,P11-1004,0,0.0204223,"Based Machine Translation Stig-Arne Grönroos Sami Virpioja Department of Signal Processing and Acoustics Department of Computer Science Aalto University, Finland Aalto University, Finland stig-arne.gronroos@aalto.fi sami.virpioja@aalto.fi Mikko Kurimo Department of Signal Processing and Acoustics Aalto University, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based"
W16-2312,K15-1017,0,0.020718,"on is detrimental, however, as longer windows of history need to be used, and useful phrases become more difficult to extract. It is therefore important to find a balance in the amount of segmentation. We consider the case that there are linguistic gold standard segmentations available for the morphologically complex target language. Even if there is no rule-based morphological analyzer for the language, a limited set of gold standard segmentations can be used for training a reasonably accurate statistical segmentation model in a supervised or semi-supervised manner (Ruokolainen et al., 2014; Cotterell et al., 2015). While using a linguistically accurate morphological segmentation in a phrase-based SMT system may sound like a good idea, there is evidence that shows otherwise. In general, oversegmentation seems to be a larger problem for 1.1 Related work Rule-based and statistical segmentation for SMT have been extensively studied in isolation (Virpi289 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 289–295, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics parallel en Test Train . monolingual fi fi preprocess we must hav"
W16-2312,P03-1021,0,0.0942991,"raining. The TheanoLM parameters were: 100 nodes in the projection layer, 300 LSTM nodes in the hidden layer, dropout rate 0.25, adam optimization with initial learning rate 0.01, and minibatch 16. 2.4 Moses configuration We used GIZA++ alignment. As decoding LMs, we used two SRILM n-gram models with modified-KN smoothing: a 3-gram and 5-gram model, trained from different data. Many Moses settings were left at their default values: phrase length 10, grow-diag-final-and alignment symmetrization, msd-bidirectional-fe reordering, and distortion limit 6. The feature weights were tuned using MERT (Och, 2003), with BLEU (Papineni et al., 2002) of the post-processed hypothesis against a development set as the metric. 20 random restarts per MERT iteration were used, with iterations repeated until convergence. The rescoring weights were tuned with a newly included script in Moses, which uses kb-MIRA instead of MERT. 2.3 Morph boundary correction One benefit of segmented translation is the ability to generate new compounds and inflections, that were not seen in the training data. However, the ability can also lead to errors, e.g when an English word frequently aligned to a compound modifier is transla"
W16-2312,W02-0603,0,0.0744088,"versity, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based on symmetry between the languages in a parallel corpus (Grönroos et al., 2015). To combine the advantages of linguistic segmentation and data-driven segmentation, we propose a hybrid approach for morphological segmentation. We optimize the segmentation in a datadriven manner, aiming for a similar granu"
W16-2312,P02-1040,0,0.107131,"arameters were: 100 nodes in the projection layer, 300 LSTM nodes in the hidden layer, dropout rate 0.25, adam optimization with initial learning rate 0.01, and minibatch 16. 2.4 Moses configuration We used GIZA++ alignment. As decoding LMs, we used two SRILM n-gram models with modified-KN smoothing: a 3-gram and 5-gram model, trained from different data. Many Moses settings were left at their default values: phrase length 10, grow-diag-final-and alignment symmetrization, msd-bidirectional-fe reordering, and distortion limit 6. The feature weights were tuned using MERT (Och, 2003), with BLEU (Papineni et al., 2002) of the post-processed hypothesis against a development set as the metric. 20 random restarts per MERT iteration were used, with iterations repeated until convergence. The rescoring weights were tuned with a newly included script in Moses, which uses kb-MIRA instead of MERT. 2.3 Morph boundary correction One benefit of segmented translation is the ability to generate new compounds and inflections, that were not seen in the training data. However, the ability can also lead to errors, e.g when an English word frequently aligned to a compound modifier is translated using such a morph, even though"
W16-2312,N09-2019,1,0.692107,"Missing"
W16-2312,W15-1844,0,0.0230193,"terwards. Additive smoothing with smoothing constant 1.0 was applied in the Viterbi search. Prior to the Viterbi training, we flattened the tree structure so that the root nodes (word forms) link directly to the leaf nodes (morphs), thus removing any shared substrings nodes that are not actual morphs. This way all word forms are segmented independently and all the restrictions are followed. Morphological segmentation An example of the morphological segmentation is shown in Table 1. 2.1.1 Omorfi segmentation We begin the morphological segmentation by applying the segmentation tool from Omorfi (Pirinen, 2015). Hyphens removed by Omorfi are reintroduced. Omorfi outputs 5 types of intra-word boundaries, which we mark in different ways. Compound modifiers, identified by the WB or wB boundary type, are marked with a reserved symbol ‘@’ at the right edge of the morph. Suffixes, identified by a leading morph boundary MB or derivation boundary DB, are marked with a ‘+’ at the left edge. Boundaries of the type STUB (other stemmer-type boundary) are removed. This marking scheme leaves the compound head, or last stem of the word, unmarked. E.g. “yli{WB}voimai{STUB}s{MB}i{MB}a” is marked as ”yli@ voimais +i"
W16-2312,W15-3022,0,0.0200185,"rejoin with Morfessor mei +llä on oltava . train Moses parallel dev set train n-gram LMs resegmentation model train TheanoLM 2 TheanoLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging c"
W16-2312,E14-4017,1,0.775696,"sing step. Over-segmentation is detrimental, however, as longer windows of history need to be used, and useful phrases become more difficult to extract. It is therefore important to find a balance in the amount of segmentation. We consider the case that there are linguistic gold standard segmentations available for the morphologically complex target language. Even if there is no rule-based morphological analyzer for the language, a limited set of gold standard segmentations can be used for training a reasonably accurate statistical segmentation model in a supervised or semi-supervised manner (Ruokolainen et al., 2014; Cotterell et al., 2015). While using a linguistically accurate morphological segmentation in a phrase-based SMT system may sound like a good idea, there is evidence that shows otherwise. In general, oversegmentation seems to be a larger problem for 1.1 Related work Rule-based and statistical segmentation for SMT have been extensively studied in isolation (Virpi289 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 289–295, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics parallel en Test Train . monolingual fi"
W16-2312,W11-2129,0,0.0172865,"oLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with Thea"
W16-2312,J13-4009,0,0.018542,"ir strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder. The standard pre-processing steps not specified in Figure 1 consist of normalization of punctuation, tokenization, an"
W16-2312,2007.mtsummit-papers.65,1,0.841019,"Missing"
W17-0208,iskra-etal-2002-speecon,0,0.0905195,"Missing"
W17-0208,W10-1310,0,0.0274908,"Missing"
W17-4727,W16-2341,0,0.168454,"Missing"
W17-4727,P17-2012,0,0.0262831,"decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims at improving the generalization performance of a task using other related tasks (Caruana, 1998). For example, Luong et al. (2016) use autoencoding, parsing, and caption generation as auxiliary tasks to improve English-to-German translation. Eriguchi et al. (2017) combine NMT with a Recurrent Neural Network Grammar. The system learns to parse the target language as an auxiliary task when translating into English. We propose an MTL approach inspired by fac296 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 296–302 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tored translation. The output of a morphological analyzer for the target sentence is used as an auxiliary prediction target, while sharing network parameters to a larger extent than in the approach of Luong"
W17-4727,W16-2209,0,0.0420161,"t words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored translation has also been successfully applied in NMT. For example, Sennrich and Haddow (2016) augment the source words with four additional factors: PoS, lemma, dependency label and subwords. García-Martínez et al. (2016) use a decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims at improving the generalization performance of a task using other related tasks (Caruana, 1998). For examp"
W17-4727,2007.mtsummit-papers.65,1,0.697651,"mpounding in synthetic languages can result in very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infreq"
W17-4727,fishel-kirik-2010-linguistically,0,0.0154268,"languages can result in very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a specia"
W17-4727,W15-3010,1,0.843286,"very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second"
W17-4727,C14-1111,1,0.802379,"weight 0.8 for the character-level cost. We use an ensemble procedure, in which the combined prediction is computed as the mean after the softmax layer of the predictions of 4 models. The primary system uses systems from 4 runs with different weights for the auxiliary task. The systems trained for comparison—a subword system based on Morfessor FlatCat and the systems in ablation experiments—were ensembled using 4 save points from a single run. To include an example of subword NMT, we also submit our FlatCat system. As preprocessing, the target side has been segmented using Morfessor FlatCat (Grönroos et al., 2014), which was tuned to produce a subword lexicon of approximately 60k symbols. Segmenting names into characters is applied in addition to the FlatCat segmentation. The FlatCat segmented system uses WMT 2016 data only, i.e., omits the Rapid corpus. The FlatCat subword system uses the standard HNMT decoder. It uses neither the hybrid wordcharacter decoder nor MTL. We did however use the improved beam search with penalties. 6 Results We evaluate the systems using characterF with β set to 1.0 and 2.0, and cased BLEU using the mteval-v13a.pl script. We also include Translation Error Rate (TER) result"
W17-4727,D07-1091,0,0.0612052,"er presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored translation has also been successfully applied in NMT. For example, Sennrich and Haddow (2016) augment the source words with four additional factors: PoS, lemma, dependency label and subwords. García-Martínez et al. (2016) use a decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims a"
W17-4727,P16-1100,0,0.311859,"have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored transla"
W18-0208,W02-0603,0,0.061433,"he work by Mohri et al. (2008). The advantage of WFST-based recognizers is that once the search network has been constructed and optimized effectively by the WFST methods, the decoding is very fast and accurate. Moreover, Kaldi’s GMM-HMMs are improved by subspace Gaussians, word-position-dependent phones and advanced silence models. 2.2 Subword lexicon FSTs and language models The small amount of training data and the morphological complexity of Northern Sámi make it problematic to build language models (LM) using words as the basic units. We applied the data-driven Morfessor Baseline method (Creutz and Lagus, 2002, 2007) to segment the words into subword units. Because all words in the language can be composed from these subword units, this approach provides an unlimited vocabulary for ASR (Hirsimäki et al., 2006). While Morfessor was developed to find units of language that resemble the surface forms of linguistic morphemes, the current implementation includes a parameter for adjusting the level of segmentation that the method produces (Virpioja et al., 2013). The optimal level of segmentation for ASR varies between languages, but a wide range of lexicon seems to produce near-optimal results (Smit et"
W18-6410,W16-3402,0,0.0285845,"Missing"
W18-6410,P17-1181,0,0.148958,"Missing"
W18-6410,P14-2017,0,0.0597286,"model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an etymological origin. We propose a language-agnostic, unsupervised method, which doesn’t require annotations, lemmatizers, analyzers or parsers. Our method can exploit both monolingual and parallel data, and can use cognates of any part-ofspeech. 2 Cognate Morfessor We introduce a new variant of Morfessor for cross-lingual segmentation.2 It is trained usin"
W18-6410,W02-0603,0,0.226497,"Missing"
W18-6410,N13-1073,0,0.0410491,"s for both languages is tried, and the pair of splits minimizing the cost function is selected, unless not splitting results in even lower cost. 3 Extracting cognates from parallel data from the news.20{14-17}.et corpora, using a language model ﬁltering technique. Finnish–Estonian cognates were automatically extracted from the shared task training data. As we needed a Finnish–Estonian parallel data set, we generated one by triangulation from the English–Finnish and English– Estonian parallel data. This resulted in a set of 679 252 sentence pairs (ca 12 million tokens per language). FastAlign (Dyer et al., 2013) was used for word alignment in both directions, after which the alignments were symmetrized using the grow-diag-ﬁnal-and heuristic. All aligned word pairs were extracted based on the symmetrized alignment. Words containing punctuation, and pairs aligned to each other fewer than 2 times were removed. The list of word pairs was ﬁltered based on Levenshtein distance. If either of the words consisted of 4 or fewer characters, an exact match was required. Otherwise, a Levenshtein distance up to a third of the mean of the lengths, rounding up, was allowed. This procedure resulted in a list of 40 47"
W18-6410,P17-4012,0,0.160201,"Missing"
W18-6410,W10-2210,1,0.853542,"E , D E ). The coding is redundant, as one language and the edits would be enough to reconstruct the second language. In the interest of symmetry between target languages, we ignore this redundancy. The intuition is that the changes in spelling between the cognates in a particular language pair is regular. Coding the diﬀerences in a way that reduces the cost of making a similar change in another word guides the model towards learning these patterns from the data. The coding of the edits is based on the Levenshtein (1966) algorithm. Let (wa , wb ) be 388 The semi-supervised weighting scheme of Kohonen et al. (2010) can be applied to Cognate Morfessor. A new weighting parameter edit_cost_weight is added, and multiplicatively applied to both the lexicon and corpus costs of the edits. The training algorithm is an iterative greedy local search very similar to the Morfessor Baseline algorithm. The algorithm ﬁnds an approximately minimizing solution to Eq 2. The recursive splitting algorithm from Morfessor Baseline is slightly modiﬁed. If a non-cognate is being reanalyzed, the normal algorithm is followed. Cognates are reanalyzed together. Recursive splitting is applied, with the restriction that if a morph i"
W18-6410,N01-1014,0,0.368489,"that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model. 1 Introduction Cognates are words in diﬀerent languages, which due to a shared etymological origin are represented as identical or nearly identical strings, and also refer to the same or similar concepts. Ideally the cognate pair is similar orthographically, semantically, and distributionally. Care must be taken with “false friends”, i.e. words with similar string representation but diﬀerent semantics. Following usage in Natural Language Processing, e.g. (Kondrak, 2001), we use this broader deﬁnition of the term cognate, without placing the same weight on etymological origin as in historical linguistics. Therefore we accept loan words as cognates. In any language pair written in the same alphabet, cognates can be found among names of persons, locations and other proper names. Cognates are more frequent in related languages, such as Finnish and Estonian. These additional cognates are words of any part-ofspeech, which happen to have a shared origin. In this work we set out to improve morphological segmentation for multilingual translation systems with one sour"
W18-6410,P11-1090,0,0.0573184,"Missing"
W18-6410,W15-3049,0,0.0172576,"omponent. Newstest is abbreviated nt. Both references are used in nt2017AB. We experimented with partially linking the embeddings of cognate morphs. In this experiment, we used morph embeddings concatenated from two parts: a part consisting of normal embedding of the morph, and a part that was shared between both halves of the cognate morph pair. Non-cognate morphs used an unlinked embedding also for the second part. After concatenation, the linked embeddings have the same size as the baseline embeddings. We evaluate the systems with cased BLEU using the mteval-v13a.pl script, and characterF (Popovic, 2015) with β set to 1.0. The latter was used for tuning. 6 Results Based on preliminary experiments, the Morfessor corpus cost weight α was set to 0.01, and the edit cost weight was set to 10. The most frequent edits are shown in Table 2. Table 3 shows the development set results for Estonian. Table 4 shows results for previous year’s test sets for Finnish. The tables show our main system and the two baselines: a multilingual model using joint BPE segmentation, and a monolingual model using Morfessor Baseline. Cognate Morfessor outperforms the comparable BPE system according to both measures for Es"
W18-6410,C16-1097,0,0.0555437,"unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an etymological origin. We propose a language-agnostic, unsupervised method, which doesn’t require annotations, lemmatizers, analyzers or parsers. Our method can exploit both monolingual and parallel data, and can use cognates of any part-ofspeech. 2 Cognate Morfessor We introduce a new variant of Morfessor for cross-lingual segmentation.2 It is trained using a bilingual corpus, so that both target languages are trained simultaneously. We allow each language to have its own subword lexicon. In essence, as a Mor"
W18-6410,P08-1084,0,0.0269002,"occur in the ends of the words. If a single letter changes in the middle of a cognate, consistent subwords that span over the location of the change are found only by chance. In order to encourage stronger consistency, we propose a segmentation model that uses automatically extracted cognates and fuzzy matching between cognate morphs. In this work we also contribute two new features to the OpenNMT translation system: Ensemble decoding, and ﬁne-tuning a pretrained model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics p"
W18-6410,W04-0109,0,0.070992,"er the location of the change are found only by chance. In order to encourage stronger consistency, we propose a segmentation model that uses automatically extracted cognates and fuzzy matching between cognate morphs. In this work we also contribute two new features to the OpenNMT translation system: Ensemble decoding, and ﬁne-tuning a pretrained model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an ety"
W18-6439,W17-4746,0,0.0911741,"Missing"
W18-6439,W17-4747,0,0.0299311,"n sources to be merged. Different strategies to include image features both on the encoder and decoder side have been explored. We are inspired by the recent success of the Transformer architecture to adapt some of these strategies for use with the Transformer. Recurrent neural networks start their processing from some initial hidden state. Normally, a zero vector or a learned parameter vector is used, but the initial hidden state is also a natural location to introduce additional context e.g. from other modalities. Initializing can be applied in either the encoder (IMGE ) or decoder (IMGD ) (Calixto et al., 2017). These approaches are not directly applicable to the Transformer, as it is not a recurrent model, and lacks a comparable initial hidden state. Double attention is another popular choice, used by e.g. Caglayan et al. (2017). In this approach, two attention mechanisms are used, one for each modality. The attentions can be separate or hierarchical. While it would be possible to use double attention with the Transformer, we did not explore it in this work. The multiple multi-head attention mechanisms in the Transformer leave open many challenges in how this integration would be done. Multi-task l"
W18-6439,I17-1014,0,0.0585738,"former, as it is not a recurrent model, and lacks a comparable initial hidden state. Double attention is another popular choice, used by e.g. Caglayan et al. (2017). In this approach, two attention mechanisms are used, one for each modality. The attentions can be separate or hierarchical. While it would be possible to use double attention with the Transformer, we did not explore it in this work. The multiple multi-head attention mechanisms in the Transformer leave open many challenges in how this integration would be done. Multi-task learning has also been used, e.g. in the Imagination model (Elliott and Kádár, 2017), where the auxiliary task consists of reconstructing the visual features from the source encoding. Imagination could also have been used with the Transformer, but we did not explore it in this work. The source sequence itself is also a possible location for including the visual information. In the IMGW approach, the visual features are encoded as a pseudo-word embedding concatenated to the word embeddings of the source sentence. When the encoder is a bidirectional recurrent network, as in Calixto et al. (2017), it is beneficial to add the pseudo-word both at the beginning and the end to make"
W18-6439,P17-4012,0,0.080597,"Missing"
W18-6439,C16-1172,0,0.0926185,"Missing"
W18-6439,W17-4733,1,0.891591,"Missing"
W18-6439,W15-3049,0,0.0230282,"d ihr brauner hund rennt auf sie zu .” when not using the image features, but as masculine “ein besitzer …” when using them. The English text contains the word “her”. The person in the image has short hair and is wearing pants. size 4096 tokens, label smoothing 0.1, Adam with initial learning rate 2 and β2 0.998. For decoding, we use an ensemble procedure, in which the predictions of 3 independently trained models are combined by averaging after the softmax layer to compute combined prediction. We evaluate the systems using uncased BLEU using multibleu. During tuning, we also used characterF (Popovic, 2015) with β set to 1.0. There are no images paired with the sentences in OpenSubtitles. When using OpenSubtitles in training multi-modal models, we feed in the mean vector of all visual features in the training data as a dummy visual feature. 4.4 Results Based on the previous experiments, we chose the Transformer architecture, Multi30k+MSCOCO+subs3MLM data sets, Detectron mask surface visual features, and domain labeling. Table 5 shows the BLEU scores for this configuration with different ways of integrating the visual features. The results are inconclusive. The ranking according to chrF-1.0 was n"
W19-0302,W16-2002,0,0.0310277,"processes that result in https://sites.google.com/view/deeplo18/home 16 allomorphs, i.e. different surface morphs corresponding to the same meaning. w 7→ y; w ∈ Σ∗ , y ∈ (Σ ∪ {◦})∗ e.g. achievability 7→ achieve ◦ able ◦ ity where Σ is the alphabet of the language, and ◦ is the boundary marker. Morphological analysis yields the lemma and tags representing the morphological properties of a word. w 7→ yt; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. took 7→ take PAST where τ is the set of morphological tags. Two related morphological tasks are reinflection and lemmatization. In morphological reinflection (see e.g. Cotterell et al., 2016), one or more inflected forms are given to identify the lexeme, together with the tags identifying the desired inflection. The task is to produce the correctly inflected surface form of the lexeme. wt 7→ y; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. taken PAST 7→ took In lemmatization, the input is an inflected form and the output is the lemma. w, y ∈ Σ∗ w 7→ y; e.g. better 7→ good Morphological surface segmentation can be formulated in the same way as canonical segmentation, by just allowing the mapping to canonical segments to be the identity. However, this formulation fails to capture the fact that the segme"
W19-0302,P12-1016,0,0.034327,"coder setting. The strings to be reconstructed can be actual words or even random noise. Surface segmentation can alternatively be formulated as structured classification w 7→ y; w ∈ Σk , y ∈ Ωk , k ∈ N e.g. uses 7→ BM ES where Ω is the segmentation tag set. Note that there is no need to generate characters from the original alphabet, instead a small tag set Ω is used. The fact that the sequence of boundary decisions is of the same length k as the input has also been made explicit. Different tag sets Ω can be used for segmentation. The minimal sets only include two labels: BM/ME (used e.g. by Green and DeNero, 2012). Either the beginning (B) or end (E) of segments is distinguished from non-boundary time-steps in the middle (M). A more fine-grained approach BMES (used e.g. by Ruokolainen et al., 2014) uses Also known as BIES, where I stands for internal. 17 l e a + n <E> h1 h2 h3 h4 h5 h6 LSTM LSTM LSTM LSTM LSTM LSTM <B> l e a + n B M E S <E> h1 h2 h3 h4 h5 LSTM LSTM LSTM LSTM LSTM <B> B M E S + s2 s3 s4 s5 LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM e 0 STM a 0 STM n 1 SUF s2 s3 s4 s5 LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM l 1 STM e 0 STM a 0 STM n 1 SUF l 1 STM (a) seq2seq B M <E> (b) neural sequence ta"
W19-0302,C14-1111,1,0.87332,"e is used: first the features for the desired words are produced using the generative model. The final segmentation can then be decoded from the discriminative model. The idea is that the features from the generative model allow the statistical patterns found in the large unannotated data to be exploited. At the same time, the capacity of the discriminative model is freed for learning to determine when the generative model’s predictions are reliable, in essence to only correct its mistakes. 3.1 Morfessor FlatCat We produce the features for our semi-supervised training using Morfessor FlatCat (Grönroos et al., 2014). Morfessor FlatCat is a generative probabilistic method for learning morphological segmentations. It uses a prior over morph lexicons inspired by the Minimum Description Length principle (Rissanen, 1989). Morfessor FlatCat applies a simple Hidden Markov model for morphotactics, providing morph category tags (stem, prefix, suffix) in addition to the segmentation. The segmentations are more consistent compared to Morfessor Baseline, particularly when splitting compound words. Morfessor FlatCat produces morph category labels in addition to the segmentation decisions. These labels can also be use"
W19-0302,D16-1097,0,0.176148,"l. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphological processing tasks There are several related morphological tasks that can be described as mapping from one sequence to another. Morphological segmentation is the task of splitting words into morphemes, meaning-bearing sub-word units. In morphological surface segmentation, the word w is segmented into a sequence of surface morphs, substrings whose concatenation is the word w. e.g. achievability 7→ achiev ◦ abil ◦ ity Canonical morphological segmentation (Kann et al., 2016) instead yields a sequence of standardized segments. The aim is to undo morphological processes that result in https://sites.google.com/view/deeplo18/home 16 allomorphs, i.e. different surface morphs corresponding to the same meaning. w 7→ y; w ∈ Σ∗ , y ∈ (Σ ∪ {◦})∗ e.g. achievability 7→ achieve ◦ able ◦ ity where Σ is the alphabet of the language, and ◦ is the boundary marker. Morphological analysis yields the lemma and tags representing the morphological properties of a word. w 7→ yt; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. took 7→ take PAST where τ is the set of morphological tags. Two related morphologi"
W19-0302,N18-1005,0,0.460579,"ion (e.g. Wang et al., 2016). We are interested to see if data-hungry neural network models are applicable to segmentation in low-resource settings, in this case for the Uralic language North Sámi. Neural sequence-to-sequence (seq2seq) models are a very versatile tool for NLP, and are used in state of the art methods for a wide variety of tasks, such as text summarization (Nallapati et al., 2016) and speech synthesis (Wang et al., 2017). Seq2seq methods are easy to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system. Moreover, we show that the semi-supervised training approach of Ruokolainen et al. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphol"
W19-0302,P17-4012,0,0.0471313,"Missing"
W19-0302,K16-1028,0,0.019041,"ained attention. For example, the workshop Deep Learning Approaches for Low-Resource NLP (DeepLo) was arranged first time in the year of writing. Neural methods have met with success in high-resource morphological segmentation (e.g. Wang et al., 2016). We are interested to see if data-hungry neural network models are applicable to segmentation in low-resource settings, in this case for the Uralic language North Sámi. Neural sequence-to-sequence (seq2seq) models are a very versatile tool for NLP, and are used in state of the art methods for a wide variety of tasks, such as text summarization (Nallapati et al., 2016) and speech synthesis (Wang et al., 2017). Seq2seq methods are easy to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system"
W19-0302,E14-4017,1,0.843392,"y to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system. Moreover, we show that the semi-supervised training approach of Ruokolainen et al. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphological processing tasks There are several related morphological tasks that can be described as mapping from one sequence to another. Morphological segmentation is the task of splitting words into morphemes, meaning-bearing sub-word units. In morphological surface segmentation, the word w is segmented into a sequence of surface morphs, substrings whose concatenation is the word w. e.g. achievability 7→ achiev ◦ abil ◦ ity Canonical morphological segmentation (Kann et a"
W19-1701,iskra-etal-2002-speecon,0,0.169959,"Missing"
