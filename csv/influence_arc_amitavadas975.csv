2020.icon-main.19,Y10-1011,0,0.0715001,"Missing"
2020.icon-main.19,W18-4401,0,0.0454009,"Missing"
2020.icon-main.19,2020.trac-1.1,0,0.03778,"Missing"
2020.icon-main.19,2020.trac-1.18,0,0.0622838,"Missing"
2020.icon-main.19,D18-1350,0,0.0676751,"Missing"
2020.icon-main.19,W18-4418,0,0.0205208,"87 9,137 16,188 144 142 630 Total 39,512 916 province.” is labeled as NAG; ”#salute you my friend” is labeled as OAG. To have a fair comparison with the results of previous works, we don’t do anything to address this. The dataset is imbalanced with maximum tweets labeled as NAG. Dataset To identify the type of aggression, we use the English train dataset, and the Facebook (fb) test dataset provided by the 2018 TRAC shared task (Kumar et al., 2018a). The data collection and annotation method is described in Kumar et al. (2018b). The training data is combined with the augmented data provided by Risch and Krestel (2018). The final distribution is given in table 1. The data has English-Hindi code-mixed tweets, which are annotated with one of three labels: 4 Preprocessing and Embeddings The tweets are first converted to lower case. Next, we remove digits, special characters, emojis, urls, and stop words. We restrict the continuous repetition of the same character in a word to 2 (e.g. ’suuuuuuper’ is converted to ’suuper’). Each tweet is tokenized and converted into a sequence of integers. The maximum sequence length is restricted to 150. To have dense representation of tokens, the following word embedding feat"
2020.icon-main.19,2020.trac-1.9,0,0.0569751,"Missing"
2020.icon-main.19,W18-4408,0,0.021288,"sm (Kumar et al., 2018b). E.g., ”Irony is your display picture at one end you are happy seeing some one innocent dying and at other end you are praying to not kill an innocent” • Glove++: Given the word, we first check whether it is present in Glove pre-trained 6b 100d embeddings, and use the embedding if it exists. For Out-Of-Vocabulary words, we use the word vectors that we train on the entire data using the Gensim library. • Overtly Aggressive (OAG): Direct and explicit form of aggression which includes derogatory comparison, verbal attack or abusive words towards a group or an individual (Roy et al., 2018). E.g., ”Shame on you assholes showing some other video and making it a fake news u chooths i hope each one you at *** news will rot in hell” • Aggression Embeddings: To have distinguishing features to separate aggressive tweets from non-aggressive tweets, we create aggression word embeddings. We take all the tweets classified as OAG and CAG and train word vectors on them. • Char Trigram: To get sub-word information, we create character trigram embeddings. • NAG: Texts which are not aggressive. E.g., ”hope car occupants are safe and unharmed.” 5 Proposed Architecture We propose an architecture"
2020.icon-main.19,2020.trac-1.20,1,0.850645,"Missing"
2020.icon-main.19,W18-4412,0,0.0209436,"ial information, learn semantic representation, and ignore words that are insignificant. CN1: The architecture is shown in figure 1. It is an ensemble of 3 subnetworks. Each SN uses Glove++ embeddings, and the CNN layers have kernel size = 3,4 and 5, respectively. CN2: Like CN1, but there is an additional biLSTM layer, having 300 units, after the capsule layer. 6 Figure 2: Confusion matrix of CN1 model Figure 3: Flatten vector of subnetwork1 Results and Discussion From table 2, we see that the CN models perform better than DL models. Both the CN models are comparable to the models proposed by Srivastava et al. (2018). This validates the usefulness of capsule networks for aggression detection. CN1 gives the best results and is better than the best model proposed by Aroyehun and Gelbukh (2018). DL2 works better than DL1, as it captures more information. The performance drops from CN1 to CN2, despite CN2 having an additional biLSTM layer. This shows that a more complex model is not necessarily better, which is in agreement with the observations of Aroyehun and Gelbukh (2018). This could be due to over-fitting. Figures 3, 4 and 5 are t-SNE (van der Maaten and Hinton, 2008) graphs, which depict the output of S"
2020.lrec-1.764,W14-3902,1,0.885747,"Missing"
2020.lrec-1.764,P18-1143,0,0.0368767,"Missing"
2020.lrec-1.764,L16-1292,1,0.794368,"For positive sampling based data augmentation, we have used actual switching points as keywords to collect more tweets, unlike the previous works, we have not used any synthetic data. 2..1 Code-Mixing Index (CMI) When comparing different language modeling techniques on CM corpus it is essential to have a measure to quantify how much mixed the data is, in particular, since error rates for various language processing applications would be expected to increase, as the level of code-mixing increases. To measure such complexity in CM corpus an index called Code-mixing Index has been introduced by (Gambäck and Das, 2016) as the following: Cu (x) = wm fm (x) + wp fp (x) N (x)−maxLi ϵL (tLi)(x) P (x) ∗ 100 + wp N N (x) (x) ∗ 100 wm ((N (x)−maxLi ϵL (tLi)(x))+wp P (x) 100 ∗ (1) N (x) = wm = As proposed by (Gambäck and Das, 2016) - there are two main sources of information are utilized to fully account for the code alternation at utterance level: the ratio of tokens belonging to the matrix language (wm ((N (x) − maxLi ϵL (tLi)(x))/N (x) as in Equation 1) and the number of code alternation points per token (wp P (x)/N (x), where P (x) is the number of code alternation points; 0 ⩽ P (x) ⩽ N (x)). (Gambäck and Das,"
2020.lrec-1.764,C82-1023,0,0.514715,"Missing"
2020.semeval-1.100,2020.semeval-1.163,0,0.0830955,"Missing"
2020.semeval-1.100,W18-3219,1,0.85107,"ma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two datasets - Spanglish an"
2020.semeval-1.100,2020.semeval-1.118,0,0.094663,"Missing"
2020.semeval-1.100,W15-4319,0,0.042123,"Missing"
2020.semeval-1.100,2020.semeval-1.172,0,0.0729464,"Missing"
2020.semeval-1.100,2020.semeval-1.182,0,0.0811279,"Missing"
2020.semeval-1.100,2020.semeval-1.175,0,0.0894491,"Missing"
2020.semeval-1.100,2020.semeval-1.121,0,0.0808882,"Missing"
2020.semeval-1.100,2020.semeval-1.119,0,0.0648513,"Missing"
2020.semeval-1.100,Q17-1010,0,0.0443846,"2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for se"
2020.semeval-1.100,2020.semeval-1.165,0,0.0668742,"Missing"
2020.semeval-1.100,W14-3908,0,0.0238306,"ttempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Co"
2020.semeval-1.100,P19-4007,0,0.0481051,"Missing"
2020.semeval-1.100,W14-5152,1,0.859608,"Missing"
2020.semeval-1.100,N19-1423,0,0.0289635,"y positive. The intention to proceed in this way is to enrich the original corpus annotations with sentiment-level labels. Moreover, the splits do not share the same distribution (i.e., development and test are more skewed than training) because we were annotating data on-demand rather than having available the entire corpus at any stage of the competition. Some annotated examples are provided in Table 2. The average CMI for the train, validation, and test sets are 21.84, 20.52, and 17.23, respectively. 5 Baseline We develop our baseline system using the pre-trained multilingual BERT (M-BERT; Devlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019"
2020.semeval-1.100,2020.semeval-1.164,0,0.0861606,"Missing"
2020.semeval-1.100,W13-1102,0,0.0133803,"are inherently multilingual environments.2 Besides, multilingual communities around the world regularly express their thoughts in social media employing and alternating different languages in the same utterance. This mixing of languages, also known as code-mixing or code-switching,3 is a norm in multilingual societies and is one of the many NLP challenges that social media has facilitated. 1.1 Code-Mixing Challenges In addition to the writing aspects in social media, such as flexible grammar, permissive spelling, arbitrary punctuation, slang, and informal abbreviations (Baldwin et al., 2015; Eisenstein, 2013), code-mixing has introduced a diverse set of linguistic challenges. For instance, multilingual speakers tend to code-mix using a single alphabet regardless of whether the languages involved belong to different writing systems ∗ 1 Equal contribution. https://ritual-uh.github.io/sentimix2020/ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 2 Statistics show that half of the messages on Twitter are in a language other than English (Schroeder, 2010). 3 We use code-mixing and code-switching intercha"
2020.semeval-1.100,L16-1292,1,0.903347,"Missing"
2020.semeval-1.100,2020.semeval-1.171,0,0.0951278,"Missing"
2020.semeval-1.100,2020.semeval-1.176,0,0.0834599,"Missing"
2020.semeval-1.100,2020.semeval-1.125,0,0.0948821,"Missing"
2020.semeval-1.100,2020.semeval-1.166,0,0.0581426,"Missing"
2020.semeval-1.100,P18-1031,0,0.0665313,"Missing"
2020.semeval-1.100,2020.semeval-1.170,0,0.079557,"Missing"
2020.semeval-1.100,2020.semeval-1.120,0,0.0613383,"Missing"
2020.semeval-1.100,2020.semeval-1.162,0,0.0710808,"Missing"
2020.semeval-1.100,2020.semeval-1.117,0,0.09011,"Missing"
2020.semeval-1.100,2020.semeval-1.103,0,0.0585778,"Missing"
2020.semeval-1.100,2020.semeval-1.126,0,0.0563388,"Missing"
2020.semeval-1.100,2020.semeval-1.177,0,0.0948764,"Missing"
2020.semeval-1.100,S13-2053,0,0.0336157,"found on social media which contains a lot of nonstandard spellings of words and unnecessary capitalization (Das and Gamb¨ack, 2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identifi"
2020.semeval-1.100,W16-5805,1,0.702393,"code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two"
2020.semeval-1.100,2020.semeval-1.124,0,0.0681298,"Missing"
2020.semeval-1.100,2020.semeval-1.169,0,0.0479718,"Missing"
2020.semeval-1.100,P19-1493,0,0.0323392,"vlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019). We use the Transformers (Wolf et al., 2019) library to implement our framework and we fine-tune the pre-trained BERT-Base, Multilingual Cased model separately for each of the two languages. Based on our observation on the training split for each dataset, we set the highest sequence length to 40 and 56 tokens for Spanglish and Hinglish, respectively. Then, we fine-tune the model for three epochs using AdamW (Loshchilov and Hutter, 2019) optimizer (η = 2e−5 ). 9 https://requester.mturk.com/ An assignment is done by a single annotator. 11 We use the assignment review policy ScoreMyKnownAnswers/"
2020.semeval-1.100,N16-1159,0,0.0256245,"d the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 20"
2020.semeval-1.100,2020.semeval-1.173,0,0.077831,"Missing"
2020.semeval-1.100,D08-1102,1,0.598825,"74 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 774–790 Barcelona, Spain (Online), December 12, 2020. (i.e., language scripts). This behavior is known as transliteration, and code-mixers rely on the phonetic patterns of their writing (i.e., the actual sound) to convey their thoughts in the foreign language (i.e., the language adapted to a new script) (Sitaram et al., 2019). Another common pattern in code-mixing is the alternation of languages at the word level. This behavior often happens by inflecting words from one language with the rules of another language (Solorio and Liu, 2008). For instance, in the second example below, the word pushes is the result of conjugating the English verb push according to Spanish grammar rules for the present tense in third person (in this case, the inflection -es). The Hinglish example shows that phonetic Latin script typing is a popular practice in India, instead of using Devanagari script to write Hindi words. We capture both transliteration and word-level code-mixing inflections in the Hinglish and Spanglish corpora of this competition, respectively. AyeHI aurHI enjoyEN kareHI Eng. Trans.: come and enjoy NoSP meSP pushesEN pleaseEN En"
2020.semeval-1.100,W14-3907,1,0.910074,"sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language ma"
2020.semeval-1.100,2020.semeval-1.122,0,0.373251,"Missing"
2020.semeval-1.100,2020.semeval-1.168,0,0.0472143,"Missing"
2020.semeval-1.100,2020.semeval-1.167,0,0.0594151,"Missing"
2020.semeval-1.100,2020.semeval-1.181,0,0.0809492,"Missing"
2020.semeval-1.100,W15-2902,0,0.181507,"Missing"
2020.semeval-1.100,2020.semeval-1.174,0,0.044579,"Missing"
2020.semeval-1.100,2020.semeval-1.179,0,0.0543759,"Missing"
2020.semeval-1.100,2020.semeval-1.183,0,0.0848258,"Missing"
2020.semeval-1.99,2020.semeval-1.112,0,0.0785168,"Missing"
2020.semeval-1.99,2020.semeval-1.157,0,0.0754856,"Missing"
2020.semeval-1.99,2020.semeval-1.102,0,0.0886475,"Missing"
2020.semeval-1.99,2020.semeval-1.146,0,0.0820491,"Missing"
2020.semeval-1.99,P19-1285,0,0.0118077,"d logistic regression for solving the task of humour classification and significant score. 765 • Hitachi: They have proposed simple but effective MODALITY ENSEMBLE that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. They fine-tuned four pre-trained visual models (i.e., InceptionResNet (Szegedy et al., 2016), Polynet (Zhang et al., 2016), SENet (Hu et al., 2017), and PNASNet (Liu et al., 2017)) and four textual models (i.e., BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2018), Transformer-XL (Dai et al., 2019), and XLNet (Yang et al., 2019)), followed by the fusion of their predictions by ensemble methods to effectively capture cross- modal correlations. Task-A Sentiment Analsysis Participant / Team Macro-F1 Comparison with baseline(+/-) Vkeswani IITK 0.35466 (+)0.13701 Guoym 0.35197 (+)0.13432 Aihaihara 0.35017 (+)0.13252 Sourya Diptadas 0.34885 (+)0.13120 Irina Bejan 0.34755 (+)0.12990 Sabino INGEOTEC 0.34689 (+)0.12924 U.Walinska Urszula Walin´ska 0.34639 (+)0.12874 Souvik Mishra Kraken 0.34627 (+)0.12862 Lb732 SESAM 0.34600 (+)0.12835 Li Zhen hit-mitlab 0.34583 (+)0.12818 George.Vlad Eduardgzah"
2020.semeval-1.99,N19-1423,0,0.56676,"challenge was a great success, involving total of 583 participants, with varying submissions in different tasks comprising of 31, 26 and 23 submissions in Task A, Task B and Task C respectively where in evaluation phase, a user is allowed for 5 submissions per day. 27 teams submitted the system description paper. A brief description of the task wise top performing models is shown below. 7.1 Top 3 Task A systems @Memotion • IITK Vkeswani: Employed wide variety of methods, ranging from a simple linear classifier such as FFNN, Naive Bayes to transformers like MMBT (Rahman et al., 2019) and BERT (Devlin et al., 2019). Implemented the model considering only text and the combination of image and text. • Guoym: Used ensembling Method considering the textual features extracted using Bi-GRU, BERT (Devlin et al., 2019), or ELMo (Peters et al., 2018), image features extracted by Resnet50 (He et al., 2015) network and fusion features of text and images. • Aihaihara: Implemented the model that is a concatenation of visual and textual features obtained from n-gram language model and VGG-16 (Simonyan and Zisserman, 2015) pretrained model respectively. 7.2 Top 3 Task B and Task C systems @Memotion • UPB George: In or"
2020.semeval-1.99,2020.semeval-1.115,0,0.202852,"Missing"
2020.semeval-1.99,2020.semeval-1.151,0,0.0791868,"Missing"
2020.semeval-1.99,2020.semeval-1.147,0,0.0680622,"Missing"
2020.semeval-1.99,2020.semeval-1.111,0,0.0777053,"Missing"
2020.semeval-1.99,2020.semeval-1.113,0,0.0538536,"Missing"
2020.semeval-1.99,2020.semeval-1.152,0,0.0786869,"Missing"
2020.semeval-1.99,2020.semeval-1.150,0,0.0502881,"Missing"
2020.semeval-1.99,2020.semeval-1.116,0,0.23958,"Missing"
2020.semeval-1.99,2020.semeval-1.154,0,0.411447,"Missing"
2020.semeval-1.99,2020.semeval-1.149,0,0.174495,"Missing"
2020.semeval-1.99,D14-1162,0,0.0841572,"ssed using varying textual contents, so as to convey different emotions. In some cases, different memes can have same images, but due to the different textual messages embedded in each of them, different sentimental reactions can be induced from all. Recognition of the emotion induced in such memes would require accurate modelling of the textual influence. To evaluate automated emotion recognition from the meme textual content, we built text binary classifier as shown in the bottom half of Fig.3, to understand different classes of emotion. We have used 100-D pre-trained Glove word embeddings (Pennington et al., 2014) to generate word embeddings from text emd(txt). These embeddings 762 Figure 1: Plot depicting category-wise data distribution of meme emotion data-set [For eg. There are approx. 2200 memes in the data-set tagged as “Not funny”]. Figure 2: A Multi-level system for the task of emotion intensity prediction (1×14 dimensional), using the emotion class multi-label output (1×4 dimensional). are given as input xi to the CNN, having 64 filters of size 1×5 with Relu as activation function to extract the textual features. To reduce the dimension of number of parameters generated by CNN layer we have use"
2020.semeval-1.99,N18-1202,0,0.0156868,"allowed for 5 submissions per day. 27 teams submitted the system description paper. A brief description of the task wise top performing models is shown below. 7.1 Top 3 Task A systems @Memotion • IITK Vkeswani: Employed wide variety of methods, ranging from a simple linear classifier such as FFNN, Naive Bayes to transformers like MMBT (Rahman et al., 2019) and BERT (Devlin et al., 2019). Implemented the model considering only text and the combination of image and text. • Guoym: Used ensembling Method considering the textual features extracted using Bi-GRU, BERT (Devlin et al., 2019), or ELMo (Peters et al., 2018), image features extracted by Resnet50 (He et al., 2015) network and fusion features of text and images. • Aihaihara: Implemented the model that is a concatenation of visual and textual features obtained from n-gram language model and VGG-16 (Simonyan and Zisserman, 2015) pretrained model respectively. 7.2 Top 3 Task B and Task C systems @Memotion • UPB George: In order to extract most salient features from text input, they opted to use the ALBERT (Lan et al., 2019)model while VGG -16 (Simonyan and Zisserman, 2015) is used for extracting the visual features from image input. To determine the h"
2020.semeval-1.99,2020.semeval-1.153,0,0.0699054,"Missing"
2020.semeval-1.99,2020.semeval-1.158,0,0.0942945,"Missing"
2020.semeval-1.99,2020.semeval-1.160,0,0.261832,"Missing"
2020.semeval-1.99,2020.semeval-1.148,0,0.0659778,"Missing"
2020.semeval-1.99,S19-2010,0,0.0926346,"Missing"
2020.semeval-1.99,2020.semeval-1.159,0,0.335744,"Missing"
2020.semeval-1.99,2020.semeval-1.145,0,0.0701982,"Missing"
2020.trac-1.20,2020.trac-1.25,0,0.0627022,"dress this problem. Aggression is a feeling of anger that results in hostile behavior and readiness to attack. According to Kumar et al. (2018c), aggression can either be expressed in a direct, explicit manner (Overtly Aggressive) or an indirect, sarcastic manner (Covertly Aggressive). Hate-speech is used to attack a person or a group of people based on their color, gender, race, sexual orientation, ethnicity, nationality, religion (Nockleby, 2000). Misogyny or Sexism is a subset of hate-speech (Waseem and Hovy, 2016) and targets the victim based on gender or sexuality (Davidson et al., 2017; Bhattacharya et al., 2020). It is essential to identify aggression and hate-speech in social networks to protect online users against such attacks, but it is quite time-consuming to do so manually. Hence, social media companies and government agencies are focusing on building a system that can automate the identification process. However, it is difficult to draw a dis1 These authors contributed equally. https://blog.microfocus.com/how-muchdata-is-created-on-the-internet-each-day/ 2 tinguishing line between acceptable content and aggressive/hateful content due to the subjectivity of the definitions and different percept"
2020.trac-1.20,N19-1423,0,0.0844451,"Missing"
2020.trac-1.20,W17-3013,0,0.0512149,"Missing"
2020.trac-1.20,W17-2902,0,0.0878508,"Missing"
2020.trac-1.20,W18-4401,0,0.158089,"I system. Facebook published its audit report3 on civil rights, which explains its strategy to tackle abusive and hateful content. The report claims that building a complete automation system to detect hate-speech is not possible, and content moderation is unavoidable. This point brings many researchers to focus on building hate-speech/aggression detection systems since a large amount of such data is diffused in social networks. To this end, several workshops have been organized, including ‘Abusive Language Online’ (ALW) (Roberts et al., 2019), ‘Trolling, Aggression and Cyberbullying’ (TRAC) (Kumar et al., 2018b), and Semantic Evaluation (SemEval) shared task on Identifying Offensive Language in Social Media (OffensEval) (Zampieri et al., 2020). This paper presents our system for TRAC-2 Shared Task on “Aggression Identification” (sub-task A) and “Misogynistic Aggression Identification” (sub-task B), in which we propose a BERT (Devlin et al., 2018) based architecture to detect misogyny and aggression using a multi-task approach. The proposed model uses attention mechanism over BERT to get relative importance of words, followed by Fully-Connected layers, and a final classification layer for each sub-t"
2020.trac-1.20,S19-2123,0,0.0353497,"Missing"
2020.trac-1.20,W18-4414,0,0.022903,"Missing"
2020.trac-1.20,D19-1174,0,0.0403019,"Missing"
2020.trac-1.20,W18-4404,0,0.024349,"Missing"
2020.trac-1.20,W18-4418,0,0.392873,"Missing"
2020.trac-1.20,2020.trac-1.1,0,0.330248,"Missing"
2020.trac-1.20,W17-1101,0,0.0492831,"Missing"
2020.trac-1.20,N16-2013,0,0.0958162,"Missing"
2020.trac-1.20,2020.semeval-1.188,0,0.023826,"he report claims that building a complete automation system to detect hate-speech is not possible, and content moderation is unavoidable. This point brings many researchers to focus on building hate-speech/aggression detection systems since a large amount of such data is diffused in social networks. To this end, several workshops have been organized, including ‘Abusive Language Online’ (ALW) (Roberts et al., 2019), ‘Trolling, Aggression and Cyberbullying’ (TRAC) (Kumar et al., 2018b), and Semantic Evaluation (SemEval) shared task on Identifying Offensive Language in Social Media (OffensEval) (Zampieri et al., 2020). This paper presents our system for TRAC-2 Shared Task on “Aggression Identification” (sub-task A) and “Misogynistic Aggression Identification” (sub-task B), in which we propose a BERT (Devlin et al., 2018) based architecture to detect misogyny and aggression using a multi-task approach. The proposed model uses attention mechanism over BERT to get relative importance of words, followed by Fully-Connected layers, and a final classification layer for each sub-task, which predicts the class. 2. Related Work Hate-speech: The interest of NLP researchers in hatespeech, aggression, and sexism detect"
C10-2027,C00-1044,0,0.117604,"Missing"
C10-2027,E06-1039,0,\N,Missing
E17-1069,baccianella-etal-2010-sentiwordnet,0,0.0334636,"n-grams are known to be useful for any kind of textual classification, all the teams tested various lengths of n-grams (uni, bi, and tri-grams). Categorical features like part-of-speech (POS), word level features like capital letters, repeated words were also used. Linguistic Inquiry Word Count (LIWC) features were used by all the teams as their baselines. LIWC (Pennebaker et al., 2015) is a handcrafted lexicon specifically designed for psycholinguistic experiments. Another psycholinguistic lexicon called MRC (Wilson, 1988) was also used by a few teams, as well as lexica such as SentiWordNet (Baccianella et al., 2010) and WordNet Affect (Strapparava and Valitutti, 2004). Two more important textual features were discussed by the participating teams. Linguistic nuances, introduced by Tomlinson et al. (2013), is the depth of the verbs in the Wordnet troponymy hierarchy. Speech act features were utilized by Appling et al. (2013): the authors manually annotated the given Facebook corpus with speech acts and reported their correlation with the personality traits. Related Work State-of-the-art sentiment analysis (SA) systems look at a fragment of text in isolation. However, in order to design a Schwartz model cla"
E17-1069,N13-1132,0,0.0284911,"data. Timeline data includes their own posts and all the posts they are tagged in, and posts other people made on their Timeline. So far, data from 114 unique users has been collected, but the data is highly imbalanced (for some value types the distributions of ‘Yes’ and ‘No’ classes were in 90:10 ratio). Crowd-sourcing is a cheap and fast way to collect data, but unfortunately some annotators chose random labels to minimize their cognitive thinking load. These annotators can be considered as spammers and make aggregation of crowd-sourced data a challenging problem, as discussed in detail by Hovy et al. (2013). To filter out spammers, the MACE (Hovy et al., 2013) tool was used and data from 54 users discarded, so the final dataset includes only 60 participants. The average number of messages per user in the Facebook corpus is 681. Facebook Corpus Facebook (FB) is the most popular social networking site in the world, with 1.65 billion monthly active users during the first quarter of 2016.6 Therefore, Facebook was a natural first choice for corpus collection, but since the privacy policy of Facebook is very stringent, accessing Facebook data is challenging. To collect the corpus, a Facebook Canvas we"
E17-1069,D14-1214,0,0.0292094,"Missing"
E17-1069,S13-2053,0,0.0625729,"Missing"
E17-1069,P11-2008,0,0.087564,"Missing"
E17-1069,D14-1160,0,0.0519273,"Missing"
E17-1069,strapparava-valitutti-2004-wordnet,0,0.106117,"Missing"
I08-5006,W03-2201,0,0.133497,"Missing"
I08-5006,W97-0312,0,0.0825691,"Missing"
I08-5006,N03-1028,0,0.349029,"features for named entity recognition in Bengali, Hindi, Telugu, Oriya and Urdu. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which corresponds to conditionally trained probabilistic finite state automata. Being conditionally trained, these CRFs can easily incorporate a large number of arbitrary, nonindependent features while still having efficient procedures for non-greedy finite-state inference and training. CRFs have shown success in various sequence modeling tasks including noun phrase segmentation (Sha and Pereira, 2003) and table extraction (Pinto et al., 2003). CRFs are used to calculate the conditional probability of values on designated output nodes given values on other designated input nodes. The conditional probability of a state sequence S s1, s 2, , sT given an observation sequence O o1, o 2, ....., oT ) is calculated as: T 1 exp( kfk (st 1, st , o, t )), where Zo t 1 k fk ( st 1, st , o, t ) is a feature function whose weight P ( s |o) k is to be learned via training. The values of the feature functions may range between ..... , but typically they are binary. To make all conditional probabilities su"
I08-5006,W03-0430,0,0.0522082,"is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are HMM (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy http://ltrc.iiit.ac.in/ner-ssea-08 33 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33–40, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing (New York University’s MENE in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine 1998), SRA’s system in (Bennet, 1997) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; McCallum and Li, 2003). There is no concept of capitalization in Indian languages (ILs) like English and this fact makes the NER task more difficult and challenging in ILs. There has been very little work in the area of NER in Indian languages. In Indian languages particularly in Bengali, the work in NER can be found in (Ekbal and Bandyopadhyay, 2007a) and (Ekbal and Bandyopadhyay, 2007b). These two systems are based on the pattern directed shallow parsing approach. An HMM-based NER in Bengali can be found in (Ekbal et al., 2007c). Other than Bengali, the work on NER can be found in (Li and McCallum, 2004) for Hind"
I08-5006,M98-1019,0,\N,Missing
I08-5006,M98-1020,0,\N,Missing
L16-1292,W14-5152,1,0.642887,"users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gruöz, 2013; Das and Gambäck, 2014). The paper is organized as follows: Section 2. describes a formal measure that can be used to compare the complexity 1850 of code-switched corpora. Section 3. then uses this corpus level switching measure in practise, applying it to a set of recently produced code-switched corpora. Finally, Section 4. sums up and elaborates on the results. 2. Measuring Code-Switching in Corpora When comparing different code-switched corpora to each other, it is desirable to have a measurement of the level of mixing between languages, in particular since error rates for various language processing application"
L16-1292,N13-1037,0,0.0216155,"of applying language processing tools to one code-switched corpus to those on another? Or more specifically: how can we compare the level of codeswitching in corpora? And for corpora containing a mix of a specific set of languages or across corpora from differThat two texts come from social media does not in itself imply that they belong to one, delimited textual domain. Rather, there is a wide spectrum of different types of texts that are transmitted through social media, and the level of formality of the language in addition depends more on the style of the writer than on the actual media (Eisenstein, 2013; Androutsopoulos, 2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change. Furthermore, although social media often convey more ungrammatical text than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lu"
L16-1292,R15-1033,1,0.860797,"13.83 22.32 35.18 13.29 14.50 20.26 21.97 31.06 49.06 17.06 Table 1: Code-switching levels in some corpora Using these weights, Equation 10 becomes   U  100 1 X 5 fm (x)+fp (x)+δ(x) +6 S Cc = U 2 x=1 = 100 U (12) max {tLi}(x)+P (x)   X U   Li ∈L 1 5 1− S +δ(x) + 2 6 N (x) x=1 To show how the measure can be used in practice to objectively compare the complexity of code-switching, Cc values as in Equation 12 were calculated for some recently produced code-switched corpora: the Dutch-Turkish chat corpus of Nguyen and Do˘gruöz (2013), the English-Hindi Twitter and Facebook chat corpus of Jamatia et al. (2015), and the four corpora6 used in the shared task on word-level language detection in code-switched text (Solorio et al., 2014) organized by the workshop on Computational Approaches to Code Switching at the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). When comparing Cc values for different corpora, it is necessary to consider their respective tagsets and annotation guidelines. So does the annotation strategy chosen for the EMNLP corpora prescribe that elements such as abbreviations should be tagged with the language they belong to, while other annotations schemes"
L16-1292,W14-1303,0,0.0121514,"13; Androutsopoulos, 2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change. Furthermore, although social media often convey more ungrammatical text than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lui and Baldwin (2014) note that users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gr"
L16-1292,D13-1084,0,0.117092,"Missing"
L16-1292,W14-3907,0,0.153917,"ts, Equation 10 becomes   U  100 1 X 5 fm (x)+fp (x)+δ(x) +6 S Cc = U 2 x=1 = 100 U (12) max {tLi}(x)+P (x)   X U   Li ∈L 1 5 1− S +δ(x) + 2 6 N (x) x=1 To show how the measure can be used in practice to objectively compare the complexity of code-switching, Cc values as in Equation 12 were calculated for some recently produced code-switched corpora: the Dutch-Turkish chat corpus of Nguyen and Do˘gruöz (2013), the English-Hindi Twitter and Facebook chat corpus of Jamatia et al. (2015), and the four corpora6 used in the shared task on word-level language detection in code-switched text (Solorio et al., 2014) organized by the workshop on Computational Approaches to Code Switching at the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). When comparing Cc values for different corpora, it is necessary to consider their respective tagsets and annotation guidelines. So does the annotation strategy chosen for the EMNLP corpora prescribe that elements such as abbreviations should be tagged with the language they belong to, while other annotations schemes treat them as language independent. Another potential problem can be to decide whether a tag is directly language related or"
L16-1292,voss-etal-2014-finding,0,0.0153529,"ext than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lui and Baldwin (2014) note that users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gruöz, 2013; Das and Gambäck, 2014). The paper is organized as follows: Section 2. describes a formal measure that can be used to compare the complexity 1850 of code-switched corpora. Section 3. then uses this corpus level switching measure in practise, ap"
L16-1292,I13-1041,0,\N,Missing
O12-1029,E09-1015,0,0.312967,"Missing"
O12-1029,I08-3012,1,0.899222,"Missing"
P11-3010,S07-1022,0,0.0743588,"Missing"
P11-3010,D08-1103,0,0.056385,"Missing"
P11-3010,W02-1011,0,0.0156809,"Missing"
P11-3010,P05-2008,0,0.07842,"Missing"
P11-3010,strapparava-valitutti-2004-wordnet,0,0.189318,"Missing"
P11-4009,S07-1022,0,0.0730324,"Missing"
P11-4009,W02-1011,0,0.013505,"Missing"
P11-4009,P05-2008,0,0.0407236,"Missing"
P11-4009,D08-1103,0,0.0344928,"Missing"
P11-4009,strapparava-valitutti-2004-wordnet,0,0.193977,"Missing"
P11-4009,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P11-4009,P06-1134,0,\N,Missing
R15-1033,W14-3914,0,0.123456,"Missing"
R15-1033,W14-3902,1,0.649564,"Missing"
R15-1033,W14-3915,0,0.0978188,"Missing"
R15-1033,sankaran-etal-2008-common,0,0.0701194,"zer,3 which is a sub-module of the CMU Twitter POS tagger (Gimpel et al., 2011). Although the CMU tokenizer was originally developed for English, empirical testing showed that it works reasonably well also for the Indian languages. 3.2 Category Noun (G_N) Pronoun (G_PRP) Part-of-Speech Tagsets We experimented with both coarse-grained and fine-grained tagsets, utilizing the fine-grained set during annotation. As can be seen in Table 2, this tagset includes both the Twitter specific tags introduced by Gimpel et al. (2011) and a set of POS tags for Indian languages that combines the ILPOST tags (Baskaran et al., 2008), the tags developed by the Central Institute of Indian Languages (LDCIL), and those suggested by the Indian Government’s Department of Information Technology (TDIL),4 that is, an approach similar to that taken for Gujarati by Dholakia and Yoonus (2014). The coarse-grained tagset instead combines Gimpel et al.’s Twitter specific tags with Google’s Universal Tagset (Petrov et al., 2011).5 The mapping between our fine-grained tagset and the Google Universal Tagset is also shown in Table 2. 3.3 Verb (G_V) Adjective (G_J) Adverb (G_R) Demonstrative (G_PRP) Quantifier (G_SYM) Particles (G_PRT) Resi"
R15-1033,Y10-1011,0,0.0773972,"Missing"
R15-1033,P11-2008,0,0.238554,"Missing"
R15-1033,W12-0601,0,0.0390013,"Missing"
R15-1033,C82-1023,0,0.438276,"s, on the other hand, concentrated on English tweets, whereas the majority of these texts now are written in other media and in other languages — or in mixes of languages. Today, code-switching is generally recognised as a natural part of bi- and multilingual language use, even though it historically often was considered a sub-standard use of language. Conversational spoken language code-switching has been a common research theme in psycho- and sociolinguists for half a century, and the first work on applying language processing methods to codeswitched text was carried out in the early 1980s (Joshi, 1982), while code-switching in social media text started to be studied in the late 1990s (Paolillo, 1996). Still, code alternation in conventional texts is not so prevalent as to spur much interest by the computational linguistic research community, and it was only recently that it became a research topic in its own right, with a code-switching workshop at EMNLP 2014 (Solorio et al., 2014), and a shared tasks at EMNLP and at Forum for Information Retrieval Evaluation, FIRE 2014. Both these shared tasks were on automatic word-level language detection in code-mixed text, but here we will assume that"
R15-1033,W14-5152,1,0.263112,"ble 2. 3.3 Verb (G_V) Adjective (G_J) Adverb (G_R) Demonstrative (G_PRP) Quantifier (G_SYM) Particles (G_PRT) Residual (G_X) Comparing Corpora Complexity The error rates for various language processing applications would be expected to be higher for more complex code-mixed text. When comparing different code-mixed corpora to each other, it is thus desirable to have a measurement of the level of mixing between languages. Kilgarriff (2001) discusses various statistical measures that can be used to compare corpora more objectively, but all those measures presume the corpora to be monolingual. In Das and Gambäck (2014) we instead suggested a Code-Mixing Index, CMI, to document the frequency of languages in a corpus, which we will use here as well. In short, the measure is defined as: if an utterance only contains language independent tokens, its CMI is zero; for other utterances, the CMI is calculated by counting the Conjunction, Pre& Postposition Numeral Determiner Twitter-Specific (Gimpel et al. 2011) (G_X) Type N_NN N_NNV N_NST N_NNP PR_PRP PR_PRL PR_PRF PR_PRC PR_PRQ V_VM V_VAUX Description Common Noun Verbal Noun Spatio-temporal Proper Noun Personal Relative Reflexive Reciprocal Wh-Word Main Auxiliary"
R15-1033,R13-1026,0,0.0259527,"Missing"
R15-1033,D14-1098,0,0.0634373,"r than the combination tagger and the one based on CRFs. There are several possible avenues that could be further explored on NLP for code-mixed texts, for example, transliteration, utterance boundary detection, language identification, and parsing. We are currently working on language modelling of code-mixed text to recognize which language is mixing into which. Language modelling has not before been applied to code-mixed POS tagging, but code-switched language models have previously been integrated into speech recognisers, although mostly by naïvely interpolating between monolingual models. Li and Funng (2014) instead obtained a code-switched language model by combining the matrix language model with a translation model from the matrix language to the mixed language. In the future, we also wish to explore language modelling on code-mixed text in order to address the problems caused by unknown words. Jannis Androutsopoulos. 2011. Language change and digital media: a review of conceptions and evidence. In Tore Kristiansen and Nikolas Coupland, editors, Standard Languages and Language Standards in a Changing Europe, pages 145–159. Novus, Oslo, Norway, Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacK"
R15-1033,N13-1037,0,0.0182022,"l, that is, in regions where languages change over short geospatial distances and people generally have at least a basic knowledge of the neighbouring languages. In particular, India is home to several hundred languages, with language diversity and dialectal changes instigating frequent code-mixing. We will here look at the tasks of collecting and annotating code-mixed English-Hindi social media text, and on automatic part-of-speech (POS) 239 Proceedings of Recent Advances in Natural Language Processing, pages 239–248, Hissar, Bulgaria, Sep 7–9 2015. this way, as discussed in detail by, e.g., Eisenstein (2013) and Androutsopoulos (2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change, which in turn has major implications for natural language processing: if we build a system that can handle a specific type of social media text today, it will be outdated tomorrow. Something which makes it very attractive to apply machine learning and adaptive techniques to the problem. In all types of social media, the level of formality of the language depends more on the style of the writer than on the"
R15-1033,W12-2101,0,0.0347566,"Missing"
R15-1033,gimenez-marquez-2004-svmtool,0,0.0687355,"Missing"
R15-1033,D11-1141,0,0.140325,"Missing"
R15-1033,N13-1039,0,0.081552,"Missing"
R15-1033,D08-1102,0,0.437157,"Missing"
R15-1033,D08-1110,0,0.808345,"Missing"
R15-1033,W14-3907,0,0.201864,"anguage code-switching has been a common research theme in psycho- and sociolinguists for half a century, and the first work on applying language processing methods to codeswitched text was carried out in the early 1980s (Joshi, 1982), while code-switching in social media text started to be studied in the late 1990s (Paolillo, 1996). Still, code alternation in conventional texts is not so prevalent as to spur much interest by the computational linguistic research community, and it was only recently that it became a research topic in its own right, with a code-switching workshop at EMNLP 2014 (Solorio et al., 2014), and a shared tasks at EMNLP and at Forum for Information Retrieval Evaluation, FIRE 2014. Both these shared tasks were on automatic word-level language detection in code-mixed text, but here we will assume that the word-level languages are known and concentrate on the task of automatic part-of-speech tagging for these types of texts. We have collected a corpus consisting of Facebook messages and tweets (which includes all The paper reports work on collecting and annotating code-mixed English-Hindi social media text (Twitter and Facebook messages), and experiments on automatic tagging of thes"
R15-1033,C12-2096,0,0.0162427,"Missing"
R15-1033,E09-1087,0,0.0139957,"Missing"
R15-1033,D14-1105,0,0.415598,"Missing"
R15-1033,A97-1004,0,0.050424,"Missing"
R15-1033,D13-1084,0,\N,Missing
R15-1033,I13-1041,0,\N,Missing
R15-1033,W11-3501,0,\N,Missing
R15-1070,P07-2056,0,0.0712326,"Missing"
R15-1070,W10-3402,1,0.838711,"2 PTE- type 3 PTE- type 4 Cosine Similarity Ranges Avg. &gt; 0.70 0.70 &lt; 0.70 &gt; 0.70 0.40 - 0.69 0.30 - 0.39 &lt; 0.30 0.35 0.70 0.46 0.35 0.25 Table 2: Ranges of cosine similarity scores 3 Bengali WordNet WordNet is a lexical semantic network to hold semantic relations like synonyms and wordsenses as the nodes of the network and relations of the synonyms and word-senses are the edges of the network. In WordNet, meaning of each word is represented by a unique word-sense and a set of its synonyms called synset. We have collected the Bengali WordNet developed by Das and Bandyopadhyay as described in (Das and Bandyopadhyay 2010), consists total 12K numbers of synsets. 3.1 Pre-Processing Text pre-processing is a vital pre-requisite while working with noisy social media text. Pre1 http://www.tweetarchivist.com 538 processing involves splitting tweet into valid tokens: words and symbols, stemming, moving out stop words and part-of-speech tagging. The CMU tweet tokenizer (Gimpel et al., 2011) has been used here. Although it is primarily developed for English but also works well for other languages like Bengali. We used the Bengali stop word list, made available publicly by ISI Kolkata2. For the POS tagging the system dev"
R15-1070,P11-2008,0,0.0177023,"e edges of the network. In WordNet, meaning of each word is represented by a unique word-sense and a set of its synonyms called synset. We have collected the Bengali WordNet developed by Das and Bandyopadhyay as described in (Das and Bandyopadhyay 2010), consists total 12K numbers of synsets. 3.1 Pre-Processing Text pre-processing is a vital pre-requisite while working with noisy social media text. Pre1 http://www.tweetarchivist.com 538 processing involves splitting tweet into valid tokens: words and symbols, stemming, moving out stop words and part-of-speech tagging. The CMU tweet tokenizer (Gimpel et al., 2011) has been used here. Although it is primarily developed for English but also works well for other languages like Bengali. We used the Bengali stop word list, made available publicly by ISI Kolkata2. For the POS tagging the system developed by (Dandapat et al., 2007) has been used. Although the POS tagger is not trained on social media text and accuracy of the tagger on tweet has not been measured. This is something we would like to do next. To trim all the surface word forms into corresponding root we developed one simple rule based Bengali Stemmer. Our stemmer concentrated on framing rules fo"
R15-1070,P14-5010,0,0.00545515,"Missing"
R15-1070,H93-1054,0,0.480918,"ge counting based method. Rada et al. (Rada, R et al., 1989), proposed a metric called distance, which determines the average minimum path length over all pair wise combinations of nodes between two subsets of nodes. Distance measure has been used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations and represents the relatedness of two words Due to the specific applications of edge counting based method like medical semantic nets (Li et al., 2003), most of the research on semantic similarity followed information theory based method (Resnik, 1993a) work is the first work on information theory based system which proposed modeled the selectional behavior of a predicate as its distributional effect on the conceptual classes of its arguments. This model experiment result suggests that many lexical relationships are better viewed in terms of underlying conceptual relationships. In a later work (Resnik, 1993b) focuses on two selectional preferences and semantic similarity as information-theoretic relationships involving conceptual classes and demonstrates the applicability of these relations to measure semantic similarity between two words."
R15-1070,Q14-1034,0,0.0544968,"Missing"
R15-1070,S12-1085,0,\N,Missing
R15-1070,W12-5114,0,\N,Missing
S19-2124,D14-1162,0,0.0945687,"oting (Kuncheva, 2004): L1-regularised Logistic Regression, L2-regularised Logistic Regression, Linear SVC, SGD, and PA. The ensemble model exhibited the best results in subtasks A and B. In subtask C, the multi-class classification problem and a severe reduction of the size of the training set led to much lower macroaveraged F1 -scores, with the ensemble model performing badly. A deep learning approach, based on an LSTM architecture (Hochreiter and Schmidhuber, 1997), was adopted specifically for this subtask. The model used a 200 dimensional GloVe embedding5 pre-trained on 2 billion tweets (Pennington et al., 2014), with trainability set to False. The embedding layer was followed by a 1D convolution layer with 64 output filters and a Rectified Linear Unit (ReLU) activation function. The output of this layer was down-sampled using a max pooling layer of size 4. These inputs were fed into an LSTM layer of 200 units and subsequently a dense layer of 3 units with a softmax activation function. The model used the ‘Adam’ optimiser and the categorical cross entropy loss function. Due to the less amount of data, overfitting was quite common on as few as 3 epochs. Therefore, the model benefited from larger dropo"
S19-2124,W17-3013,1,0.83596,"Missing"
S19-2124,W17-1101,0,0.0349419,"licit (Waseem and Hovy, 2016), but is considerably less reliable when considering implicit abuse (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011). They propose a typology that can synthesise different offensive language detection subtasks. Zampieri et al. (2019a) expand on these ideas and propose a hierarchical three-level-annotation model, which is used in the OffensEval 2019 shared task. Another issue is whether the datasets should be balanced or not (Waseem and Hovy, 2016), since there are much fewer offensive comments than benign comments in randomly sampled real-life data (Schmidt and Wiegland, 2017). timent scores (Van Hee et al., 2015; Davidson et al., 2017) also having been used (Schmidt and Wiegland, 2017). More recently, meta information about the users have been suggested as features, but no consistent correlation between user information and tendency for offensive behaviour online has been shown, with Waseem and Hovy (2016) claiming gender information leading to improvements in classifier performance, but with Unsv˚ag and Gamb¨ack (2018) challenging this and reporting user-network data to be more important instead. Wulczyn et al. (2017) concluded that anonymity leads to an increase"
S19-2124,gao-huang-2017-detecting,0,0.0193989,"ulczyn et al. (2017) concluded that anonymity leads to an increase in the likelihood of a comment being an attack. Classical Machine learning algorithms have been wielded to some success in automated offensive language detection, mainly Logistic Regression (Davidson et al., 2017; Waseem and Hovy, 2016; Burnap and Williams, 2015) and Support Vector Machines (Xu et al., 2012; Dadvar et al., 2013). Recently, however, deep learning models have outperformed their traditional machine learning counterparts, with both Recurrent Neural Networks (RNN) — such as LSTM (Pitsilis et al., 2018) and Bi-LSTM (Gao and Huang, 2017) — and Convolutional Neural Networks (CNN) having been used. Gamb¨ack and Sikdar (2017) utilised a CNN model with word2vec embeddings to obtain higher F1 -score and precision than a previous logistic regression model (Waseem and Hovy, 2016), while Zhang et al. (2018) combined a CNN model with a Gated Recurrent Unit (GRU) layer. Malmasi and Zampieri (2018) used an ensemble system much like ours to separate profanity from hate speech, but reported no significant improvement over a single classifier system. The training dataset used for the shared task, the Offensive Language Identification Datas"
S19-2124,W18-5110,1,0.893947,"Missing"
S19-2124,P11-2008,0,0.275852,"Missing"
S19-2124,R15-1086,0,0.0634104,"Missing"
S19-2124,P82-1020,0,0.796928,"Missing"
S19-2124,W17-3012,0,0.0308943,"ge on Social Media’. However, what we consider offensive is often a grey area, as is evident by the low inter-annotator agreement 2 Related Work Most datasets for offensive language detection represent multiclass classification problems (Davidson et al., 2017; Founta et al., 2018; Waseem and Hovy, 2016), with the annotations often obtained via crowd-sourcing portals, with 696 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 696–703 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics varying degrees of success. Waseem et al. (2017b) state that annotation via crowd-sourcing tends to work best when the abuse is explicit (Waseem and Hovy, 2016), but is considerably less reliable when considering implicit abuse (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011). They propose a typology that can synthesise different offensive language detection subtasks. Zampieri et al. (2019a) expand on these ideas and propose a hierarchical three-level-annotation model, which is used in the OffensEval 2019 shared task. Another issue is whether the datasets should be balanced or not (Waseem and Hovy, 2016), since there are muc"
S19-2124,W16-3638,0,0.0320908,"t is unclear whether this was due to a more strict definition provided by the task organisers. For example, it is not immediately clear why tweets such as:1 “@USER Ouch!” (23159), “@USER He is a beast” (50771), and “@USER That shit weird! Lol” (31404) were annotated as offensive. The annotators furthermore seemed to disagree over the cathartic and emphatic use of swearing, as in “@USER Oh my Carmen. He is SO FRICK3 In terms of features, simple bag of words models have proven to be highly predictive (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Burnap and Williams, 2015). Mehdad and Tetreault (2016) endorsed the use of character n-grams over token n-grams citing their ability to glaze over the spelling errors that are frequent in online texts. Nobata et al. (2016); Chen et al. (2012) showed small improvements by including features capturing the frequency of different entities such as URLs and mentions, with other features such as part-of-speech (POS) tags (Xu et al., 2012; Davidson et al., 2017) and senData 1 697 In the examples, tweet IDs are given in parenthesis. built on the following five classifiers combined by plurality voting (Kuncheva, 2004): L1-regularised Logistic Regression, L"
S19-2124,S19-2010,0,0.303497,"be difficult, due to the broad spectrum in which language can be used to convey an insult. The nature of the abuse can be implicit — drawing from sarcasm and humour rather than offensive terms — as well as explicit, by making extensive use of traditional offensive terms and profanity. It does not help that the reverse is also entertained, with profanity often being used to imply informality in speech or for emphasis. Coincidentally, these are also the reasons why lexical detection methods have been unfruitful in classifying text as offensive or non-offensive. The OffensEval 2019 shared task (Zampieri et al., 2019b) is one of several endeavours to further the state-of-the-art in addressing the offensive language problem. The paper describes the insights obtained when tackling the shared task using an ensemble of traditional machine learning classification models and a Long Short-Term Memory (LSTM) deep learning model. Section 2 first discusses other related approaches to detecting hate speech and offensive language. Then Section 3 describes the dataset and Section 4 the ideas and methodology behind our approach. Section 5 reports the results obtained, while Section 6 discusses those results with a part"
S19-2124,N12-1084,0,0.53594,"has been shown, with Waseem and Hovy (2016) claiming gender information leading to improvements in classifier performance, but with Unsv˚ag and Gamb¨ack (2018) challenging this and reporting user-network data to be more important instead. Wulczyn et al. (2017) concluded that anonymity leads to an increase in the likelihood of a comment being an attack. Classical Machine learning algorithms have been wielded to some success in automated offensive language detection, mainly Logistic Regression (Davidson et al., 2017; Waseem and Hovy, 2016; Burnap and Williams, 2015) and Support Vector Machines (Xu et al., 2012; Dadvar et al., 2013). Recently, however, deep learning models have outperformed their traditional machine learning counterparts, with both Recurrent Neural Networks (RNN) — such as LSTM (Pitsilis et al., 2018) and Bi-LSTM (Gao and Huang, 2017) — and Convolutional Neural Networks (CNN) having been used. Gamb¨ack and Sikdar (2017) utilised a CNN model with word2vec embeddings to obtain higher F1 -score and precision than a previous logistic regression model (Waseem and Hovy, 2016), while Zhang et al. (2018) combined a CNN model with a Gated Recurrent Unit (GRU) layer. Malmasi and Zampieri (201"
S19-2124,N19-1144,0,0.294066,"be difficult, due to the broad spectrum in which language can be used to convey an insult. The nature of the abuse can be implicit — drawing from sarcasm and humour rather than offensive terms — as well as explicit, by making extensive use of traditional offensive terms and profanity. It does not help that the reverse is also entertained, with profanity often being used to imply informality in speech or for emphasis. Coincidentally, these are also the reasons why lexical detection methods have been unfruitful in classifying text as offensive or non-offensive. The OffensEval 2019 shared task (Zampieri et al., 2019b) is one of several endeavours to further the state-of-the-art in addressing the offensive language problem. The paper describes the insights obtained when tackling the shared task using an ensemble of traditional machine learning classification models and a Long Short-Term Memory (LSTM) deep learning model. Section 2 first discusses other related approaches to detecting hate speech and offensive language. Then Section 3 describes the dataset and Section 4 the ideas and methodology behind our approach. Section 5 reports the results obtained, while Section 6 discusses those results with a part"
W09-3517,P06-2025,1,0.893972,"Missing"
W09-3517,2003.mtsummit-papers.17,0,0.504975,"ering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard run and two nonstan"
W09-3517,C00-1056,0,0.396199,"Missing"
W09-3517,P04-1021,0,0.304488,"ji Bandyopadhyay Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009"
W09-3517,2005.mtsummit-papers.36,0,0.226978,"Missing"
W09-3517,I08-1009,0,0.162852,"Missing"
W09-3517,W03-1508,0,0.0702944,"Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard"
W09-3517,P02-1051,0,0.119126,"Missing"
W09-3517,W09-3501,0,\N,Missing
W09-3517,W09-3502,0,\N,Missing
W09-3517,J98-4003,0,\N,Missing
W10-2411,I08-1009,0,0.0692551,"Missing"
W10-2411,W09-3517,1,0.31321,"ithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil Non-Standard Run-1: TNSR1). Abstract This paper reports about our work in the NEWS 2010 Shared Task on Transliteration Generation held"
W10-2411,W03-1508,0,0.211651,"niversity of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hind"
W10-2411,P02-1051,0,0.0210547,"e alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliterati"
W10-2411,P06-2025,1,0.798718,"uage Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tam"
W10-2411,2003.mtsummit-papers.17,0,0.0288078,"Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Ru"
W10-2411,C00-1056,0,0.0909754,"Missing"
W10-2411,P04-1021,0,0.131127,"al Linguistics4 University of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Benga"
W10-2411,2005.mtsummit-papers.36,0,0.0629612,"Missing"
W10-2411,W10-2402,0,\N,Missing
W10-2411,P97-1017,0,\N,Missing
W10-3208,S07-1022,0,0.0273142,"Missing"
W10-3208,W02-1011,0,0.0157695,"Missing"
W10-3208,P05-2008,0,0.141591,"Missing"
W10-3208,esuli-sebastiani-2006-sentiwordnet,0,0.801056,"Missing"
W10-3208,P06-1134,0,0.0246901,"Missing"
W10-3208,C00-1044,0,0.0281407,"Missing"
W10-3208,H05-1044,0,0.0578517,"Missing"
W10-3208,P07-1123,0,0.0326869,"Missing"
W10-3208,D08-1103,0,\N,Missing
W10-3208,P06-1034,0,\N,Missing
W10-3402,kipper-etal-2006-extending,0,0.0658009,"Missing"
W10-3402,J05-1004,0,0.061272,"Missing"
W10-3603,J95-3006,0,0.550791,"Missing"
W10-3603,W01-0708,0,0.0406621,"Missing"
W12-3707,W11-0311,0,0.0386498,"Missing"
W12-3707,W10-3208,1,0.797918,"Missing"
W12-3707,D09-1061,0,0.0365449,"Missing"
W12-3707,esuli-sebastiani-2006-sentiwordnet,0,0.294433,"Missing"
W12-3707,W02-1011,0,0.016747,"Missing"
W12-3707,P05-1015,0,0.0873971,"Missing"
W12-3707,C10-2146,0,0.0426259,"Missing"
W12-3707,W11-1710,0,0.0258363,"Missing"
W12-3707,P02-1053,0,0.0153326,"Missing"
W12-3707,J06-3003,0,0.0505959,"Missing"
W12-3707,W10-4116,0,\N,Missing
W12-3707,C04-1200,0,\N,Missing
W12-3707,W09-3539,0,\N,Missing
W12-3707,P97-1023,0,\N,Missing
W12-3707,W10-3402,1,\N,Missing
W12-3707,W10-3209,0,\N,Missing
W12-3707,P05-1017,0,\N,Missing
W14-3902,W12-2108,0,0.0277643,"ndom Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration. 1 Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data itIntroduction Automatic processing and understanding of Social Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Althoug"
W14-3902,dey-fung-2014-hindi,0,0.0842282,"Missing"
W14-3902,hughes-etal-2006-reconsidering,0,0.0200292,"and sequence labelling using Conditional Random Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration. 1 Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data itIntroduction Automatic processing and understanding of Social Media Content (SMC) is currently attracting much attention from the Natural L"
W14-3902,C82-1023,0,0.591308,"Missing"
W14-3902,N13-1131,0,0.030764,"4 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turki"
W14-3902,D12-1039,0,0.0264776,"Missing"
W14-3902,P08-1099,0,0.0156156,"004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turkish and Dutch forum data. Experiments have been carried out using language models, dictionaries, logistic regression classification and Conditional Random Fields. They find that language models are more robust than dictionaries and that contextual information is helpful for the task. self and the annotation process; in Section 4, we list the tools and resources which we use in our language iden"
W14-3902,D08-1102,0,0.0504151,"ocial Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language ident"
W14-3902,D08-1110,0,0.39477,"ocial Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language ident"
W14-3902,S13-2052,0,0.0212039,"Missing"
W14-3902,W14-3907,0,0.181932,"Missing"
W14-3902,D13-1084,0,0.127537,"Missing"
W14-3902,S14-2036,1,0.830181,"Missing"
W14-3902,N13-1039,0,0.0250362,"Missing"
W14-3902,P12-1102,0,0.0127428,"opular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved"
W14-3902,W13-2249,1,0.82795,"Missing"
W14-3902,N10-1027,0,\N,Missing
W14-5152,N10-1027,0,0.0481886,"Missing"
W14-5152,W14-3902,1,0.358818,"on-wide communication. Language diversity and dialect changes instigate frequent codemixing in India. Hence, Indians are multi-lingual by adaptation and necessity, and frequently change and mix languages in social media contexts. 3.1 Data Acquisition English-Hindi and English-Bengali language mixing were selected for the present study. These language combinations were chosen as Hindi and Bengali are the two largest languages in India in terms of first-language speakers (and 4th and 7th worldwide). In our study, we include corpora collected both by ourselves for this study and by Utsab Barman (Burman et al., 2014), hereforth called EN-BN1 and EN-HN1 resp. EN-BN2 and EN-HN2. Various campus Facebook groups Tag Description en English word bn hi ne ne+en_suffix ne+bn_suffix ne+hi_suffix univ Bengali word Hindi word Named Entity (NE) NE + English suffix NE + Bengali suffix NE + Hindi suffix Universal Tag en+bn_suffix en+hi_suffix bn+en_suffix hi+en_suffix acro acro+en_suffix acro+bn_suffix acro+hi_suffix undef Description English word + Bengali suffix English word + Hindi suffix Bengali word + English suffix Hindi word + English suffix Acronym Acronym + English suffix Acronym + Bengali suffix Acronym + Hind"
W14-5152,O09-5003,0,0.110054,"Missing"
W14-5152,D12-1039,0,0.0229006,"creases performance (by 0.5). 4.2 Dictionary-Based Detection Use of most-frequent-word dictionaries is another established method in language identificaˇ uˇrek and Kolkus, 2009). We tion (Alex, 2008; Reh˚ incorporated a dictionary-based language detection technique for the present task, but were faced with some challenges for the dictionary preparation, in particular since social media text is full of noise. A fully edited electronic dictionary may not have all such distorted word forms as are used in these texts (e.g., ‘gr8’ rather than ‘great’). Therefore a lexical normalisation dictionary (Han et al., 2012) prepared for Twitter was used for English. Unfortunately, no such dictionary is available for Hindi or Bengali, so we used the Samsad English-Bengali dictionary (Bi´sv¯as, 2000). The Bengali part of the Samsad dictionary is written in Unicode, but in our corpus the Bengali texts are written in transliterated/phonetic (Romanized) form. Therefore the Bengali lexicon was transliterated into Romanized text using the ModifiedJoint-Source-Channel model (Das et al., 2010). The same approach was taken when creating the Hindi dictionary, using Hindi WordNet (Narayan et al., 2002). In order to capture"
W14-5152,C82-1023,0,0.49505,"Missing"
W14-5152,N13-1131,0,0.0553511,"Missing"
W14-5152,W14-1303,0,0.0263079,"Missing"
W14-5152,Q14-1003,0,0.0667828,"Missing"
W14-5152,D13-1084,0,0.0822776,"Missing"
W14-5152,P14-2110,0,0.0397074,"Missing"
W14-5152,D08-1110,0,0.399909,"Missing"
W14-5152,voss-etal-2014-finding,0,0.0475633,"Missing"
W14-5152,P12-1102,0,0.0604059,"Missing"
W14-5152,W10-2411,1,\N,Missing
W15-5938,P96-1025,0,0.372296,"texts, sentence boundary detection has been considered a more or less solved problem since the 1990s, but the proliferation of social media has added new 254 challenges to language processing and new difficulties for SBD, with state-of-the-art systems failing to perform well on social media, due to the coarse nature of the texts. In spite of its important role for language processing, sentence boundary detection has so far not received enough attention. Previous research in the area has been confined to formal texts only, and either has not addressed the process of SBD directly (Brill, 1994; Collins, 1996), or not the performance related issues of sentence boundary detection (Cutting et al., 1992). In particular, no SBD research to date has addressed the problem in informal texts such as Twitter and Facebook posts. The growth of social media is a global phenomenon where people are communicating both using single languages and using mixes of several languages. The social media texts are informal in nature, and posts on Twitter and Facebook tend to be full of misspelled words, show extensive use of home-made acronyms and abbreviations, and contain plenty of punctuation applied in creative and non"
W15-5938,A92-1018,0,0.721034,"since the 1990s, but the proliferation of social media has added new 254 challenges to language processing and new difficulties for SBD, with state-of-the-art systems failing to perform well on social media, due to the coarse nature of the texts. In spite of its important role for language processing, sentence boundary detection has so far not received enough attention. Previous research in the area has been confined to formal texts only, and either has not addressed the process of SBD directly (Brill, 1994; Collins, 1996), or not the performance related issues of sentence boundary detection (Cutting et al., 1992). In particular, no SBD research to date has addressed the problem in informal texts such as Twitter and Facebook posts. The growth of social media is a global phenomenon where people are communicating both using single languages and using mixes of several languages. The social media texts are informal in nature, and posts on Twitter and Facebook tend to be full of misspelled words, show extensive use of home-made acronyms and abbreviations, and contain plenty of punctuation applied in creative and non-standard ways. The punctuation markers are also often ambiguous in these types of texts — in"
W15-5938,N09-2061,0,0.908979,"Missing"
W15-5938,P11-2008,0,0.0643046,"Missing"
W15-5938,R15-1033,1,0.863576,"Missing"
W15-5938,J06-4003,0,0.820996,"Missing"
W15-5938,J02-3002,0,0.438312,"Missing"
W15-5938,J97-2002,0,0.692022,"Missing"
W15-5938,A97-1004,0,0.534917,"Missing"
W15-5938,H89-2048,0,0.875076,"Missing"
W16-6322,baccianella-etal-2010-sentiwordnet,0,0.124312,"Missing"
W16-6322,P11-2008,0,0.0214171,"Missing"
W16-6322,D14-1214,0,0.0659469,"Missing"
W16-6322,strapparava-valitutti-2004-wordnet,0,0.333743,"Missing"
W17-7532,W93-0231,0,0.765314,"e of ten basic Values based on people’s motivation. The Schwartz model was proved to be very successful in psychological research as well as in other fields. The ten basic Values are related to various outcomes and effects of a person’s role in a society [Argando˜na2003]. The Values have also proved to provide an important and powerful explanation of consumer behaviour and how they influence it [Kahle et al.1986]. In the recent years, there have been few initiatives to automatically identify various Big5 Personality traits of individuals from their language usage and behaviour in social media [Goldberg1993]. A milestone in this area was the 2013 Workshop and Shared Task on Computational Personality Recognition1 , repeated in 20142 . Further research work in the area of developing computational model for identifying Personality from language usage on Facebook and Twitter has been done in [Park et al.2015] and [Quercia et al.2011] respectively. However, no computational model for Schwartz’ Values has been tested or examined before. 2 3 Related Work There has been a little research in the field of computational models to predict the degree of optimism. [Ruan et al.2016] developed the first computat"
W17-7532,S13-2053,0,0.0622441,"Missing"
W17-7532,P16-2052,0,0.125118,"Missing"
W17-7552,baccianella-etal-2010-sentiwordnet,0,0.0246873,"Missing"
W17-7552,S13-2053,0,0.0843018,"Missing"
Y10-1092,S07-1022,0,0.0669519,"Missing"
Y10-1092,W10-3208,1,0.154305,"Missing"
Y10-1092,esuli-sebastiani-2006-sentiwordnet,0,0.249077,"Missing"
Y10-1092,P07-1123,0,0.0472979,"Missing"
Y10-1092,D08-1103,0,0.0250758,"Missing"
Y10-1092,W02-1011,0,0.0114942,"Missing"
Y10-1092,P05-2008,0,0.0868094,"Missing"
Y10-1092,strapparava-valitutti-2004-wordnet,0,0.452938,"Missing"
Y10-1092,P06-1134,0,0.0695823,"Missing"
Y10-1092,H05-1044,0,0.0270228,"Missing"
