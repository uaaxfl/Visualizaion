2020.acl-main.159,P19-1376,1,0.740346,"result from overapplication of the regular past tense (e.g. throw– throwed)—a type of error observed in human language learners as well—as opposed to the unattested forms produced by Rumelhart and McClelland’s model. K&C conclude that modern neural networks can learn human-like behavior for English past tense without recourse to explicit symbolic structure, and invite researchers to move beyond the ‘rules’ debate, asking instead whether the learner correctly generalizes to a range of novel inputs, and whether its errors (and other behavior) are human-like. This challenge was first taken up by Corkery et al. (2019), who showed that, on novel English-like words designed to elicit some irregular generalizations from humans, the ED model’s predictions do not closely match the human data. While these results suggest possible problems with the ED model, English may not be the best test case to fully understand these, since the sole regular inflectional class is also by far the most frequent. In contrast, many languages have multiple inflectional classes which can act ‘regular’ under various conditions (Seidenberg and Plaut, 2014; Clahsen, 2016). In this paper, we examine German number inflection, which has b"
2020.acl-main.159,P16-2090,0,0.146389,"Missing"
2020.acl-main.159,L16-1498,0,0.0255326,"Missing"
2020.acl-main.159,W18-1817,0,0.0582117,"Missing"
2020.acl-main.159,D19-1331,0,0.0590498,"Missing"
2020.acl-main.159,P15-2111,0,0.020628,"Missing"
2020.cmcl-1.8,P19-1376,1,0.883714,"f. Marcus et al. 1995). Based on the artificial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Fi"
2020.cmcl-1.8,2020.acl-main.159,1,0.904286,"ficial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Figure 1 (lower), resulting in a majority us"
2020.cmcl-1.8,P16-2090,0,0.0632739,"Missing"
2020.cmcl-1.8,2020.acl-main.597,0,0.0579472,"Missing"
2020.cmcl-1.8,2020.scil-1.58,0,0.026412,". Based on the artificial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Figure 1 (lower), res"
2020.emnlp-main.642,N18-1007,0,0.0417865,"Missing"
2020.emnlp-main.642,N09-2021,0,0.0327803,"achine learning models (Wightman and Ostendorf, 1994; Levow, 2005; Gregory and Altun, 2004), and neural models (Fernandez et al., 2017; Stehwien and Vu, 2017; Stehwien et al., 2018). Stehwien and Vu (2017) and Stehwien et al. (2018) (henceforth, SVS18) showed that neural methods can perform comparably to traditional methods using a relatively small amount of speech context— just a single word on either side of the target word. However, since pitch accents are deviations from a speaker’s average pitch, intensity, and duration, we hypothesize that, as in some non-neural models (e.g. Levow 2005; Rosenberg and Hirschberg 2009), a wider input context will allow the model to better determine the speaker’s baseline for these features and therefore improve its ability to detect deviations. In addition, we hypothesize that a recurrent model (rather than the CNN used by SVS18) will also improve performance, since it is better adapted to processing long-distance dependencies. In this paper, we test these hypotheses by building a new neural pitch accent prediction model that takes in prosodic speech features, text features, or both. Our main contribution is showing that these context-enhancing innovations in the speechonly"
2020.emnlp-main.642,P04-1086,0,\N,Missing
2020.emnlp-main.642,D14-1162,0,\N,Missing
2021.acl-long.79,H89-2017,0,0.537777,"ackets is called the reparandum, which is immediately followed by the interruption point. Disfluencies in SWBD-NXT are marked in the constituency parse annotation, where the reparandum is marked as a constituent with the label EDITED. The interruption point is the right edge of this constituent. Our analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989). Pauses. Although pauses may be the most intuitive potential cue to SU boundaries, previous work suggests that long pauses also characterize interruption points (Wagner and Watson, 2010; Shriberg, 2001). Indeed, our analysis shows that longer pauses (&gt; 0.05s) are over-represented in both locations. If pause types were distributed uniformly, 16 percent of both SU boundaries and interruption The second duration feature is the token length normalized by the maximum length of any token in the input, to normalize for speaking rate. Initially, this feature looks helpful: SU-final words have mean va"
2021.acl-long.79,N13-1023,0,0.0328193,"Roark et al. (2006)). However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020). D (ASR) transcript; we plan to use ASR output in future work. We build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different experimental conditions for each model: inputting text features only and inputting both text and prosodic features. Using the Switchboard corpus of English conversational dialogue, we find that when only transcripts are used, the turn-based parser performs considerably worse than the SU-based parser, which is not surprising given that it needs to perform two tasks instead of one. However, when prosodic features are inclu"
2021.acl-long.79,D12-1096,0,0.30946,"sody boosts SU segmentation accuracy to near-perfect levels, which explains why the parser performance is similar (and much better than without prosody). Comparing the two text-only models reveals a more interesting pattern: while the pipeline model achieves much better segmentation F1, its parsing performance is worse. This is unexpected, as parsing and segmentation performance are usually correlated. This effect seems to arise because the two models err in different directions on segmentation: The pipeline model under-segments turns (corre5.1 Error types We use the Berkeley Parser Analyser (Kummerfeld et al., 2012) to determine what types of errors each of the SU-based and end-to-end turn-based models makes. Figure 2 summarizes the output of the Analyser. Overall, the SU-based parser shows only small effects from prosody, but the turn-based model does significantly worse on certain error types without prosody. Even for the turn-based model, prosody only affects error types that have to do with the shape of the tree. The different label category shows errors where two identically shaped trees have different constituent labels, and prosody has no effect on these. For the turn-based model, poor SU segmenta"
2021.acl-long.79,D14-1162,0,0.0839201,"Missing"
2021.acl-long.79,roark-etal-2006-sparseval,0,0.104552,"e important include Gotoh and Renals (2000); Kol´aˇr et al. (2006); Kahn et al. (2004); Kahn and Ostendorf (2012), who all used traditional statistical models (e.g., HMMs, finite state machines, and decision trees), and Xu et al. (2014), who used a neural model. Kahn et al. (2004) and Kahn and Ostendorf (2012) also looked at downstream parsing accuracy on the same corpus we use. Like us, Kahn and Ostendorf (2012) don’t use gold SU boundaries, but direct comparison is impossible because they use ASR output instead of human transcriptions and a different metric for parse performance (SParseval; Roark et al. (2006)). However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013"
2021.acl-long.79,P17-1076,0,0.0207485,"om seeds. For the test set, we use the model that has the median dev. set performance out of 10 randomly seeded models. D Text only Text+pros. ∆ Input length (# tokens) 1 2–8 9–22 23–255 98.36 93.00 89.22 84.30 99.18 94.91 92.74 89.80 0.82 1.91 3.52 5.5 Table 2: F1 performance of the text-only and text+prosody turn-based models on inputs of various lengths in the development set. The inputs are divided into bins of approximately equal size by token length. RE TR AC TE parser based on Kitaev and Klein (2018)’s textonly parser, with a transformer-based encoder and a chart-style decoder based on Stern et al. (2017) and Gaddy et al. (2018). This encoder-decoder is augmented with a CNN on the input side that handles prosodic features (Tran et al., 2019). For further description of the model and hyperparameters, see Appendices A.1 and A.2. The text is encoded using 300-dimensional GloVe embeddings (Pennington et al., 2014).4 Of the four types of prosodic features described in Section 3, pause and duration features are already token-level. However, pitch and intensity features are extracted from the speech signal at the frame level. In order to map from these frame-level features to a token-level representa"
2021.acl-long.79,N18-1007,0,0.0523023,"Missing"
2021.acl-long.79,2020.clssts-1.11,0,0.0341009,"However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020). D (ASR) transcript; we plan to use ASR output in future work. We build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different experimental conditions for each model: inputting text features only and inputting both text and prosodic features. Using the Switchboard corpus of English conversational dialogue, we find that when only transcripts are used, the turn-based parser performs considerably worse than the SU-based parser, which is not surprising given that it needs to perform two tasks instead of one. However, when prosodic features are included, there is no di"
2021.eacl-main.127,K17-1037,0,0.0163091,"esult does not tell us whether the phonetic or the phonolexical explanation is more parsimonious—a question that should be addressed in the future. One could interpret our main result differently: that our model, in fact, has succeeded in learning phonological systems from speech data and cannot be considered a purely phonetic model. Indeed, we know that deep neural networks can learn to encode various types of linguistic structure without explicit supervision (e.g., Manning et al., 2020; Linzen and Baroni, 2021). In particular, speech models can achieve high accuracy in phone discrimination (Alishahi et al., 2017) and classification (Chung et al., 2019), a finding sometimes interpreted as a successful acquisition of phonetic/phonological categories. While our model can discriminate at least some phone contrasts, too (Simulation 1), this does not necessarily mean that it learns phonetic categories (see Schatz et al., 2019, for a relevant discussion). More importantly, what our model does not do is store explicit phonolexical representations in its memory, whereas the (imprecise) storage of word forms is one of the key premises of the phonolexical account explaining non-native speakers’ difficulties in s"
2021.eacl-main.127,H92-1073,0,0.1419,"s one instance of a word (represented as a speech sequence), encodes it into a vector of a fixed dimensionality (an acoustic embedding), and then tries to reconstruct the other instance in the pair, as shown in Figure 1. Formally, each training item is a pair of acoustic words (X, X 0 ). Each word is represented as a sequence of vectors: X = (~x1 , . . . , ~xT ) and X 0 = (~x01 , . . . , ~x0T 0 ). The loss for a single training item is: 1 EN 2 ZH 3 RU 1 2 3 4 5 B. Test data. WSJ 9:39 Buckeye 9:01 AIShell 58:45 GlobalPhone 11:51 GlobalPhone 11:01 47 20 165 48 57 Wall Street Journal CSR corpus (Paul and Baker, 1992). Multilingual text and speech database (Schultz, 2002). Buckeye corpus of conversational speech (Pitt et al., 2005). Corpus of spontaneous Japanese (Maekawa, 2003). Open-source Mandarin speech corpus (Bu et al., 2017). Table 1: Corpus samples used in the simulations. 0 0 `(X, X ) = T X ||~x0t − f~t (X)||2 (1) t=1 where X is the input and X 0 the target output sequence, and ft (X) is the tth decoder output conditioned on the embedding z. At inference time, we can encode a sequence of arbitrary duration (e.g., a phone or a word) into a fixed-dimensional acoustic embedding in the model’s represe"
2021.insights-1.11,D15-1166,0,0.0366796,"e model, followed by postprocessing to convert the map into an alignment. 3.1 p(y1:K |x1:T ) = p(y1 |x1:T ) K Y p(yk |y1:k−1 , x1:T ). k=2 We emphasize that, in our setting, x and y are always given, also known as teacher forcing (Lamb et al., 2016), and we are interested in the attention map α, not how well the model maps x to y. Note that the assignments to x and y can be swapped since both are given. For example, we can align word transcriptions to phonetic transcriptions, or Sequence-to-Sequence Models Below is a review of sequence-to-sequence models. Readers should refer to, for example, Luong et al. (2015) for a detailed exposition. 3 We do report generalization results in the Appendix, for completeness, though these do not change our main story. 68 Ptk t=sk αt,k , is maximized, while respecting the connectedness constraint, i.e., sk+1 = tk + 1. This can be achieved by finding the maximum weighted path in a graph with edges as word segments and weights of the edges as the attention weights a segment covers. See a detailed description in Appendix A.1 and (Tang et al., 2017). vice versa. However, the choice of directionality has two implications. First, for each output yk , some parts of x will h"
2021.sigmorphon-1.9,Q17-1010,0,0.483342,"erivational morphology is an intriguing and contested issue within linguistics (e.g. Stump, 2005), and the question of how to model it computationally requires much more attention. 4.2 subword length of 2 characters, and used it to cluster words from the same cell rather than the same paradigm (e.g. clustering together English verbs in the third person singular such as “walks” and “jumps”). We attempted to follow this procedure, but it proved too difficult, as paradigm cell information was not explicitly included in the development data for this shared task. 3) We used the method described by Bojanowski et al. (2017) to identify important subwords within a word, in hopes of combining them with AG segmentations. However, the identified subwords did not consistently align with stem-adfix segementations as we had hoped, and did not seem to provide any additional benefit. Brown clustering Part of speech tags could provide latent structure as a higher-order grouping for paradigm clusters — for example, verbs would be expected to have paradigms more similar to other verbs than to nouns. Brown clusters (Brown et al., 1992) have been used for unsupervised induction of word classes approximating part of speech tag"
2021.sigmorphon-1.9,D13-1034,0,0.0478443,"Missing"
2021.sigmorphon-1.9,W13-2603,0,0.0730678,"Missing"
2021.sigmorphon-1.9,J92-4003,0,0.649613,"uded in the development data for this shared task. 3) We used the method described by Bojanowski et al. (2017) to identify important subwords within a word, in hopes of combining them with AG segmentations. However, the identified subwords did not consistently align with stem-adfix segementations as we had hoped, and did not seem to provide any additional benefit. Brown clustering Part of speech tags could provide latent structure as a higher-order grouping for paradigm clusters — for example, verbs would be expected to have paradigms more similar to other verbs than to nouns. Brown clusters (Brown et al., 1992) have been used for unsupervised induction of word classes approximating part of speech tags. We used a spectral clustering algorithm (Stratos et al., 2014) to learn Brown clusters, but they did not reliably correspond to part of speech categories on our development language data. Things that didn’t work 5 We attempted a number of unsupervised approaches beyond AG segmentations, with the goal of incorporating them during the clustering process; however, we could not consistently improve performance with any of them. It seems likely to us that these methods could still be used to improve AG-seg"
2021.sigmorphon-1.9,K18-3001,0,0.140003,"Missing"
2021.sigmorphon-1.9,2020.acl-main.598,0,0.0770183,"particular subtree σ is roughly proportional to the number of times X has previously expanded to σ. This leads to a “rich-get-richer” effect as more Introduction While the task of supervised morphological inflection has seen dramatic gains in accuracy over recent years (e.g. Cotterell et al., 2016, 2017, 2018; Vylomova et al., 2020), unsupervised morphological analysis remains an open challenge. This is evident in the results of the 2020 SIGMORPHON Shared Task 2 on Unsupervised Morphological Paradigm Completion, in which no submission consistently outperformed the baseline (Kann et al., 2020; Jin et al., 2020). The 2021 Shared Task 2 (Wiemerslage et al., 2021) focuses on a subproblem from the 2020 task: given raw text input, cluster tokens together based on membership in the same morphological paradigm. For example, given the sentence “My dog met some other dogs”, a successful system would assign “dog” and “dogs” to the same paradigm because they are two inflected forms of the same lemma “dog”, while each other word would occupy its own cluster. Furthermore, a successful system needs to cluster typologically diverse, morphologically rich languages such as Finnish and Navajo, with inflectional parad"
2021.sigmorphon-1.9,N09-1036,1,0.708419,"nd each grammar, resulting in 6 segmentations for each word. We then use frequency-based metrics frequently sampled subtrees gain higher probability over the segmentations to identify the language’s within the conditional adapted distribution. Given adfix direction, i.e. whether it is predominantly an AG specification, MCMC sampling can be used prefixing or suffixing, as described in Section 3.3. to infer values for the PCFG rule probabilities θ Finally, we iterate over the entire vocabulary and (Johnson et al., 2007a) and PYP hyperparameters apply frequency-based scores to generate paradigm (Johnson and Goldwater, 2009). clusters, as described in Section 3.4 . 2.2 Word walked jumping walking jump Segmentation walk-ed jump-ing walk-ing jump AGs for Morphological Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, resear"
2021.sigmorphon-1.9,N07-1018,1,0.769271,"the related task of unsupervised morphological segmentation (Eskander et al., 2020). This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions. 1 Adaptor Grammars 2.1 Model Adaptor Grammars (AGs; Johnson et al., 2007b) are a class of nonparametric Bayesian probabilistic models which learn structured representations, or parses, of natural language input strings. An AG has two components: a Probabilistic Context-Free Grammar (PCFG) and one or more adaptors. The PCFG is a 5-tuple (N, W, R, S, θ) which specifies a base distribution over parse trees. Parse trees are generated top-down by expanding non-terminals N (including the start symbol S ∈ N ) to nonterminals N (excluding S) and terminals W , using the set of allowed expansion rules R with expansion probability θr for each rule r ∈ R. PCFGs have very stro"
2021.sigmorphon-1.9,2020.sigmorphon-1.3,0,0.152829,"of Cx returning a particular subtree σ is roughly proportional to the number of times X has previously expanded to σ. This leads to a “rich-get-richer” effect as more Introduction While the task of supervised morphological inflection has seen dramatic gains in accuracy over recent years (e.g. Cotterell et al., 2016, 2017, 2018; Vylomova et al., 2020), unsupervised morphological analysis remains an open challenge. This is evident in the results of the 2020 SIGMORPHON Shared Task 2 on Unsupervised Morphological Paradigm Completion, in which no submission consistently outperformed the baseline (Kann et al., 2020; Jin et al., 2020). The 2021 Shared Task 2 (Wiemerslage et al., 2021) focuses on a subproblem from the 2020 task: given raw text input, cluster tokens together based on membership in the same morphological paradigm. For example, given the sentence “My dog met some other dogs”, a successful system would assign “dog” and “dogs” to the same paradigm because they are two inflected forms of the same lemma “dog”, while each other word would occupy its own cluster. Furthermore, a successful system needs to cluster typologically diverse, morphologically rich languages such as Finnish and Navajo, with"
2021.sigmorphon-1.9,2020.lrec-1.879,0,0.32211,"l Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, researchers have successfully applied AGs to the task of unsupervised morphological segmentation (Sirts and Goldwater, 2013; Eskander et al., 2016). Eskander et al. (2020) found that a languageindependent AG framework achieved state-of-theart results on 12 typologically distinct languages. 3 3.1 Grammar selection An adaptor grammar builds upon an initial PCFG specification, and many such grammars can be applied to model word structure. As a first step, we evaluate various grammar specifications on the development languages and select the grammars for our final model. To train the adaptor grammar representations, we use MorphAGram (Eskander et al., 2020), a framework which extends the adaptor grammar implementation of Johnson et al. (2007b). Eskander et al. (202"
2021.sigmorphon-1.9,Q13-1021,1,0.895391,"walk-ed jump-ing walk-ing jump AGs for Morphological Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, researchers have successfully applied AGs to the task of unsupervised morphological segmentation (Sirts and Goldwater, 2013; Eskander et al., 2016). Eskander et al. (2020) found that a languageindependent AG framework achieved state-of-theart results on 12 typologically distinct languages. 3 3.1 Grammar selection An adaptor grammar builds upon an initial PCFG specification, and many such grammars can be applied to model word structure. As a first step, we evaluate various grammar specifications on the development languages and select the grammars for our final model. To train the adaptor grammar representations, we use MorphAGram (Eskander et al., 2020), a framework which extends the adaptor grammar implementation"
C00-2097,A97-1001,0,0.113553,"ation becomes increasingly burdensome. The grammar tends to become large and unwieldy, with many rules appearing in multiple versions that constantly need to be kept in step with each other. It represents a large development cost, is hard to maintain, and does not usually port well to new applications. It is tempting to consider the option of moving towards a more expressive grammar formalism, like uni cation grammar, writing the original grammar in uni cation grammar form and compiling it down to the context-free notation required by the underlying toolkit. At least one such system (Gemini; (Moore et al 1997)) has been implemented and used to build successful and non-trivial applications, most notably CommandTalk (Stent et al 1999). Gemini accepts a slightly constrained version of the uni cation grammar formalism originally used in the Core Language Engine (Alshawi 1992), and compiles it into context-free grammars in the GSL formalism supported by the Nuance Toolkit. The Nuance Toolkit compiles GSL grammars into sets of probabilistic nite state graphs (PFSGs), which form the nal language model. The relative success of the Gemini system suggests a new question. Uni cation grammars have been used ma"
C00-2097,P99-1024,1,\N,Missing
D10-1056,N10-1083,0,0.565401,"Missing"
D10-1056,P06-3002,0,0.106796,"this improves the corpus probability. [clark]: Class-based n-grams with morphology (Clark, 2003). This system uses a similar model to the previous one, and also clusters word types (rather than tokens, as the rest of the systems do). The main differences between the systems are that clark uses a slightly different approximate search procedure, and that he augments the probabilistic model with a prior that prefers clusterings where morphologically similar words are clustered together. The morphology component is implemented as a single-order letter HMM. [cw]: Chinese Whispers graph clustering (Biemann, 2006). Unlike the other systems we consider, this one induces the value of |C |rather than taking it as an input parameter.2 The system uses a graph clustering algorithm called Chinese Whispers that is based on contextual similarity. The algorithm works in two stages. The first clusters the most frequent 10,000 words (target words) based on their context statistics, with contexts formed from the most frequent 150-250 words (feature words) that appear ei1 Implementations were obtained from: brown: http://www.cs.berkeley.edu/∼pliang/ software/brown-cluster-1.2.zip (Percy Liang), clark: http://www.cs."
D10-1056,J92-4003,0,0.676257,"in many cases, being more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driv"
D10-1056,E03-1009,0,0.810921,"g more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning sy"
D10-1056,erjavec-2004-multext,0,0.0126186,"Section 2. We first present results for the same WSJ corpus used above. However, because most of the systems were initially developed on this corpus, and often evaluated only on it, there is a question of whether their methods and/or hyperparameters are overly specific to the domain or to the English language. This is a particularly pertinent question since a primary argument in favor of unsupervised systems is that they are easier to port to a new language or domain than supervised systems. To address this question, we evaluate all the systems as well on the multilingual Multext East corpus (Erjavec, 2004), without changing any of the parameter settings. |C| was set to 45 for all of the experiments reported in this section. Based on our assessment of evaluation 580 Figure 2: Performance of the different systems on WSJ, using three different measures [|C|:45, |T |:45] system brown clark cw bhmm vbhmm pr feat runtime e10 min. e40 min. e10 min. e4 hrs. e10 hrs. e10 hrs.* e40 hrs.* Table 2: Runtimes for the different systems on WSJ [|C|:45]. *pr and feat have multithreading implementations and ran on 16 cores. measures above, we report VM scores as the most reliable measure across different systems"
D10-1056,D08-1036,0,0.0869431,"one mapping accuracy (also known as cluster purity) maps each cluster to the gold standard tag that is most common for the words in that cluster (henceforth, the preferred tag), and then computes the proportion of words tagged correctly. More than one cluster may be mapped to the same gold standard tag. This is the most commonly used metric across the literature as it is intuitive and creates a meaningful POS sequence out of the cluster identifiers. However, it tends to yield higher scores as |C |increases, making comparisons difficult when |C |can vary. [crossval]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. In this measure, the first half of the corpus is used to obtain the many-to-one mapping of clusters to tags, and this mapping is used to compute the accuracy of the clustering on the second half of the corpus. [1-to-1]: One-to-one mapping accuracy (Haghighi and Klein, 2006) constrains the mapping from clusters to tags, so that at most one cluster can be mapped to any tag. The mapping is performed greedily. In general, as the number of clusters increases, fewe"
D10-1056,P07-1094,1,0.932131,"based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead u"
D10-1056,N06-1041,0,0.297038,"cludes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make sinc"
D10-1056,D07-1031,0,0.703065,"stos.c@ed.ac.uk sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know ho"
D10-1056,J93-2004,0,0.0413613,"ard tagging to compute. This is normally available during development of a system, but if the system is deployed on a novel language a gold standard may not be available. 578 We mentioned a few strengths and weaknesses of each evaluation method above; in this section we present some empirical results to expand on these claims. First, we examine the effects of varying |C| on the behavior of the evaluation measures, while keeping the number of gold standard tags the same (|T |= 45). Results were obtained by training and evaluating each system on the full WSJ portion of the Penn Treebank corpus (Marcus et al., 1993). Figure 1 shows the results from the Brown system for |C |ranging from 1 to 200; the same trends were observed for all other systems.3 In addition, Table 1 provides results for the two extremes of |C |= 1 (all words assigned to the same cluster) and |C |equal to the size of the corpus (a single word per cluster), as 3 The results reported in this paper are only a fraction of the total from our experiments; given the number of parameters, models and measures tested, we obtained over 15000 results. The full set of results can be found at http://homepages.inf.ed.ac.uk/s0787820/pos/. Figure 1: Sc"
D10-1056,J94-2001,0,0.620472,"h University of Edinburgh christos.c@ed.ac.uk sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight"
D10-1056,P09-1057,0,0.25444,"steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-s"
D10-1056,D07-1043,0,0.128472,"l entropy of each clustering conditioned on the other. More formally, V I(C, T ) = H(T |C) + H(C|T ) = H(C) + H(T ) − 2I(C, T ), where H(.) is the entropy function and I(.) is the mutual information. VI and other entropy-based measures have been argued to be superior to accuracy-based measures such as those above, because they consider not only the majority tag in each cluster, but also whether the remainder of the cluster is more or less homogeneous. Unlike the other measures we consider, lower scores are better (since VI measures the difference between clusterings in bits). [vm]: V-Measure (Rosenberg and Hirschberg, 2007) is another entropy-based measure that is designed to be analogous to F-measure, in that it is defined as the weighted harmonic mean of two values, homogeneity (h, the precision analogue) and completeness (c, the recall analogue): H(T |C) H(T ) H(C|T ) c = 1− H(C) (1 + β)hc VM = (βh) + c h = 1− In addition, there is the question of whether the gold standard itself is “correct”. Recently, Frank et al. (2009) proposed this novel evaluation measure that requires no gold standard, instead using the concept of substitutability to evaluate performance. Instead of comparing the system’s clusters C to"
D10-1056,P05-1044,0,0.311309,"Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-speech (POS) induction is"
D10-1056,D09-1071,0,0.0460356,"Missing"
D10-1056,W09-0210,0,0.0708737,"Missing"
D10-1119,C04-1180,1,0.305945,"(Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a"
D10-1119,W03-1013,0,0.0248429,"ntries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle & Mooney, 1996"
D10-1119,J07-4004,0,0.145444,"r S|N P/N P λyλx.state(x) ∧ next to(x, y) &gt; S|N P λx.state(x) ∧ next to(x, tex) &gt; texas S|N P/(S|N P ) SN P/N P λf λx.state(x)∧f (x) λyλx.next to(x, y) S λx.state(x) ∧ next to(x, tex) NP tex &gt;B &gt; &gt; Figure 1: Two examples of CCG parses with different logical form representations. defined as: eθ·φ(x,y,z) θ·φ(x,y 0 ,z 0 ) (y 0 ,z 0 ) e P (y, z|x; θ, Λ) = P (1) Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in Λ are used in the parse y. For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below. The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters θ and lexicon Λ are known: f (x) = arg max p(z|x; θ, Λ) z (2) where the probability of the logical form is found by summing over all parses that produce it: X p(z|x; θ, Λ) = p(y, z|x; θ, Λ) (3) y In this approach the distribution over parse trees y is modeled as a hidden variable. The sum over parses in Eq. 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm. To estimate the parameters themselves, we use"
D10-1119,P06-2034,0,0.0924891,"007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representatio"
D10-1119,P06-1115,0,0.935826,"g representations, or both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representat"
D10-1119,D08-1082,1,0.902357,"natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the fact that there is no standard representation for logical f"
D10-1119,P96-1008,0,0.0802861,"en designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) con"
D10-1119,J03-1002,0,0.00259079,"tures: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a),i) for the predicate argument-type relation. Initialization The weights for the semantic features are initialized to zero. The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1. We compute translation scores for (word, constant) pairs that cooccur in examples in the training data. The initial weight for each φL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ΛN P which are set to 10 (equivalent to the highest possible coocurrence score). We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations. These values were selected with cross validation on 1230 the Geo880 development set, describe"
D10-1119,N06-1056,0,0.88739,"both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the"
D10-1119,P07-1121,0,0.766071,"Λ) [φ(xi , y, zi )] −Ep(y,z|xi ;θ,Λ) [φ(xi , y, z)] • Set θ = θ + γ∆ Output: Lexicon Λ and parameters θ. Figure 2: The UBL learning algorithm. WASP system (Wong & Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &"
D10-1119,D07-1071,1,0.85741,"Missing"
D10-1119,P09-1110,1,0.900752,"Missing"
D11-1059,D10-1083,0,\N,Missing
D11-1059,D07-1031,0,\N,Missing
D11-1059,J93-2004,0,\N,Missing
D11-1059,N10-1083,0,\N,Missing
D11-1059,E03-1009,0,\N,Missing
D11-1059,W06-2920,0,\N,Missing
D11-1059,N06-1041,0,\N,Missing
D11-1059,P07-1094,1,\N,Missing
D11-1059,P10-2040,0,\N,Missing
D11-1059,D10-1056,1,\N,Missing
D11-1059,P08-1084,0,\N,Missing
D11-1059,J92-4003,0,\N,Missing
D11-1059,P02-1017,0,\N,Missing
D11-1059,P05-1044,0,\N,Missing
D11-1059,D07-1043,0,\N,Missing
D11-1059,P09-1057,0,\N,Missing
D11-1059,P06-3002,0,\N,Missing
D11-1059,J03-1002,0,\N,Missing
D11-1059,E95-1020,0,\N,Missing
D11-1059,dzeroski-etal-2006-towards,0,\N,Missing
D11-1059,erjavec-2004-multext,0,\N,Missing
D11-1140,C04-1180,1,0.278766,"s learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“Show me flights to Boston” and zi = λ x. f light(x) ∧to(x, bos). Model We will represent"
D11-1140,P10-1129,1,0.585752,"e recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified"
D11-1140,W03-1013,0,0.00562958,"y available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z r"
D11-1140,J07-4004,0,0.026062,"c parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored lexicon. We evaluate the appr"
D11-1140,W10-2903,0,0.551773,"ncluding ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and fro"
D11-1140,P06-2034,0,0.0236428,"guage-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on uns"
D11-1140,P11-1149,0,0.158983,"ls (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a met"
D11-1140,P02-1043,1,0.652263,"obabilistic CCG grammars for semantic parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored l"
D11-1140,D10-1119,1,0.165291,"ction for Semantic Parsing Tom Kwiatkowski∗ Luke Zettlemoyer† Sharon Goldwater∗ Mark Steedman∗ t.m.kwiatkowksi@sms.ed.ac.uk lsz@cs.washington.edu sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk † Computer ∗ School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Science & Engineering University of Washington Seattle, WA 98195 Abstract and the goal is to learn a grammar that can map new, unseen, sentences onto their corresponding meanings, or logical forms. One approach to this problem has developed algorithms for leaning probabilistic CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). These grammars are well-suited to the task of semantic parsing, as they closely link syntax and semantics. They can be used to model a wide range of complex linguistic phenomena and are strongly lexicalized, storing all language-specific grammatical information directly with the words in the lexicon. For example, a typical learned lexicon might include entries such as: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pai"
D11-1140,P09-1011,0,0.0264637,"ations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the"
D11-1140,P11-1060,0,0.573776,"chine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon a"
D11-1140,D08-1082,1,0.938058,"ed CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York ` NP : nyc New York ` NN : λ f λ x. f rom(x, nyc) ∧ f (x) A"
D11-1140,P96-1008,0,0.0661923,"d an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011"
D11-1140,P06-2080,0,0.0157424,"ge independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environme"
D11-1140,J03-1002,0,0.00538065,"elation; and φ(p,Ty(a),i) for the predicate argument-type relation. Boolean operator features look at predicates that occurr together in conjunctions and disjunctions. For each variable vi that fills argument slot i in two conjoined predicates p1 and p2 we introduce a binary indicator feature φcon j(i,p1 ,p2 ) . We introduce similar features φdis j(i,p1 ,p2 ) for variables vi that are shared by predicates in a disjunction. Initialization The weights for lexeme features are initialized according to coocurrance statistics between words and logical constants. These are estimated with the Giza++ (Och and Ney, 2003) implementation of IBM Model 1. The initial weights for templates are set by adding −0.1 for each slash in the syntactic category and −2 if the template contains logical constants. Features on lexeme-template pairs and all parse features are initialized to zero. Systems We compare performance to all recentlypublished, directly-comparable results. For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer System ZC07 UBL FUBL Exact Match Rec. Pre. F1 74.4 87.3 80.4 65.6 67.1 66.3 81.9 82.1 82.0 System ZC07 HY06 UBL FUBL Exact Match Rec. Pre. F1. 84.6 85.8 85.2 71.4 72.1 71.7 82.8 82.8 82.8 Partial"
D11-1140,D09-1001,0,0.0645657,"(2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach."
D11-1140,P10-1031,0,0.0256343,"Missing"
D11-1140,P06-1115,0,0.933771,"odel systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x,"
D11-1140,W00-1317,0,0.0167049,"od, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific i"
D11-1140,P10-1083,0,0.0173826,"d on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the tra"
D11-1140,W99-0909,0,0.0192112,"antly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“"
D11-1140,N06-1056,0,0.926806,"on in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York"
D11-1140,P07-1121,0,0.892548,"rsing datasets: GeoQuery, which is made up of natural language queries to a database of geographical information; and Atis, which contains natural language queries to a flight booking system. The Geo880 dataset has 880 (English-sentence, logicalform) pairs split into a training set of 600 pairs and a test set of 280. The Geo250 data is a subset of the Geo880 sentences that have been translated into Japanese, Spanish and Turkish as well as the original English. We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). The Atis dataset contains 5410 (sentence, logical-form) pairs split into a 4480 example training set, a 480 example development set and a 450 example test set. 1519 Evaluation Metrics We report exact match Recall (percentage of sentences for which the correct logical-form was returned), Precision (percentage of returned logical-forms that are correct) and F1 (harmonic mean of Precision and Recall). For Atis we also report partial match Recall (percentage of correct literals returned), Precision (percentage of returned literals that are correct) and F1, computed as described by Zettlemoyer an"
D11-1140,D07-1071,1,0.893493,"three lexical items separately is inefficient, since each word of this class (such as “fare”) will require three similarly structured lexical entries differing only in predicate name. There may also be systemtatic semantic variation between entries for a certain class of words. For example, in (6) “Boston” is paired with the constant bos that represents its meaning. However, item (7) also adds the predicate from to the logical form. This might be used to analyse somewhat elliptical, unedited sentences such as “Show me flights Boston to New York,” which can be challenging for semantic parsers (Zettlemoyer and Collins, 2007). This paper builds upon the insight that a large proportion of the variation between lexical items for a given class of words is systematic. Therefore it should be represented once and applied to a small set of basic lexical units. 1 We develop a factored lexicon that captures this insight by distinguishing lexemes, which pair words with logical constants, from lexical templates, which map lexemes to full lexical items. As we will see, this can lead to a significantly more compact lexicon that can be learned from less data. Each word or phrase will be associated with a few lexemes that can be"
D11-1140,P09-1110,1,0.855677,"ree of success on all of these datasets. 2 Related work There has been significant previous work on learning semantic parsers from training sentences labelled with logical form meaning representations. We extend a line of research that has addressed this problem by developing CCG grammar induction techniques. Zettlemoyer and Collins (2005, 2007) presented approaches that use hand generated, English-language specific rules to generate lexical items from logical forms as well as English specific type-shifting rules and relaxations of the CCG combinators to model spontaneous, unedited sentences. Zettlemoyer and Collins (2009) extends this work to the case of learning in context dependent environments. Kwiatkowski et al. (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translat"
D13-1004,N10-1083,0,0.0572949,"gory and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syn"
D13-1004,P11-1087,0,0.0581607,"1) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) an"
D13-1004,D10-1056,1,0.945212,"ber 2013. 2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984). These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng,"
D13-1004,D11-1059,1,0.882313,"comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly"
D13-1004,E03-1009,0,0.290804,"levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility. Languages differ in the richness of their morphology and strictness of word order. These characteristics appear to be (anti)co"
D13-1004,D07-1023,0,0.0218669,"os et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentation"
D13-1004,N13-1012,0,0.0220972,"earn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, a"
D13-1004,P12-1020,1,0.802314,"r, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sou"
D13-1004,W13-2603,0,0.0167602,"on from the stronger cue. The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity. 2 Model The task is to assign word tokens to part of speech categories and simultaneously segment the tokens into morphemes. We assume a relatively simple yet commonly used concatenative morphology which models a word as a stem plus (possibly null) suffix2 . 1 There are languages with much richer morphology than Spanish, but none with a child-directed corpus suitably annotated for evaluation. 2 Fullwood and O’Donnell (2013) recently presented a model of non-concatenative morphology that could be integrated into this model; however, it does not perform well on English (and presumably other mostly concatenative languages). Since this is an unsupervised model, the inferred categories and morphemes lack meaningful labels, but ideally will correspond to gold standard categories and morphemes. 2.1 generated from Dirichlet-multinomials conditioned on the tag t: κ∼ Dir(ακ ) t|κ ∼ Mult(κ) σ∼ Dir(αs ) s|t, σ ∼ Mult(σt ) φ∼ Dir(α f ) f |t, φ ∼ Mult(φt ) Word Order We model a sequence of words as a Hidden Markov Model (HMM)"
D13-1004,P07-1094,1,0.810299,"ferent syntactic categories. Most recent models have included a constraint forcing all tokens of a given type into the same category, which improves performance but often complicates inference. The Bayesian HMM’s performance is therefore not stateof-the-art, but is comparable to other token-based models (Christodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions. This part of the model is parametric, operating over a fixed number of tags T , and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology. 2.2 Morphology The morphology model introduced by Goldwater et al. (2006) generates morphological analyses for a set of tokens. These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words. Both stem s and suffix f are 32 ∑ P(s|t)P( f |t)P(t) (1) t,s, f s.t. s⊕ f =w where s ⊕ f = w denotes that the concatenation of ste"
D13-1004,P06-1111,0,0.0339909,"odels. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordintern"
D13-1004,J11-2002,0,0.0398389,"Missing"
D13-1004,P08-1046,0,0.0200408,"in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the mod"
D13-1004,E12-1024,1,0.822483,"ther than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them a"
D13-1004,D07-1043,0,0.0368605,"Missing"
D13-1004,N12-1045,0,0.328378,"Missing"
D13-1004,P05-1044,0,0.0261761,"eral, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories"
D13-1004,P06-1124,0,0.0452368,"nd suffix f are 32 ∑ P(s|t)P( f |t)P(t) (1) t,s, f s.t. s⊕ f =w where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. To resolve this problem, a Pitman-Yor process (PYP) is placed over the generating distribution above. The Pitman-Yor process has been found to be useful for representing the power-law distributions common in natural language (Teh, 2006; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011). The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. A series of customers (tokens z = z1 . . . zN ) enter a restaurant with an infinite number of initially empty tables. Upon entering, each customer is seated at a table k with probability p(zi = k|z1 . . . zi−1 , a, b) = ( nk −a if 1 ≤ k ≤ K i−1+b Ka+b if k = K + 1 i−1+b (2) tk sk zi fk lk wi K ti−2 N where nk is the number o"
D13-1004,W11-0301,0,\N,Missing
D13-1004,D10-1083,0,\N,Missing
D13-1005,P12-2017,0,0.0347966,"Missing"
D13-1005,P13-1148,0,0.105815,"Missing"
D13-1005,W11-0601,0,0.030909,"Missing"
D13-1005,D08-1113,0,0.10302,"Missing"
D13-1005,P12-1020,1,0.511537,"learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-channel setup where phonetic variation is introduced by a finite-state transducer (Neubig et al., 2010; Elsner et al., 2012). This integrated"
D13-1005,P08-1016,0,0.588196,"want, ... juwant, ... Probabilities for each word (sparse) G0 0 α p(ði) = .1, p(a) = .05, p(want) = .01... Conditional probabilities for each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word bou"
D13-1005,P08-1046,0,0.0260435,"esembles GGJ’s on clean data, and therefore the behavior of human learners. 5.2 Phonetic variability We next analyze the model’s ability to normalize variations in the pronunciation of tokens, by inspecting the mtk score. The “segment only” baseline is predictably poor, F : 44.8. The pipeline model scores 48.8, and our oracle transducer model matches this exactly. The EM transducer scores better, F : 49.6. Although the confidence intervals overlap slightly, the EM system also outperforms the pipeline on the other F -measures; altogether, these results suggest at least a weak learning synergy (Johnson, 2008) between segmentation and phonetic learning. It is interesting that EM can perform better than the oracle. However, EM is more conservative about which sound changes it will allow, and thus tends to avoid mistakes caused by the simplicity of the transducer model. Since the transducer works segmentby-segment, it can apply rare contextual variations out of context. EM benefits from not learning these variations to begin with. We can also compare the bigram and unigram versions of the model. The unigram model is a reasonable segmenter, though not quite as good as the bigram model, with boundary F"
D13-1005,P09-1012,0,0.61566,"s /2/, the word is likely to be /w2n/ “one” and the next word begins with /t/; if instead we posit that the vowel is /O/, the word is probably /wOnt/ “want”. Thus, inference methods that change only one character at a time are unlikely to mix well. Since they cannot simultaneously change the vowel and resegment the /t/, they must pass through a low-probability intermediate state to get from one state to the other, so will tend to get stuck in a bad local minimum. A Gibbs sampler which inserts or deletes a single segment boundary in each step (Goldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010), we can write the original GGJ model as a Hidden Sem"
D13-1005,P08-2042,0,0.0457234,"r each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their mode"
D13-1005,J01-3002,0,0.0639611,"GGJ 09 Generator for possible words a, b, ..., ju, ... want, ... juwant, ... Probabilities for each word (sparse) G0 0 α p(ði) = .1, p(a) = .05, p(want) = .01... Conditional probabilities for each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (200"
E12-1024,D11-1131,0,0.0825834,"Missing"
E12-1024,D10-1119,1,0.517337,"learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once. We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus (MacWhinney, 2000) using online variational Bayesian EM. We evaluate the learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1 Similar to referential uncertainty but relating to propositions rather than referents. 234 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguis"
E12-1024,D11-1140,1,0.744611,"Missing"
E12-1024,D08-1082,1,0.28321,"learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approac"
E12-1024,sagae-etal-2004-adding,0,0.0577893,"ex function to read all of the lexical items off from the derivations in each {t}0 . In the parameter update step, the training algorithm updates the pseudocounts associated with each of the productions a → b that have ever been seen during training according to Equation (17). Only non-zero pseudocounts are stored in our model. The count vector is expanded with a new entry every time a new production is used. While Data The Eve corpus, collected by Brown (1973), contains 14, 124 English utterances spoken to a single child between the ages of 18 and 27 months. These have been hand annotated by Sagae et al. (2004) with labelled syntactic dependency graphs. An example annotation is shown in Figure 3. While these annotations are designed to represent syntactic information, the parent-child relationships in the parse can also be viewed as a proxy for the predicate-argument structure of the semantics. We developed a template based deterministic procedure for mapping this predicateargument structure onto logical expressions of the type discussed in Section 2.1. For example, the dependency graph in Figure 3 is automatically transformed into the logical expression λe.have(you,another(y, cookie(y)), e) (18) ∧"
E12-1024,N06-1056,0,0.0359022,"een data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. I"
E12-1024,P07-1121,0,0.25359,"Missing"
E12-1024,D07-1071,1,0.775604,"Missing"
E12-1024,P09-1110,1,0.394634,"Missing"
E17-1032,W02-0606,0,0.0995474,"This section describes our set of features, with examples shown in Table 3. Presence in Training Data We want features that signal which parents are valid words. Narasimhan et al. (2015) used each word’s log frequency. However the majority of words in the training data (word frequency lists) occur only once, which makes their frequency information unreliable.3 Instead, we use an out-of-vocabulary feature (OOV) for parents that don’t occur in the training data. Semantic Similarity Morphologically related words exhibit semantic similarity among their word embeddings (Schone and Jurafsky, 2000; Baroni et al., 2002). Semantic similarity was an important feature in MorphoChains: Narasimhan et al. (2015) concluded that up to 25 percent of their model’s precision was due to the semantic similarity feature. We use the same feature here (COS). For a child-parent pair (wA , wB ) with word embeddings vwA and vwB respectively we compute semantic similarity as: ∗ ,z) z∈C(w∗ )  eθ·φ(w,z)  − λkθk2 (3) cos(wA , wB ) = vwA · vwB kvwA kkvwB k (4) Affixes Candidate pairs where the child contains a frequently occurring affix are more likely to be correct. To identify possible affixes to use as features, Narasimhan et"
E17-1032,J01-2001,0,0.74819,"Missing"
E17-1032,W04-0105,1,0.695844,"the sparse data problem in under-resourced languages. While surface segmentation can help, the example above illustrates its limitations: for more effective parameter sharing, a system should recognize that -ed, -d, -ted, and -ped share the same linguistic function. The importance of identifying underlying morphemes rather than surface morphs is widely recognized, for example by the MorphoChallenge organizers, who in later years provided datasets and evaluation measures to encourage this deeper level of analysis (Kurimo et al., 2010). Nevertheless, only a few systems have attempted this task (Goldwater and Johnson, 2004; Naradowsky and Goldwater, 2009), and as far as we know, only one, the rule-based MORSEL (Lignos et al., 2009; Lignos, 2010), has come close to the level of performance achieved by segmentation systems such as Morfessor (Virpioja et al., 2013). We present a system that adapts the unsupervised MorphoChains segmentation system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes. Like MorphoChains, our system uses an unsupervised log-linear model whose parameters are learned using contrastive estimation (Smi"
E17-1032,W10-2211,0,0.0306323,"d), etc. A major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages. While surface segmentation can help, the example above illustrates its limitations: for more effective parameter sharing, a system should recognize that -ed, -d, -ted, and -ped share the same linguistic function. The importance of identifying underlying morphemes rather than surface morphs is widely recognized, for example by the MorphoChallenge organizers, who in later years provided datasets and evaluation measures to encourage this deeper level of analysis (Kurimo et al., 2010). Nevertheless, only a few systems have attempted this task (Goldwater and Johnson, 2004; Naradowsky and Goldwater, 2009), and as far as we know, only one, the rule-based MORSEL (Lignos et al., 2009; Lignos, 2010), has come close to the level of performance achieved by segmentation systems such as Morfessor (Virpioja et al., 2013). We present a system that adapts the unsupervised MorphoChains segmentation system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes. Like MorphoChains, our system uses an unsu"
E17-1032,W11-0301,0,0.0413299,"Missing"
E17-1032,Q15-1012,0,0.169872,"ecognized, for example by the MorphoChallenge organizers, who in later years provided datasets and evaluation measures to encourage this deeper level of analysis (Kurimo et al., 2010). Nevertheless, only a few systems have attempted this task (Goldwater and Johnson, 2004; Naradowsky and Goldwater, 2009), and as far as we know, only one, the rule-based MORSEL (Lignos et al., 2009; Lignos, 2010), has come close to the level of performance achieved by segmentation systems such as Morfessor (Virpioja et al., 2013). We present a system that adapts the unsupervised MorphoChains segmentation system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes. Like MorphoChains, our system uses an unsupervised log-linear model whose parameters are learned using contrastive estimation (Smith and Eisner, 2005). The original MorphoChains system learns to identify child-parent pairs of morphologically related words, where the child (e.g., stopping) is formed from the parent (stop) by adding an affix and possibly a spelling transformation (both represented as features in the model). However, these spelling transformations are never used to"
E17-1032,W02-0604,0,0.115785,"Missing"
E17-1032,N09-1024,0,0.169817,"Missing"
E17-1032,W00-0712,0,0.167441,"mprove overall performance. This section describes our set of features, with examples shown in Table 3. Presence in Training Data We want features that signal which parents are valid words. Narasimhan et al. (2015) used each word’s log frequency. However the majority of words in the training data (word frequency lists) occur only once, which makes their frequency information unreliable.3 Instead, we use an out-of-vocabulary feature (OOV) for parents that don’t occur in the training data. Semantic Similarity Morphologically related words exhibit semantic similarity among their word embeddings (Schone and Jurafsky, 2000; Baroni et al., 2002). Semantic similarity was an important feature in MorphoChains: Narasimhan et al. (2015) concluded that up to 25 percent of their model’s precision was due to the semantic similarity feature. We use the same feature here (COS). For a child-parent pair (wA , wB ) with word embeddings vwA and vwB respectively we compute semantic similarity as: ∗ ,z) z∈C(w∗ )  eθ·φ(w,z)  − λkθk2 (3) cos(wA , wB ) = vwA · vwB kvwA kkvwB k (4) Affixes Candidate pairs where the child contains a frequently occurring affix are more likely to be correct. To identify possible affixes to use as fe"
E17-1032,P05-1044,0,0.115136,"004; Naradowsky and Goldwater, 2009), and as far as we know, only one, the rule-based MORSEL (Lignos et al., 2009; Lignos, 2010), has come close to the level of performance achieved by segmentation systems such as Morfessor (Virpioja et al., 2013). We present a system that adapts the unsupervised MorphoChains segmentation system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes. Like MorphoChains, our system uses an unsupervised log-linear model whose parameters are learned using contrastive estimation (Smith and Eisner, 2005). The original MorphoChains system learns to identify child-parent pairs of morphologically related words, where the child (e.g., stopping) is formed from the parent (stop) by adding an affix and possibly a spelling transformation (both represented as features in the model). However, these spelling transformations are never used to output underlying morphemes, instead the system just returns a segmentation by post-processing the inferred child-parent pairs. We extend the MorphoChains system in sev337 Proceedings of the 15th Conference of the European Chapter of the Association for Computationa"
E17-1032,C10-1116,0,0.0527913,"Missing"
E17-1032,Q13-1021,1,\N,Missing
E17-1116,D16-1120,0,0.0622866,"Missing"
E17-1116,E14-1011,0,0.183292,"Missing"
E17-1116,W15-4302,0,0.114184,"ence and linguistic identity on social media Philippa Shoemark∗ p.j.shoemark@ed.ac.uk Debnil Sur† debnil@stanford.edu Iain Murray∗ luke.shrimpton@ed.ac.uk Sharon Goldwater∗ i.murray@ed.ac.uk sgwater@inf.ed.ac.uk † Department of Computer Science ∗ School of Informatics University of Edinburgh Stanford University Abstract of regional variation in general US English (Doyle, 2014; Huang et al., 2015), African American English (Jones, 2015), and global Spanish (Gonc¸alves and S´anchez, 2014), and to study variation associated with factors such as race/ethnicity (Jones, 2015; Blodgett et al., 2016; Jørgensen et al., 2015) and gender (Bamman et al., 2014). These studies have shown that tweets mirror spoken language in many ways, such as displaying dialect variation not only in the use of distinct lexical items, but also in the use of non-standard spellings to indicate nonstandard pronunciation—in fact, these spellings even reflect the phonological processes found in spoken language (Eisenstein, 2015). There is also evidence that, as in spoken language, individuals may shift their style of language in response to the audience. In particular, studies have found that when the expected audience of a tweet is larger"
E17-1116,P12-3005,0,0.18381,"Missing"
E17-2076,J93-2003,0,0.0703492,"1). In creating a translation model from this data, we face a difficulty that does not arise in the parallel texts that are normally used to train translation models: the pseudotext does not represent all of the source words, since the discovered segments do not cover the full audio (Fig. 1). Hence we must not assume that our MT model can completely recover the translation of a test sentence. In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).2 In particular, for each pseudoterm, we learn a translation distribution over possible target words. To translate a pseudoterm in test data, we simply return its highest-probability translation (or translations, as discussed in §5). This setup implies that in order to translate, we must apply UTD on both the training and test audio. Using additional (not only training) audio in UTD increases the likelihood of discovering more clusters. We therefore generate pseudotext for the combined audio, t"
E17-2076,N16-1109,0,0.299767,"(Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsuperv"
E17-2076,N13-1073,0,0.0130345,"parallel texts that are normally used to train translation models: the pseudotext does not represent all of the source words, since the discovered segments do not cover the full audio (Fig. 1). Hence we must not assume that our MT model can completely recover the translation of a test sentence. In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).2 In particular, for each pseudoterm, we learn a translation distribution over possible target words. To translate a pseudoterm in test data, we simply return its highest-probability translation (or translations, as discussed in §5). This setup implies that in order to translate, we must apply UTD on both the training and test audio. Using additional (not only training) audio in UTD increases the likelihood of discovering more clusters. We therefore generate pseudotext for the combined audio, train the MT model on the pseudotext of the training audio, and apply it to the pseudotext of the tes"
E17-2076,D16-1263,0,0.377478,"translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsupervised or minimallysupervised way, but in practice they used supervised ASR/phone recognition. Additionally, their evaluation focused on phone error rate rather than translation. In contrast to these approaches, our method can make translation predictions for audio input not seen during training, and we evaluate it on real multi-speaker speech data. Our simple system (§2) builds on unsupervised speec"
E17-2076,D16-1133,0,0.709692,"Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsupervised or minimallysupervised way,"
E17-2076,Q15-1028,0,0.0199519,"Missing"
E17-2076,2010.amta-workshop.1,0,0.0325618,"nslation (MT) (Waibel and Fugen, 2008). But high-quality ASR requires hundreds of hours of transcribed audio, while high-quality MT requires millions of words of parallel text—resources available for only a tiny fraction of the world’s estimated 7,000 languages (Besacier et al., 2014). Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used thes"
E17-2076,2013.iwslt-papers.14,1,0.100114,"eech data. Our simple system (§2) builds on unsupervised speech processing (Versteegh et al., 2015; Lee et al., 2015; Kamper et al., 2016b), and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech (Park and Glass, 2008; Jansen and Van Durme, 2011). The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-ofwords translation model. We test our system on the CALLHOME Spanish-English speech translation corpus (Post et al., 2013), a noisy multi-speaker corpus of telephone calls in a variety of Spanish di474 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 474–479, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics alects (§3). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in crossspeaker clustering (§4). Despite these difficulties, we demonstrate that the system lea"
H05-1085,N04-4015,0,0.592275,"cal tags, and use a MaxEnt model to combine the different levels of representation in the translation model. The results from these papers indicate that on corpus sizes up to 60,000 parallel sentences, the restructuring operations yielded a large improvement in translation quality, but the morphological decomposition provided only a slight additional benefit. However, since German is not as morphologically complex as Czech, we might expect a larger benefit from morphological analysis in Czech. Another project utilizing morphological analysis for statistical machine translation is described by Lee (2004). Lee’s system for Arabic-English translation takes as input POS-tagged English and Arabic text, where the Arabic words have been pre-segmented into stems and affixes. The system performs an initial alignment of the Arabic morphemes to the English words. Based on the consistency of the English POS tag that each Arabic morpheme aligns to, the system determines whether to keep that morpheme as a separate item, merge it back onto the stem, or delete it altogether. In addition, multiple occurrences of the determiner Al within a single Arabic noun phrase are deleted (i.e. only one occurrence is all"
H05-1085,C00-2162,0,0.252734,"n using morphological analysis for statistical machine translation. In Section 3, we describe several methods for utilizing morphological information in a statistical translation model. Section 4 presents the results of our experiments using these methods. Sections 5 and 6 discuss the results of our experiments and conclude the paper. 2 Previous Work Until recently, most machine translation projects involved translating between languages with relatively little morphological structure. Nevertheless, a few research projects have investigated the use of morphology to improve translation quality. Niessen and Ney (2000; 2004) report work on German-English translation, where they investigate various types of morphosyntactic restructuring, including merging German verbs with their detached prefixes, annotating a handful of frequent ambiguous German words with POS tags, combining idiomatic multi-word expressions into single words, and undoing question in676 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 676–683, Vancouver, October 2005. 2005 Association for Computational Linguistics version and do-insertion in both Ger"
H05-1085,J04-2003,0,0.419652,"English translation, where they investigate various types of morphosyntactic restructuring, including merging German verbs with their detached prefixes, annotating a handful of frequent ambiguous German words with POS tags, combining idiomatic multi-word expressions into single words, and undoing question in676 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 676–683, Vancouver, October 2005. 2005 Association for Computational Linguistics version and do-insertion in both German and English. In addition, Niessen and Ney (2004) decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. The results from these papers indicate that on corpus sizes up to 60,000 parallel sentences, the restructuring operations yielded a large improvement in translation quality, but the morphological decomposition provided only a slight additional benefit. However, since German is not as morphologically complex as Czech, we might expect a larger benefit from morphological analysis in Czech. Another project"
H05-1085,J03-1002,0,0.0040092,"llel corpus. Some statistics on the parallel corpus are shown in the graph in Figure 3. This graph illustrates the sparse data problem in Czech that our morphological analysis is intended to address. Although the number of infrequently occurring lemmas is about the same in both English and Czech, the number of infrequently occurring inflected wordforms is approximately twice as high in Czech.1 For all of our experiments, we used the same language model, trained with the CMU Statistical Language Modelling Toolkit (Clarkson and Rosenfeld, 1997). Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modi1 Although we did not use it for the experiments in this paper, the PCEDT corpus does contain lemma information for the English data. There is a slight discrepancy between the English and Czech data in the lemma information for pronouns, in that English pronouns (including accusitive, possessive, and other forms) are assigned themselves as lemmas, whereas Czech pronouns are reduced to uninflected forms. Given that pronouns generally have many tokens, this discrepancy should not affect the data in Figure 3. 4 3.5 x 10 word-to-word lemmatize all except Pro except Pro, V, N lemmati"
H05-1085,cmejrek-etal-2004-prague,0,0.0427447,"Missing"
H05-1085,E03-1004,0,\N,Missing
K17-2002,E17-1005,0,0.00549682,"ach prepended with a + X s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to t"
K17-2002,W02-0606,0,0.108183,"oduce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Fi"
K17-2002,P17-1031,0,0.0164218,"s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to twice as many autoencoder"
K17-2002,N15-1186,0,0.0377683,"mples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs with the same differences"
K17-2002,P17-1029,0,0.0531855,"Missing"
K17-2002,J11-2002,0,0.0734975,"Missing"
K17-2002,P17-1182,1,0.740658,"Missing"
K17-2002,W16-2010,1,0.383881,"Missing"
K17-2002,W17-4111,1,0.800423,"Missing"
K17-2002,W10-2211,0,0.0774086,"ion training pairs (any duplicate lemmas or target forms are included only once). Our second autoencoder variant, AE-RS, uses randomly generated strings as inputs, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities"
K17-2002,W02-0604,0,0.507225,"mber of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs wi"
K17-2002,W00-0712,0,0.0402176,"puts, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Tabl"
N07-1018,W06-1673,0,0.0467676,"Missing"
N07-1018,P04-1061,0,0.0368722,"is task is that simple PCFGs are not accurate models of English syntactic structure. We know that PCFGs 144 α = (1.0, 1.0) α = (0.5, 1.0) α = (0.1, 1.0) 5 4 P(θ1 |α) 3 2 1 0 0 0.2 0.4 0.6 0.8 Binomial parameter θ1 1 Figure 2: A Dirichlet prior α on a binomial parameter θ1 . As α1 → 0, P(θ1 |α) is increasingly concentrated around 0. that represent only major phrasal categories ignore a wide variety of lexical and syntactic dependencies in natural language. State-of-the-art systems for unsupervised syntactic structure induction system uses models that are very different to these kinds of PCFGs (Klein and Manning, 2004; Smith and Eisner, 2006).1 Our goal in this section is modest: we aim merely to provide an illustrative example of Bayesian inference using MCMC. As Figure 2 shows, when the Dirichlet prior parameter αr approaches 0 the prior probability PD (θr |α) becomes increasingly concentrated around 0. This ability to bias the sampler toward sparse grammars (i.e., grammars in which many productions have probabilities close to 0) is useful when we attempt to identify relevant productions from a much larger set of possible productions via parameter estimation. The Bantu language Sesotho is a richly agglut"
N07-1018,P06-1072,0,0.0107366,"FGs are not accurate models of English syntactic structure. We know that PCFGs 144 α = (1.0, 1.0) α = (0.5, 1.0) α = (0.1, 1.0) 5 4 P(θ1 |α) 3 2 1 0 0 0.2 0.4 0.6 0.8 Binomial parameter θ1 1 Figure 2: A Dirichlet prior α on a binomial parameter θ1 . As α1 → 0, P(θ1 |α) is increasingly concentrated around 0. that represent only major phrasal categories ignore a wide variety of lexical and syntactic dependencies in natural language. State-of-the-art systems for unsupervised syntactic structure induction system uses models that are very different to these kinds of PCFGs (Klein and Manning, 2004; Smith and Eisner, 2006).1 Our goal in this section is modest: we aim merely to provide an illustrative example of Bayesian inference using MCMC. As Figure 2 shows, when the Dirichlet prior parameter αr approaches 0 the prior probability PD (θr |α) becomes increasingly concentrated around 0. This ability to bias the sampler toward sparse grammars (i.e., grammars in which many productions have probabilities close to 0) is useful when we attempt to identify relevant productions from a much larger set of possible productions via parameter estimation. The Bantu language Sesotho is a richly agglutinative language, in whic"
N09-1036,W08-2109,0,0.0320289,"ons, each of which expands to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and im320 plementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabli"
N09-1036,P08-1016,0,0.0331332,"to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and im320 plementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabling it to captu"
N09-1036,P06-1085,1,0.875941,"ceived considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units"
N09-1036,N07-1018,1,0.652939,"lities θ, where θr is the probability of rule r ∈ R, A ⊆ N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so CX is the adaptor for adapted nonterminal X ∈ A. Informally, an adaptor CX nondeterministically maps a stream of trees from a base distribution HX whose support is TX (the set of subtrees whose root node is X ∈ N generated by the grammar’s rules) into another stream of trees whose support is also TX . In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of nat319 ural numbers z1 , z2 , . . ., where z1 = 1 and each zn+1 ≤ m + 1 where m = max(z1 , . . . , zn ). In the “Chinese Restaura"
N09-1036,P08-1046,1,0.928247,"ecause they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units such as words, syllables and collocations"
N09-1062,E03-1005,0,0.326439,", not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner. The net result is a grammar"
N09-1062,P05-1022,0,0.0061087,"ing latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local c"
N09-1062,A00-2018,0,0.0299597,"rammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can b"
N09-1062,C02-1126,0,0.04097,"component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim"
N09-1062,P07-1035,0,0.0165933,"Missing"
N09-1062,P06-1085,1,0.0765468,"bution in more detail below. Rather than representing the distribution Gc explicitly, we integrate over all possible values of Gc . The resulting distribution over ei , conditioned on e&lt;i = e1 . . . ei−1 and the root category c is: (1) Since the sequence of elementary trees can be split into derivations, each of which completely specifies a tree, P (t|e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the work in our model is done by the prior distribution over elementary trees. Note that this is analogous to the Bayesian model of word segmentation presented by Goldwater et al. (2006); indeed, the problem of inferring e from t can be viewed as a segmentation problem, where each full tree must be segmented into one or more elementary trees. As in Goldwater et al. (2006), we wish to favour solutions employing a relatively small number of elementary units (here, elementary trees). This can be done using a Dirichlet process (DP) prior. Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as Gc |αc , P0 ∼ DP(αc , P0 (·|c)) e|c ∼ Gc where P0 (·|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, an"
N09-1062,N07-1018,1,0.524373,"required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone. Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint. 3 Model Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferrin"
N09-1062,J98-4004,0,0.13043,"ures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as"
N09-1062,J02-1005,0,0.622903,"as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and ef"
N09-1062,D07-1072,0,0.0361547,"operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to"
N09-1062,N07-1051,0,0.0121277,"e 3: TSG used to generate synthetic data. All production probabilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3. This produces binary trees with A and B internal nodes and ‘a’ and ‘b’ as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requi"
N09-1062,P06-1055,0,0.780747,"thout the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs"
N09-1062,D07-1058,0,0.406805,"moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be ex"
N09-1062,J03-4003,0,\N,Missing
N18-1126,W16-2007,0,0.0240979,"model is inspired by recent work on morphological reinflection. As defined by two recent Shared Tasks (Cotterell et al., 2016, 2017), a morphological reinflection system gets as input 1 https://sites.google.com/site/ morfetteweb/ 2 http://cistern.cis.lmu.de/lemming 3 https://github.com/onkarpandit00786/ neural-lemmatizer 1392 some inflected wordform (and possibly its morphosyntactic tags) along with a set of target tags. The system must produce the correct inflected form for the target tags. In the 2016 SIGMORPHON Shared Task, various neural sequence-to-sequence models gave the best results (Aharoni et al., 2016; ¨ Kann and Sch¨utze, 2016; Ostling, 2016). We base our work closely on one of these (Kann and Sch¨utze, 2016), which also won one of the 2017 tasks (Bergmanis et al., 2017). Our lemmatization task can be viewed as a specific type of reinflection, but instead of assuming that tags are given in the input (or that the system simply has to guess the tags from the wordform itself, as in some of the Shared Tasks), we investigate whether the information available from the tags can instead be inferred from sentence context. 3 Model Description Our model is based on the network architecture proposed"
N18-1126,K17-2002,1,0.902767,"Missing"
N18-1126,Q17-1010,0,0.0328085,"p < 0.01. 8 Training parameters were tunned/verified on the standard splits of UDT training and development sets for Spanish and Catalan, therefore the results on these languages are not included in our evaluation. 1394 we evaluate the current model’s lemmatization exact match accuracy on the development set and keep this model if it performs better than the previous best model. When making predictions we use beam-search decoding with a beam of size 12. Baselines To train models we use the default settings for Morfette and Lemming. Ch-2017 requires word embeddings, for which we use fastText9 (Bojanowski et al., 2017). For Ch-2017 we set the number of training epochs to 100 and implement early stopping with patience 10.10 We leave the remaining model parameters as suggested by Chakrabarty et al. (2017). We also use a lookup-based baseline (Baseline). For words that have been observed in training, it outputs the most frequent lemma (or the first observed lemma, if the options are equally frequent). For unseen words it outputs the wordform itself as the hypothesized lemma. Context Representation We aim to use a context representation that works well across multiple languages, rather than to tune the context"
N18-1126,P17-1136,0,0.183587,"ectly lemmatize ambiguous forms such as the example above. Previous researchers have also assumed that context should help in lemmatizing unseen words (Chrupała, 2006; M¨uller et al., 2015)—i.e., that the context contains useful features above and beyond those in the wordform itself. Nevertheless, we are not aware of any previous work that has attempted to quantify how much (or even whether) context actually helps in both of these cases. Several previous papers on contextsensitive lemmatization have reported results on unseen words (Chrupała, 2006; Chrupała et al., 2008; M¨uller et al., 2015; Chakrabarty et al., 2017), and some have compared versions of their systems that use context in different ways (M¨uller et al., 2015; Chakrabarty et al., 2017), but there are few if any direct comparisons between context-sensitive and context-free systems, nor have results been reported on ambiguous forms. The main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of context actually helps in these cases. We introduce Lematus, a lemmatizer based on a standard encoder-decoder architecture, whi"
N18-1126,W14-4012,0,0.147256,"Missing"
N18-1126,chrupala-etal-2008-learning,0,0.67214,"Missing"
N18-1126,K17-2001,0,0.373039,"e.g. swims, swimming, swam, swum). Data-driven lemmatizers face two main challenges: first, to generalize beyond the training data in order to lemmatize unseen words; and second, to disambiguate ambiguous wordforms from their sentence context. In Latvian, for example, the wordform “cel¸u” is ambiguous when considered in isolation: it could be an inflected variant of the verb “celt” (to lift) or the nouns “celis” (knee) or “cel¸sˇ” (road); without context, the lemmatizer can only guess. This paper presents Lematus—a system that adapts the neural machine translation framework of Sennrich et al. (2017) to learn context sensitive lemmatization using an encoder-decoder model. Context is represented simply using the character contexts of each form to be lemmatized, meaning that our system requires fewer training resources than previous systems: only a corpus with its lemmatized forms, without the need for POS tags (Chrupała et al., 2008; M¨uller et al., 2015) or word embeddings trained on a much larger corpus (Chakrabarty et al., 2017). We evaluate Lematus on data from 20 typologically varied languages, both using the full training data from the Universal Dependencies project (Nivre et al., 20"
N18-1126,P98-1062,0,0.617156,"gether, then, our results suggest that context-free neural lemmatization is surprisingly effective, and may be a reasonable option if the language contains many unseen words but few ambiguous ones. Context is likely to help in most languages, but the main boost is for languages with higher ambiguity. 2 Background and Baseline Systems Early work on context-sensitive lemmatization focused on disambiguation: given a set of analyses produced by a hand-built morphological analyzer (typically including both lemmas and morphosyntactic tags), choose the best one in context (Oflazer and Kuru¨oz, 1994; Ezeiza et al., 1998; Hakkani-T¨ur et al., 2002). Here, we focus on systems learning to generate the lemmas and tags without a pre-existing analyzer (Erjavec and Dˇzeroski, 2004; Chrupała, 2006). The three systems we use as baselines follow Chrupała (2006) in treating the task as a classification problem, where the system learns to choose which of a set of edit scripts or edit trees (previously induced from the aligned wordform-lemma pairs) should be applied to transform each wordform into the correct lemma. Two of our baselines, Morfette1 (Chrupała et al., 2008) and Lemming2 (M¨uller et al., 2015), learn from mo"
N18-1126,P08-1103,0,0.0342976,"onsists of two log-linear components (a classifier for lemmatization and a sequence model for tagging), which are combined either using a pipeline (first tag, then lemmatize) or through joint inference. The lemmatization model uses a variety of features from the edit trees, alignments, orthography of the lemma, and morphosyntactic tags. In experiments on six languages, M¨uller et al. (2015) showed that the joint Lemming model worked better than the pipelined model, and that adding morphosyntactic features helped. They also demonstrated improvements over an earlier context-free baseline model (Jiampojamarn et al., 2008). However, they did not evaluate on ambiguous forms, nor directly compare context-sensitive and context-free versions of their own model. Our third baseline, Ch-20173 (Chakrabarty et al., 2017) uses a neural network rather than a log-linear model, but still treats lemmatization as a classification task to choose the correct edit tree. (Like our model, Ch-2017 does not perform morphological tagging.) The model composes syntactic and semantic information using two successive bidirectional GRU networks. The first bidirectional GRU network is similar to the character to word model by Ling et al. ("
N18-1126,W16-2010,0,0.103649,"Missing"
N18-1126,D15-1176,0,0.0309533,"et al., 2008). However, they did not evaluate on ambiguous forms, nor directly compare context-sensitive and context-free versions of their own model. Our third baseline, Ch-20173 (Chakrabarty et al., 2017) uses a neural network rather than a log-linear model, but still treats lemmatization as a classification task to choose the correct edit tree. (Like our model, Ch-2017 does not perform morphological tagging.) The model composes syntactic and semantic information using two successive bidirectional GRU networks. The first bidirectional GRU network is similar to the character to word model by Ling et al. (2015) and learns syntactic information. The semantic information comes from word embeddings pre-trained on much larger corpora. The second GRU uses a composition of the semantic and syntactic embeddings for the edit tree classification task. Rather than treating lemmatization as classification, our own model is inspired by recent work on morphological reinflection. As defined by two recent Shared Tasks (Cotterell et al., 2016, 2017), a morphological reinflection system gets as input 1 https://sites.google.com/site/ morfetteweb/ 2 http://cistern.cis.lmu.de/lemming 3 https://github.com/onkarpandit007"
N18-1126,D15-1272,0,0.146429,"Missing"
N18-1126,A94-1024,0,0.716039,"Missing"
N18-1126,W16-2003,0,0.0288397,"l reinflection. As defined by two recent Shared Tasks (Cotterell et al., 2016, 2017), a morphological reinflection system gets as input 1 https://sites.google.com/site/ morfetteweb/ 2 http://cistern.cis.lmu.de/lemming 3 https://github.com/onkarpandit00786/ neural-lemmatizer 1392 some inflected wordform (and possibly its morphosyntactic tags) along with a set of target tags. The system must produce the correct inflected form for the target tags. In the 2016 SIGMORPHON Shared Task, various neural sequence-to-sequence models gave the best results (Aharoni et al., 2016; ¨ Kann and Sch¨utze, 2016; Ostling, 2016). We base our work closely on one of these (Kann and Sch¨utze, 2016), which also won one of the 2017 tasks (Bergmanis et al., 2017). Our lemmatization task can be viewed as a specific type of reinflection, but instead of assuming that tags are given in the input (or that the system simply has to guess the tags from the wordform itself, as in some of the Shared Tasks), we investigate whether the information available from the tags can instead be inferred from sentence context. 3 Model Description Our model is based on the network architecture proposed by Sennrich et al. (2017), which implements"
N18-1126,E17-3017,0,0.0795205,"Missing"
N18-1126,P16-1162,0,0.0249112,", Polish and Turkish) were also used in our main experiments, while three (Bulgarian, Hebrew, and Persian) were not, due to problems getting all the baseline systems to run on those languages. For the word level context representation (Words), we use all words in the left and the right sentence contexts. For the character level context representations (N-Ch) we experiment with N = 0, 5, 10, 15, 20, or 25 characters of left and right contexts. For the sub-word unit context representation, we use byte pair encoding (BPE) (Gage, 1994), which has shown good results for neural machine translation (Sennrich et al., 2016). BPE is a data compression algorithm that iteratively replaces the most frequent pair of symbols (here, characters) in a sequence with a single new symbol. BPE has 9 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md 10 We do so because it is unclear what stopping criterion was used by Chakrabarty et al. (2017) Their suggested default for the number of training epochs is 6, yet the values used in their experiments vary from 15 for Hindi to 80 for Bengali. a single parameter—the number of merge operations. Suitable values for this parameter depend on the applicati"
N18-1126,P17-1184,0,0.0140897,"iteratively replaces the most frequent pair of symbols (here, characters) in a sequence with a single new symbol. BPE has 9 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md 10 We do so because it is unclear what stopping criterion was used by Chakrabarty et al. (2017) Their suggested default for the number of training epochs is 6, yet the values used in their experiments vary from 15 for Hindi to 80 for Bengali. a single parameter—the number of merge operations. Suitable values for this parameter depend on the application and vary from 10k in language modeling (Vania and Lopez, 2017) to 50k in machine translation (Sennrich et al., 2016). We aim to use BPE to extract a few salient and frequently occurring strings, such as affixes, therefore we set the number of BPE merge operations to 500. We use BPE-encoded left and right sentence contexts that amount up to 20 characters of the original text. Since we hoped to use context to help with ambiguous words, we looked specifically at ambiguous word performance in choosing the best context representation.11 Table 1 summarizes Lematus’ performance on ambiguous tokens using different sentence context representations. There is no co"
N18-2113,C16-1013,0,0.264385,"ime period and even within a single author, since orthography only became standardized in many languages fairly recently. Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014; Pettersson et al., 2014), statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, Sharon Goldwater School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk 2013), and most recently neural network models (Bollmann and Søgaard, 2016; Bollmann et al., 2017; Korchagina, 2017). However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other. Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks. This paper br"
N18-2113,W17-0504,0,0.84804,"rthography only became standardized in many languages fairly recently. Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014; Pettersson et al., 2014), statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, Sharon Goldwater School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk 2013), and most recently neural network models (Bollmann and Søgaard, 2016; Bollmann et al., 2017; Korchagina, 2017). However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other. Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks. This paper brings together best practices for evaluatin"
N18-2113,W13-5617,0,0.26427,"Missing"
N18-2113,P17-1183,0,0.190987,"searchers have rarely examined whether their systems actually improve performance on downstream tasks. This paper brings together best practices for evaluating historical text normalization systems, highlighting in particular the need to report results on unseen tokens and to consider the naïve baseline. We focus our evaluation on two recent neural models: one that has been previously tested only on a German collection that is not widely available (Bollmann et al., 2017), and one that is adapted from work on morphological re-inflection, but has not been used for historical text normalization (Aharoni et al., 2017). Both are encoderdecoder models; the former with soft attention, and the latter with hard monotonic attention. We present results on five languages, for both seen and unseen words and for various amounts of training data. The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models (Pettersson et al., 2014). However, on unseen words (which we argue are what matters), both neural models do well. Unfortunately, these positive results did not 1 Some authors have focussed on unsupervised normalizat"
N18-2113,P17-1031,0,0.273723,"single author, since orthography only became standardized in many languages fairly recently. Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014; Pettersson et al., 2014), statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, Sharon Goldwater School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk 2013), and most recently neural network models (Bollmann and Søgaard, 2016; Bollmann et al., 2017; Korchagina, 2017). However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other. Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks. This paper brings together best prac"
N18-2113,W14-0605,0,0.639295,"ith several different orthographic forms, which may not correspond to the modern form. For example, the modern English word said might be realized as sayed, seyd, said, sayd, etc. Spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently. Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014; Pettersson et al., 2014), statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, Sharon Goldwater School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk 2013), and most recently neural network models (Bollmann and Søgaard, 2016; Bollmann et al., 2017; Korchagina, 2017). However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.1 These issues"
N18-2113,rognvaldsson-etal-2012-icelandic,0,0.0984883,"ral models at different training data sizes starting from 1k tokens. On seen tokens, the baseline was best in all cases except for 1k tokens in Hungarian and Icelandic (where the soft attention model was slightly better) and the largest two data sizes in German (where the hard attention model was slightly better). This supports our claim that learned models should typically only be applied to unseen tokens. Accuracy on unseen tokens is shown in Figure 1. Note that the set of unseen items gets smaller 6 English: Markus (1999); German: Scheible et al. (2011); Hungarian: Simon (2014); Icelandic: Rögnvaldsson et al. (2012); Swedish: Fiebranz et al. (2011). For details of their dates and contents, see Pettersson et al. (2014). 722 7 We obtained our datasets from Pettersson et al. but our baseline results are slightly different from what they report. The differences (theirs–ours) are -0.1, 0.2, 0.4, 1.2, 0.6 for Eng, Ger, Hun, Ice, Swe respectively. This could be due to differences in tie-breaking methods, or to another unknown factor. These differences suggest using caution in directly comparing their non-baseline results to ours. 8 When we varied the training data sizes, we found that the soft attention model a"
N18-2113,W11-0415,0,0.0265674,"ll performance. We also compared the accuracy of the two neural models at different training data sizes starting from 1k tokens. On seen tokens, the baseline was best in all cases except for 1k tokens in Hungarian and Icelandic (where the soft attention model was slightly better) and the largest two data sizes in German (where the hard attention model was slightly better). This supports our claim that learned models should typically only be applied to unseen tokens. Accuracy on unseen tokens is shown in Figure 1. Note that the set of unseen items gets smaller 6 English: Markus (1999); German: Scheible et al. (2011); Hungarian: Simon (2014); Icelandic: Rögnvaldsson et al. (2012); Swedish: Fiebranz et al. (2011). For details of their dates and contents, see Pettersson et al. (2014). 722 7 We obtained our datasets from Pettersson et al. but our baseline results are slightly different from what they report. The differences (theirs–ours) are -0.1, 0.2, 0.4, 1.2, 0.6 for Eng, Ger, Hun, Ice, Swe respectively. This could be due to differences in tie-breaking methods, or to another unknown factor. These differences suggest using caution in directly comparing their non-baseline results to ours. 8 When we varied t"
N18-2113,W13-2409,0,0.277492,"Missing"
N19-1006,L18-1531,0,0.101856,"Missing"
N19-1006,D16-1263,0,0.026857,"ge differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and Engl"
N19-1006,W17-0123,0,0.0192522,"urce and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and English as the target. After training an"
N19-1006,N18-1008,0,0.180871,"plications, there may be some source language audio paired with target language text translations. In these scenarios, end-to-end ST is appealing. Recently, Weiss et al. (2017) showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST. But this result required over 150 hours of translated audio for training, still a substantial resource requirement. By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (Bansal et al., 2018). Other low-resource systems have similarly low accuracies (Anastasopoulos and Chiang, 2018; B´erard et al., 2018). To improve end-to-end ST in low-resource settings, we can try to leverage other data resources. For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (Anastasopoulos and Chiang, 2018; Weiss et al., 2017; B´erard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest. Could we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b;"
N19-1006,Q17-1024,0,0.0580446,"Missing"
N19-1006,D17-1039,0,0.0440099,"Missing"
N19-1006,P07-2045,0,0.0124442,"Missing"
N19-1006,P16-1162,0,0.129045,"Missing"
N19-1006,W07-0734,0,0.0797966,"models.5 In low-resource settings, BLEU scores tend to be low, difficult to interpret, and poorly correlated with model performance. This is because BLEU requires exact four-gram matches only, but low four-gram accuracy may obscure a high unigram accuracy and inexact translations that partially capture the semantics of an utterance, and these can still be very useful in situations like language documentation and crisis response. Therefore, we also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches. To compute these scores, we use METEOR (Lavie and Agarwal, 2007) with default settings for English and French.6 For example, METEOR assigns “eat” a recall of 1 against reference “eat” and a recall of 0.8 against reference “feed”, which it considers a synonym match. 5 Spanish-English ST In the following, we denote an ST model by S-TNh, where S and T are source and target language codes, and N is the size of the training set in hours. For example, sp-en-20h denotes a Spanish-English ST model trained using 20 hours of data. We use the code mb for Mboshi and fr for French. Naive baselines. We also include evaluation scores for a naive baseline model that predi"
N19-1006,D15-1166,0,0.0599554,"Missing"
N19-1006,2010.amta-workshop.1,0,0.0217312,"e on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and English as the target. After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only"
N19-1006,P02-1040,0,0.103413,"Missing"
N19-1006,2013.iwslt-papers.14,1,0.888041,"ariety of Spanish dialects, split into 140K utterances. To simulate low-resource conditions, we construct smaller train1 Using a shared vocabulary of characters or subwords is an interesting direction for future work, but not explored here. 59 ing corpora consisting of 50, 20, 10, 5, or 2.5 hours of data, selected at random from the full training data. The development and test sets each consist of around 4.5 hours of speech, split into 4K utterances. We do not use the corresponding Spanish transcripts; our target text consists of English translations that were collected through crowdsourcing (Post et al., 2013, 2014). quency of character sequences, so it must be computed with respect to a specific corpus. For English, we use the full 160-hour Spanish-English ST target training text. For French, we use the Mboshi-French ST target training text. 3.3 Model architecture for ASR and ST Speech encoder. As shown schematically in Figure 1, MFCC feature vectors, extracted using a window size of 25 ms and a step size of 10ms, are fed into a stack of two CNN layers, with 128 and 512 filters with a filter width of 9 frames each. In each CNN layer we stride with a factor of 2 along time, apply a ReLU activation"
N19-1006,D16-1163,0,0.032263,"sopoulos and Chiang, 2018; Weiss et al., 2017; B´erard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest. Could we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b; Deng et al., 2013). For MT, transfer learning (Thrun, 1995) has been very effective: pretraining a model for a high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (Zoph et al., 2016; Johnson et al., 2017). Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model’s parameters for low-resource ST. We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We"
N19-1418,P17-1136,0,0.0196404,", and especially on unseen words) in comparison to a baseline that does not use context. 1 Introduction Many lemmatizers work on isolated wordforms (Wicentowski, 2002; Dreyer et al., 2008; Rastogi et al., 2016; Makarov and Clematide, 2018b,a). Lemmatizing in context can improve accuracy on ambiguous and unseen words (Bergmanis and Goldwater, 2018), but most systems for contextsensitive lemmatization must train on complete sentences labeled with POS and/or morphological tags as well as lemmas, and have only been tested with 20k-300k training tokens (Chrupała et al., 2008; M¨uller et al., 2015; Chakrabarty et al., 2017).1 1 The smallest of these corpora contains 20k tokens of Bengali annotated only with lemmas, which Chakrabarty et al. (2017) reported took around two person months to create. Intuitively, though, sentence-annotated data is inefficient for training a lemmatizer, especially in low-resource settings. Training on (say) 1000 word types will provide far more information about a language’s morphology than training on 1000 contiguous tokens, where fewer types are represented. As noted above, sentence data can help with ambiguous and unseen words, but we show here that when data is scarce, this effect"
N19-1418,chrupala-etal-2008-learning,0,0.0477257,"Missing"
N19-1418,E17-2025,0,0.0482656,"Missing"
N19-1418,P13-1057,0,0.0441203,"Missing"
N19-1418,N16-1076,0,0.111572,"ith the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from Wikipedia that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the model successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use context. 1 Introduction Many lemmatizers work on isolated wordforms (Wicentowski, 2002; Dreyer et al., 2008; Rastogi et al., 2016; Makarov and Clematide, 2018b,a). Lemmatizing in context can improve accuracy on ambiguous and unseen words (Bergmanis and Goldwater, 2018), but most systems for contextsensitive lemmatization must train on complete sentences labeled with POS and/or morphological tags as well as lemmas, and have only been tested with 20k-300k training tokens (Chrupała et al., 2008; M¨uller et al., 2015; Chakrabarty et al., 2017).1 1 The smallest of these corpora contains 20k tokens of Bengali annotated only with lemmas, which Chakrabarty et al. (2017) reported took around two person months to create. Intuitiv"
N19-1418,W16-2010,0,0.0565971,"Missing"
N19-1418,P07-2045,0,\N,Missing
N19-1418,D08-1113,0,\N,Missing
N19-1418,D15-1272,0,\N,Missing
N19-1418,E17-3017,0,\N,Missing
N19-1418,K17-2002,1,\N,Missing
N19-1418,N18-1126,1,\N,Missing
N19-1418,D18-1314,0,\N,Missing
N19-1418,W18-6011,0,\N,Missing
P06-1085,P98-2206,0,0.0943693,"Missing"
P06-1085,J01-3002,0,0.142733,"s independent of its local context, while the second incorporates bigram dependencies between adjacent words. The algorithms we use to search for likely segmentations do differ, but so long as the segmentations they produce are close to optimal we can be confident that any differences in the segmentations reflect differences in the probabilistic models, i.e., in the kinds of dependencies between words. We are not the first to propose explicit probabilistic models of word segmentation. Two successful word segmentation systems based on explicit probabilistic models are those of Brent (1999) and Venkataraman (2001). Brent’s ModelBased Dynamic Programming (MBDP) system assumes a unigram word distribution. Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS). Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar. Moreover, the segmentation accuracy of the NGS unigram, bigram, and trigram models hardly differ, suggesting that contextual dependencies are irrelevant to word segmentation. HowDeveloping better methods for segmenting continuous text into wor"
P06-1085,J04-1004,0,0.104648,"Missing"
P06-1085,C98-2201,0,\N,Missing
P07-1094,J92-4003,0,0.486197,"Missing"
P07-1094,W00-0717,0,0.345089,"ork is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods. In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all. We introduce the use of a new information-theoretic criterion, variation of informa"
P07-1094,N06-1041,0,0.614153,"HMM 744 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult"
P07-1094,N07-1018,1,0.110463,"variable, t, from observed data, w, chooses a value of t robust to uncertainty in θ. (a) Posterior distribution on θ given w. (b) Probability that t = 1 given w and θ as a function of θ. preferred; and when β &lt; 1, high probability is assigned to sparse multinomials, where one or more parameters are at or near 0. Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions). Consequently, it makes sense to use a Dirichlet prior with β &lt; 1. However, as noted by Johnson et al. (2007), this choice of β leads to difficulties with MAP estimation. For a sequence of draws x = (x1 , . . . , xn ) from a multinomial distribution θ with observed counts n1 , . . . , nK , a symmetric Dirichlet(β) prior nk +β−1 over θ yields the MAP estimate θk = n+K(β−1) . When β ≥ 1, standard MLE techniques such as EM can be used to find the MAP estimate simply by adding “pseudocounts” of size β − 1 to each of the expected counts nk at each iteration. However, when β &lt; 1, the values of θ that set one or more of the θk equal to 0 can have infinitely high posterior probability, meaning that MAP estim"
P07-1094,P02-1017,0,0.0612523,"ge of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. 1 Introduction Unsupervised learning of linguistic structure is a difficult problem. Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and ∗ This work was supported by grants NSF 0631518 and ONR MURI N000140510388. We would also like to thank Noah Smith for providing us with his data sets. Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging. Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a"
P07-1094,J94-2001,0,0.985138,"find improvements both when training from data alone, and using a tagging dictionary. 1 Introduction Unsupervised learning of linguistic structure is a difficult problem. Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and ∗ This work was supported by grants NSF 0631518 and ONR MURI N000140510388. We would also like to thank Noah Smith for providing us with his data sets. Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging. Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM 744 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); se"
P07-1094,W97-0309,0,0.0933371,"005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods. In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all. We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels. We also"
P07-1094,E95-1020,0,0.971093,"Missing"
P07-1094,P05-1044,0,0.866812,"E) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. 1 Introduction Unsupervised learning of linguistic structure is a difficult problem. Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and ∗ This work was supported by grants NSF 0631518 and ONR MURI N000140510388. We would also like to thank Noah Smith for providing us with his data sets. Before describing our approach in more detail, we briefly review previous work on unsupervised POS"
P08-1044,W96-0213,0,0.0946589,"nels (uh-huh, mm-hm), guesses (where the transcribers were unsure of the correct words), and full words (everything else). Error rates for each of these types can be found in Table 1. The remainder of our analysis considers only the 36159 invocabulary full words in the reference transcriptions (70 OOV full words are excluded). We collected the following features for these words: Speaker sex Male or female. Broad syntactic class Open class (e.g., nouns and verbs), closed class (e.g., prepositions and articles), or discourse marker (e.g., okay, well). Classes were identified using a POS tagger (Ratnaparkhi, 1996) trained on the tagged Switchboard corpus. Log probability The unigram log probability of each word, as listed in the system’s language model. Word length The length of each word (in phones), determined using the most frequent pronunciation BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFr yeah i i i think you should um ask for the ref- recommendation Figure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, and fragments; first, middle, and last words in a repeated sequence. found for that word in the recognition lattices. 3.2 Resu"
P09-2085,D08-1033,0,0.0317874,"Missing"
P09-2085,P07-1035,0,0.0288854,"present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼ DP(α0 , P0 ) ∼G where"
P09-2085,P06-1085,1,0.89977,"we present an efficient method for sampling from the HDP (and related models, such as the hierarchical PitmanYor process) that considerably decreases the memory footprint of such models as compared to the naive implementation. As we have noted, the issues described in this paper apply to models for various kinds of NLP tasks; for concreteness, we will focus on n-gram language modeling for the remainder of the paper, closely following the presentation in GGJ06. The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 2 Introduction The Chinese Restaurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram"
P09-2085,D07-1072,0,0.0157343,"aurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼"
P12-1020,E09-3001,0,0.0289407,"the speech stream, but they are tested on an artificial corpus with only 80 vocabulary items that was constructed so as to “avoid strong word-to-word dependencies” (R¨as¨anen, 2011). Here, we use a naturalistic corpus, demonstrating that lexical-phonetic learning is possible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic in"
P12-1020,W11-0601,0,0.0951404,"Missing"
P12-1020,D08-1113,0,0.214064,"Missing"
P12-1020,P08-1016,0,0.0608466,"sen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these models even maintains an explicit lexicon), nor do they model or learn from phonetic variation in the input. 3 Lexical-phonetic model Our lexical-phonetic model is defined using the standard noisy channel framework: first a sequence of intended word tokens is generated using a language model, and then each token is transformed by a pr"
P12-1020,N06-1041,0,0.0331693,"Missing"
P12-1020,H94-1050,0,0.0377277,"int probability (l, x, r) as p(x)p(l|x)p(r|x) rather than as a leftto-right chain p(l)p(x|l)p(r|x). Given our independence assumption above, these two quantities are mathematically equivalent, so the difference matters only because we are using smoothed estimates. Our factorization leads to a symmetric treatment of left and right contexts, which simplifies implementation: we can store all the context parameters locally as PL (·|x) rather than distributed over various P (x|·). Next, we explain our transducer T . A weighted finite-state transducer (WFST) is a variant of a finitestate automaton (Pereira et al., 1994) that reads an input string symbol-by-symbol and probabilistically produces an output string; thus it can be used to specify a conditional probability on output strings given an input. Our WFST (Figure 3) computes a weighted edit distance, and is implemented using OpenFST (Allauzen et al., 2007). It contains a state for each triplet of (previous, current, next) phones; conditioned on this state, it emits a character output which can be thought of as a possible surface realization of current in its particular environment. The output can be the empty string , in which case current is deleted. T"
P12-1020,W10-2902,0,0.0286415,"nd five manner values (stop, nasal stop, fricative, vowel, other). Empty segments like  and • are assigned a special value “no-value” for all features. 187 Figure 4: Some features generated for (•, D, i) → d. Each black factor node corresponds to a positional template. The features instantiated for the (curr)→out and →out template are shown in full, and we show some of the features for the (curr,next)→out template. tic Optimality Theory models (Goldwater and Johnson, 2003; Hayes and Wilson, 2008). 4 Inference Global optimization of the model posterior is difficult; instead we use Viterbi EM (Spitkovsky et al., 2010; Allahverdyan and Galstyan, 2011). We begin with a simple initial transducer and alternate between two phases: clustering together surface forms, and reestimating the transducer parameters. We iterate this procedure until convergence (when successive clustering phases find nearly the same set of merges); this tends to take about 5 or 6 iterations. In our clustering phase, we improve the model posterior as much as possible by greedily making type merges, where, for a pair of intended word forms u and v, we replace all instances of xi = u with xi = v. We maintain the invariant that each intende"
P12-1020,P06-1124,0,0.323419,"x1 . . . xn . As shown in Figure 2, si is produced from xi by a transducer T : si ∼ T (xi ), which models phonetic changes. Each xi is sampled from a distribution θ which represents word frequencies, and its left and right context words, li and ri , are drawn from distributions conditioned on xi , in order to capture information about the environments in which xi appears: li ∼ PL (xi ), ri ∼ PR (xi ). Because the number of word types is not known in advance, θ is drawn from a Dirichlet process DP (α), and PL (x) and PR (x) have Pitman-Yor priors with concentration parameter 0 and discount d (Teh, 2006). 186 Our generative model of xi is unusual for two reasons. First, we treat each xi independently rather than linking them via a Markov chain. This makes the model deficient, since li overlaps with xi−1 and so forth, generating each token twice. During inference, however, we will never compute the joint probability of all the data at once, only the probabilities of subsets of the variables with particular intended word forms u and v. As long as no two of these words are adjacent, the deficiency will have no effect. We make this independence assumption for computational reasons—when deciding w"
P12-1020,P08-2042,0,0.121566,"ible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these mo"
P12-1020,J01-3002,0,0.178409,"iation (many English vowels are reduced to [@] in unstressed positions) and the context—if the next word is “want”, “you” is a plausible choice. To date, most models of infant language learning have focused on either lexicon-building or phonetic learning in isolation. For example, many models of word segmentation implicitly or explicitly build a lexicon while segmenting the input stream of phonemes into word tokens; in nearly all cases the phonemic input is created from an orthographic transcription using a phonemic dictionary, thus abstracting away from any phonetic variability (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Goldwater et al., 2009, among others). As illustrated in Figure 1, these models attempt to infer line (a) from line (d). However, (d) is an idealization: real speech has variability, and behavioral evidence suggests that infants are still learning about the phonetics and phonology of their language even after beginning to segment words, rather than learning to neutralize 184 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184–193, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics the variat"
P12-1051,D11-1131,1,0.826426,"Missing"
P12-1051,N10-1068,0,0.0115772,"g by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framework for principled handling of data sparsity and 488 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics prior knowledge. Graehl et al. (2008) present an EM training procedure for top down tree transducers, but while there are Bayesian approaches to string transducers (Chiang et al., 2010) and PCFGs (Kurihara and Sato, 2006), there has yet to be a proposal for Bayesian inference in tree transducers. Our variational algorithm produces better semantic parses than EM while remaining general to a broad class of transducers appropriate for other domains. In short, our contributions are three-fold: we present a new state-of-the-art semantic parsing model, propose a broader theory for tree transformation based semantic parsing, and present a general inference algorithm for the tree transducer framework. We recommend the last of these as just one benefit of working within a general the"
P12-1051,C69-0101,0,0.627125,"). We indicate states using all capital letters: NUM → population(PLACE). Intuitively, an RTG is a CFG where the yield of every parse is itself a tree. In fact, for any CFG G, it 1 See Liang et al. (2011) for work in representing lambda calculus expressions with trees. 489 is straightforward to produce a corresponding RTG that generates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right"
P12-1051,N04-1035,0,0.0212804,"ransformative machine that takes one as input and produces another as output. Different semantic parsing approaches have taken one or the other view, and both can be captured in this single framework. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. The most significant difference from our approach is that they use machine translation techniques for automatically extracting rules from parallel corpora; similar techniques can be applied to tree transducers (Galley et al., 2004). In fact, synchronous grammars and tree transducers can be seen as instances of the same more general class of automata (Shieber, 3 The addition of W symbols is a convenience; it is easier to design transducer rules where every substring on the right side corresponds to a subtree on the left. Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to substrings of the output and are removed in a post-processing step. (a) The (M"
P12-1051,W04-3312,0,0.198227,"Missing"
P12-1051,W05-0602,0,0.620725,"e mapping. Introduction Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unc"
P12-1051,J08-3004,0,0.145628,"Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framewor"
P12-1051,P06-1115,0,0.788705,"on Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments i"
P12-1051,D10-1119,1,0.506978,"ith the most probable derivation. 8 Experimental setup and evaluation We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. We present here additional translations of the full 880 sentences into 4 Because of the resemblance to EM, this procedure has been called VBEM. Unlike EM, however, this procedure alternates between two estimation steps and has no maximization step. 494 German, Greek, and Thai. For evaluation, following from Kwiatkowski et al. (2010), we reserve 280 sentences for test and train on the remaining 600. During development, we use cross-validation on the 600 sentence training set. At test, we run once on the remaining 280 and perform 10 fold cross-validation on the 250 sentence sets. To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard. We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences"
P12-1051,P01-1067,0,0.0752881,"alize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of"
P12-1051,N06-1056,0,\N,Missing
P12-1051,D08-1082,0,\N,Missing
P12-1051,P11-1060,0,\N,Missing
P14-1101,D10-1056,1,0.899207,"Missing"
P14-1101,D07-1043,0,0.0123358,"my, peach, cookie, daddy, bib in one topic, shoe, let, put, hat, pants in another). The word-topic assignments are used to calculate unsmoothed situation-topic distributions θ used by the TLD model. 6.5 Evaluation We evaluate against adult categories, i.e., the ‘goldstandard’, since all learners of a language eventually converge on similar categories. (Since our model is not a model of the learning process, we do not compare the infant learning process to the learning algorithm.) We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; Rosenberg and Hirschberg, 2007).6 VM is the harmonic mean of two components, similar to F-score, where the components (VC and VH) are measures of cross entropy between the gold and model categorization. 6 Other clustering measures, such as 1-1 matching and pairwise precision and recall (accuracy and completeness) showed the same trends, but VM has been demonstrated to be the most stable measure when comparing solutions with varying numbers of clusters (Christodoulopoulos et al., 2010). 1079 200 400 600 85 F1 VM 90 80 LD-all TLD-all LD-w TLD-w 75 24 Cons 1000 1200 3500 15 Cons Dataset 6 Cons Figure 3: Vowel evaluation. ‘all’"
P14-1101,D13-1005,1,\N,Missing
P14-1101,P06-1124,0,\N,Missing
P14-2044,P10-1132,0,0.0174697,". A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features."
P14-2044,W13-3520,0,0.0466473,"Missing"
P14-2044,N10-1083,0,0.135339,"Missing"
P14-2044,P06-3002,0,0.0580063,"Missing"
P14-2044,P11-1087,0,0.129669,"Missing"
P14-2044,N06-1041,0,0.0466985,"Missing"
P14-2044,D13-1169,0,0.0235492,"Missing"
P14-2044,D10-1083,0,0.0183087,"hristodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linea"
P14-2044,W13-3512,0,0.0246285,"be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distanc"
P14-2044,N13-1090,0,0.0350707,"of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model"
P14-2044,petrov-etal-2012-universal,0,0.0558409,"Missing"
P14-2044,D10-1056,1,0.856919,"unction, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear,"
P14-2044,W09-1121,0,0.0417273,"Missing"
P14-2044,D11-1059,1,0.889971,"et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS indu"
P14-2044,D07-1043,0,0.363702,"Missing"
P14-2044,E03-1009,0,0.0278881,"h syntactic function, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and mor"
P14-2044,P12-1020,1,0.853847,"following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest"
P14-2044,erjavec-2004-multext,0,0.126496,"clusters. The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2 , which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning. We apply our model to the English section of the the Multext-East corpus (Erjavec, 2004) in order to evaluate both against the coarse-grained and 265 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265–271, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics fine-grained tags, where the fine-grained tags encode detailed morphological classes. We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches. 2 Related work Unsupervised POS tagging has a long history in NLP. This paper focuses on the"
P14-2044,N12-1045,1,0.820633,"Missing"
P14-2044,E12-1003,0,0.0244364,", the probability of customer i following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from among"
P14-2044,P10-1040,0,0.00921189,"es can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Bl"
P14-2044,D09-1071,0,0.0418626,"Missing"
P14-2044,D12-1086,0,0.019225,"ty tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS induction, but they use morphology in the likelihood term of a parametric sequence model, th"
P19-1376,N18-1126,1,0.852463,"tially follow K&C in computing the Spearman (rank) correlation against the production probabilities, and later also examine Pearson (linear) correlations and ratings data. 3 3.1 Methods Model and hyperparameters We adopt the encoder-decoder architecture used by K&C, as well as their implementation framework and hyperparameters. Encoder-decoder models are a type of recurrent neural network (RNN) introduced for machine translation (Sutskever et al., 2014) but also often used for other sequence-tosequence transductions, such as morphological inflection and lemmatization (Kann and Sch¨utze, 2016; Bergmanis and Goldwater, 2018). The encoder is an RNN that reads in the input sequence (here, a sequence of characters representing the phonemes in the present tense verb form) and creates a fixed-size vector representation of it. The decoder is another RNN that takes this vector as input and decodes it sequentially, outputting one symbol at each timestep (here, the phonemes of the past tense form). The ED model with attention (Bahdanau et al., 2015) is implemented in OpenNMT (Klein et al., 2017).2 It has two bidirectional LSTM encoder layers and two LSTM decoder layers, 300dimensional character embeddings in the encoder,"
P19-1376,W16-2010,0,0.0554699,"Missing"
P19-1376,P17-4012,0,0.0373921,"other sequence-tosequence transductions, such as morphological inflection and lemmatization (Kann and Sch¨utze, 2016; Bergmanis and Goldwater, 2018). The encoder is an RNN that reads in the input sequence (here, a sequence of characters representing the phonemes in the present tense verb form) and creates a fixed-size vector representation of it. The decoder is another RNN that takes this vector as input and decodes it sequentially, outputting one symbol at each timestep (here, the phonemes of the past tense form). The ED model with attention (Bahdanau et al., 2015) is implemented in OpenNMT (Klein et al., 2017).2 It has two bidirectional LSTM encoder layers and two LSTM decoder layers, 300dimensional character embeddings in the encoder, and 100-dimensional hidden layers in the encoder and decoder. The Adadelta optimizer (Zeiler, 2012) is used for training, with the default beam size of 12 for decoding. The batch size is 20, and dropout is applied between layers with a probability of 0.3. Except where otherwise noted below, all models were trained for 100 epochs. 3.2 Training data To compare our results to both A&H and K&C, we use their corresponding training sets, both based on data from CELEX (Baay"
P19-1376,Q18-1045,0,\N,Missing
Q13-1006,W12-1913,1,0.886164,"Missing"
Q13-1006,N09-1009,0,0.0308058,"elines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark. c Submitted 9/2012; Published 3/2013. 2013 Association for Computational Linguist"
Q13-1006,D11-1005,0,0.0258363,"upport the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark. c Submitted 9/2012; Published 3/2013. 2013 Association for Computational Linguistics. To our knowledge"
Q13-1006,N09-1012,0,0.175603,"-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from"
Q13-1006,N10-1005,0,0.0221363,"011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huang and Harper (2010) used annotated prosodic breaks as a kind of punctuation in a supervised PCFG, while prosodic breaks learned in a semi-supervised way have been used as features for parse reranking (Kahn et al., 2005) or PCFG statesplitting (Dreyer and Shafran, 2007). In contrast to these methods, our approach observes neither parse trees nor prosodic annotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) becau"
Q13-1006,W07-2416,0,0.0160657,"f an an UNK’d word in the model. αUNK affects only dependents in Pchoose , and treats a dependent as UNK iff it did not occur on that particular side of that particular head word in any sentence. We used both global UNK cutoffs (optimized on the dev set) and these αUNK hyperparameters. wsj10 brent swbdnxt10 Word tokens Word types Sentences Word tokens Word types Sentences Word tokens Word types Sentences Train 42,505 7,804 6,007 24,998 2,647 3,998 20,954 1,390 6,249 Dev 1,765 818 233 2,980 760 488 2,127 482 424 Test 2,571 1,134 357 3,052 767 491 2,206 488 449 to-dependency” conversion tool of Johansson and Nugues (2007) to obtain high-quality CoNLL-style dependency parses. 4.1.2 swbdnxt10 Table 1: Statistics for our three corpora. Proot over both streams jointly and independently, respectively. 4 Experimental Setup 4.1 Datasets We evaluate on three datasets: wsj10, sentences of length 10 or less from the Wall Street Journal portion of the Penn Treebank; swbdnxt10, sentences of length 10 or less from the Switchboard dataset of ADS used by Pate and Goldwater (2011); and brent, part of the Brent corpus of CDS (Brent and Siskind, 2001). Table 1 presents corpus statistics. 4.1.1 wsj10 We present a new evaluation"
Q13-1006,D07-1031,0,0.0481192,"to distributions that condition on only the head POS. In the equation above, λ is a scalar parameter. However, it actually specifies a probability distribution over the decision to back off (B) or not back off (¬B), and we can use different notation to reflect this view. Specifically, λstop (·) and λchoose (·) will represent our backoff distributions for the Stop and Choose decision, respectively. Using hp and dp to represent head and dependent POS tag and hw and dw to represent head and dependent word, one of the models Headden et al. explored estimates: Pˆ choose (dp |hw , hp , dir , val) = Johnson, 2007), which finds an approximation to the posterior using an iterative EM-like algorithm. In the E-step of VBEM, expected counts E(ri ) are gathered for each latent variable using the Inside-Outside algorithm, exactly as in the E-step of traditional EM. The Maximization step differs from the M-Step of EM in two ways. First, the expected counts for each value of the latent variable ri are incremented by the hyperparameter αi . Second, the numerator and denominator are scaled by the function exp(ψ(·)), which reduces the probability of rare events. Specifically, the Pchoose distribution is estimated"
Q13-1006,H05-1030,0,0.0569873,"words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huang and Harper (2010) used annotated prosodic breaks as a kind of punctuation in a supervised PCFG, while prosodic breaks learned in a semi-supervised way have been used as features for parse reranking (Kahn et al., 2005) or PCFG statesplitting (Dreyer and Shafran, 2007). In contrast to these methods, our approach observes neither parse trees nor prosodic annotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) because they provide information about prosodic structure, but because they are a direct cue to syntactic predictability. It is well-established that talkers tend to pronounce words more quickly when they"
Q13-1006,P04-1061,0,0.535188,"ause prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. This paper explores the hypothesis that word duration can help with lear"
Q13-1006,W11-0603,1,0.715973,"nd many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per"
Q13-1006,P11-1108,0,0.378286,"that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark. c Su"
Q13-1006,H91-1073,0,0.666995,"nts exploit this correlation. Specifically, if infants can learn about prosodic phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. 64 tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the proso"
Q13-1006,P11-1067,0,0.0245326,"Missing"
Q13-1006,P07-1049,0,0.571883,"cted and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics,"
Q13-1006,D11-1118,0,0.206549,"directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action E"
Q13-1006,W11-0303,0,0.0352405,"directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) ‡ 1 By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action E"
Q13-1006,N10-1083,0,\N,Missing
Q13-1021,N09-1019,0,0.0108841,"g., a common prefix) will tend to be parsed consistently, as this permits the model to treat the subtree spanning that string as a cached subtree, assigning it higher probability than under the PCFG distribution. Adaptor Grammars have been applied to a wide variety of tasks, including segmenting utterances into words (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010), classifying documents according to perspective (Hardisty et al., 2010), machine transliteration of names (Huang et al., 2011), native language identification (Wong et al., 2012), and named entity clustering (Elsner et al., 2009). There have also been AG experiments with morphological segmentation, but more as a proof of concept than an attempt to achieve state-of-the-art results (Johnson et al., 2007; Johnson, 2008b). grammar SubMorphs: 3 For capturing the rules of morphotactics, a grammar with linguistically motivated non-terminals can be created. There are many plausible options and the best-performing grammar may be somewhat language-dependent. Rather than experimenting extensively, we designed our third grammar to replicate as closely as possible the grammar that is implicitly implemented in the Morfessor system."
Q13-1021,J01-2001,0,0.894796,"systems may not be ideal if even a small amount of segmented training data is Adaptor Grammars (AGs) are a nonparametric Bayesian modelling framework that can learn latent tree structures over an input corpus of strings. For example, they can be used to define a morphological grammar where each word consists of zero or more prefixes, a stem, and zero or more suffixes; the actual forms of these morphs (and the segmentation of words into morphs) are learned from the data. In this general approach AGs are similar to many other unsupervised morphological segmentation systems, such as Linguistica (Goldsmith, 2001) and the Morfessor family (Creutz and Lagus, 2007). A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily. For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 201"
Q13-1021,D10-1028,0,0.0163155,"es (and all hyperparameters of the model, including PCFG probabilities in the base distribution and PYP hyperparameters). Any frequently recurring substring (e.g., a common prefix) will tend to be parsed consistently, as this permits the model to treat the subtree spanning that string as a cached subtree, assigning it higher probability than under the PCFG distribution. Adaptor Grammars have been applied to a wide variety of tasks, including segmenting utterances into words (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010), classifying documents according to perspective (Hardisty et al., 2010), machine transliteration of names (Huang et al., 2011), native language identification (Wong et al., 2012), and named entity clustering (Elsner et al., 2009). There have also been AG experiments with morphological segmentation, but more as a proof of concept than an attempt to achieve state-of-the-art results (Johnson et al., 2007; Johnson, 2008b). grammar SubMorphs: 3 For capturing the rules of morphotactics, a grammar with linguistically motivated non-terminals can be created. There are many plausible options and the best-performing grammar may be somewhat language-dependent. Rather than ex"
Q13-1021,P11-2094,0,0.0280555,"robabilities in the base distribution and PYP hyperparameters). Any frequently recurring substring (e.g., a common prefix) will tend to be parsed consistently, as this permits the model to treat the subtree spanning that string as a cached subtree, assigning it higher probability than under the PCFG distribution. Adaptor Grammars have been applied to a wide variety of tasks, including segmenting utterances into words (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010), classifying documents according to perspective (Hardisty et al., 2010), machine transliteration of names (Huang et al., 2011), native language identification (Wong et al., 2012), and named entity clustering (Elsner et al., 2009). There have also been AG experiments with morphological segmentation, but more as a proof of concept than an attempt to achieve state-of-the-art results (Johnson et al., 2007; Johnson, 2008b). grammar SubMorphs: 3 For capturing the rules of morphotactics, a grammar with linguistically motivated non-terminals can be created. There are many plausible options and the best-performing grammar may be somewhat language-dependent. Rather than experimenting extensively, we designed our third grammar"
Q13-1021,C10-1060,0,0.147266,"stica (Goldsmith, 2001) and the Morfessor family (Creutz and Lagus, 2007). A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily. For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010). These word segmentation papers demonstrated both the power of the AG approach and the synergistic behavior that occurs when learning multiple levels of structure simultaneously. However, the bestperforming grammars were selected using the same corpus that was used for final testing, and each paper dealt with only one language. The ideal unsupervised learner would use a single grammar tuned on 255 Transactions of the Association for Computational Linguistics, 1 (2013) 255–266. Action Editor: Kristina Toutanova. c Submitted 11/2012; Published 5/2013. 2013 Association for Computational Linguist"
Q13-1021,N09-1036,1,0.932966,"ation systems, such as Linguistica (Goldsmith, 2001) and the Morfessor family (Creutz and Lagus, 2007). A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily. For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010). These word segmentation papers demonstrated both the power of the AG approach and the synergistic behavior that occurs when learning multiple levels of structure simultaneously. However, the bestperforming grammars were selected using the same corpus that was used for final testing, and each paper dealt with only one language. The ideal unsupervised learner would use a single grammar tuned on 255 Transactions of the Association for Computational Linguistics, 1 (2013) 255–266. Action Editor: Kristina Toutanova. c Submitted 11/2012; Published 5/2013. 2013 Association"
Q13-1021,W08-0704,0,0.801535,"ological segmentation systems, such as Linguistica (Goldsmith, 2001) and the Morfessor family (Creutz and Lagus, 2007). A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily. For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010). These word segmentation papers demonstrated both the power of the AG approach and the synergistic behavior that occurs when learning multiple levels of structure simultaneously. However, the bestperforming grammars were selected using the same corpus that was used for final testing, and each paper dealt with only one language. The ideal unsupervised learner would use a single grammar tuned on 255 Transactions of the Association for Computational Linguistics, 1 (2013) 255–266. Action Editor: Kristina Toutanova. c Submitted 11/2012; Publ"
Q13-1021,P08-1046,0,0.0257697,"ological segmentation systems, such as Linguistica (Goldsmith, 2001) and the Morfessor family (Creutz and Lagus, 2007). A major difference, however, is that the morphological grammar is specified as an input to the program, rather than hard-coded, which allows different grammars to be explored easily. For the task of segmenting utterances into words, for example, Johnson and colleagues have experimented with grammars encoding different kinds of sub-word and super-word structure (e.g., syllables and collocations), showing that the best grammars far outperform other systems on the same corpora (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010). These word segmentation papers demonstrated both the power of the AG approach and the synergistic behavior that occurs when learning multiple levels of structure simultaneously. However, the bestperforming grammars were selected using the same corpus that was used for final testing, and each paper dealt with only one language. The ideal unsupervised learner would use a single grammar tuned on 255 Transactions of the Association for Computational Linguistics, 1 (2013) 255–266. Action Editor: Kristina Toutanova. c Submitted 11/2012; Publ"
Q13-1021,W10-2210,0,0.178756,"hen replaced with their mappings and based on that, F1-score on matching segments is calculated. Using EMMA we can evaluate the induced segmentations of German words against gold standard analyses. EMMA has a freely available implementation,9 but is slow to compute because it uses Integer Linear Programming. For our dev results, we computed both scores using the entire dev set, but for the large test sets, the evaluation is done on batches of 1000 word types selected randomly from the test set. This procedure is repeated 10 times and the average is reported, just as in the MC2010 competition (Kohonen et al., 2010a). 4.3 Baseline Models We compare our AG models to several other morphology learning systems. We were able to obtain implementations of two of the best unsupervised systems from MC2010, Morfessor (Creutz and Lagus, 2007) and Morsel (Lignos, 2010), and we use these for comparisons on both the dev and test sets. We also report test results from MC2010 for the only semi-supervised system in the competition, semisupervised Morfessor (Kohonen et al., 2010a; Kohonen et al., 2010b). No dev results are reported on this system since we were unable to obtain an implementation. This section briefly revi"
Q13-1021,W11-0301,0,0.504141,"th AG Select and semisupervised AG are computationally more demanding than the comparison systems. Since we do inference over tree structures, the complexity is cubic in the input word length, while most segmentation systems are quadratic or linear. Even compared to the unsupervised AG, AG Select is more expensive, because of the larger grammar and number of cached symbols. Nevertheless, our systems can feasibly be run on the large Morpho Challenge datasets. Other recent unsupervised systems have reported state-of-the art results by incorporating additional information from surrounding words (Lee et al., 2011), multilingual alignments (Snyder and Barzilay, 2008), or overlapping context features in a log-linear model (Poon et al., 2009), but they have only been run on Semitic languages and English (and in the latter case, a very small corpus). Since they explicitly enumerate and sample from all possible segmentations of each word (often with some heuristic constraints), they could have trouble with the much longer words of the agglutinative languages tested here. In any case the results are not directly comparable to ours. 6 Conclusion In this paper we have introduced three new methods for Adaptor G"
Q13-1021,N09-1024,0,0.554997,"ree structures, the complexity is cubic in the input word length, while most segmentation systems are quadratic or linear. Even compared to the unsupervised AG, AG Select is more expensive, because of the larger grammar and number of cached symbols. Nevertheless, our systems can feasibly be run on the large Morpho Challenge datasets. Other recent unsupervised systems have reported state-of-the art results by incorporating additional information from surrounding words (Lee et al., 2011), multilingual alignments (Snyder and Barzilay, 2008), or overlapping context features in a log-linear model (Poon et al., 2009), but they have only been run on Semitic languages and English (and in the latter case, a very small corpus). Since they explicitly enumerate and sample from all possible segmentations of each word (often with some heuristic constraints), they could have trouble with the much longer words of the agglutinative languages tested here. In any case the results are not directly comparable to ours. 6 Conclusion In this paper we have introduced three new methods for Adaptor Grammars and demonstrated their usefulness for minimally supervised morphological segmentation. First, we showed that AG models c"
Q13-1021,P08-1084,0,0.230467,"tationally more demanding than the comparison systems. Since we do inference over tree structures, the complexity is cubic in the input word length, while most segmentation systems are quadratic or linear. Even compared to the unsupervised AG, AG Select is more expensive, because of the larger grammar and number of cached symbols. Nevertheless, our systems can feasibly be run on the large Morpho Challenge datasets. Other recent unsupervised systems have reported state-of-the art results by incorporating additional information from surrounding words (Lee et al., 2011), multilingual alignments (Snyder and Barzilay, 2008), or overlapping context features in a log-linear model (Poon et al., 2009), but they have only been run on Semitic languages and English (and in the latter case, a very small corpus). Since they explicitly enumerate and sample from all possible segmentations of each word (often with some heuristic constraints), they could have trouble with the much longer words of the agglutinative languages tested here. In any case the results are not directly comparable to ours. 6 Conclusion In this paper we have introduced three new methods for Adaptor Grammars and demonstrated their usefulness for minimal"
Q13-1021,C10-1116,0,0.22866,"texts of a mixed corpus of Estonian.7 Gold standard segmentations of some of these words are available from the Estonian morphologically disambiguated corpus;8 we used these for the test set, with small subsets selected randomly for the labelled and dev sets. For semi-supervised tests of the AG Compounding grammar we annotated the morphemes in the English, Finnish and Estonian labelled sets as prefixes, stems or suffixes. We could not do so for Turkish because none of the authors knows Turkish. 4.2 Evaluation We evaluate our results with two measures: segment border F1-score (SBF1) and EMMA (Spiegler and Monson, 2010). SBF1 is one of the simplest and most popular evaluation metrics for morphological segmentations. It computes F1-score from the precision and recall of ambiguous segment boundaries— i.e., word edges are not counted. It is easy and quick to compute but has the drawback that it gives no credit for one-morpheme words that have been segmented correctly (i.e., are assigned no segment borders). Also it can only be used on systems and gold standards where the output is just a segmentation of the surface string (e.g., availabil+ity) rather than a morpheme analysis (e.g., available+ity). For this reas"
Q13-1021,D12-1064,0,0.0264428,"rameters). Any frequently recurring substring (e.g., a common prefix) will tend to be parsed consistently, as this permits the model to treat the subtree spanning that string as a cached subtree, assigning it higher probability than under the PCFG distribution. Adaptor Grammars have been applied to a wide variety of tasks, including segmenting utterances into words (Johnson, 2008a; Johnson and Goldwater, 2009; Johnson and Demuth, 2010), classifying documents according to perspective (Hardisty et al., 2010), machine transliteration of names (Huang et al., 2011), native language identification (Wong et al., 2012), and named entity clustering (Elsner et al., 2009). There have also been AG experiments with morphological segmentation, but more as a proof of concept than an attempt to achieve state-of-the-art results (Johnson et al., 2007; Johnson, 2008b). grammar SubMorphs: 3 For capturing the rules of morphotactics, a grammar with linguistically motivated non-terminals can be created. There are many plausible options and the best-performing grammar may be somewhat language-dependent. Rather than experimenting extensively, we designed our third grammar to replicate as closely as possible the grammar that"
U11-1005,D11-1131,1,0.836476,"example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learning a mapping between trees, one of which may be latent. This mapping can"
U11-1005,J93-2003,0,0.0166146,"tup We use Tiburon (May and Knight, 2006), a tree transducer toolkit, to train our transducer using 40 iterations of its inside-outside-like EM training procedure, and modify it slightly to include the mean field VB approximation for a symmetric Dirichlet prior over the multinomial parameters as just described. Decoding is handled the same by Tiburon for both training procedures, producing the MR input tree with the tree transducer derivation that maximizes the probability over derivations of equation 2. In keeping with the original hybrid tree, we run 100 iterations of IBM alignment model 1 (Brown et al., 1993) to initialize the word distribution parameters. Also in keeping with Lu et al. (2008), we use the standard noun phrase list from the given language to help initialize the word distributions for their counterparts in the meaning representation language. 9 Results To evaluate our models, we use the the GeoQuery corpus, a standard benchmark data set. The corpus contains English sentences (questions about U.S. geography) paired with an MR in a database query language, 250 of which were translated into Japanese (among other languages) yielding two training sets using the same MRs. For testing we r"
U11-1005,J08-3004,0,0.257175,"1970) are generalizations of finite state machines that take trees as inputs and either output a string or another tree. Mirroring the branching nature of its input, the tree transducer may simultaneously transition to any number of successor states, assigning a separate state to process each sub-tree. Although they were originally conceived of by Rounds (1970) as a way to formalize tree transformations in linguistic theory, they have since received far more interest in theoretical computer science. Recently, however, they have also been used for syntax-based statistical machine translation (Graehl et al., 2008; Knight and Greahl, 2005). Figure 1 presents an example of a tree-to-tree transducer. It is defined using tree transformation rules, where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right hand side describes a fragment of the output tree. Variables xi stand for entire sub-trees. There are many classes of transducer, each with its own selection of algorithms (Knight and Greahl, 2005). In this paper we restrict consideration primarily to the extended left hand side, root-to-frontier, linear, nondeleting tree transducers (Maletti et al., 200"
U11-1005,P06-1115,0,0.0285966,"nguage sentences to formal representations of their meaning, a problem that arises in developing natural language interfaces, for example. Given a set of (sentence, meaning representation) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are"
U11-1005,D10-1119,1,0.915678,"ample. Given a set of (sentence, meaning representation) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learnin"
U11-1005,P11-1060,0,0.0143339,"n a standard PCFG) distinguishes between functions and predicates with the same name but different semantics (Wong and Mooney, 2006). To formally define the probability of the MR, let paths be the set of paths from the root to every node in the MR where paths are represented using a variety of Gorn’s notation (Gorn, 1962)2 . Let argsi be the set of indices of the children of the node at path i; and Ri be the grammar rule that derives the symbol at i according to the MR parse. Then, the following 1 With a pre-parsing step, it may also be possible to represent lambda expressions with trees (see Liang et al. (2011)). 2 I.e., paths are represented by strings where the empty string ǫ is the path to the root, and if i is a path and j is the index of a child of the node at i, i · j is the path to that child. 21 equation defines P (MR). Y P (MR) =P (Rǫ ) Y P (Ri·j |j, Ri ) (1) i∈paths j∈argsi In other words, each node in the tree is generated according to the probability of the MR rule that derives it conditioned on (1) the MR rule Ri that derives its parent symbol and (2) its position j beneath that parent. The hybrid tree model then re-orders and extends this basic skeleton to include the NL. The probabili"
U11-1005,D08-1082,0,0.103566,"n) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learning a mapping between trees, one of which may"
U11-1005,C69-0101,0,0.637729,"frontier, linear, non-deleting, tree-to-tree transducer (a) and an example derivation (b). Numbered arrows in the derivation indicate which rules apply during that step. Rule [1] is the only rule with an extended left hand side. framework, and add a small extension, made easy by the framework. We also update a standard tree transducer training algorithm to incorporate a Variational Bayes approximation. The result is the first purely generative model to achieve state-of-the-art results on a standard data set. 2 Extended, root-to-frontier, linear, non-deleting tree transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that take trees as inputs and either output a string or another tree. Mirroring the branching nature of its input, the tree transducer may simultaneously transition to any number of successor states, assigning a separate state to process each sub-tree. Although they were originally conceived of by Rounds (1970) as a way to formalize tree transformations in linguistic theory, they have since received far more interest in theoretical computer science. Recently, however, they have also been used for syntax-based statistical machine tr"
U11-1005,N06-1056,0,\N,Missing
W00-0312,P93-1008,1,0.825914,"CommandTalk agents can be found in Moore et al. (1997). CommandTalk's dialogue component is described in detail in Stent et al. (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME's), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. We used this unification grammar for both natural language understanding and generation, and, using a grammar compiler we developed, compiled it into a context-free form suitable for the speech recognizer as well. The effe~s_ of this single-grammar approach on the robustness of the CommandTalk system were twofold. On the negative side, we presumably ended up with a recognition language model with less coverage than a statistical model would have had. Our attempts to deal with this are discussed in the next section. On the positive side, we eliminated th"
W00-0312,A97-1001,0,0.0239234,"ns contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either express or implied, of the Defense Advanced Research Projects Agency of the U.S. Government. ? Currently affiliated with GO.corn 61 CommandTalk consists of a number of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk's dialogue component is described in detail in Stent et al. (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME's), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar for"
W00-0312,C00-2097,1,0.760851,"for our parsing a-nd recognition robustness methods. The one-grammar approach is key to our eliminating the necessity for robust parsing, renders a large corpus for generating a recognition model unnecessary, and has other advantages as well. Yet our experience in the ATIS domain suggests that further research into this approach is needed. Our ATIS grammar is based on a grammar of general English and has a very different structure from that of CommandTalk's semantic grammar, but we were unable to isolate the factor or factors responsible for its poor recognition performance. Recent research (Rayner et al., 2000) suggests that it may be possible to compile a useful recognition model from a general English unification grammar if the grammar is constructed carefully and a few compromises are made. We also believe that using an appropriate grammar approximation algorithm to reduce the complexity of the recognition model may prove fruitful. This would reintroduce some discrepancy between the recognition and understanding language models, but maintain the other advantages of the one-grammar approach. In either case, the effectiveness of our recognition robustness techniques remains an open question. We kno"
W00-0312,P99-1024,1,0.80656,"eted as necessarily representing the official policies, either express or implied, of the Defense Advanced Research Projects Agency of the U.S. Government. ? Currently affiliated with GO.corn 61 CommandTalk consists of a number of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk's dialogue component is described in detail in Stent et al. (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME's), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. We used this unification grammar for both natural language understanding"
W00-0312,P94-1011,0,0.0204375,"In either case, the effectiveness of our recognition robustness techniques remains an open question. We know they have no significant negative impact on ingrammar recognition, but whether they are helpful in recognizing and~ more importantly, interpreting out-of-grammar utterances is unknown. We have been unable to evaluate them so far in the CommandTalk or any other domain, although we hope to do so in the future. Another possible solution to the problem of producing a workable robust recognition grammar would return to a statistical approach rather than using word insertions and deletions. Stolcke and Segal (1994) describe a method for combining a context-free grammar with an n-gram model generated from a small corpus of a few hundred utterances to create a more accurate n-gram model. This method would provide a robust recognition model based on the context-free grammar compiled from 64 our unification grammar. We would'still have to write only one grammar for the system, it would still influence the recognition model, and we could still be sure that the system would never say anything it couldn't recognize. This approach Would require using robust parsing methods, but might be the best solution for ot"
W00-0312,H93-1008,1,\N,Missing
W04-0105,W02-0603,0,0.0527446,"ly applicable, and possibly more successful, with phonological transcriptions. However, since we wish to have an entirely unsupervised system and we require a morphological segmentation as input, we are currently limited by the capabilities of Linguistica, which requires standard textual input. For the remainder of this paper, we use “phonology” and “phonological rules” in a broad sense to include orthography as well.    lift            jump  roll ×            walk  ...  −s −ed −ing ...          Figure 1: An example signature segmentation (Goldsmith, 2001; Creutz and Lagus, 2002), discovery of syllabicity and sonority (Ellison, 1993), and learning constraints on vowel harmony and consonant clusters (Ellison, 1994). However, our work shows that a straightforward MDL approach, where the prior − log Pr(H) depends on the length of the phonological rules and the rest of the grammar in the obvious way, does not result in a successful system for learning phonological rules. We discuss why this is so, and then present several changes that can be made to the prior in order to learn phonological rules successfully. Our conclusion is that, although Bayesian techniques can be suc"
W04-0105,J01-2001,0,0.609483,"etermine the prior probabilities of various hypotheses. In this paper, we compare the results of using two different prior distributions for an unsupervised learning task in the domain of morpho-phonology. Our goal is to learn transformation rules of the form x → y / C, where x and y are individual characters (or the empty character ) and C is some representation of the context licensing the transformation. Our input is an existing segmentation of words from the Penn Treebank (Marcus et al., 93) into stems and suffixes. This segmentation is provided by the Linguistica morphological analyzer (Goldsmith, 2001; Goldsmith, 2004b), itself an unsupervised algorithm. Using the transformation rules we learn, we are able to output a new segmentation that more closely matches our linguistic intuitions. 1 We are not the first to apply Bayesian learning techniques for unsupervised learning of morphology and phonology. Several other researchers have also pursued these methods, usually within a Minimum Description Length (MDL) framework (Rissanen, 1989). In MDL approaches, − log Pr(H) is taken to be proportional to the length of H in some standard encoding, and − log Pr(D|H) is the length of D using the encod"
W04-0105,P84-1070,1,0.76251,"of similar signature pairs, the rules relating them, and the possible contexts for those rules, we need to determine which rules are actually phonologically legitimate and which are simply accidents of the data. We do this by simply considering each rule and context in turn, proceeding from the most attested to least attested rules and from most likely to least likely contexts. For each rule-context pair, we add the rule to the grammar 4 The reasoning we use to finding conditioning contexts for deletion rules was also described by Goldsmith (2004a), and is similar to the much earlier work of Johnson (1984). FIND P HONO RULES () 1 G ← grammar produced by Linguistica 2 R ← ordered set of possible rules 3 for each r ∈ R 4 do 5 Cr ← ordered set of possible contexts for r 6 C←∅ 7 while Cr 6= ∅ 8 do c ← next c ∈ Cr 9 Cr ← Cr  {c} 10 C ← C ∪ {c} 11 G0 ← collapseInContext(G, r, C) 12 G0 ← pruneRules(G0 ) 13 if score(G0 ) &lt; score(G) 14 then G ← G0 15 return G COLLAPSE I N C ONTEXT (G, r, C) 1 2 3 4 for each σi ∈ G do for each σj ∈ G do if (σi → σj ) ∧ (∀(t, f ) ∈ σi , ctx(t, f ) ∈ C) r then collapseSigs(σi , σj ) Figure 4: Pseudocode for our search algorithm with that context and collapse any pairs of"
W04-0105,J93-2004,0,\N,Missing
W05-0615,W95-0101,0,0.0392109,"slation. However, most of the successful work to date has used supervised learning techniques. Unsupervised algorithms that can learn from raw linguistic data, as humans can, remain a challenge. In a statistical Despite the advantages of maximum likelihood estimation and its implementation via various instantiations of the EM algorithm, it is widely regarded as ineffective for unsupervised language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) recently achieved more encouraging results using an EM-like algorithm to induce syntactic constituent grammars, based on a deficient probability model. It has been suggested that EM often"
W05-0615,W01-0714,0,0.126039,"language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) recently achieved more encouraging results using an EM-like algorithm to induce syntactic constituent grammars, based on a deficient probability model. It has been suggested that EM often yield poor results because it is overly sensitive to initial parameter values and tends to converge on likelihood maxima that are local, but not global (Carroll and Charniak, 1992). In this paper, we present a series of experiments indicating that for the task of learning a syllable structure grammar, the initial parameter weights are not crucial. Rather, it is the choice of the model class, i.e., the"
W05-0615,P02-1017,0,0.0987533,"Missing"
W05-0615,J94-2001,0,0.592124,"text-Free Grammars (PCFGs). Introduction The use of statistical methods in computational linguistics has produced advances in tasks such as parsing, information retrieval, and machine translation. However, most of the successful work to date has used supervised learning techniques. Unsupervised algorithms that can learn from raw linguistic data, as humans can, remain a challenge. In a statistical Despite the advantages of maximum likelihood estimation and its implementation via various instantiations of the EM algorithm, it is widely regarded as ineffective for unsupervised language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) rec"
W05-0615,P01-1053,0,0.341835,"Missing"
W05-0615,W02-0608,0,0.292493,"Missing"
W05-0615,W98-1223,0,0.0543507,"Missing"
W05-0615,J01-3002,0,0.0321306,"cedures and representations that can lead to successful unsupervised language learning in both computers and humans. Our work has some similarity to that of M¨uller, 113 who trains a PCFG of syllable structure from a corpus of words with syllable boundaries marked. We, too, use a model defined by a grammar to describe syllable structure.1 However, our work differs from M¨uller’s in that it focuses on how to learn the model’s parameters in an unsupervised manner. Several researchers have worked on unsupervised learning of phonotactic constraints and word segmentation (Elman, 2003; Brent, 1999; Venkataraman, 2001), but to our knowledge there is no previously published work on unsupervised learning of syllable structure. In the work described here, we experimented with two different classes of models of syllable structure. Both of these model classes are presented as PCFGs. The first model class, described in M¨uller (2002), encodes information about the positions within a word or syllable in which each phoneme is likely to appear. In this positional model, each syllable is labeled as initial (I), medial (M), final (F), or as the one syllable in a monosyllabic word (O). Syllables are broken down into an"
W05-0615,C04-1080,0,\N,Missing
W10-2001,D07-1043,0,0.0112285,"es. In effect this condition is a pseudo-baseline, testing the effects of less- or noninformative sentence features on our proposed models. 4 V I(C, K) = H(C) + H(K) − 2I(C, K) = H(C|K) + H(K|C) A lower score implies closer clusterings, since each clustering has less information not shared with the other: two identical clusterings have a VI of zero. However, VI’s upper bound is dependent on the maximum number of clusters in C or K, making it difficult to compare clustering results with different numbers of clusters. As a third, and, in our view, most informative measure, we use V-measure (VM; Rosenberg and Hirschberg (2007)). Like VI, VM uses the conditional entropy of clusters and categories to evaluate clusterings. However, it also has the useful characteristic of being analogous to the precision and recall measures commonly used in NLP. Homogeneity, the precision analogue, is defined as VH = 1− H(C|K) . H(C) VH is highest when the distribution of categories within each cluster is highly skewed towards a small number of categories, such that the conditional entropy is low. Completeness (recall) is defined symmetrically to VH as: H(K|C) VC = 1 − . H(K) Evaluation Measures Evaluation of fully unsupervised part o"
W10-2001,P07-1094,1,0.843606,"evious work investigating the usefulness of this kind of information for syntactic category acquisition. In other domains, intonation has been used to identify sentence types as a means of improving speech recognition language models. Specifically, (Taylor et al., 1998) found that using intonation to recognize dialogue acts (which to a significant extent correspond to sentence types) and then using a specialized language model for each type of dialogue act led to a significant decrease in word error rate. In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. We then discuss the distinctions in sentence type that we used and our evaluation measures, and finally our experimental results. We perform experiments on corpora in four different languages: English, Spanish, Cantonese, and Dutch. Our results on Spanish show no difference between the baseline and the models incorporating sentence type, possibly due to the small size of the Spanish corpus. Results on all other corpora show a small improvement in pe"
W11-0603,N04-1011,0,0.0252732,"ibit prosodic phenomena, but they are not addressed here. word boundaries. In Mainstream American English ToBI, for example, the boundary between a clitic and its base word (e.g. “do” and “n’t” of “don’t”) is 0, representing a very weak boundary, while the boundary following a word at the end of an intonational phrase is 4, indicating a very strong boundary. Below we examine how useful these break indices are for identifying syntactic boundaries. Finally, we note that our work is not the first computational approach to using prosody for identifying syntactic structure. However, previous work (Gregory et al., 2004; Kahn et al., 2005; Dreyer and Shafran, 2007; N¨oth et al., 2000) has focused on supervised parsing rather than unsupervised chunking, and also makes different assumptions about prosody. For example, Gregory et al. (2004) assume that prosody is an acoustically-realized substitute for punctuation; our own treatment is much less constrained. Kahn et al. (2005) and Dreyer and Shafran (2007) use ToBI labels to represent prosodic information, whereas we explore both ToBI and direct acoustic measures. Finally, N¨oth et al. (2000) do not use ToBI, instead developing a novel prosodic annotation syste"
W11-0603,H05-1030,0,0.0936953,"a, but they are not addressed here. word boundaries. In Mainstream American English ToBI, for example, the boundary between a clitic and its base word (e.g. “do” and “n’t” of “don’t”) is 0, representing a very weak boundary, while the boundary following a word at the end of an intonational phrase is 4, indicating a very strong boundary. Below we examine how useful these break indices are for identifying syntactic boundaries. Finally, we note that our work is not the first computational approach to using prosody for identifying syntactic structure. However, previous work (Gregory et al., 2004; Kahn et al., 2005; Dreyer and Shafran, 2007; N¨oth et al., 2000) has focused on supervised parsing rather than unsupervised chunking, and also makes different assumptions about prosody. For example, Gregory et al. (2004) assume that prosody is an acoustically-realized substitute for punctuation; our own treatment is much less constrained. Kahn et al. (2005) and Dreyer and Shafran (2007) use ToBI labels to represent prosodic information, whereas we explore both ToBI and direct acoustic measures. Finally, N¨oth et al. (2000) do not use ToBI, instead developing a novel prosodic annotation system designed specific"
W11-0603,P04-1061,0,0.358757,"producing a flat structure, performs competitively with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants. 1 Introduction Young children routinely learn to group words into phrases, yet computational methods have so far struggled to accomplish this task without supervision. Previous work on unsupervised grammar induction has made progress by exploiting information such as gold-standard part of speech tags (e.g. Klein and Manning (2004)) or punctuation (e.g. Seginer (2007)). While this information may be available in some NLP contexts, our focus here is on the computational problem facing language-learning infants, who do not have access to either part of speech tags or punctuation. However, infants do have access to certain cues that have not been well explored by NLP researchers focused on grammar induction from text. In particular, we consider the cues to syntactic structure that might be available from prosody (roughly, the structure of speech conveyed through rhythm and intonation) and its acoustic realization. The idea"
W11-0603,D07-1072,0,0.0277396,"tally (Mehler et al., 1988) and that 9-month-olds use prosodic cues to distinguish verb phrases from nonconstituents (Soderstrom et al., 2003). However, as far as we know, there has so far been no direct computational evaluation of the prosodic bootstrapping hypothesis. In this paper, we provide the first such evaluation by exploring the utility of acoustic cues for unsupervised syntactic chunking, i.e., grouping words into non-hierarchical syntactic phrases. Nearly all previous work on unsupervised grammar induction has focused on learning hierarchical phrase structure (Lari and Young, 1990; Liang et al., 2007) or dependency structure (Klein and Manning, 2004); we are aware of only one previous paper on unsupervised syntactic chunking (Ponvert et al., 2010). Ponvert et al. describe a simple method for chunking that uses only bigram counts and punctuation; when the chunks are combined using a rightbranching structure, the resulting trees achieve unlabeled bracketing precision and recall that is competitive with other unsupervised parsers. The sys20 Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 20–29, c Portland, Oregon, June 2011. 2011 Association for Comp"
W11-0603,H91-1073,0,0.747013,"or variation in spectral tilt, these are merely observable acoustic correlates that provide evidence of varying quality about the hidden prosodic structure, which specifies such hidden variables as contrastive stress or question intonation. Prosody has been hypothesized to be useful for learning syntax because it imposes a grouping structure on word sequences that sometimes coincides with traditional constituency analyses (Ladd, 1996; Shattuck-Hufnagel and Turk, 1996). Moreover, laboratory experiments have shown that adults use prosody both for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and, crucially, in learning the syntax of an artificial language (Morgan et al., 1987). Accordingly, if prosodic structure is sufficiently prominent in the acoustic signal, and coincides often enough with syntactic structure, then it may provide children with useful information about how to combine words into phrases. Although there are several theories of how to represent and annotate prosodic structure, one of the most influential is the ToBI (Tones and Break Indices) theory (Beckman et al., 2005), which we will use in some of our experiments. ToBI proposes, among other things, that the pro"
W11-0603,P07-1049,0,0.101545,"ely with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants. 1 Introduction Young children routinely learn to group words into phrases, yet computational methods have so far struggled to accomplish this task without supervision. Previous work on unsupervised grammar induction has made progress by exploiting information such as gold-standard part of speech tags (e.g. Klein and Manning (2004)) or punctuation (e.g. Seginer (2007)). While this information may be available in some NLP contexts, our focus here is on the computational problem facing language-learning infants, who do not have access to either part of speech tags or punctuation. However, infants do have access to certain cues that have not been well explored by NLP researchers focused on grammar induction from text. In particular, we consider the cues to syntactic structure that might be available from prosody (roughly, the structure of speech conveyed through rhythm and intonation) and its acoustic realization. The idea that prosody provides important init"
W11-0603,N03-1028,0,0.0289021,"osody is an acoustically-realized substitute for punctuation; our own treatment is much less constrained. Kahn et al. (2005) and Dreyer and Shafran (2007) use ToBI labels to represent prosodic information, whereas we explore both ToBI and direct acoustic measures. Finally, N¨oth et al. (2000) do not use ToBI, instead developing a novel prosodic annotation system designed specifically to provide cues to syntax and for annotation efficiency. However, their system is supervised and focuses on improving parse speed rather than accuracy. 3 Models Following previous work (e.g. Molina and Pla (2002) Sha and Pereira (2003)), we formulate chunking as a tagging task. We use Hidden Markov Models (HMMs) and their variants to perform the tagging, with carefully specified tags and constrained transition distributions to allow us to interpret the results as a bracketing of the input. Specifically, we use four chunk tags: B (“Begin”) and E (“End”) tags are interpreted as the first and last words of a chunk, respectively, with I (“Inside”) corresponding to other words inside a chunk and O (“Outside”) to all other words. The transition matrices are constrained to afford 0 probability to transitions that violate these def"
W11-0603,W00-0726,0,0.344093,"Missing"
W12-1913,P11-1048,0,0.0470754,"todoulopoulos† , Sharon Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for"
W12-1913,D11-1059,1,0.837648,"rvised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have described BMMM: a PoS induction system that makes it is easy to incorporate multiple features either at the type or token level. For the dependency induction system we chose the DMV model of Klein and Manning (2004) because of its simplicity and its popularity. Both systems are described briefly in section 3. Using these two systems we performed an iterated learning experiment. The term is borrowed from the language evolution literature meaning “the process by which the output of one individual’s learning becomes the input to other individuals’ learning” (Smith et al., 2003). Here we"
W12-1913,D11-1005,0,0.0742278,"n Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use de"
W12-1913,N06-1041,0,0.224034,"Missing"
W12-1913,C08-1042,0,0.139855,"es that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use dependency parsing as an extrinsic evaluation for unsupervised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have de"
W12-1913,N09-1012,0,0.304428,"Missing"
W12-1913,P04-1061,0,0.726608,"a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use dependency parsing as an extrinsic evaluation for unsupervised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have described BMMM: a PoS induction system that makes it is easy to inco"
W12-1913,D11-1109,0,0.042809,"on Christos Christodoulopoulos† , Sharon Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pip"
W13-1810,P13-1091,1,0.791979,"y edge and then attaching the new subgraph. Rule r3 has an external vertex sequence of 1 to 5, and these are fused to the incident vertices of the nonterminal edge Nsay 1 → 5. The edge to be replaced in each step has been highlighted in red to ease reading. 3 Tree Decompositions We now introduce one additional piece of theoretical machinery, the tree decomposition 56 (Bodlaender, 1993). Tree decompositions play an important role in graph theory, feature prominently in the junction tree algorithm from machine learning (Pearl, 1988), and have proven valuable for efficient parsing (Gildea, 2011; Chiang et al., 2013). Importantly, Lautemann (1988) proved that every HRG parse identifies a particular tree decomposition, and by restricting ourselves to a certain type of tree we will draw an even tighter relationship, allowing us to identify parses given tree decompositions. A tree decomposition of a graph g is a tree whose nodes identify subsets of the vertices of g which satisfy the following three properties:1 • Vertex Cover: Every vertex of g is contained by at least one tree node. • Edge Cover: For every edge e of the graph, there is a tree node η such that each vertex of α(e) is in η. • Running Intersec"
W13-1810,W08-1301,0,0.021476,"Missing"
W13-1810,P99-1069,1,0.725561,"Missing"
W13-1810,C12-1083,1,0.721826,"syntactic subject, so could a graph grammar represent lake as a constituent in a parse of the corresponding semantic graph. In fact, picking a formalism that is so similar to the PCFG makes it easy to adapt proven, familiar techniques for training and inference such as the inside-outside algorithm, and because HRG is context-free, parses can be represented by trees, facilitating the use of many more tools from tree automata (Knight and Graehl, 2005). Furthermore, the operational parallelism with PCFG makes it easy to integrate graph-based systems with syntactic models in synchronous grammars (Jones et al., 2012). Probabilistic versions of deep syntactic models such as Lexical Functional Grammar and HPSG (Johnson et al., 1999; Riezler et al., 2000) are one grammar-based approach to Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing, pages 54–62, c St Andrews–Sctotland, July 15–17, 2013. 2013 Association for Computational Linguistics modeling graphs represented in the form of feature structures. However, these models are tied to a particular linguistic paradigm, and they are complex, requiring a great deal of effort to engineer and annotate the nece"
W13-1810,C12-1094,0,0.031056,"ibes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to define generative models of graph languages. 1 Introduction While most work in natural language processing (NLP), and especially within statistical NLP, has historically focused on strings and trees, there is increasing interest in deeper graph-based analyses which could facilitate natural language understanding and generation applications. Graphs have a long tradition within knowledge representation (Sowa, 1976), natural language semantics (Titov et al., 2009; Martin and White, 2011; Le and Zuidema, 2012), and in models of deep syntax (Oepen et al., 2004; de Marneffe and Manning, 2008). Graphs seem particularly appropriate for representing semantic structures, since a single concept could play multiple roles within a sentence. For instance, in the semantic representation at the bottom right of Figure 1 lake is an argument of both rich-in and own in the sentence, “The lake is said to be rich in fish but is privately owned.” However, work 54 Mark Johnson† mark.johnson@mq.edu.au † Department of Computing Macquarie University Sydney, Australia on graphs has been hampered, due, in part, to the abse"
W13-1810,W11-1609,0,0.0147533,"eneral graphs. This paper describes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to define generative models of graph languages. 1 Introduction While most work in natural language processing (NLP), and especially within statistical NLP, has historically focused on strings and trees, there is increasing interest in deeper graph-based analyses which could facilitate natural language understanding and generation applications. Graphs have a long tradition within knowledge representation (Sowa, 1976), natural language semantics (Titov et al., 2009; Martin and White, 2011; Le and Zuidema, 2012), and in models of deep syntax (Oepen et al., 2004; de Marneffe and Manning, 2008). Graphs seem particularly appropriate for representing semantic structures, since a single concept could play multiple roles within a sentence. For instance, in the semantic representation at the bottom right of Figure 1 lake is an argument of both rich-in and own in the sentence, “The lake is said to be rich in fish but is privately owned.” However, work 54 Mark Johnson† mark.johnson@mq.edu.au † Department of Computing Macquarie University Sydney, Australia on graphs has been hampered, du"
W13-1810,P00-1061,1,0.697607,"king a formalism that is so similar to the PCFG makes it easy to adapt proven, familiar techniques for training and inference such as the inside-outside algorithm, and because HRG is context-free, parses can be represented by trees, facilitating the use of many more tools from tree automata (Knight and Graehl, 2005). Furthermore, the operational parallelism with PCFG makes it easy to integrate graph-based systems with syntactic models in synchronous grammars (Jones et al., 2012). Probabilistic versions of deep syntactic models such as Lexical Functional Grammar and HPSG (Johnson et al., 1999; Riezler et al., 2000) are one grammar-based approach to Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing, pages 54–62, c St Andrews–Sctotland, July 15–17, 2013. 2013 Association for Computational Linguistics modeling graphs represented in the form of feature structures. However, these models are tied to a particular linguistic paradigm, and they are complex, requiring a great deal of effort to engineer and annotate the necessary grammars and corpora. It is also not obvious how to define generative probabilistic models with such grammars, limiting their utilit"
W13-1810,J11-1008,0,\N,Missing
W17-4607,D16-1263,0,0.0546957,"Missing"
W17-4607,D16-1133,1,0.879375,"Lopez♦ David Chiang♠ ♠ Department of Computer Science and Engineering, University of Notre Dame ♦ School of Informatics, University of Edinburgh Abstract native speakers themselves (Bird et al., 2014; Blachon et al., 2016; Adda et al., 2016). Nevertheless, even translation takes time and language knowledge, so there may still be little translated data relative to the amount of recorded audio. An important goal, then, is to bootstrap language technology from this small parallel corpus in order to provide tools to annotate more data or make the data more searchable. We build on the approach of Anastasopoulos et al. (2016), who developed a system that performs joint inference to identify recurring segments of audio and cluster them while aligning them to words in a text translation. Here, we extend the method to be able to search for new instances of the latent clusters within the unlabeled audio, effectively providing keyword translations for some of the unlabeled speech. We evaluate our method on a Spanish-English corpus used in previous work, and on two datasets from endangered languages (narratives in Arapaho and Ainu). No previous computational methods have been tested on the latter data, to our knowledge."
W17-4908,D15-1256,0,0.0485011,"Missing"
W17-4908,E17-1116,1,0.415163,"in Twitter data Philippa Shoemark∗ p.j.shoemark@ed.ac.uk James Kirby† j.kirby@ed.ac.uk ∗ School of Informatics University of Edinburgh † sgwater@inf.ed.ac.uk Dept. of Linguistics and English Language University of Edinburgh Abstract whether results obtained on a particular user sample generalize to another sample. For example, previous studies have suggested that Twitter users modulate their use of regional and non-standard language depending on the expected size of the audience (operationalized as whether a Tweet contains hashtags, @-mentions, or neither) (Pavalanathan and Eisenstein, 2015a; Shoemark et al., 2017). However, these studies did not sufficiently control for possible effects of topic, which may be confounded with audience size: e.g., users may use more hashtags when discussing political events than when discussing daily routines. These studies also did not look at the degree to which their results generalize across different populations of users. In this work we study two largely disjoint groups of (mainly) Scottish Twitter users: one group sent tweets geotagged within Scotland, while the other used hashtags related to the 2014 Scottish independence referendum. We use mixed-effects models t"
W17-4908,E14-1034,0,0.0601761,"Missing"
W17-4908,P15-1073,0,0.0303302,"g mixedeffects models, we show that audience and topic have independent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups,underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data. 1 Sharon Goldwater∗ Introduction Linguistic variation in social media is a growing research area, with interest stemming both from the engineering goal of developing tools that work well across different styles and dialects (Hovy, 2015; Stoop and van den Bosch, 2014; Vyas et al., 2014; Huang and Yates, 2014), and from the social science goal of studying user behaviour (Bamman et al., 2014; Eisenstein, 2015; Huang et al., 2016; Nguyen et al., 2015). However, this type of research is often complicated by the messy nature of social media data, which can make it hard to control for different explanatory factors and to know 59 Proceedings of the Workshop on Stylistic Variation, pages 59–68 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics with audience size. We were unable to find a clea"
W17-4908,D14-1105,0,0.031877,"e and topic have independent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups,underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data. 1 Sharon Goldwater∗ Introduction Linguistic variation in social media is a growing research area, with interest stemming both from the engineering goal of developing tools that work well across different styles and dialects (Hovy, 2015; Stoop and van den Bosch, 2014; Vyas et al., 2014; Huang and Yates, 2014), and from the social science goal of studying user behaviour (Bamman et al., 2014; Eisenstein, 2015; Huang et al., 2016; Nguyen et al., 2015). However, this type of research is often complicated by the messy nature of social media data, which can make it hard to control for different explanatory factors and to know 59 Proceedings of the Workshop on Stylistic Variation, pages 59–68 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics with audience size. We were unable to find a clear explanation of this difference. Nevertheless, it"
W17-4908,E14-1001,0,0.0188323,"dependent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups,underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data. 1 Sharon Goldwater∗ Introduction Linguistic variation in social media is a growing research area, with interest stemming both from the engineering goal of developing tools that work well across different styles and dialects (Hovy, 2015; Stoop and van den Bosch, 2014; Vyas et al., 2014; Huang and Yates, 2014), and from the social science goal of studying user behaviour (Bamman et al., 2014; Eisenstein, 2015; Huang et al., 2016; Nguyen et al., 2015). However, this type of research is often complicated by the messy nature of social media data, which can make it hard to control for different explanatory factors and to know 59 Proceedings of the Workshop on Stylistic Variation, pages 59–68 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics with audience size. We were unable to find a clear explanation of this difference. Nevertheless, it highlights the difficul"
W18-6101,W17-1202,0,0.194354,"Missing"
W18-6101,P08-1088,0,0.131318,"Missing"
W18-6101,W17-4908,1,0.843761,"previous dataset, we noticed that many of the terms identified by SAGE as distinctively American were actually from AAVE. To create our GenAm/AAVE seed pairs, we manually cross-referenced the most distinctively ‘American’ terms with the AAVE phonological processes described by Rickford (1999). We then selected terms that reflected these processes, paired with their GenAm equivalents, e.g. (about, bou), (brother, brudda). The full list of 19 open-class and 20 closed-class seed pairs is included in the Supplement. BrEng/Scottish For our first test case, we combined the two datasets collected by Shoemark et al. (2017a), consisting of complete tweet histories from Aug-Oct 2014 by users who had posted at least one tweet in the preceding year geotagged to a location in Scotland, or that contained a hashtag relating to the 2014 Scottish Independence referendum. The corpus contains 9.4M tweets. For seeds, we used the 64 pairs curated by Shoemark et al. (2017b). Half are discourse markers or open-class words (dogs, dugs), (gives, gees) and half are closed-class words (have, hae), (one, yin). The full list is included in the Supplement. 4 Evaluation Procedure We evaluate our systems using Precision@K, the percen"
W18-6101,E17-1116,1,0.821309,"previous dataset, we noticed that many of the terms identified by SAGE as distinctively American were actually from AAVE. To create our GenAm/AAVE seed pairs, we manually cross-referenced the most distinctively ‘American’ terms with the AAVE phonological processes described by Rickford (1999). We then selected terms that reflected these processes, paired with their GenAm equivalents, e.g. (about, bou), (brother, brudda). The full list of 19 open-class and 20 closed-class seed pairs is included in the Supplement. BrEng/Scottish For our first test case, we combined the two datasets collected by Shoemark et al. (2017a), consisting of complete tweet histories from Aug-Oct 2014 by users who had posted at least one tweet in the preceding year geotagged to a location in Scotland, or that contained a hashtag relating to the 2014 Scottish Independence referendum. The corpus contains 9.4M tweets. For seeds, we used the 64 pairs curated by Shoemark et al. (2017b). Half are discourse markers or open-class words (dogs, dugs), (gives, gees) and half are closed-class words (have, hae), (one, yin). The full list is included in the Supplement. 4 Evaluation Procedure We evaluate our systems using Precision@K, the percen"
W18-6101,W15-4302,0,0.0823907,"overy of sociolinguistic variables: linguistic items with identifiable variants that are correlated with social or contextual traits such as class, register, or dialect. For example, the choice of the term rabbit versus bunny might correlate with audience or style, while fitba is a characteristically Scottish variant of the more general British football. To date, most large-scale social media studies have studied the usage of individual variant forms (Eisenstein, 2015; Pavalanathan and Eisenstein, 2015). Studying how a variable alternates between its variants controls better for ‘Topic Bias’ (Jørgensen et al., 2015), but identifying the relevant variables/variants may not be straightforward. 1. need to come hame fae the football need to come home from the football 2. miss the fitba miss the football 3. awwww man a wanty go tae the fitbaw awwww man I want to go to the football The lexical variable induction task is challenging: we cannot simply classify documents containing fitba as Scottish, since the football variant may also occur in otherwise distinctively Scottish texts, as in (1). Moreover, if we start by knowing only a few variables, we would like a way to learn what other likely variables might be"
W18-6101,P12-3005,0,0.117079,"Missing"
W98-1115,H90-1053,0,0.0422779,"have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence. Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework"
W98-1115,P97-1003,0,0.104889,"y more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others interested in PCFG"
W98-1115,P96-1024,0,0.142388,"Missing"
W98-1115,W97-0302,0,0.0885422,"1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather"
W98-1115,H91-1045,0,0.196176,"Missing"
W98-1115,H91-1044,0,0.0428787,"ssibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses"
W98-1115,P95-1037,0,0.0839334,"g other, strictly more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others int"
W98-1115,J93-2004,0,0.0255481,"edge B ~ '7- to yield the edge A ~ a B . fl, corresponds to the left-factored productions ' a B ' ~ a B if fl is non-empty or A ~ ' a ' B if fl is empty. Thus in general a single 'new' non-terminal in a C K Y parse using the left-factored grammar abbreviates several incomplete edges in the Earley algorithm. 4 Figure h r/vs. P o p p e d Edges 4o0 3oo 2oo 1.0 1.5 ! 2.0 N o r m a l i z a t i o n constant Figure 2: r] vs. Precision and Recall 76 ~ 74 130 , ! . . . . lO0 The Experiment For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing. All sentences of length greater than 40 were ignored for testing purposes as done in b o t h C&C and Goodman (1997). We applied the binarization technique described above to the grammar. We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse. This method has the advantage of being platform independent, as well as providing a measure of ""perfection"". Here, perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse"
W98-1115,H94-1051,0,0.135544,"Missing"
W98-1115,W97-0301,0,0.025136,"upin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather than simply completed"
W98-1115,H91-1042,0,\N,Missing
