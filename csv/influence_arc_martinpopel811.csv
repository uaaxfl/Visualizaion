2016.eamt-2.1,W15-3009,1,0.880814,"Missing"
2016.eamt-2.1,W15-5711,1,0.827881,"Missing"
2016.eamt-2.1,W15-3006,1,0.877207,"Missing"
2016.eamt-2.1,P15-4020,0,0.0200302,"Missing"
2016.eamt-2.1,P13-4014,0,0.0192755,"Missing"
2016.eamt-2.1,W15-2505,0,0.0245204,"Missing"
2016.eamt-2.1,P15-3002,0,0.0279562,"Missing"
2020.wmt-1.17,2020.ngt-1.4,1,0.775989,"our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized back to float32. Despite the extra quantization and de-quantization involved, the increased speed at which 8-bit integer multiplication is performed more than compensates for it. Bogoychev et al. (2020) observe that smaller student presets lose BLEU when quantized. In order to counteract that, we perform model fine tuning following the work of Aji and Heafield (2020): We replace the GEMM routine implementation with a custom one that is damaged, according to the quan7 For example, for handling HTML tags in translated texts. 8 https://github.com/kpu/intgemm 194 4 Results In Table 4, we show the performance of the three models in terms of BLEU scores for the WMT 2020 cs↔en test sets and translation speed. Teacher models ran on an Nvidia GeForce GTX 1080 with a batch size of 16. Student models were run on a single CPU core on an Intel Intel(R) Xeon(R) CPU E5-2680 0 @ 2.70GHz with a batch size of 64. It should be noted that we made no effort to optimize the te"
2020.wmt-1.17,2016.amta-researchers.10,0,0.0683014,"Missing"
2020.wmt-1.17,D17-1300,0,0.0148403,"nd perform several thousand minibatch updates of the model. The damaged GEMM implementation can only produce 255 unique float values (corresponding to the 8-bit integer dequantization range) and the model quickly learns to work with those values and recovers some of the BLEU lost compared to untuned quantized model. Quantized Models Floating point operations are computationally more expensive than integer operations. However, as Han et al. (2016) have shown, neural network inference does not require the high precision of representation and computation that 32-bit floating point numbers offer. Devlin (2017) suggests a simple quantization mechanism for quantizing parameters to 16-bit integer precision and notes that support for off-the-shelf 8-bit integer matrix multiplication is lacking. Bogoychev et al. (2020) fill that gap and provide an 8-bit quantization and fine tuning scheme for Marian based on the intgemm library;8 we used that scheme for our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized b"
2020.wmt-1.17,N13-1073,0,0.196044,"Missing"
2020.wmt-1.17,2020.ngt-1.1,1,0.698048,"th the teacher model to generate the training set D0 . 3. Train a small student model on D0 . Introduction The conventional set-up of the WMT Shared Tasks on News Translation emphasizes translation quality (however measured) above all else. Constraints on the data that may be used for training in the ‘constrained’ track establish a level playing field in terms of the information available to the translation model and its training process, but there are no constraints on the computational power and effort spent to achieve the results. In contrast, the WNGT Shared Task on Efficient Translation (Heafield et al., 2020) encourages participants to submit systems that are both accurate and efficient during inference (i.e., translation). So far, there has been little interaction between the two tasks. With our joint submission between the University of Edinburgh (UEDIN) and Charles University, Prague (CUNI), we strive to bridge this gap. We submitted small, efficient systems that distilled knowledge from a more powerful teacher model via sequence-level knowledge distillation (Kim and 1 Bogoychev et al. (2020) report translation speeds of up to 3135 source words per second on a single CPU thread; the actual thro"
2020.wmt-1.17,E17-2068,0,0.0666085,"Missing"
2020.wmt-1.17,P18-4020,1,0.873993,"Missing"
2020.wmt-1.17,W18-2716,1,0.833283,"sion to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student mode"
2020.wmt-1.17,D16-1139,0,0.0795642,"beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student models were trained by UEDIN with the Marian NMT toolkit 3 3.1 Student Model Architectures The student models use the"
2020.wmt-1.17,N03-1017,0,0.0630476,"Missing"
2020.wmt-1.17,D18-2012,0,0.0346137,"hypotheses selected over the respective beam ranks. For the monolingual data, for which we obviously have no human reference translations, we simply chose the highest-scoring translation. Sentence pairs where the translation contained the same whitespace-separated sequence of words three or more times in a row, or the same sequence of one or more characters in five or more subsequent repetitions (which can happen when the recursive decoder goes into a loop) were discarded. We subsequently tokenized the synthetic teaching data (source and translations by the teacher model) with SentencePiece (Kudo and Richardson, 2018), 193 using a joint vocabulary for both languages with a size of 32,000 tokens. This vocabulary is also used by the final systems. The tokenized training data was word-aligned in both translation directions with FastAlign (Dyer et al., 2013). Directional word alignments were then symmetrized with the growdiag-final-and symmetrization algorithm (Koehn et al., 2003). These word alignments serve mainly three purposes: (a) to guide the attention mechanism during training of the student models (Liu et al., 2016) with guided alignment (Chen et al., 2016); (b) to produce shortlists of translation can"
2020.wmt-1.17,C16-1291,0,0.0504752,"Missing"
2020.wmt-1.17,W18-6424,1,0.836123,"t. value size checkpoints avg. back-translation beam search alpha max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary subm"
2020.wmt-1.17,2020.wmt-1.28,1,0.73264,". Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data"
2020.wmt-1.17,W18-6319,0,0.0332871,"Missing"
2020.wmt-1.17,P16-1009,0,0.0423625,"max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”;"
2020.wmt-1.17,W18-1819,0,0.0673101,"Missing"
2020.wmt-1.28,D19-5545,0,0.0282449,"level decoding and we used this for our last-year sentence-level submission “Transformer T2T 2019” (Popel et al., 2019). • We can split each input document into nonBecause of the increased limits of training sequences, we increased also the decoding limits two times: pre-context of up to 400 characters, main content of up to 1000 characters and post-context of up to 1800 characters minus the length of the pre-context and main content. 5 Robust training with noising To make the model more robust to real-world usergenerated data, we added a noise to the training data. We followed an approach of Náplava and Straka (2019) and made the source side of the training data more noisy by introducing both grammatical and spelling errors. The basic set of noising operations introducing grammatical errors consisted of the following operations: token replacement with one of its spelling dictionary proposals, token deletion and insertion and swapping of two nearby words. Moreover, we also allowed to replace phrases with one of their most frequent variants, add or delete punctuation and allowed to strip diacritics. We applied this technique only to our Czech→English sentence-level system by noising the source=Czech side of"
2020.wmt-1.28,W18-6424,1,0.734942,"d “CUNI-DocTransformer” (document-level). We trained them for English↔Czech and the former one also for English↔Polish (no parallel document-level data was provided for EnglishPolish, thus we could not train the latter one). 2 sentence pairs (M) Common settings Both our systems are implemented in the Tensor2Tensor framework (Vaswani et al., 2018) and have the same Transformer (Vaswani et al., 2017) architecture – transformer_big with 12 encoder layers instead of the default 6 (while keeping 6 layers in the decoder). The 32k joint English-Czech subword vocabulary is exactly the same as used by Popel (2018) and Popel et al. (2019), which are the systems we submitted to WMT in the last two years. Also most of the hyperparameters (except for the encoder depth) and the training regime are the same. The main improvement of our sentence-level system relative to our last-year submission stems from using slightly larger and better-filtered training data – CzEng 2.0 (Kocmi et al., 2020b) with 61M authentic parallel and 127M synthetic (back-translated) 3 Document-level training Our last-year document-level submission (Popel et al., 2019) introduced a method of training-data context augmentation, where mu"
2021.emnlp-main.801,W14-0307,0,0.0593636,"Missing"
2021.emnlp-main.801,W17-3204,0,0.0599099,"Missing"
2021.emnlp-main.801,2013.mtsummit-wptp.1,0,0.0657736,"Missing"
2021.emnlp-main.801,2020.eamt-1.13,0,0.0876269,"Missing"
2021.emnlp-main.801,W19-6626,0,0.0261339,"Missing"
2021.emnlp-main.801,2013.mtsummit-wptp.5,0,0.0528047,"Missing"
2021.emnlp-main.801,2020.wmt-1.28,1,0.703983,"ith clearly marked document boundaries. For clarity, we will refer to the whole set simply as “file” and the individual parts as “documents”. 3.2 cloud.google.com/translate azure.microsoft.com/en-us/services/cognitiveservices/translator/ 6 We also experimented with BERTScore but its Pearson correlation with BLEU is 0.9939. This would lead to the same observations and conclusions. 5 Model M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 Google Microsoft Machine Translation Models In total, we used 13 MT models of various quality. Models M01–M11 are based on the setup, training procedure and data of Popel (2020). We chose this particular approach because it has been reported to reach human translation quality (Popel et al., 2020). For our purposes, we reproduce the training, stopping it at various stages of the training process. All MT systems translate sentences in isolation, with the exception of M11, which is a document-level system (replicating CUNI-DocTransformer in Popel (2020)). Systems MT01–MT10 differ only in the number of training steps, which affects also the ratio of authentic- and synthetic- data checkpoints in the hourly checkpoint averaging (Popel et al., 2020): the best devset BLEU wa"
2021.emnlp-main.801,W15-3049,0,0.0606843,"Missing"
2021.emnlp-main.801,W18-6319,0,0.0252958,"4.65).6 Most of the systems are concentrated in the upper half of the range. 4 This better reflects realistic scenarios in localization workflows where users can typically decide among several engines of comparable but not identical performance. TER BLEU Steps [k] 0.729 0.678 0.655 0.648 0.622 0.624 0.604 0.600 0.603 0.600 0.601 0.623 0.632 25.35 31.61 33.09 33.63 35.22 35.68 36.58 36.41 37.40 37.44 37.37 37.56 33.06 25.4 29.0 29.3 33.0 72.8 997.1 1015.2 1022.4 1055.0 1058.6 698.5 – – ACh 8 8 8 8 6 0 5 6 8 6 5 – – Table 1: Overview of MT systems used. TER and BLEU were measured by SacreBLEU7 (Post, 2018). Steps mark the number of training steps in thousands. ACh is the number of authentic-data-trained checkpoints in an average of 8 checkpoints. 3.3 Translation Process We carried out the translation in two stages: MT post-editing stage and final revision stage. For both stages, we used Memsource as the computerassisted translation (CAT) tool. (1) Post-editing The documents were first translated by all 13 MT systems. In addition, we included a variant with no translation (”Source”) and with a pre-existing reference translation (”Reference”).8 The translated files were shuffled at document bound"
2021.emnlp-main.801,2016.amta-researchers.2,0,0.103117,"ss to the original text as well. Finally, Koponen (2013) comments on the high variance of post-editors, which is a common problem in postediting research (Koponen, 2016). Interactive MT is an alternative use case of computer-assisted translation and it is possible that effort or behavioural patterns in interactive MT could be used as a different proxy extrinsic measure for MT quality. Post-editor productivity has also been measured in contrast to interactive translation prediction by Sanchis-Trilles et al. (2014). Additionally, our focus is state-of-the-art NMT systems, which was not true for Sanchez-Torron and Koehn (2016), who constructed 9 artificially severely degraded statistical phrase-based MT systems. The experiment by Koehn and Germann (2014) used only 4 MT systems. Our focus is motivated by the industry’s direct application: Considering the cost of skilled staff and model training, what are the practical benefits of improving MT performance? In contrast to the previous setups, we evaluate two additional settings: post-editing human reference and translating from scratch, corresponding to a theoretical2 BLEU of 100 and 0, respectively. We also consider the quality of the PE output and not only the proce"
2021.emnlp-main.801,2009.mtsummit-posters.20,0,0.208863,"Missing"
2021.emnlp-main.801,tiedemann-2012-parallel,0,0.0132279,"the translation process. NMT system quality and PE effort is not a simple one and that older results based on statistical MT 2 In fact, humans never produce the same translation, so may not directly carry over to NMT. The first of BLEU of 100 is unattainable, and the source text often conthe six challenges listed by Koehn and Knowles tains some tokens appearing also in the output, so not translating can reach BLEU scores of e.g. 3 or 4. (2017) suggests that fluency over adequacy can be 3 Document-level BLEU of 19.3 on miscellaneous a critical issue: NMT systems have lower quality FI→SV OPUS (Tiedemann, 2012) data. Current state of the out of domain, to the point that they completely ¨ benchmark (Tiedemann et al., art is 29.5 on the FIKSMO sacrifice adequacy for the sake of fluency. 2020). 10205 3.1 Documents In total, we used 99 source lines (segments) of 8 different parallel English documents for which Czech human reference translations were available. One line can contain more than one sentence, which is reflected by the rather high average sentence length of 25 words. We chose the domains to mirror common use-cases in localization: 36 lines of news texts (WMT19 News testset), 29 lines from a l"
2021.emnlp-main.801,2020.lrec-1.470,0,0.0240506,"Missing"
2021.emnlp-main.801,2020.wmt-1.41,1,0.825737,"Missing"
2021.findings-emnlp.303,P15-1136,0,0.0504828,"Missing"
2021.findings-emnlp.303,N09-1037,0,0.136783,"Missing"
2021.findings-emnlp.303,guillou-etal-2014-parcor,0,0.0314253,"henomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for Engli"
2021.findings-emnlp.303,2020.lrec-1.641,0,0.0768802,"Missing"
2021.findings-emnlp.303,hendrickx-etal-2008-coreference,0,0.126363,"Missing"
2021.findings-emnlp.303,N19-1419,0,0.0170907,"ctic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, whi"
2021.findings-emnlp.303,W05-0303,0,0.12226,"cuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but i"
2021.findings-emnlp.303,D19-1588,0,0.0125746,"ount, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phe"
2021.findings-emnlp.303,J94-4002,0,0.242889,"the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limis"
2021.findings-emnlp.303,L18-1065,0,0.0239068,"syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for English datasets converted to a harmonize"
2021.findings-emnlp.303,N18-2108,0,0.0459646,"Missing"
2021.findings-emnlp.303,2020.findings-emnlp.245,0,0.031795,"Missing"
2021.findings-emnlp.303,L16-1026,1,0.753884,"ructures correspond by design. In the latter case, To the best of our knowledge, there are only two coreference annotations made use either of conlarge-scale coreference-annotated datasets where syntax is closely linked to coreference relations. stituency trees – an English dataset from OntoNotes In AnCora-CO (Recasens and Martí, 2010), co- (Weischedel et al., 2011), and Spanish and Catalan referring mentions are nodes in constituency trees, datasets from the AnCora project (Recasens and and in the Prague Dependency corpora (Hajiˇc et al., Martí, 2010)), or of dependency trees – a Czech 2020; Nedoluzhko et al., 2016; Mikulová et al., dataset from the Prague Dependency Treebank (Hajiˇc et al., 2020), and English and Czech datasets 2017), coreference relations are annotated directly from the Prague Czech-English Dependency Treebetween syntactic heads in dependency trees and bank (Nedoluzhko et al., 2016). mention spans are implicitly defined as subtrees of the heads. The selection resulted in 9 datasets, for which Finkel and Manning (2009) deal with issues simi- we use their CorefUD labels: (1) English-GUM: lar to our work and have developed a model that per- Georgetown Multilayer Corpus (Zeldes, 2017) (th"
2021.findings-emnlp.303,P02-1014,0,0.105273,"utational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite p"
2021.findings-emnlp.303,2020.lrec-1.497,1,0.883163,"Missing"
2021.findings-emnlp.303,P13-1051,1,0.859969,"Missing"
2021.findings-emnlp.303,L18-1061,0,0.0383519,"Missing"
2021.findings-emnlp.303,2020.acl-main.622,0,0.0142617,"s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generati"
2021.findings-emnlp.303,K17-3009,0,0.0297901,"is a collection of coreference datasets unidependency trees, would be beneficial in the long fied under a common scheme. Mention spans in term from various linguistic and computational per- all 9 datasets result from manual annotation. Despectives, especially if we hypothesize that: pendency trees available in the collection follow the Universal Dependencies (UD) scheme (Nivre 1. mentions are not just unconstrained subseet al., 2020) and result from manual annotation in quences of tokens, but mostly correspond to one case and from automatic parsing with UDPipe syntactically meaningful units, (Straka and Straková, 2017) in the 8 remaining 2. certain types of coreference relations are man- cases. In all cases, the dependency trees came into ifested primarily by syntactic means (such as existence independently of the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to prono"
2021.humeval-1.13,2020.emnlp-main.5,0,0.0403366,"Missing"
2021.humeval-1.13,W13-2305,0,0.0101524,"he 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fashion, on a 0–10 scale, where all the evaluated translations are shown on the same screen, allowing thus better reliability in comparisons; see Section 5 for details. 3 Automatic analysis of references Table 1 shows the translation quality of the three references and two selected MT systems according to two manual evaluations, DA (Direct Assessment, Graham et al., 2013) and RankME, and four types of BLEU scores. The first three types use http://www.statmt.org/wmt06 till wmt20 2 The additional references R EF 2 and R EF 3 were not available before our RankME evaluation started. We plan to evaluate them in future. 114 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 114–119 Online, April 19, 2021. ©2021 Association for Computational Linguistics manual system REF 1 REF 2 REF 3 CUNI-D OC T RANSFORMER O NLINE -B BLEU DA RankME R EF 1 R EF 2 R EF 3 R EF 2+3 85.6 – – 82.8 70.5 8.17 – – 7.39 5.62 – 28.90 24.20 35.88 41.11 28.91 – 26.45"
2021.humeval-1.13,N18-2012,0,0.0118233,"-G and O NLINE -Z. We focused on three translations: the official reference, R EF 1; the best-performing MT system (according to the official WMT manual evaluation), CUNI-D OC T RANSFORMER (Popel, 2020); and the best-performing online system, O NLINE -B. We hired two professional translators (native Czech speakers) to translate the whole WMT20 test set, thus creating additional references R EF 2 and R EF 3. We also hired 18 annotators to judge the translation quality of R EF 1, CUNID OC T RANSFORMER and O NLINE -B.2 The annotators assessed 90 of the 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fashion, on a 0–10 scale, where all the evaluated translations are shown on the same screen, allowing thus better reliability in comparisons; see Section 5 for details. 3 Automatic analysis of references Table 1 shows the translation quality of the three references and two selected MT systems according to two manual evaluations, DA (Direct Assessment, Graham et al., 2013) and RankME, and four types of BLEU scores. The f"
2021.humeval-1.13,2020.wmt-1.28,1,0.692658,"nally written 1 in English – news stories downloaded from web. The test set comes with an official reference translation into Czech (R EF 1) provided by the WMT organizers and done by a professional translation agency. There are also 8 machine translations submitted by the participants of the WMT news translation shared task and 4 translations by online systems anonymized as O NLINE -A, O NLINE -B, O NLINE -G and O NLINE -Z. We focused on three translations: the official reference, R EF 1; the best-performing MT system (according to the official WMT manual evaluation), CUNI-D OC T RANSFORMER (Popel, 2020); and the best-performing online system, O NLINE -B. We hired two professional translators (native Czech speakers) to translate the whole WMT20 test set, thus creating additional references R EF 2 and R EF 3. We also hired 18 annotators to judge the translation quality of R EF 1, CUNID OC T RANSFORMER and O NLINE -B.2 The annotators assessed 90 of the 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fas"
2021.humeval-1.13,W15-3049,0,0.0792015,"Missing"
2021.humeval-1.13,W18-6319,0,0.0273676,"pril 19, 2021. ©2021 Association for Computational Linguistics manual system REF 1 REF 2 REF 3 CUNI-D OC T RANSFORMER O NLINE -B BLEU DA RankME R EF 1 R EF 2 R EF 3 R EF 2+3 85.6 – – 82.8 70.5 8.17 – – 7.39 5.62 – 28.90 24.20 35.88 41.11 28.91 – 26.45 36.50 31.08 24.18 26.43 – 30.17 26.39 37.22 – – 47.59 41.00 Table 1: Manual and automatic evaluation scores of the systems in our study. DA is the source-based Direct Assessment average score (un-normalized). RankME is the average Overall quality score over all 90 documents (not sentences) evaluated in our study. BLEU is computed with SacreBLEU (Post, 2018) with signature BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.13. The best score in each column is in bold. R EF 1, R EF 2 and R EF 3, respectively, as the reference translation in BLEU. The fourth type uses BLEU with two reference translations: R EF 2+3. While both manual evaluations, DA and RankME, agree that R EF 1 is better than both CUNI-D OC T RANSFORMER and O NLINE -B, the automatic metric BLEU evaluates one of the two MT systems as better than R EF 1.3 For brevity, we report only BLEU, but we confirmed this with several other automatic metrics, e.g. chrF (Popovi´c, 2015). Th"
2021.wnut-1.38,W18-6111,0,0.0648404,"Missing"
2021.wnut-1.38,W19-4423,0,0.0239064,"Missing"
2021.wnut-1.38,W13-1703,0,0.01923,"ype. The noising aspect probabilities are estimated by frequency analysis.3 To accurately model the distribution of amount of errors in different sentences, we also measure the standard deviation of the token edit probability per sentence. We collected M2 files from various grammaticalerror-correction corpora in 4 languages: English, Czech, Russian and German. The majority of annotated content comes from Second Learners of the particular language and in addition, more speaker groups are available in English and Czech: • English • Natives: LOCNESS v2.1 (Granger, 1998) • Second Learners: NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), Write & Improve (Yannakoudakis et al., 2018) • Czech • Natives: essays of Czech primary schools students, in submission process • Natives Informal: web discussions data, in submission process • Second Learners: AKCES-GEC (Šebesta et al., 2019) • Romani: AKCES-GEC (Šebesta et al., 2019) – Romani ethnic minority children and teenagers using Czech 2 3 http://aspell.net/ 342 GEC file format since the CoNLL-2013 shared task We refer to the published source code for details. Language Corpus Sentences Error rate Domain English NUCLE (Dahlmeier et al., 2013) FCE (Ya"
2021.wnut-1.38,N19-1423,0,0.0351424,"s and a selection of error aspects in multiple languages. The NLP tasks include morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of GLUE benchmark and reading comprehension. We conclude that: • The amount of noise is far more important than the distribution of error types. • Sensitivity to noise differs greatly among NLP tasks. While tasks such as lemmatization require correcting the input text, only an approximate understanding is sufficient for others. Although there has recently been an amazing progress in variety of NLP tasks (Vaswani et al., 2017; Devlin et al., 2019) with some models even We also compare two approaches for increasing reaching performance comparable to humans on models robustness to noise: training with noise and certain domains (Ge et al., 2018; Popel et al., 2020), external grammatical-error-correction (GEC) preit has been shown that the models are very sensitive processing. Our findings suggest that training with to noise in data (Belinkov and Bisk, 2017; Rychal- noise is beneficial for models with large capacity ska et al., 2019). and large training data (neural machine translation), Multiple areas of NLP have been studied to eval- whi"
2021.wnut-1.38,P18-2103,0,0.0984343,"correction (GEC) preit has been shown that the models are very sensitive processing. Our findings suggest that training with to noise in data (Belinkov and Bisk, 2017; Rychal- noise is beneficial for models with large capacity ska et al., 2019). and large training data (neural machine translation), Multiple areas of NLP have been studied to eval- while the preprocessing with grammatical-erroruate the effect of noise in data (Belinkov and Bisk, correction is more suitable for limited-data classifi2017; Heigold et al., 2018; Ribeiro et al., 2018; cation tasks, such as morpho-syntactic analysis. Glockner et al., 2018) and a framework for text Finally, we also offer an evaluation on authentic corruption to test NLP models robustness is also noise: We assembled a new dataset with authentic available (Rychalska et al., 2019). However, all Czech noisy sentences translated into English and these systems introduce noise in a custom-defined, we evaluate the noise-mitigating strategies in the arbitrary level and typically for a single language. neural machine translation task on this dataset. 340 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 340–350 November"
2021.wnut-1.38,W19-5303,0,0.0187539,"ciation for Computational Linguistics 2 Related Work for training GEC systems whereas we also use it to asses model performance in noisy scenarios. Many empirical findings have shown the fact that Authentic Noise Evaluation The growing interest data with natural noise deteriorate NLP systems in developing production-ready machine translaperformance. Belinkov and Bisk (2017) found that tion models that are robust to natural noise resulted natural noise such as misspellings and typos cause significant drops in BLEU scores of character- in the First Shared Task on Machine Translation Robustness (Li et al., 2019). The shared task used the level machine translation models. To increase the MTNT dataset (Michel and Neubig, 2018), which model’s robustness, they trained the model on a consists of noisy texts collected from Reddit and mixture of original and noisy input and found out that it learnt to address certain amount of errors. their translations between English and French and Similar findings were observed by Heigold et al. English and Japanese. (2018) who tested machine translation and mor- Improving Model Robustness Using Noisy Data phological tagging under three types of word-level Majority of re"
2021.wnut-1.38,D18-1050,0,0.0189234,"es model performance in noisy scenarios. Many empirical findings have shown the fact that Authentic Noise Evaluation The growing interest data with natural noise deteriorate NLP systems in developing production-ready machine translaperformance. Belinkov and Bisk (2017) found that tion models that are robust to natural noise resulted natural noise such as misspellings and typos cause significant drops in BLEU scores of character- in the First Shared Task on Machine Translation Robustness (Li et al., 2019). The shared task used the level machine translation models. To increase the MTNT dataset (Michel and Neubig, 2018), which model’s robustness, they trained the model on a consists of noisy texts collected from Reddit and mixture of original and noisy input and found out that it learnt to address certain amount of errors. their translations between English and French and Similar findings were observed by Heigold et al. English and Japanese. (2018) who tested machine translation and mor- Improving Model Robustness Using Noisy Data phological tagging under three types of word-level Majority of research on improving model robusterrors. ness is dedicated to training on a mixture of origiRibeiro et al. (2018) de"
2021.wnut-1.38,D19-5545,1,0.850417,"le corpus error level. be better in scenarios with small amount of annotated data. In such cases, only few iterations over training data are typically performed to prevent overfitting, and we suppose that learning the task itself and denoising at the same time would harm its performance a lot. Contrarily, with enough data and appropriate model capacity, learning the denoising and the task jointly may reduce the amount of potential false positives that might be otherwise proposed by the external language corrector. 6.1 External Correction Model We use the grammatical-error-correction system of Náplava and Straka (2019) in our experiments. Their models trained on Czech, German and Russian achieve state-of-the-art results and slightly below state-of-the-art results on English. We use their “pretrained” version. We modified the pipeline of Náplava and Straka (2019) to train on detokenized text. Furthermore, we also trained new grammatical-error-correction models which only make corrections that strictly keep the given tokenization (important in morphosyntactic annotations). To sum up, we trained two types of grammatical-error-correction models: 1. detokenized error correction model (for NMT) 2. tokenization-pr"
2021.wnut-1.38,W18-6319,0,0.0207803,"Missing"
2021.wnut-1.38,W18-1807,0,0.0249609,"Missing"
2021.wnut-1.38,P18-2124,0,0.0299159,"and De Meulder, 2003); for Czech, we used a fine-grained Czech Named Entity Corpus 2.0 (Ševˇcíková et al., 2007) with 46 types of nested entities. Metric The evaluation metric is F1 score. 80 60 40 CS Native Speaker CS Second Learners EN Native Speaker EN Second Learners 20 0 Diacritics Casing Spelling Affixes Figure 1: Proportional distribution of the first 4 aspects (diacritics, casing, spelling, affixes) in Czech and English. 4.5 Reading Comprehension Model We utilize a BERT base architecture with a standard SQuAD classifier on top (Devlin et al., 2019). Dataset We employ English SQuAD 2 (Rajpurkar et al., 2018) and its Czech translation (Macková and Straka, 2020). Metric Our experiments are evaluated using F1 score. 5 Robustness to Noise We evaluated the models robustness both to the amount of noise (Figure 2) and to error types (FigWe select a subset of GLUE (Wang et al., 2018) ures 3 and 4). tasks, namely Microsoft Research Paraphrase CorA unifying trend can be observed in models pus (MRPC), Semantic Textual Similarity Bench- performance with respect to increasing percentmark (STS-B), Quora Question Pairs (QQP) and age of token edits. Solid lines in Figure 2 display The Stanford Sentiment Treebank"
2021.wnut-1.38,D18-1541,0,0.0154935,"lockner et al. (2018) crein machine translation and GEC often use so called ated a new test set for natural language inference and showed that current systems do not general- back-translation (Sennrich et al., 2016). A reverse model translating in the opposite direction (i.e. ize well even for a single-word replacements by from the target language to the source language or synonyms and antonyms. Rychalska et al. (2019) implemented a frame- from the clean sentence into noisy sentence, respectively) is trained (Rei et al., 2017; Náplava, 2017; work for introducing multiple noise types into text Kasewa et al., 2018; Xie et al., 2018). It is then such as removing or swapping articles, rewriting used on the corpus of clean sentences to generate digit numbers into words or introducing errors in noisy input data. While this approach might genspelling. They found out in four NLP tasks that even recent state-of-the-art systems based on con- erate high-quality synthetic data, it requires large textualized word embeddings are not completely volumes of training data. We evaluate two approaches to alleviate perforrobust against such natural noise. They also remance drop on noisy data: We either train the systrain"
2021.wnut-1.38,W17-5032,0,0.0240693,"answering To generate synthetic training data, researchers and sentiment analysis. Glockner et al. (2018) crein machine translation and GEC often use so called ated a new test set for natural language inference and showed that current systems do not general- back-translation (Sennrich et al., 2016). A reverse model translating in the opposite direction (i.e. ize well even for a single-word replacements by from the target language to the source language or synonyms and antonyms. Rychalska et al. (2019) implemented a frame- from the clean sentence into noisy sentence, respectively) is trained (Rei et al., 2017; Náplava, 2017; work for introducing multiple noise types into text Kasewa et al., 2018; Xie et al., 2018). It is then such as removing or swapping articles, rewriting used on the corpus of clean sentences to generate digit numbers into words or introducing errors in noisy input data. While this approach might genspelling. They found out in four NLP tasks that even recent state-of-the-art systems based on con- erate high-quality synthetic data, it requires large textualized word embeddings are not completely volumes of training data. We evaluate two approaches to alleviate perforrobust agains"
2021.wnut-1.38,P18-1079,0,0.105814,"ains (Ge et al., 2018; Popel et al., 2020), external grammatical-error-correction (GEC) preit has been shown that the models are very sensitive processing. Our findings suggest that training with to noise in data (Belinkov and Bisk, 2017; Rychal- noise is beneficial for models with large capacity ska et al., 2019). and large training data (neural machine translation), Multiple areas of NLP have been studied to eval- while the preprocessing with grammatical-erroruate the effect of noise in data (Belinkov and Bisk, correction is more suitable for limited-data classifi2017; Heigold et al., 2018; Ribeiro et al., 2018; cation tasks, such as morpho-syntactic analysis. Glockner et al., 2018) and a framework for text Finally, we also offer an evaluation on authentic corruption to test NLP models robustness is also noise: We assembled a new dataset with authentic available (Rychalska et al., 2019). However, all Czech noisy sentences translated into English and these systems introduce noise in a custom-defined, we evaluate the noise-mitigating strategies in the arbitrary level and typically for a single language. neural machine translation task on this dataset. 340 Proceedings of the 2021 EMNLP Workshop W-NUT:"
2021.wnut-1.38,Q19-1001,0,0.0466382,"Missing"
2021.wnut-1.38,J17-4002,0,0.0194758,"certain conditions. Moreover, since we defined the individual error types with no language-specific rules, we can apply 3 Modeling Natural Noise from Corpora it to multiple languages with an available annotated grammatical-error corpus. Robustness of NLP models to natural noise would Grammatical-error corpora are typically used as ideally be evaluated on texts with authentic noise, training data for estimating error statistics in GEC with error corrections annotated by humans. (We systems. In a setting similar to ours, Choe et al. present such authentic data evaluation in Section 7.) (2019); Rozovskaya et al. (2017) also estimated This perfect-world setting, however, requires an error statistics and used them to generate additional immense annotation effort, as multiple target dotraining data for GEC systems. However, compared mains have to be covered by well-educated human to our approach, they defined only a small set of annotators for multiple NLP tasks in a range of lanpredefined error categories and used it specifically guages. To ease the annotation burden, we propose 341 a new framework, named KaziText, for introducing natural-like errors in a text. The core of KaziText is a set of several common"
2021.wnut-1.38,P16-1009,0,0.0391532,"The same procedure is usually used for generating both the test corpus and traintion rules that produce semantically equivalent text variants. They used them to test systems in ma- ing data (Belinkov and Bisk, 2017; Heigold et al., 2018; Ribeiro et al., 2018; Rychalska et al., 2019). chine comprehension, visual question answering To generate synthetic training data, researchers and sentiment analysis. Glockner et al. (2018) crein machine translation and GEC often use so called ated a new test set for natural language inference and showed that current systems do not general- back-translation (Sennrich et al., 2016). A reverse model translating in the opposite direction (i.e. ize well even for a single-word replacements by from the target language to the source language or synonyms and antonyms. Rychalska et al. (2019) implemented a frame- from the clean sentence into noisy sentence, respectively) is trained (Rei et al., 2017; Náplava, 2017; work for introducing multiple noise types into text Kasewa et al., 2018; Xie et al., 2018). It is then such as removing or swapping articles, rewriting used on the corpus of clean sentences to generate digit numbers into words or introducing errors in noisy input dat"
2021.wnut-1.38,P19-1527,1,0.882507,"Missing"
2021.wnut-1.38,W18-5446,0,0.0669387,"Missing"
2021.wnut-1.38,2020.emnlp-demos.6,0,0.0679474,"Missing"
2021.wnut-1.38,N18-1057,0,0.0184782,"crein machine translation and GEC often use so called ated a new test set for natural language inference and showed that current systems do not general- back-translation (Sennrich et al., 2016). A reverse model translating in the opposite direction (i.e. ize well even for a single-word replacements by from the target language to the source language or synonyms and antonyms. Rychalska et al. (2019) implemented a frame- from the clean sentence into noisy sentence, respectively) is trained (Rei et al., 2017; Náplava, 2017; work for introducing multiple noise types into text Kasewa et al., 2018; Xie et al., 2018). It is then such as removing or swapping articles, rewriting used on the corpus of clean sentences to generate digit numbers into words or introducing errors in noisy input data. While this approach might genspelling. They found out in four NLP tasks that even recent state-of-the-art systems based on con- erate high-quality synthetic data, it requires large textualized word embeddings are not completely volumes of training data. We evaluate two approaches to alleviate perforrobust against such natural noise. They also remance drop on noisy data: We either train the systrained the systems on n"
2021.wnut-1.38,P11-1019,0,0.0283436,"ilities are estimated by frequency analysis.3 To accurately model the distribution of amount of errors in different sentences, we also measure the standard deviation of the token edit probability per sentence. We collected M2 files from various grammaticalerror-correction corpora in 4 languages: English, Czech, Russian and German. The majority of annotated content comes from Second Learners of the particular language and in addition, more speaker groups are available in English and Czech: • English • Natives: LOCNESS v2.1 (Granger, 1998) • Second Learners: NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), Write & Improve (Yannakoudakis et al., 2018) • Czech • Natives: essays of Czech primary schools students, in submission process • Natives Informal: web discussions data, in submission process • Second Learners: AKCES-GEC (Šebesta et al., 2019) • Romani: AKCES-GEC (Šebesta et al., 2019) – Romani ethnic minority children and teenagers using Czech 2 3 http://aspell.net/ 342 GEC file format since the CoNLL-2013 shared task We refer to the published source code for details. Language Corpus Sentences Error rate Domain English NUCLE (Dahlmeier et al., 2013) FCE (Yannakoudakis et al., 2011) W&I (Yan"
2021.wnut-1.38,K18-2001,1,0.849114,"Missing"
bojar-etal-2012-joy,bojar-etal-2010-evaluating,1,\N,Missing
bojar-etal-2012-joy,C00-2163,0,\N,Missing
bojar-etal-2012-joy,W07-1709,0,\N,Missing
bojar-etal-2012-joy,P02-1040,0,\N,Missing
bojar-etal-2012-joy,H05-1066,0,\N,Missing
bojar-etal-2012-joy,P07-2045,1,\N,Missing
bojar-etal-2012-joy,W10-1703,0,\N,Missing
bojar-etal-2012-joy,W09-3939,1,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
L16-1296,W15-5705,1,0.838417,"Missing"
L16-1296,P02-1040,0,0.134922,"Missing"
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
P09-2037,W04-2201,0,0.0120485,"finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representaWe would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of depen"
P09-2037,J93-2003,0,0.0228161,"Missing"
P09-2037,N03-1017,0,0.021086,"used for labeling nodes of a dependency tree, interpreted as revealing the hidden states1 in the tree nodes, given another (observable) labeling of the nodes of the same tree. The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees. Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model. This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (Koehn et al., 2003)), in which translation and language models are trainable separately too. Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967))"
P09-2037,W01-1406,0,0.0229009,"l target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representaWe would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we s"
P09-2037,W08-0325,1,0.877289,"Missing"
P09-2037,2001.mtsummit-ebmt.4,0,\N,Missing
P13-1051,C00-2143,0,0.133964,"rinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling"
P13-1051,W06-2920,0,0.0228052,"section, we identify the CS styles defined in the previous section as used in the primary treebank data sources; statistical observations (such as the amount of annotated shared modifiers) presented here, as well as experiments on CS-style convertibility presented in Section 5.2, are based on the normalized shapes of the treebanks as contained in the HamleDT 1.0 treebank collection (Zeman et al., 2012).15 Some of the treebanks were downloaded individually from the web, but most of them came from previously published collections for dependency parsing campaigns: six languages from CoNLL-2006 (Buchholz and Marsi, 2006), seven languages from CoNLL-2007 (Nivre et al., 2007), two languages from CoNLL-2009 (Hajiˇc and others, 2009), three languages from ICON-2010 (Husain et al., 2010). Obviously, there is a certain risk that the CS-related information contained in the source treebanks was slightly biased by the properties of the CoNLL format upon conversion. In addition, many of the treebanks were natively dependency-based (cf. the 2nd column of Table 1), but some were originally based on constituents and thus specific converters to the CoNLL format had to be created (for instance, the Spanish phrase-structure"
P13-1051,dzeroski-etal-2006-towards,0,0.138307,"CS head, or attaching shared modifiers below the nearest conjunct). Even if it does not make sense to create the full Cartesian product of all dimensions because some values cannot be combined, it allows to explore the space of possible CS styles systematically.9 One can find various arguments supporting the particular choices. MTT possesses a complex set of linguistic criteria for identifying the governor of a relation (see Mazziotta (2011) for an overview), which lead to MS. MS is preferred in a rule-based dependency parsing system of Lombardo and Lesmo (1998). PS is advocated by ˇ ep´anek (2006) who claims that it can represent Stˇ shared modifiers using a single additional binary attribute, while MS would require a more complex co-indexing attribute. An argumentation of Tratz and Hovy (2011) follows a similar direction: We would like to change our [MS] handling of coordinating conjunctions to treat the coordinating conjunction as the head [PS] because this has fewer ambiguities than [MS]. . . We conclude that the influence of the choice of coordination style is a well-known problem in dependency syntax. Nevertheless, published works usually focus only on a narrow ad-hoc selection of"
P13-1051,W12-0503,1,0.894933,"Missing"
P13-1051,afonso-etal-2002-floresta,0,0.0183801,"hers, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinatio"
P13-1051,W03-2405,0,0.0337781,"Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling variants defined in Section 3.2. Variations in representing coordination structures Our analysis of variations in representing coordination structures is based on observations from a set of dependency treebanks for 26 languages.7 5 We use the already establishe"
P13-1051,W09-1201,1,0.0605496,"Missing"
P13-1051,W12-3603,1,0.894647,"Missing"
P13-1051,ramasamy-zabokrtsky-2012-prague,1,0.894745,"Missing"
P13-1051,E09-1047,0,0.144276,"Missing"
P13-1051,W98-0502,0,0.490473,"tactic surface means of expressing coordination relations is fuzzy. Some languages can use enclitics instead of conjunctions/prepositions, e.g. Latin “Senatus Populusque Romanus”. Purely hypotactic surface means such as the preposition in “John with Mary” occur too.4 Related work Let us first recall the basic well-known characteristics of CSs. In the simplest case of a CS, a coordinating conjunction joins two (usually syntactically and semantically compatible) words or phrases called conjuncts. Even this simplest case is difficult to represent within a dependency tree because, in the words of Lombardo and Lesmo (1998): Dependency paradigms exhibit obvious difficulties with coordination because, differently from most linguistic structures, it is not possible to characterize the coordination construct with a general schema involving a head and some modifiers of it. Proper formal representation of CSs is further complicated by the following facts: • Careful semantic analysis of CSs discloses additional complications: if a node is modified by a CS, it might happen that it is the node itself (and not its modifiers) what should be semantically considered as a conjunct. Note the difference between “red and white"
P13-1051,J93-2004,0,0.0420156,"rent granularity of syntactic labels. 3 3.1 Topological variations We distinguish the following dimensions of topological variations of CS styles (see Figure 1): Family – configuration of conjuncts. We divide the topological variations into three main groups, labeled as Prague (fP), Moscow (fM), and vided by IXA Group) (Aduriz and others, 2003), Bulgarian: BulTreeBank (Simov and Osenova, 2005), Czech: Prague Dependency Treebank 2.0 (Hajiˇc et al., 2006), Danish: Danish Dependency Treebank (Kromann et al., 2004), Dutch: Alpino Treebank (van der Beek and others, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treeban"
P13-1051,D07-1013,0,0.158065,"Missing"
P13-1051,taule-etal-2008-ancora,0,0.0175746,"Missing"
P13-1051,zeman-etal-2012-hamledt,1,0.0729458,"Missing"
P13-1051,D11-1116,0,\N,Missing
P13-1051,J03-4003,0,\N,Missing
P13-1051,D07-1096,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2006-generating,0,\N,Missing
rosa-etal-2014-hamledt,zeman-2008-reusable,1,\N,Missing
rosa-etal-2014-hamledt,J93-2004,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2014-universal,0,\N,Missing
rosa-etal-2014-hamledt,C00-2143,0,\N,Missing
rosa-etal-2014-hamledt,W08-1301,0,\N,Missing
rosa-etal-2014-hamledt,W13-3721,0,\N,Missing
rosa-etal-2014-hamledt,D11-1006,0,\N,Missing
rosa-etal-2014-hamledt,P13-1051,1,\N,Missing
rosa-etal-2014-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
rosa-etal-2014-hamledt,berovic-etal-2012-croatian,0,\N,Missing
rosa-etal-2014-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
rosa-etal-2014-hamledt,W03-2405,0,\N,Missing
rosa-etal-2014-hamledt,P13-2017,0,\N,Missing
rosa-etal-2014-hamledt,taule-etal-2008-ancora,0,\N,Missing
rosa-etal-2014-hamledt,W10-1819,0,\N,Missing
rosa-etal-2014-hamledt,afonso-etal-2002-floresta,0,\N,Missing
W09-0422,bojar-etal-2008-czeng,1,0.888991,"Missing"
W09-0422,W08-0309,0,0.0625391,"ion of prepositional group would be difficult otherwise. After the capitalization of the beginning of each sentence (and each named entity instance), we obtain the final translation by flattening the surface tree. Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in English→Czech translation, as evaluated on the official WMT09 unseen test set. Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but “lower scoring” PC Translator this year and also in Callison-Burch et al. (2008). System BLEU Moses T 14.24 Moses T+C 13.86 Google 13.59 U. of Edinburgh 13.55 Moses T+C+C&T+T+G 84k 10.01 Eurotran XP 09.51 PC Translator 09.42 TectoMT 07.29 NIST 5.175 5.110 4.964 5.039 4.360 4.381 4.335 4.173 Rank -3.02 (4) – -2.82 (3) -3.24 (5) -2.81 (2) -2.77 (1) -3.35 (6) Table 3: Automatic scores and preliminary human rank for English→Czech translation. Systems in italics are provided for comparison only. Best results in bold. Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly 4.4 Preliminary Error Analysis because it is easier to tune them pr"
W09-0422,P05-1045,0,0.0043121,"ons: 15 33 43 . 5 Later, we found out that the grow-diag-final-and heuristic provides insignificantly superior results. 4 ˇ In some previous experiments (e.g.Zabokrtsk´ y et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. 2 126 with the option to resort to (2) an independent translation of lemma→lemma and tag→tag finished by a generation step that combines target-side lemma and tag to produce the final target-side form. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization). This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation. We use three language models in this setup (3-grams of forms, 3-grams of lemmas, and 10-grams of tags). Due to the increased complexity of the setup, we were able to train this model on 84k parallel sentences only (the Commentaries section) and we use the target-side"
W09-0422,P07-2045,1,0.010104,"naming conventions. However, we were unable to reliably determine the series number and the episode number from the file names. We employed a two-step procedure to automatically pair the TV series subtitle files. For every TV series: We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for English→Czech translation, and (2) a complex ˇ system in the TectoMT platform (Zabokrtsk´ y et al., 2008). 1. We clustered the files on both sides to remove duplicates 2. We found the best matching using a provisional translation dictionary. This proved to be a successful technique on a small sample of manually paired test data. The process was facilitated by the fact that the correct pairs of episodes usually share some named entities which the human translator chose to keep in the original English form. 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the"
W09-0422,J03-1002,0,0.00228104,"0.4 Table 2: Czech-English data sizes and sources. ∗ The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK ˇ ˇ LC536 and FP6-IST-5-034291-STP 52408/2008, MSMT CR (EuroMatrix). 1 www.opensubtitles.org and titulky.com Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125–129, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 125 2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0"
W09-0422,W07-1709,0,0.170323,"Missing"
W09-0422,W08-0325,1,0.861517,"emented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajiˇc and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer). The analysis was implemented using TectoMT, ˇ (Zabokrtsk´y et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajiˇc’s Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set ˇ of features as described in Nov´ak and Zabokrt"
W09-0422,J03-4003,0,\N,Missing
W09-0422,H05-1066,0,\N,Missing
W09-0422,W08-0319,1,\N,Missing
W09-0422,2008.eamt-1.16,1,\N,Missing
W10-1730,N07-1008,0,0.0186993,"(SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of th"
W10-1730,W04-3250,0,0.149325,"mes) instead of word forms. In particular, our target-language tree model (TreeLM) predicts the probability of node’s lemma and formeme given its parent’s lemma and formeme. The optimal (lemma and formeme) labeling is found by tree-modified Viterbi search; ˇ for details see (Zabokrtsk´ y and Popel, 2009). 4 Experiments When included into the above described translation scenario, the MaxEnt TM outperforms the baseline TM, be it used together with or without TreeLM. The results are summarized in Table 1. The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004). In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models. 205 5 Conclusions Jan Hajiˇc. 2004. Disambiguation of Rich Inflection – Computational Morphology of Czech. Charles University – The Karolinum Press, Prague. We have introduced a maximum entropy translation model in dependency-based MT which enables exploiting a large number of feature functions in order to obtain more accurate translations. The BLEU evaluation p"
W10-1730,H05-1066,0,0.157359,"Missing"
W10-1730,P02-1038,0,0.0667274,"antic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combin"
W10-1730,W03-0420,0,0.0157367,"el data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abstracts from language-specific means (inflection, agg"
W10-1730,J96-1002,0,0.0200957,"model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The idea of using maximum entropy for constructing forward translation models is not new. It naturally allows to make use of various features potentially important for correct choice of targetlanguage expressions. Let us adopt a motivating example of such a feature from (Berger et al., 1996) (which contains the first usage of maxent translation model we are aware of): “If house appears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely [French] translation [of in].” Incorporating non-local features extracted from the source sentence into the standard noisychannel model in which only the backward translation model is available, is not possible. This drawback of the noisy-channel approach is typically compensated by using large target-language n-gram models, which can – in a result – play a role similar to that of a m"
W10-1730,W96-0213,0,0.208466,"wever, we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) t"
W10-1730,A00-2018,0,0.0559398,"d be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abs"
W10-1730,W07-1709,0,0.179472,"Missing"
W10-1730,P00-1006,0,0.0343477,"v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the tran"
W10-1730,P09-2037,1,0.49307,"; coreference is also resolved. Collapsing edges are depicted by wider lines in the Figure 1a. 3 Training the two models In this section we describe two translation models used in the experiments: a baseline translation model based on maximum likelihood estimates (3.2), and a maximum entropy based model (3.3). Both models are trained using the same data (3.1). In addition, we describe a target-language tree model (3.4), which can be combined with both the translation models using the Hidden Tree Markov Model approach and tree-modified Viterbi ˇ search, similarly to the approach of (Zabokrtsk´ y and Popel, 2009). 5. The transfer phase follows, whose most difficult part consists in labeling the tree with target-side lemmas and formemes5 (changes of tree topology are required relatively infrequently). See Figure 1c. 3.1 6. Finally, surface sentence shape (Figure 1d) is synthesized from the tectogrammatical tree, which is basically a reverse operation for the Data preprocessing common for both models We used Czech-English parallel corpus CzEng 0.9 ˇ (Bojar and Zabokrtsk´ y, 2009) for training the translation models. CzEng 0.9 contains about 8 million sentence pairs, and also their tectogrammatical analy"
W10-1730,D09-1023,0,0.0237895,"e implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as foll"
W10-1730,W08-0325,1,0.702701,"(1) tectogrammatics largely abstracts from language-specific means (inflection, agglutination, functional words etc.) of expressing non-lexical meanings and thus tectogrammatical trees are supposed to be highly similar across languages,2 (2) it enables a natural transfer factorization,3 (3) and local tree contexts in tectogrammatical trees carry more information (especially for lexical choice) than local linear contexts in the original sentences.4 In order to facilitate transfer of sentence ‘syntactization’, we work with tectogrammatical nodes ˇ enhanced with the formeme attribute (Zabokrtsk´ y et al., 2008), which captures the surface morphosyntactic form of a given tectogrammatical node in a compact fashion. For example, the value n:pˇred+4 is used to label semantic nouns that should appear in an accusative form in a prepositional group with the preposition pˇred in Czech. For English we use formemes such as n:subj (semantic noun (SN) in subject position), n:for+X (SN with preposition for), n:X+ago (SN with postposition ago), n:poss (possessive form of SN), v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), ad"
W11-2101,J08-4004,0,0.0118721,"e lengths. We see that the interannotator agreement (when excluding references) is reasonably high only for sentences up to 10 words in length – according to Landis and Koch (1977), and as cited by the WMT overview paper, not even ‘moderate’ agreement can be assumed if κ is less than 0.4. Another popular (and controversial) rule of thumb (Krippendorff, 1980) is more strict and says that κ < 0.67 is not suitable even for tentative conclusions. 4 2.2.2 Estimating P (E), the Expected Agreement by Chance Several agreement measures (usually called kappas) were designed based on the Equation 1 (see Artstein and Poesio (2008) and Eugenio and Glass (2004) for an overview and a discussion). Those measures differ from each other in how to define the individual components of Equation 2, and hence differ in what the expected agreement by chance (P (E)) would be:8 • The S measure (Bennett et al., 1954) assumes a uniform distribution over the categories. • Scott’s π (Scott, 1955) estimates the distribution empirically from actual annotation. • Cohen’s κ (Cohen, 1960) estimates the distribution empirically as well, and further assumes a separate distribution for each annotator. Given that the WMT10 overview paper assumes"
W11-2101,W10-1703,1,0.863735,"Missing"
W11-2101,2010.eamt-1.12,0,0.0147419,"Missing"
W11-2101,vilar-etal-2006-error,0,0.00766891,"Missing"
W11-2153,P05-1022,0,0.248854,"g to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU."
W11-2153,W07-2416,0,0.584308,"might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 However, the parser bottleneck of the dependency-based MT approach was observed also by other researchers (R"
W11-2153,P03-1054,0,0.0229106,"have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 Howeve"
W11-2153,W10-1730,1,0.897432,"Missing"
W11-2153,E06-1011,0,0.0143919,"ion that, n:sb – semantic noun in a subject position, n:for+X – semantic noun in a prepositional group introduced with preposition for, adj:attr – semantic adjective in an attributive position. 435 Involved Parsers We performed experiments with parsers from three families: graph-based parsers, transitionbased parsers, and phrase-structure parsers (with constituency-to-dependency postprocessing). 3.1 Graph-based Parser In graph-based parsing, we learn a model for scoring graph edges, and we search for the highest-scoring tree composed of the graph’s edges. We used Maximum Spanning Tree parser (Mcdonald and Pereira, 2006) which is capable of incorporating second order features (MST for short). 3.2 Transition-based Parsers Transition-based parsers utilize the shift-reduce algorithm. Input words are put into a queue and consumed by shift-reduce actions, while the output parser is gradually built. Unlike graph-based parsers, transition-based parsers have linear time complexity and allow straightforward application of non-local features. We included two transition-based parsers into our experiments: • Malt – Malt parser introduced by Nivre et al. (2007) 6 5 http://ucnk.ff.cuni.cz We used stackeager algorithm, libl"
W11-2153,H05-1066,0,0.288682,"Missing"
W11-2153,W07-1709,0,0.0270164,"Missing"
W11-2153,P07-1031,0,0.0148678,"d be parsed independently of the rest of the sentence. This was shown to improve not only parsing accuracy of the parenthesed word sequence (which is forced to remain in one subtree), but also the rest of the sentence.10 In our experiments, SentChunk is used only in combination with the three genuine dependency parsers. 4 Experiments and Evaluation 4.1 Data for Parsers’ Training and Evaluation The dependency trees needed for training the parsers and evaluating their UAS were created from the Penn Treebank data (enriched first with internal noun phrase structure applied via scripts provided by Vadas and Curran (2007)) by Penn Converter (Johansson and Nugues, 2007) with the -conll2007 option (PennConv for short). All the parsers were evaluated on the same data – section 23. All the parsers were trained on sections 02–21, except for the Stanford parser which was trained on sections 01–21. We were able to retrain the parser models only for MST and Malt. For the other parsers we used pretrained models available on the Internet: CJ’s default model ec50spfinal, Stanford’s wsjPCFG.ser.gz model, and 10 Edge length is a common feature in dependency parsers, so “deleting” parenthesed words may give higher scores to"
W11-2153,P09-2037,1,0.946992,"ependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors have already been addressed by improvements based on Hidden Markov Tree Models for lexical and syntactic choice as shown by ˇ Zabokrtsk´ y and Popel (2009), and by context sensitive translation models based on maximum entropy as described by Mareˇcek et al. (2010). Our study proceeds along two directions. First, we train two state-of-the-art dependency parsers on training sets with varying size. Second, we use five parsers based on different parsing techniques. In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric). The motivation behind the first set of experiments is that we can extrapolate the learning curve and try to pred"
W11-2153,W08-0325,1,0.950709,"study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU. 1 Introduction In the last years, statistical n-gram models dominated the field of Machine Translation (MT). However, their results are still far from perfect. Therefore we believe it makes sense to investigate alternative statistical approaches. This paper is focused on an analysis-transfer-synthesis translation system called TectoMT whose transfer representation has a shape of a deep-syntactic dependency tree. The system has ˇ been introduced by Zabokrtsk´ y et al. (2008). The translation direction under consideration is Englishto-Czech. It has been shown by Popel (2009) that the current accuracy of the dependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors h"
W11-2153,P11-2033,0,0.167717,"of pronouns (personal and possessive pronouns referring to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence"
W11-2153,zhang-etal-2004-interpreting,0,0.0936613,"Missing"
W12-3132,hajic-etal-2012-announcing,1,0.802661,"Missing"
W12-3132,W04-3250,0,0.264493,"Missing"
W12-3132,W06-1606,0,0.0610868,"Missing"
W12-3132,W10-1730,1,0.899159,"Missing"
W12-3132,W01-1406,0,0.0190747,"actic description, mainly within valency lexicons, starting probably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysi"
W12-3132,P02-1040,0,0.0829475,". 6.1 Czech Synthesis The synthesis phase of the TectoMT system relies heavily on the information included in formemes, as its rule-based blocks use solely formemes and grammar rules to gradually change a deep tree node into a surface subtree. To directly measure the suitability of our changes for the synthesis stage of the TectoMT system, we used a Czech-to-Czech round trip—deep analysis of Czech PDT 2.0 development set sentences using the CzEng 1.0 pipeline (Bojar et al., 2012b), followed directly by the synthesis part of the TectoMT system. The results were evaluated using the BLEU metric (Papineni et al., 2002) with the original sentences as reference; they indicate a higher suitability of the new formemes for deep Czech synthesis (see Table 2). 6.2 Version Original formemes Revised formemes BLEU 0.6818 0.7092 Table 2: A comparison of formeme versions in Czech-toCzech round trip. Version Original formemes Revised formemes BLEU 0.1190 0.1199 Table 3: A comparison of formeme versions in Englishto-Czech TectoMT translation on the WMT12 test set. two translation scenarios—one using the original formemes and the second using the revised formemes in the formeme-to-formeme translation model. Due to time re"
W12-3132,W11-2153,1,0.890651,"Missing"
W12-3132,P05-1034,0,0.0232767,"robably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory"
W12-3132,W08-0325,1,0.921231,"Missing"
W12-3132,P09-2037,1,0.875964,"oMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986), proceeding over two layers of structural description, from shallow (analytical) to deep (tectogrammatical) (see Section 3.1). The transfer phase of the system is based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It is factorized into three subtasks: lemma, formeme and grammatemes translation (see Sections 3.2 and 3.3). The subsequent generation phase consists of rulebased components that gradually change the deep target language representation into a shallow one, which is then converted to text (cf. Section 6.1). The version of TectoMT submitted to WMT122 builds upon the WMT11 version. Several rule-based components were slightly refined. However, most of the effort was devoted to creating a better and bigger parallel treebank—CzEng 1.03 (Bojar et al., 2012b), and re-training the statistical componen"
W12-3132,W09-0422,1,\N,Missing
W12-3132,2001.mtsummit-ebmt.4,0,\N,Missing
W12-3132,bojar-etal-2012-joy,1,\N,Missing
W12-4205,hajic-etal-2012-announcing,0,0.0852801,"Missing"
W12-4205,W06-2920,0,0.0367148,"targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in"
W12-4205,N10-1015,0,0.0755019,"011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,W10-1703,0,0.0245956,"parser training data, so that the training sentences resemble SMT output. We evaluate the modified parser on DEP FIX , a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality. 1 Introduction The machine translation (MT) quality is on a steady rise, with mostly statistical systems (SMT) dominating the area (Callison-Burch et al., 2010; CallisonBurch et al., 2011). Most MT systems do not employ structural linguistic knowledge and even the stateof-the-art MT solutions are unable to avoid making serious grammatical errors in the output, which often leads to unintelligibility or to a risk of misinterpretations of the text by a reader. ∗ This research has been supported by the EU Seventh Framework Programme under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310 and GA201/09/H057. This problem is particularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the"
W12-4205,W11-2103,0,0.0450256,"Missing"
W12-4205,P10-1003,0,0.0232916,"ek et al., 2011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,P99-1065,0,0.316623,"Missing"
W12-4205,P08-2056,0,0.0358,"Missing"
W12-4205,D09-1127,0,0.0558467,"Missing"
W12-4205,P07-2045,0,0.0114404,"g one, creating the respective alignment link from word A (in the reference) to word B (in the SMT output) and deleting all scores of links from A or to B, so that one-to-one alignments are enforced. The process is terminated when no links with a score higher than a given threshold are available; some words may thus remain unaligned. The score is computed as a linear combination of the following four features: • word form (or lemma if available) similarity based on Jaro-Winkler distance (Winkler, 1990), 1. We translated the English side of PCEDT5 to Czech using SMT (we chose the Moses system (Koehn et al., 2007) for our experiments) and tagged the resulting translations using the Morˇce tagger (Spoustov´a et al., 2007). • fine-grained morphological tag similarity, • similarity of the relative position in the sentence, 2. We aligned the Czech side of PCEDT, now serving as a reference translation, to the SMT output using our Monolingual Greedy Aligner (see Section 4.2). 3. Collecting the counts of individual errors, we estimated the Maximum Likelihood probabilities of changing a correct fine-grained morphological tag (of a word from the reference) into a possibly incorrect fine-grained morphological ta"
W12-4205,J93-2004,0,0.0469362,"er. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR"
W12-4205,H05-1066,0,0.30865,"Missing"
W12-4205,W06-2932,0,0.0233358,"entence first and include features computed over the parsed source sentence in the set of features used for parsing SMT output. We first align the source and SMT output sentences on the word level and then use alignment-wise local features – i.e. for each SMT output word, we add features computed over its aligned source word, if applicable (cf. Section 3.4 for a listing). 3.2 Parsers Used We have reimplemented the MST parser (McDonald et al., 2005) in order to provide for a simple insertion of the parallel features into the models. We also used the original implementation of the MST parser by McDonald et al. (2006) for comparison in our experiments. To distinguish the two variants used, we denote the original MST parser as M C D parser,2 and the new reimplementation as RUR parser. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the"
W12-4205,D11-1006,0,0.0305903,"010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and"
W12-4205,J03-1002,0,0.0141147,"IRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR parser follows those described by McDonald et al. (2005). For parsing, we use the features described below. The individual features are computed for both the parent node and the child node of an edge and conjoined in various ways. The coarse morphological tag and lemma are provided by the Morˇce tagger (Spoustov´a"
W12-4205,P02-1040,0,0.0832465,"Missing"
W12-4205,W07-1709,0,0.0974333,"Missing"
W12-4205,stymne-ahrenberg-2010-using,0,0.0223864,"icularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the relations between individual words using morphological agreement instead of word order, together with the word order itself being relatively free, choosing the correct inflection becomes crucial. Since the output of phrase-based SMT shows frequent inflection errors (even in adjacent words) due to each word belonging to a different phrase, a possible way to address the grammaticality problem is a combination of statistical and structural approach, such as SMT output post-editing (Stymne and Ahrenberg, 2010; Mareˇcek et al., 2011). In this paper, we focus on improving SMT output parsing quality, as rule-based post-editing systems rely heavily on the quality of SMT output analysis. Parsers trained on gold standard parse trees often fail to produce the expected result when applied to SMT output with grammatical errors. This is partly caused by the fact that when parsing highly inflected free word-order languages the parsers have to rely on morphological agreement, which, as stated above, is often erroneous in SMT output. Training a parser specifically by creating a manually annotated treebank of M"
W12-4205,J97-3002,0,0.138024,"ation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in a joint scenario. We concentrate here on using dependency parsers because of tools and training data availability for the examined language pair. Regarding treebank adaptation for parser robustness, Foster et al. (2008) introduce various kinds of artificial errors into the training data to make the final parser less sensitive to grammar errors. However, their approach concentrates on mistakes made by humans (such as misspellings, word repetition or omission etc.) and the error models used are handcrafted. Our work focuse"
W12-4205,P10-1062,0,0.0211229,"ncy scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT output sentences and, ultimately, improve rule-based SMT post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDo"
W12-4205,I08-3008,0,0.0945642,"post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences"
W12-4205,P09-1007,0,0.0247054,"“RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual"
W12-4205,W11-2152,1,\N,Missing
W12-4205,D11-1007,0,\N,Missing
W12-4205,W09-1201,0,\N,Missing
W12-4205,D07-1096,0,\N,Missing
W12-4205,W10-1705,0,\N,Missing
W13-2216,2011.mtsummit-papers.35,0,0.0473899,"Missing"
W13-2216,W11-2101,1,0.872402,"Missing"
W13-2216,W09-0405,0,0.0153382,"in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables, leading to spurious ambiguity and thus less diversity in n-best lists of a given size (see Chen et al. (2009) for one possible solution). CComb does not suffer from the spurious ambiguity issue, but it does not allow to tune special features for the individual first-stage systems. serves well-formed syntactic sentence structures, and the SPE (Moses) fixes low fluency wordings. 2.2 MT Output Combination An SPE system is trained to improve the output of a single first-stag"
W13-2216,W09-0419,0,0.0745508,"nd P HRASE F IX – statistical post-editing of T ECTO MT using Moses (Koehn et al., 2007). We also report on experiments with another hybrid method where T EC TO MT is used to produce additional (so-called synthetic) parallel training data for Moses. This method was used in CU-B OJAR and CU-D EPFIX submissions, see Bojar et al. (2013). 2 Overview of Related Work The number of approaches to system combination is enormous. We very briefly survey those that form the basis of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several"
W13-2216,W08-0328,0,0.125383,"of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables, leading to spurious ambiguity and thus less diversity in n-best lists of a given size (see Chen et al. (2009) for one possible solution). CComb does not suffer from the spurious ambiguity issue, but it does not allow to tune special features for the individual first-stage systems. serves well-formed syntactic sentence structures, and the SPE (Moses) fixes low fluency wordings. 2.2 MT Output Combination An SPE system is trained to improve the output of"
W13-2216,N07-1029,0,0.022578,"slation of each sentence is a combination of phrases from several systems. In both approaches, the systems are treated as black boxes, so only their outputs are needed. In the simplest setting, all systems are supposed to be equally good/reliable, and the final output is selected by voting, based on the number of shared ngrams or language model scores. The number and the identity of the systems to be combined therefore do not need to be known in advance. More sophisticated methods learn parameters/weights specific for the individual systems. These methods are based e.g. on confusion networks (Rosti et al., 2007; Matusov et al., 2008) and joint optimization of word alignment, word order and lexical choice (He and Toutanova, 2009). 2.3 In our experiments, we use both CComb and PTComb approaches. In PTComb, we use T EC TO MT as the only first-stage system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated c"
W13-2216,D09-1125,0,0.0152484,"ted as black boxes, so only their outputs are needed. In the simplest setting, all systems are supposed to be equally good/reliable, and the final output is selected by voting, based on the number of shared ngrams or language model scores. The number and the identity of the systems to be combined therefore do not need to be known in advance. More sophisticated methods learn parameters/weights specific for the individual systems. These methods are based e.g. on confusion networks (Rosti et al., 2007; Matusov et al., 2008) and joint optimization of word alignment, word order and lexical choice (He and Toutanova, 2009). 2.3 In our experiments, we use both CComb and PTComb approaches. In PTComb, we use T EC TO MT as the only first-stage system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated corpus. 3 T ECTO MT T ECTO MT is a linguistically-motivated tree-totree deep-syntactic translation system with transfer b"
W13-2216,W07-0733,0,0.0297807,"to use the n firststage systems to prepare synthetic parallel data and include them in the training data for the SMT. Corpus Combination (CComb) The easiest method is to use these n newly created parallel corpora as additional training data, i.e. train Moses on a concatenation of the original parallel sentences (with human-translated references) and the new parallel sentences (with machinetranslated pseudo-references). Phrase Table Combination (PTComb) Another method is to extract n phrase tables in addition to the original phrase table and exploit the Moses option of multiple phrase tables (Koehn and Schroeder, 2007). This means that given the usual five features (forward/backward phrase/lexical log probability and phrase penalty), we need to tune 5 · (n + 1) features. Because such MERT (Och, 2003) tuning may be unstable for higher n, several methods were proposed where the n+1 phrase tables are merged into a single one 142 Corpus Sents CzEng tmt (CzEng) Czech Web Corpus WMT News Crawl 15M 15M 37M 25M Tokens Czech English 205M 236M 197M 236M 627M – 445M – T ECTO MT P HRASE F IX Filtering Mark Reliable Phr. Mark Identities Table 1: Statistics of used data. 4 BLEU 14.71±0.53 17.73±0.54 14.68±0.50 17.87±0.55"
W13-2216,N07-1064,0,0.349038,"ed MT because of the complementary nature of weaknesses and advantages of rule-based and statistical approaches. SPE is usually done with an off-the-shelf SMT system (e.g. Moses) which is trained on output of the first-stage system aligned with reference translations of the original source text. The goal of SPE is to produce translations that are better than both the first-stage system alone and the second-stage SMT trained on the original training data. Most SPE approaches use the reference translations from the original training parallel corpus to train the second-stage system. In contrast, Simard et al. (2007) use human-post-edited firststage system outputs instead. Intuitively, the latter approach achieves better results because the human-post-edited translations are closer to the first-stage output than the original reference translations. Therefore, SPE learns to perform the changes which are needed the most. However, creating human-post-edited translations is laborious and must be done again for each new (version of the) first-stage system in order to preserve its full advantage over using the original references.2 Rosa et al. (2013) have applied SPE on English→Czech SMT outputs. They have used"
W13-2216,P07-2045,1,0.0124533,"e put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from T ECTO MT to train a statistical MT system (SMT). We confirm that P HRASE F IX (SPE) improves the output of T ECTO MT, and we use this to analyze errors in T EC TO MT. However, we also show that extending data for SMT is more effective. 1 Introduction This paper describes two submissions to the WMT 2013 shared task:1 T ECTO MT – a deepsyntactic tree-to-tree system and P HRASE F IX – statistical post-editing of T ECTO MT using Moses (Koehn et al., 2007). We also report on experiments with another hybrid method where T EC TO MT is used to produce additional (so-called synthetic) parallel training data for Moses. This method was used in CU-B OJAR and CU-D EPFIX submissions, see Bojar et al. (2013). 2 Overview of Related Work The number of approaches to system combination is enormous. We very briefly survey those that form the basis of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are availab"
W13-2216,2006.amta-papers.25,0,0.03865,"etic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here slightly differ from the results shown at http://matrix.statmt. org/matrix/systems_list/1720 because of different tokenization and"
W13-2216,spoustova-spousta-2012-high,0,0.0943894,"Missing"
W13-2216,W04-3250,0,0.0861943,"ed to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here slightly differ from the results shown at http://matrix.statmt. org/matrix/syste"
W13-2216,W10-1730,1,0.910175,"Missing"
W13-2216,P09-2037,1,0.839154,"age system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated corpus. 3 T ECTO MT T ECTO MT is a linguistically-motivated tree-totree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It employs some rule-based components, but the most important tasks in the analysistransfer-synthesis pipeline are based on statistics and machine learning. There are three main reasons why it is a suitable candidate for SPE and other hybrid methods. • T ECTO MT has quite different distribution and characteristics of errors compared to standard SMT (Bojar et al., 2011). • T ECTO MT is not tuned for BLEU using MERT (its development is rather driven by human inspection of the errors although different setups are regularly evaluated with BLEU as an additional guidance). • T ECTO MT uses deep-sy"
W13-2216,J03-1002,0,0.00478822,"omitted), see Table 1 for statistics. We translated the English side of CzEng with T ECTO MT to obtain “synthetic Czech”. This way we obtained a new parallel corpus, denoted tmt (CzEng), with English ↔ synthetic Czech sentences. Analogically, we translated the WMT 2013 test set (newstest2013) with T ECTO MT and obtained tmt (newstest2013). Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt (newstest2013). For all MERT tuning, newstest2011 was used. 4.1 Alignment All our parallel data were aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the “grow-diag-final-and” heuristics. This applies also to the synthetic corpora tmt (CzEng), tmt (newstest2013),3 and smt (newstest2013). For the SPE experiments, we decided to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the"
W13-2216,P03-1021,0,0.00803066,"llel corpora as additional training data, i.e. train Moses on a concatenation of the original parallel sentences (with human-translated references) and the new parallel sentences (with machinetranslated pseudo-references). Phrase Table Combination (PTComb) Another method is to extract n phrase tables in addition to the original phrase table and exploit the Moses option of multiple phrase tables (Koehn and Schroeder, 2007). This means that given the usual five features (forward/backward phrase/lexical log probability and phrase penalty), we need to tune 5 · (n + 1) features. Because such MERT (Och, 2003) tuning may be unstable for higher n, several methods were proposed where the n+1 phrase tables are merged into a single one 142 Corpus Sents CzEng tmt (CzEng) Czech Web Corpus WMT News Crawl 15M 15M 37M 25M Tokens Czech English 205M 236M 197M 236M 627M – 445M – T ECTO MT P HRASE F IX Filtering Mark Reliable Phr. Mark Identities Table 1: Statistics of used data. 4 BLEU 14.71±0.53 17.73±0.54 14.68±0.50 17.87±0.55 17.87±0.57 1-TER 35.61±0.60 35.63±0.65 35.47±0.57 35.57±0.66 35.85±0.68 Table 2: Comparison of several strategies of SPE. Best results are in bold. Common Experimental Setup Corpus (Sp"
W13-2216,P02-1040,0,0.0911933,"mt (CzEng), tmt (newstest2013),3 and smt (newstest2013). For the SPE experiments, we decided to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here"
W13-2216,W12-4205,1,0.900973,"Missing"
W13-2216,P13-3025,0,0.277699,"Missing"
W13-2216,W12-3102,0,\N,Missing
W13-2216,W07-0704,0,\N,Missing
W14-3322,W11-2138,1,0.918214,"Missing"
W14-3322,W13-2208,1,0.864737,"DEPFIX, Depfix post-processing is added; and CU - FUNKY also employs documentspecific language models. Introduction 2 TectoMT (§2.4) Factored Moses (§2.1) Adapted LM (§2.2) Document-specific LMs (§2.3) Depfix (§2.5) Y NK FI X -D -FU CU EP R -BO JA CU CU -TE CT OM T In this paper, we describe translation systems submitted by Charles University (CU or CUNI) to the Translation task of the Ninth Workshop on Statistical Machine Translation (WMT) 2014. In §2, we present our English→Czech systems, CU - TECTOMT, CU - BOJAR , CU - DEPFIX and CU FUNKY . The systems are very similar to our submissions (Bojar et al., 2013) from last year, the main novelty being our experiments with domainspecific and document-specific language models. In §3, we describe our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the fol"
W14-3322,P13-2121,0,0.043073,"Missing"
W14-3322,2005.mtsummit-papers.11,0,0.0931675,"our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the following) is similar to last year – we trained a factored Moses model on the concatenation of CzEng (Bojar et al., 2012) and Europarl (Koehn, 2005), see Table 2. We use two factors: tag, which is the part-of-speech tag, and stc, which is “supervised truecasing”, i.e. the surface form with letter case set according to the lemma; see (Bojar et al., 2013). Our factored Moses system translates from English stc to Czech stc |tag in one translation step. Our basic language models are identical to last year’s submission. We added an adapted language English→Czech Our submissions for English→Czech build upon last year’s successful C HIMERA system (Bojar et al., 2013). We combine several different approaches: • factored phrase-based Moses model ("
W14-3322,W10-1730,1,0.935053,"Missing"
W14-3322,P00-1056,0,0.191162,"on of vocabulary reported in the paper is to roughly one half. In our case, the vocabulary is reduced much more, so we opted for a more conservative back-off, namely “nosuf2”. Baseline System The baseline system was eventually our bestperforming one. Its design is completely straightforward – it uses one phrase table trained on all parallel data (we translate from “supervisedtruecased” English into Hindi forms) and one 5gram language model trained on all monolingual data. We used KenLM (Heafield et al., 2013) for estimating the model as the data was rather large (see Table 6). We used GIZA++ (Och and Ney, 2000) as our word alignment tool. We experimented with several coarser representations to make the final alignment more reliable. Table 7 shows the results. The factor “stem4” refers to simply taking the first four characters of each word. For lemmas, we used the outputs of the tools mentioned above. However, lemmas as output by the Hindi tagger were not much coarser than surface forms – the ratio between the number of types is merely 1.11 – so we also tried “stemming” the lemmas (lemma4). Of these variants, stem4-stem4 alignment worked best and we used it for the rest of our experiments. 3.2 BLEU"
W14-3322,P02-1040,0,0.0888957,"we do not use any stopwords or keyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→"
W14-3322,2006.amta-papers.25,0,0.0448398,"eyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→Hindi translation. We used Hin"
W14-3322,W07-1709,0,0.0300845,"Missing"
W14-3322,P09-2037,1,0.836354,"er General General General News stc stc tag stc 4 7 10 6 2.4 Sents Tokens ARPA.gz Trie [M] [M] [GB] [GB] 201.31 3430.92 28.2 11.8 24.91 444.84 13.1 8.1 14.83 205.17 7.2 3.0 0.25 4.73 0.2 – Table 4: Czech LMs used in CU - BOJAR. The last small model is described in §2.2. 1 Document-Specific Language Models TectoMT2 was one of the three key components in last year’s C HIMERA. It is a linguisticallymotivated tree-to-tree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models ˇ (Zabokrtsk´ y and Popel, 2009). It is trained on the WMT-provided data: CzEng 1.0 (parallel data) and News Crawl (2007–2012 Czech monolingual sets). We maintain the same approach to combining TectoMT with Moses as last year – we translate WMT test sets from years 2007–2014 and use them as additional synthetic parallel training data – a corpus consisting of the test set source side (English) and TectoMT output (synthetic Czech). We then use the standard extraction pipeline to create 2 http://lucene.apache.org 196 TectoMT Deep-Syntactic MT System http://ufal.mff.cuni.cz/tectomt/ an additional phrase table from this corpus. T"
W14-3322,W12-3148,1,\N,Missing
W14-3322,bojar-etal-2014-hindencorp,1,\N,Missing
W15-2111,W09-1206,0,0.0853316,"Missing"
W15-2111,W14-2902,1,0.890401,"Missing"
W15-2111,1992.tmi-1.8,0,0.471548,"valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findings with parallel data and a manually created bilingual valency lexicon, we have moved to add bilingual 1 Given the “automatic” nature of the word sen"
W15-2111,S01-1001,0,0.0274924,"Missing"
W15-2111,H93-1039,0,0.448683,"Missing"
W15-2111,I05-1081,0,0.0862138,"Missing"
W15-2111,W09-1201,1,0.803063,"Missing"
W15-2111,J93-2004,0,0.0496536,"pts on the deep layer; for the purpose of our experiments, it is important that the deep layer links each verb node (occurrence) to the corresponding valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findin"
W15-2111,hajic-etal-2012-announcing,1,0.809152,"Missing"
W15-2111,H05-1066,1,0.611891,"Missing"
W15-2111,C14-1003,0,0.0380015,"Missing"
W15-2111,A00-2013,0,0.174257,"Missing"
W15-2111,J03-1002,0,0.00589822,"in the IBM Candide SMT system had been given in the Brown et al. (1992) paper. 2 3 http://hunch.net/~vw http://ufal.mff.cuni.cz/pdt2.0 82 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 82–90, Uppsala, Sweden, August 24–26 2015. radit2 ACT(1) PAT(4;k+3;aby) ADDR(3) help1 ACT() PAT() ADDR() Figure 1: Valency frame examples from PDTVallex and EngVallex (Czech radit = ‘give advice, help’). translation into Czech. Sentences have been manually aligned during the human translation process, and words have been then aligned automatically using GIZA++ (Och and Ney, 2003). We have used valency frame annotation (and other features) of the PCEDT 2.0 in our previous work; however, billingual alignment information has not been used before. 2.2 Figure 2: PCEDT trees aligned using the CzEngVallex mapping Valency lexicons PDT-Vallex4 (Hajiˇc et al., 2003; Urešová, 2011) is a valency lexicon of Czech verbs (and nouns), manually created during the annotation of the PDT/PCEDT 2.0. Each entry in the lexicon contains a headword (lemma), according to which the valency frames (i.e., senses) are grouped. Each valency frame includes the valency frame members and the following"
W15-2111,J05-1004,0,0.0239311,"PAT, ADDR, EFF, ORIG, TWHEN, LOC, CAUS (actor, patient, addressee, effect, origin, time, location, cause),5 • its semantic “obligatoriness” attribute, • subcategorization: its required surface form(s) using morphosyntactic and lexical constraints. Most valency frames are further accompanied by a note or an example which explains their meaning and usage. The version of PDT-Vallex used here contains 11,933 valency frames for 7,121 verbs. EngVallex6 (Cinková, 2006) is a valency lexicon of English verbs based also on the FGD framework, created by an automatic conversion from PropBank frame files (Palmer et al., 2005) and subsequent manual refinement.7 EngVallex was used for the annotation of the English part of the PCEDT 2.0. Currently, it contains 7,148 valency frames for 4,337 verbs. EngVallex does not contain the explicitly formalized subcategorization information. 2.3 CzEngVallex: Valency lexicon mapping CzEngVallex (Urešová et al., 2015a; Urešová et al., 2015b) is a manually annotated Czech-English valency lexicon linking the Czech and English valency lexicons, PDT-Vallex and EngVallex. It contains 19,916 frame (verb sense) pairs. CzEngVallex builds links not only between corresponding frames but als"
W15-2111,W12-4205,1,0.894553,"Missing"
W15-2111,W02-0808,0,0.157321,"Missing"
W15-2111,P12-1073,0,0.0646998,"Missing"
W15-2111,W04-3250,0,0.0177928,"increase its usefulness to the classifier. 3.4 Results The results of the individual settings are given in Tables 1 and 2. The figures include the sense detection F-measure in an unlabeled (just detecting a verb occurrence whose sense must be inferred) and labeled setting (also selecting the correct sense) as well as the accuracy of the sense detection alone (in total and in ambiguous verbs with two or more senses). We can see that just using the Vowpal Wabbit classifier with the same features provides a substantial performance boost. The aligned lemma 12 We used paired bootstrap resampling (Koehn, 2004) with 1,000 resamples to assess statistical significance. 85 PCEDT annotation error. On the whole, the positive effects of using information from parallel data are prevailing. 4 tionaries have been used in POS tagging (Hajiˇc, 2000). More distant is the approach of, e.g., Brown et al. (1992) and Ide et al. (2002), where parallel text is used for learning supervision, but not for feature extraction; Diab and Resnik (2002) use an unsupervised method. We should also mention the idea of using parallel corpora as hidden features, a task first performed by (Brown et al., 1992) for WSD and subsequent"
W15-2111,W07-1709,1,0.845918,"Missing"
W15-2111,P97-1009,0,0.432533,"Missing"
W15-2111,P14-5003,1,0.893555,"Missing"
W15-2111,C04-1192,0,0.0753298,"Missing"
W15-2111,W15-1613,1,0.749109,"Missing"
W15-2111,F14-1005,0,0.0227862,"Missing"
W15-2111,N09-1004,0,\N,Missing
W15-2111,P02-1033,0,\N,Missing
W15-2111,W12-3132,1,\N,Missing
W15-2111,cinkova-2006-propbank,0,\N,Missing
W15-3009,bojar-etal-2012-joy,1,0.894425,"Missing"
W15-3009,P13-3023,1,0.902684,"Missing"
W15-3009,P15-1044,1,0.88208,"Missing"
W15-3009,W12-3132,1,0.897194,"Missing"
W15-3009,hajic-etal-2012-announcing,0,0.0376956,"Missing"
W15-3009,P14-5003,0,0.120191,"Missing"
W15-3009,W10-1730,1,0.934479,"Missing"
W15-3009,W15-4103,0,0.0311546,"uages indicated that it is sufficient in most cases. The output sentence is then obtained by just combining all the nodes in the resulting surface dependency tree. 5 WMT 2015 Translation Task Results TectoMT reached a BLEU score of 13.9 for the English-to-Czech direction in the WMT 2015 Translation Task. This ranks it among the last systems, which is consistent with results from previous years. However, English-to-Czech TectoMT has also been used in the Chimera system combination, which ranks first in both automatic and human evaluation results. TectoMT plays a very important role in Chimera (Tamchyna and Bojar, 2015). TectoMT’s Czech-to-English translation reached a BLEU score of 12.8, and finished last 4. Subject-predicate agreement in number and person is enforced – predicates have their number and person filled based on their subject(s). 5. Auxiliary words are added. These are based on the contents of formemes (prepositions, subordinating conjunction, infinitive particles, possessive markers) and t-lemmas (phrasal verb particles). 10 Alternatively, an n-gram language model could be used to select the word forms. Flect uses just a short context of neighboring lemmas, but it generalizes also to unseen wo"
W15-3009,H05-1066,0,0.0520577,"Missing"
W15-3009,P09-2037,1,0.891858,"as, formemes, and grammatemes are translated using separate models. The t-lemma and formeme translation models are an interpolation of maximum entropy discriminative models (MaxEnt) of Mareˇcek et al. (2010) and simple conditional probability models. The MaxEnt models are in fact an ensemble of models, one for each individual source t-lemma/formeme. The combined translation models provide several translation options for each node along with their estimated probability (see Section 1). The best options are then selected using a Hidden Markov Tree Model (HMTM) with a target-language tree model (Žabokrtský and Popel, 2009), which roughly corresponds to the target-language n-gram model in phrase-based MT. Grammateme transfer is rule-based; in most cases, grammatemes remain the same as in the source language. Adding New Language Pairs Using different languages in an MT system with deep transfer is mainly hindered by differences in the analysis and synthesis of the individual languages. To overcome these problems, we decided to use existing multilingual annotation standards (see Section 3.1) and to simplify and automate translation model training (see Section 3.2). In addition, we introduce an easier way of combin"
W15-3009,W13-3307,1,0.84883,"and Rudolf Rosa∗ ∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics {odusek,mnovak,popel,rosa}@ufal.mff.cuni.cz ‡ University of Lisbon, Faculty of Sciences, Department of Informatics luis.gomes@di.fc.ul.pt Abstract system in the shared task. The performance of the current version leaves a lot of room for improvement, but proves the potential of TectoMT for different language pairs. The original TectoMT system for EnglishCzech translation has seen just small changes, e.g., adding specialized translation models for selected pronouns (Novák et al., 2013a; Novák et al., 2013b) and fine-tuning of a handful of rules. Therefore, its performance is virtually identical to that of the last year’s version. This paper is structured as follows: in Section 2, we introduce the TectoMT basic architecture. In Section 3, we describe the improvements to TectoMT that were added for an easier support of new language pairs. Section 4 then details the Czech-to-English TectoMT system submitted to WMT15. We discuss TectoMT’s performance in the task and examine the most severe error sources in Section 5. Section 6 then concludes the paper. The TectoMT tree-to-tree"
W15-3009,W08-0325,0,0.622363,"Missing"
W15-3009,I13-1142,1,0.831364,"Missing"
W15-3009,zeman-etal-2012-hamledt,1,0.902954,"Missing"
W15-3009,P02-1040,0,0.0941085,"ining New Language Pairs • HM-P – harmonic mean of probabilities, Other improvements to support adding new language pairs quickly are rather technical. We automated the translation model training in a set of makefiles. To train a new translation pair, one only needs to implement analysis and synthesis pipelines for both languages and edit a configuration file. Debugging and testing of the new analysis and synthesis pipelines is supported by monolingual “roundtrip” experiments: a development data set is first analyzed up to t-layer, then synthesized back to word forms. BLEU score measurements (Papineni et al., 2002) and a direct comparison of the results are then used to improve performance before the translation models are trained and other transfer blocks are implemented.3 3.3 • GM-Log-P – geometric mean of logarithmic probabilities,7 • HM-Log-P – harmonic mean of logarithmic probabilities.8 We compared the functions against a baseline of just using the first option given by each of the models (regardless of compatibility). We used corpora of 1,000 sentences from the IT domain collected in the QTLeap project to evaluate all variants in English-to-Czech, English-toSpanish, and English-to-Portuguese tran"
W15-3009,zeman-2008-reusable,0,0.205436,"o-tree machine translation (MT) system (Žabokrtský et al., 2008) has been competing in WMT translation tasks since 2008 and has seen a number of improvements. Until now, the only supported translation direction was English to Czech. This year, as a part of the QTLeap project,1 we have enhanced TectoMT and its underlying natural language processing (NLP) framework, Treex (Popel and Žabokrtský, 2010), to support more language pairs. We simplified the training pipeline to be able to retrain the translation models faster, and we use abstracted language-independent rules with the help of Interset (Zeman, 2008) where possible. Together with our partners on the QTLeap project, we have implemented translation systems for other language pairs (English to and from Dutch, Spanish, Basque, and Portuguese) which are not part of WMT shared Translation Task this year. However, we were also able to submit the results of a newly built Czech-English translation 1 2 The TectoMT Translation System TectoMT (Žabokrtský et al., 2008) is a tree-totree MT system system consisting of an analysistransfer-synthesis pipeline, with transfer on the level of deep syntax. It is based on the Prague Tectogrammatics theory (Sgal"
W15-5711,D11-1033,0,0.0265192,"models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we"
W15-5711,J96-1002,0,0.23014,"of rules which collapse auxiliaries and assign all the required attributes to each t-node. 2.2 Transfer In the transfer phase, an initial target t-tree is obtained as a copy of the source t-tree. Target t-lemmas and formemes of the t-nodes are suggested by a set of TMs, and the other attributes are transferred by a set of rules. For both t-lemmas and formemes, we use two separate TMs: • MaxEnt TM – a discriminative model whose prediction is based on features extracted from the source tree. The discriminative TM (Mareˇcek et al., 2010) is in fact an ensemble of maximum entropy (MaxEnt) models (Berger et al., 1996), each trained for one specific source t-lemma/formeme. However, as the number of types observed in the parallel treebank may be too large, infrequent source t-lemmas/formemes are not covered by this type of TM. • Static TM – this is only a dictionary of possible translations with relative frequencies (no contextual features are taken into account). This model is available for most source t-lemmas/formemes seen in training data.3 1 http://ufal.mff.cuni.cz/treex and https://github.com/ufal/treex The modules used for the analysis in the individual languages vary, but all of them follow the same"
W15-5711,2011.iwslt-evaluation.18,0,0.0393918,"erforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusio"
W15-5711,bojar-etal-2012-joy,1,0.910006,"Missing"
W15-5711,W04-3237,0,0.0447917,"he linear interpolation constant is tuned on a development set. • A LL – concatenation of training data. • W EIGHT – as A LL, but the out-of-domain training examples are downweighted so the in-domain examples (which are typically much fewer) have bigger effect on the resulting model. The weight is chosen by cross-validation. • P RED – the prediction of the out-of-domain model is used as an additional feature for training the final model on the in-domain data. • P RIOR – out-of-domain weights are used as a prior (via the regularization term) when training the final model on the in-domain data (Chelba and Acero, 2004). 4 http://metashare.metanet4u.eu/go2/qtleapcorpus http://www.statmt.org/wmt13/translation-task.html 6 See cuni_train/Makefile in https://github.com/ufal/qtleap. 5 93 • E ASYA DAPT (called AUGMENT in the original paper, sometimes referred to as the “Frustratingly Easy Domain Adaptation”) – create three variants of each feature: general, in-specific and outspecific; train on concatenation of in- and out-of-domain data, where on in-domain data, the general and in-specific features are active and on the out-of-domain data, the general and out-specific features are active. Daumé III (2009) showed"
W15-5711,W10-2608,0,0.0563296,"Missing"
W15-5711,W14-3326,1,0.79882,"Missing"
W15-5711,P13-3023,1,0.892993,"Missing"
W15-5711,W12-3132,1,0.901012,"Missing"
W15-5711,W15-3009,1,0.84035,"two possible ways of combining the lists: 1. Just using the first item of both lists (the simplest way, but its performance may not be ideal since incompatible combinations are sometimes produced). 2. Using a Hidden Markov Tree Model (Žabokrtský and Popel, 2009), where a Viterbi search is used to find the best t-lemma/formeme combinations globally over the whole tree. In the current TectoMT version, HMTM is only used in EN→CS translation. HMTM for the remaining languages will be added in the near future. 2.3 Synthesis The synthesis is a pipeline of rule-based modules (Žabokrtský et al., 2008; Dušek et al., 2015) that gradually change the translated t-tree into an a-tree (surface dependency tree), adding auxiliary words and punctuation and resolving morphological attributes. Some basic word-order rules are also applied. The individual a-tree nodes/words are then inflected using a morphological dictionary (Straková et al., 2014) or a statistical tool trained on an annotated corpus (Dušek and Jurˇcíˇcek, 2013). The resulting tree is then simply linearized into the output sentence. 3 Domain Adaptation by Model Interpolation The general approach of domain adaptation by model interpolation is rather simple"
W15-5711,eck-etal-2004-language,0,0.0138674,"ments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, a"
W15-5711,2005.eamt-1.19,0,0.0123169,"neral-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset."
W15-5711,W07-0733,0,0.0220004,"NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented o"
W15-5711,2005.mtsummit-papers.11,0,0.105998,"nswers) as the in-domain training data. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on develo"
W15-5711,W02-1405,0,0.0494824,"c features are active. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) o"
W15-5711,2007.mtsummit-papers.42,0,0.0276696,"a. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on development data to set TM pruning thresholds and i"
W15-5711,2011.iwslt-papers.5,0,0.0195592,"s-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we will obtain additional in-domain data, whi"
W15-5711,W10-1730,1,0.924267,"Missing"
W15-5711,H05-1066,0,0.33499,"Missing"
W15-5711,P10-2041,0,0.034953,"bine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up"
W15-5711,W08-0320,0,0.0279926,"ctive. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combi"
W15-5711,C10-2124,0,0.0226748,"III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et"
W15-5711,W07-1709,0,0.0772611,"Missing"
W15-5711,P14-5003,0,0.0926819,"Missing"
W15-5711,P09-2037,1,0.952471,"ne translation system with a tree-to-tree transfer on the deep syntax layer, first introduced by Žabokrtský et al. (2008). It is based on the Prague “tectogrammatics” theory of Sgall et al. (1986). The system uses two layers of structural description with dependency trees: surface syntax (a-layer, a-trees) and deep syntax (t-layer, t-trees). The analysis phase is two-step and proceeds from plain text over a-layer to t-layer (see Section 2.1). The transfer phase of the system is based on maximum entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Markov Tree Models (Žabokrtský and Popel, 2009) (see Section 2.2). The subsequent generation phase consists of rule-based components that gradually change the deep target language representation into a shallow one, which is then converted to text (see Section 2.3). 2.1 Analysis The analysis phase consists of a pipeline of standard NLP tools that perform the analysis to the a-layer, followed by a rule-based conversion to t-layer. In the analysis pipeline, the input is first segmented into sentences and tokenized using rule-based modules from the Treex toolkit1 (Popel and Žabokrtský, 2010). A statistical part-of-speech tagger and dependency"
W15-5711,W08-0325,0,0.606583,"Missing"
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W16-2334,W16-2332,1,0.730263,"f performance of the forced non-translations from the evaluation section: we simply always apply it in the TectoMT system, but never in the Moses system.9 4 Moses 4.2 TectoMT TectoMT is a hybrid MT system, combining statistical and rule-based Treex blocks to perform translation with transfer on the layer of tectogrammatical (deep) syntax. MT Systems We use two systems, Moses (Koehn et al., 2007) ˇ and TectoMT (Zabokrtsk´ y et al., 2008), as well as their combination Chimera (Bojar et al., 2013); see also a more detailed description of the Moses and TectoMT systems within the QTLeap project by Gaudio et al. (2016) in these proceedings. 10 W2A::ResegmentSentences W2A::TokenizeMoses 12 W2A::TokenizeMorphoDiTa for EN→CS 13 W2W::NormalizeEnglishSentence 14 W2A::EscapeMoses 15 W2A::TruecaseMoses 16 A2A::ProjectCase 17 A2W::CapitalizeSentStart 11 9 This holds even for the Chimera combination, i.e. this method is applied in its TectoMT component but not in the Moses component. 452 System Annotations (not adapted) XXX XML (not adapted) XXX XML →ES 22.23 23.61 24.22 26.01 26.89 27.40 →NL 23.40 24.89 25.41 21.82 23.52 23.26 →PT 14.01 15.47 15.58 13.11 14.19 14.21 We use TectoMT’s translation model interpolation"
W16-2334,P07-2045,1,0.0394348,"rained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focu"
W16-2334,P03-1021,0,0.0324255,"ems domain-adapted: they are trained and tuned on the Batch1 and Batch2 parts of the in-domain training data, as described below. Thus, even without the domain adaptation through in-domain lexicons (which were also provided by the task organizers), the systems constitute strong baselines within the IT domain. Still, the lexicons were not used to train nor tune the systems. 3.5 4.1 Forced non-translations Moses is a standard phrase-based statistical machine translation system. We train Moses on general-domain training data and tune it on the Batch2 part from in-domain training data using MERT (Och, 2003). We perform domain adaptation of Moses using either XXX placeholders or XML annotations. EN→CS uses factored translation (with part-of-speech tags as additional target-side factors), which is not compatible with the XML annotations, and thus only XXX placeholders are used for EN→CS. We apply some rather standard pre- and postprocessing steps (implemented as Treex blocks). Preprocessing: • segmentation into sentences10 • tokenization11,12 • normalization of quotes, dashes and contracted forms (for EN→CS)13 • entity escaping14 • truecasing (for EN→NL)15 /lowercasing Postprocessing: • projection"
W16-2334,P02-1040,0,0.100304,"of the in-domain training data. Unlike Moses, TectoMT does not support automatic tuning of parameters; however, some parameters were tuned manually using Batch2 from in-domain training data. We only experiment with domain adaptation of TectoMT via Treex wild attributes (§ 3.4). Table 1: BLEU evaluation of two forced translation styles for Moses: XXX placeholders and XML markup. For comparison, the non-adapted system is also included. 4.3 5 Moses Chimera Chimera We use the WMT16 IT task test set (i.e. Batch3 from the QTLeap corpus19 ) to evaluate our experiments using (case-insensitive) BLEU (Papineni et al., 2002). First, in Table 1, we compare the two annotation styles we can use for Moses. In general, the XML annotations perform better, in half of the cases leading to a result better by about +0.5 BLEU than that of the XXX placeholders while performing worse only once. Although the documentation in the Moses manual is not very detailed in this respect, we believe that the XML annotations are more palatable to the language model, which can then make meaningful decisions at the boundaries of the force-translated entities, while the XXX placeholders simply constitute out-ofvocabulary items for the langu"
W16-2334,W12-3146,1,0.930914,"Missing"
W16-2334,W15-5711,1,0.848136,"Missing"
W16-2334,W08-0325,0,0.736805,"g. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focuses on high-quality translation for the"
W16-2334,P07-1033,0,\N,Missing
W16-2334,W13-2208,1,\N,Missing
W16-6401,W10-1705,1,0.929434,"oses + Moses post-editing, simple Moses + Moses post-editing, TwoStep Google Translate + TectoMT post-editing Moses + TectoMT post-editing § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setup"
W16-6401,W15-3006,1,0.779365,"nslation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been"
W16-6401,W12-3130,1,0.860797,"s, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd Deep Machine Translation Workshop (DMTW 2016), pages 1–10, Lisbon, Portugal, 21 October 2016. Figure 1: TectoMT 2.1.1 Factored Moses In the more recent experiments that we report, the Moses system used is actually the Factored Moses of Bojar et al. (2012). It translates the source English text into a factored representation of Czech, where each word is represented by a tuple of a word form and a corresponding part-of-speech (PoS) tag. This enables Moses to use an additional language model which operates on PoS tags instead of word forms. This helps overcome data sparsity issues of the word-based language model and thus leads to a higher output quality, especially to its better grammaticality. Factored Moses is trained on parallel corpora pre-analyzed by Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically"
W16-6401,W13-2208,1,0.933672,"to perform forced translation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at"
W16-6401,W15-3009,1,0.853769,"y Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically motivated NLP framework. It consists of a large number of smaller components performing a specific NLP-task (blocks), both Treex-specific as well as Treex-wrapped external tools, which can be flexibly combined into processing pipelines. Sentences are represented by surface and deep syntactic dependency trees, richly annotated with numerous linguistic attributes, similarly to the Prague Dependency Treebank (Hajiˇc, 1998). 2.2.1 TectoMT The main application of Treex is TectoMT3 (Žabokrtský et al., 2008; Dušek et al., 2015), a linguistically motivated hybrid machine translation system. Its pipieline consists of three main steps: analysis of each source sentence up to t-layer (a deep syntactic representation of the sentence in a labelled dependency t-tree), transfer of the source t-tree to the target t-tree (i.e., the translation per se), and generation of the target sentence from the target t-tree (see Figure 1). The transfer is performed by copying the t-tree structure and grammatemes4 (attributes describing grammatical meaning) from source, and predicting target lemmas and formemes5 (deep morphosyntactic attri"
W16-6401,W12-3132,1,0.90614,"Missing"
W16-6401,W13-2216,1,0.89877,"Missing"
W16-6401,P07-2045,1,0.0127177,"rtcomings cancel out. In our paper, we review a set of such attempts, performed with Moses, a prominent representative of the PB-SMT systems, and Treex, a linguistically motivated NLP framework, featuring, among other, a full-fledged deep syntactic MT system, TectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to othe"
W16-6401,W09-0424,0,0.0237791,"u-plain Moses output downloaded from http://matrix.statmt.org/systems/show/2807, test set downloaded from http://matrix.statmt.org/test_sets/list. 10 Figure 8: Moses with TectoMT pre- and post-processing 6 Figure 9: Two-headed Chimera Zeman (2010) used several pre-processing steps to make the source English text more similar to Czech, such as removing articles, marking subjects by artificial suffixes (“/Sb”), and reordering auxiliary verbs to neighbor their main verbs. Of course, the SMT system was also trained on texts preprocessed in that way; in these experiments, the Joshua PB-SMT system (Li et al., 2009) was used instead of Moses. This approach may seem too aggressive, prone to making the input noisier as well as being potentially lossy. However, the author showed that with careful selection and tuning of the pre-processing steps, a significant improvement of translation quality can be achieved; moreover, this was also confirmed on English-to-Hindi translation. Rosa et al. (2016) successfully apply Treex pre-processing and post-processing to Moses, but this time with the main objective being an adaptation of Moses trained on general-domain data to a specific domain (namely the domain of Infor"
W16-6401,J03-1002,0,0.0116228,"ectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd De"
W16-6401,W07-0704,0,0.0827717,"Missing"
W16-6401,W12-3146,1,0.888935,"Missing"
W16-6401,W16-2334,1,0.910577,"ng § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setups have been properly described and evaluated in a peer-reviewed publication, others, especially some of the unsuccessful ones, were ne"
W16-6401,N07-1064,0,0.0327481,"Majliš (2009) Zeman (2010) Table 2: Base systems. Figure 3: PhraseFix translating one t-node with two or more t-nodes or deleting some t-nodes.8 It also uses MERT tuning and it should scale with more training data. In the experiments with two factors (Popel, 2013), two language models were used: one for lemmas and one for formemes. Unfortunately, the TectoMoses experiment brought negative results, presumably due to additional noise introduced by the added transformations. 3.2 PhraseFix: TectoMT with Moses Post-editing The PhraseFix system of Galušˇcáková et al. (2013) is based on the work of Simard et al. (2007), who introduced the idea of automatically post-editing a first-stage MT system by a second-stage MT system, trained to “translate” the output of the first-stage system into a reference translation. This has been shown to be particularly beneficial for conceptually different MT systems. In PhraseFix, the source English side of the CzEng parallel corpus of Bojar and Žabokrtský (2009) is translated by TectoMT into Czech, and Moses is then trained in a monolingual setting to translate the TectoMT-Czech into reference-Czech, i.e., the target side of CzEng (see Figure 3). Evaluation shows that this"
W16-6401,W07-1709,0,0.0726314,"Missing"
W16-6401,W16-2325,1,0.836879,"parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been shown to have a positive effect, e.g., in choosing the correct inflection of a word when the language model encounters an unknown context, or in generating a translation for a word that constitutes an out-of-vocabulary item for Moses (as TectoMT can abstract from word forms to lemmas and beyond, which Moses cannot). 3.8 Chimera: Moses with Additional TectoMT Phrase-table and Depfix Post-editing The Three-headed Chimera, or simply Chimera (Bojar et al., 2013b; Tamchyna et al., 2016), is a combination of TectoMT and Moses, as in Section 3.7, complemented by a final post-editing step performed by Depfix, as in Section 3.5 (see Figure 10). It has been repeatedly confirmed as the best system by both automatic and manual evaluations, not only among the ones reported in this paper, but also in general, 12 http://www.statmt.org/moses/?n=Advanced.Hybrid 7 Figure 10: Three-headed Chimera being the winner of the WMT English-to-Czech translation task in the years 2013, 2014 and 2015 (Bojar et al., 2013a; Bojar et al., 2014; Bojar et al., 2015). 4 Conclusion We reviewed a range of e"
W16-6401,W08-0325,0,0.0651825,"Missing"
W17-0412,P05-1013,0,0.0703427,"Block ud.AddMwt splits multi-word tokens into words based on language-specific rules – there are subclasses for several languages, e.g. ud.cs.AddMwt for Czech. Overall statistics (number of words, empty words, multi-word tokens, sentences) can be printed with util.Wc. Advanced statistics about nodes matching a given condition (relative to other nodes) can be printed with util.See. For evaluation, eval.Parsing computes the standard UAS and LAS, while eval.F1 computes Precision/Recall/F1 of various attributes based on the longest common subsequence. Tree projectivization and deprojectivization (Nivre and Nilsson, 2005) can be performed using transform.Proj and transform.Deproj. 3 load (s) 2,501 158 24 7 9 CoNLL-U, iterating over all nodes sorted by word order while allowing changes of the word order too) and other technical issues was much bigger than the effort spent on the use cases described in Section 2. For example, to provide access to structured attributes FEATS and MISC (e.g. node .feats['Case'] = 'Nom') while allowing access to the serialized data (e.g. node.feats = 'Case=Nom|Person=1'), Udapi maintains both representations (string and dict) and synchronizes them transparently, but lazily. Table 1"
W17-0412,L16-1680,0,0.0889923,"Missing"
W17-0412,E12-2021,0,\N,Missing
W18-6424,W15-3049,0,0.0421704,"Missing"
W18-6424,W18-6319,0,0.0494111,"Missing"
W18-6424,W17-4739,0,0.0242155,"performed all other systems in WMT2018 evaluation (Section 8). 1 482 https://github.com/tensorflow/tensor2tensor Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 482–487 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64051 sets are translated (by a target-to-source MT system) to the source language, resulting in synthetic parallel data, which is used as additional training data (in addition to authentic parallel) for the final (source-to-target) NMT system. Sennrich et al. (2017) compared two regimes of how to incorporate synthetic training data created using backtranslation of monolingual data. In the fine-tuned regime, a system is trained first on the authentic parallel data and then after several epochs it is trained on a 1:1 mix of authentic and synthetic data. In the mixed regime, the 1:1 mixed data is used from the beginning of training. In both cases, the 1:1 mix means shuffling the data randomly at the sentence level, possibly oversampling the smaller of the two data sources. We used a third approach, termed concat regime, where the authentic and synthetic par"
W18-6424,W16-2309,0,0.0336171,"training steps. For the CZtuned model, we selected a checkpoint with the best performance on wmt13-CZ (Czech-origin portion of newstest2013), which was at 774k steps. Similarly, for the nonCZ-tuned model, we selected the checkpoint with the best performance on wmt13-nonCZ, which was at 788k steps. Note that both the models were trained jointly in one experiment, just selecting checkpoints at two different moments. 7 RegEx Post-processing We applied two simple post-processings to the translations, using regular expressions. 484 English→Czech system BLEU cased BLEU uncased chrF2 cased Nematus (Sennrich et al., 2016b) T2T (Popel and Bojar, 2018) 22.80 23.84 23.29 24.40 0.5059 0.5164 our mixed backtranslation our concat backtranslation + higher quality backtranslation + CZ/nonCZ tuning 24.85 (+1.01) 25.77 (+0.92) 26.60 (+0.83) 26.81 (+0.21) 25.33 26.29 27.10 27.30 0.5267 0.5352 0.5410 0.5431 Table 2: Automatic evaluation on (English→Czech) newstest2017. The three scores in parenthesis show BLEU difference relative to the previous line. natures of the three metrics are: We deleted phrases repeated more than twice (immediately following each other); we kept just the first occurrence. We considered phrases o"
W18-6424,W04-3250,0,0.154692,"f all English↔Czech systems submitted to WMT2018, according to both automatic and manual evaluation. For the automatic evaluation, we use the same three metrics as in the previous section (just with wmt18 instead of wmt17). For the manual evaluation, we report the reference-based direct assessment (refDA) scores, provided by the WMT organizers. Our Transformer is the best system in English→Czech and Czech→English WMT2018 news task. It is significantly (p &lt; 0.05) better than the second-best system – UEdin NMT, in both translation directions and both according to BLEU bootstrap resampling test (Koehn, 2004) and according to refDA Wilcoxon rank-sum test. Table 2 evaluates the relative improvements described in Sections 4 and 5 on English→Czech newstest2017 and compares the results with the WMT2017 winner – Nematus (Sennrich et al., 2016b), and with the result of Popel and Bojar (2018) – T2T without any backtranslation. The three reported automatic metrics are: casesensitive (cased) BLEU, case-insensitive (uncased) BLEU and a character-level metric chrF2 (Popovi´c, 2015). We compute all the three metrics with sacreBLEU (Post, 2018). The reported cased and uncased variants of BLEU differ also in th"
W18-6424,P16-1009,0,0.206967,"training steps. For the CZtuned model, we selected a checkpoint with the best performance on wmt13-CZ (Czech-origin portion of newstest2013), which was at 774k steps. Similarly, for the nonCZ-tuned model, we selected the checkpoint with the best performance on wmt13-nonCZ, which was at 788k steps. Note that both the models were trained jointly in one experiment, just selecting checkpoints at two different moments. 7 RegEx Post-processing We applied two simple post-processings to the translations, using regular expressions. 484 English→Czech system BLEU cased BLEU uncased chrF2 cased Nematus (Sennrich et al., 2016b) T2T (Popel and Bojar, 2018) 22.80 23.84 23.29 24.40 0.5059 0.5164 our mixed backtranslation our concat backtranslation + higher quality backtranslation + CZ/nonCZ tuning 24.85 (+1.01) 25.77 (+0.92) 26.60 (+0.83) 26.81 (+0.21) 25.33 26.29 27.10 27.30 0.5267 0.5352 0.5410 0.5431 Table 2: Automatic evaluation on (English→Czech) newstest2017. The three scores in parenthesis show BLEU difference relative to the previous line. natures of the three metrics are: We deleted phrases repeated more than twice (immediately following each other); we kept just the first occurrence. We considered phrases o"
W18-6424,W13-2202,0,0.0392747,"Missing"
W19-5337,W19-5301,1,0.839454,"Missing"
W19-5337,N13-1073,0,0.0558634,"ed T2T system achieved an insignificant improvement (+0.1 BLEU) over our last year’s sentence-level T2T system, but applying this system on sentences led to a significant worsening (−0.6 BLEU). We hypothesized that by providing the translation model with larger attendable context, the resulting translations display larger lexical consistency. We could demonstrate it by finding less examples where an English polysemous word is translated to two or more Czech non-synonymous lemmata within one document. To evaluate the hypothesis, we word-aligned the source and target sentences using fast_align (Dyer et al., 2013).5 We then lemmatized the aligned words (both English and Czech) using MorphoDiTa (Straková et al., 2014) and considered all instances where a single English lemma was aligned to at least two Czech lemmata in a single document. Since our focus was on evaluating the difference between non-context and document-level models, we selected only the English lemmata with different number of aligned Czech lemmata in the two types of systems. Two pairs of models were compared: “DocTransformer T2T” vs. “Transformer T2T 2019” and “DocTransformer Marian” vs. “Transformer Marian”. The final pool of examples"
W19-5337,P18-4020,0,0.0759104,"Missing"
W19-5337,D18-2012,0,0.0320416,"mup 20000 --lr-decay-inv-sqrt 20000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --label-smoothing 0.1 --learn-rate 0.0002 --exponential-smoothing 3 Document-Level Systems Our document-level models were created by training on the context-augmented data described in Section 2.2. We used different strategies for document-level decoding in Marian and in T2T. We used the same learning rate as T2T and estimated the number of warmup training steps so the model consumed approximately the same number of sentences as T2T in warmup. Instead of T2T’s default SubwordTextEncoder, we used SentencePiece (Kudo and Richardson, 2018) with its default parameters to obtain a shared vocabulary of 32,000 entries from untokenized training data. We set the maximal sentence length to 150 and decoded with beam size 4. We could not use Adafactor (Shazeer and Stern, 2018) optimizer as in T2T, because it is not implemented in Marian. We used Adam instead. We did not set the batch size manually, but used the --mini-batch-fit parameter to determine the mini-batch size automatically based on sentence lengths to fit the available memory. We estimated the workspace memory to 13,900 MB as the largest possible on our hardware. We shuffled"
W19-5337,D18-1512,0,0.0561563,"Missing"
W19-5337,W18-6424,1,0.812702,"coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence. 1 Since assessing the performance of documentlevel systems is one of the goals of WMT19 (Barrault et al., 2019), we decided to build NMT systems trained for translation of longer segments than single sentences. In this paper, we describe our five NMT systems submitted to WMT19 English→Czech news translation task (see Table 1). They are based on the Transformer model (Vaswani et al., 2017) and on our submission from WMT18 (Popel, 2018). Our new contributions are (i) adaptation of the baseline single-sentence models to translate multiple adjacent sentences in a document at once, so the Transformer can attend to inter-sentence relations and achieve better document-level translation quality, as was already showed to be effective by Jean et al. (2017); and (ii) reimplementation of our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the qual"
W19-5337,W18-6319,0,0.0236236,"6 29.20 28.13 0.5474 29.00 27.89 0.5516 Table 4: Automatic evaluation on newstest2019. Significantly different BLEU scores (p < 0.05 bootstrap resampling) are separated by a horizontal line. „lower and upper“. This is considered as standard in Czech formal texts. For Marian, we applied only the conversion of quotation symbols. 4 Results 4.1 Automatic Evaluation Table 4 reports the automatic metrics of our English→Czech systems submitted to WMT2019, plus the best other system – UEdin (Marian system trained by University of Edinburgh). The automatic metrics are calculated using sacreBLEU 1.3.2 (Post, 2018) and their signatures are: • pre-context: sentences which are ignored in the translation and serve only as a context for better translation of the main content, • BLEU+case.mixed+lang.encs+numrefs.1+smooth.exp+tok.13a, • main content: sentences which are used for the final translation, • BLEU+case.lc+lang.encs+numrefs.1+smooth.exp+tok.intl and • post-context: sentences which are ignored, similarly to the pre-context. • chrF2+case.mixed+lang.encs+numchars.6+numrefs.1+space.False. Based on a small dev-set BLEU hyperparameter search, we selected the following length limits: pre-context of up to 2"
W19-5337,P14-5003,0,0.0292515,"Missing"
W19-5337,W18-6312,0,0.0177422,"f our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the quality of professional human translations. Hassan et al. (2018) report achieving a “human parity” on Chinese→English news translation. Bojar et al. (2018, p. 291) report that our last year’s English→Czech system (Popel, 2018) was evaluated as significantly better (p < 0.05) than the human reference. However, it has been shown (Läubli et al., 2018; Toral et al., 2018) that evaluating the quality of translation of news articles on isolated sentences without the context of the whole document is not sufficient. It can bias the evaluation results because systems that ignore the context are not penalized in the evaluation for these context-related errors; and vice versa: sysThis paper is organized as follows: In Section 2, we describe our training data and its augmentation to overlapping multi-sentence sequences. We describe also the hyper-parameters of our models in the two frameworks. Section 3 follows with a description of the document-level decoding strateg"
W19-5337,W18-1819,0,0.0615439,"use only the data allowed in WMT2018, which does not include CS NewsCrawl 2018 and WikiTitles. All the data were preprocessed, filtered and backtranslated by the same process as in Popel (2018). We selected the originally English part of newstest2016 for validation, following the idea of CZ/nonCZ tuning in Popel (2018), but excluding the CZ tuning because the WMT2019 test set was announced to contain only original English sentences and no translationese. 2.2 2.3 2.3.1 Model Hyper-parameters Tensor2Tensor Our three systems with “T2T” in the name are implemented in the Tensor2Tensor framework (Vaswani et al., 2018), version 1.6.0. The model and training parameters this year are identical to our last year’s (WMT18) submission (Popel, 2018), with just two exceptions: First, we trained on 10 GPUs instead of 8 GPUs, thus using the effective batch size of 29k subwords instead of 23k subwords. Second, we used max_length=200 instead of 150. This means we discard all training sequences longer than 200 subwords. With our 32k joint subword vocabulary, a word contains on average 1.5 subwords. Thus effectively, the sequence-length limit used in T2T training was in most cases lower than 1000 characters – on average"
W19-5337,W18-6401,1,\N,Missing
W19-5364,W14-3302,0,0.0363432,"model outputs at all and learning rates larger than 10−4 cause the models to diverge immediately. The models in our final submission were fine-tuned with a learning rate of 10−4 . The CUNI Transformer model Our original plan was to train a system that would be robust by itself and would not require further fine-tuning on the MTNT dataset. As the baseline model, we use the Transformer “Big” model (Vaswani et al., 2017) as implemented in Tensor2Tensor (Vaswani et al., 2018). We train the model using the procedure 6 Results We evaluate the results on four datasets. The first one is neswtest2014 (Bojar et al., 2014), a standard WMT test set consisting of manually translated newspaper texts where one half is originally in English and the other half originally in French. 540 English-French French-English WMT14 WMT15 MTNT blind WMT14 WMT15 MTNT blind MTNT baseline + fine-tuning 33.5 — 33.0 — 21.8 29.7 22.1 — 28.9 — 30.8 — 23.3 30.3 25.6 — CUNI Transformer + fine-tuning 43.6 43.5 41.6 41.6 34.0 36.6 37.0 38.5 42.9 41.5 39.6 40.9 39.9 42.1 42.6 44.8 fr-en 41.4 38.5 36.4 — 24.2 22.1 47.9 44.8 43.6 40.2 29.9 25.6 BLEU Naver Labs Europe this work Baidu & Oregon State Uni. Johns Hopkins Uni. Fraunhofer FOKUS – VI"
W19-5364,P18-1163,0,0.041967,"m 539 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 539–543 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 3 Related Work 22,520k 2,007k 200k 12,886k 3,224k Parallel # Sentences 109 English-French Corpus Europarl News Commentary UN Corpus Common Crawl French News Crawl (’08–’14) English News Crawl (’11–’17) 37,320k 127,554k Table 1: Overview of the data used to train the CUNI Transformer baseline system. There have been several attempts to increase the robustness of MT systems in recent years. Cheng et al. (2018) employ an adversarial training scheme in a multi-task learning setup in order to increase the system robustness. For each training example, its noisy counterpart is randomly generated. The network is trained to yield such input representations such that it is not possible to train a discriminator that decides (based on the input representation) which input is the noisy one. This method improves both the robustness and the translation quality on the clean data. Liu et al. (2018) attempt to make the translation more robust towards noise from homophones. This type of noise is common in languages"
W19-5364,W18-2709,0,0.0420615,"eline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain. 1 Introduction Machine translation (MT) is usually evaluated on text coming from news written by a professional journalist. However, in practice, MT should cover more domains, including informal and not carefully spelled text that we encounter in the online world. Although machine translation quality increased dramatically in recent years (Bojar et al., 2018), several studies (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018) has shown that the current systems are sensitive to the source-side noise. It is also an issue that was not studied intensively in the past because neural systems appear to be more noise-sensitive than the previously used statistical systems (Khayrallah and Koehn, 2018). Recently, Michel and Neubig (2018) prepared a dataset called Machine Translation of Noisy Text (MTNT) that focuses exclusively on translating texts from the online environment. This dataset is used for the WMT19 Robustness Task. 2 MTNT Dataset and Baselines The MTNT dataset consists of sentences collected from Reddit1 posts."
W19-5364,D18-1050,0,0.257786,"rnalist. However, in practice, MT should cover more domains, including informal and not carefully spelled text that we encounter in the online world. Although machine translation quality increased dramatically in recent years (Bojar et al., 2018), several studies (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018) has shown that the current systems are sensitive to the source-side noise. It is also an issue that was not studied intensively in the past because neural systems appear to be more noise-sensitive than the previously used statistical systems (Khayrallah and Koehn, 2018). Recently, Michel and Neubig (2018) prepared a dataset called Machine Translation of Noisy Text (MTNT) that focuses exclusively on translating texts from the online environment. This dataset is used for the WMT19 Robustness Task. 2 MTNT Dataset and Baselines The MTNT dataset consists of sentences collected from Reddit1 posts. Unlike the standard corpora 1 http://www.reddit.com 539 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 539–543 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 3 Related Work 22,520k 2,007k 200k 12,886k 3,224k"
W19-5364,W18-6424,1,0.748574,"chniques for noise induction, one employing hand-crafted rules, and one based on back-translation. The techniques offer a similar translation quality gains as fine-tuning on MTNT data. 4 Corpus Mono in the LSTMs is 1,024 and the word embedding size is 512. The model that was used as a baseline for the Robustness Task was trained on the WMT15 parallel data. Additionally, simple fine-tuning using stochastic gradient descent on the MTNT data is shown to improve the translation quality by a large margin. The translation quality of the system is tabulated among our systems in Table 2. described in Popel (2018) and Popel and Bojar (2018), which was the best-performing method for Czech-to-English and English-to-Czech translation at WMT18 News Translation shared task (Bojar et al., 2018). We trained our model on all parallel data available for the WMT15 News Translation task (Bojar et al., 2015). We acquired additional synthetic data by back-translation of the WMT News Crawl corpora (from years 2008–2014 for French and 2011– 2017 for English). We did not include the News Discussion corpus that we considered too noisy for training the system. Table 1 gives an overview of the training data composition."
W19-5364,N19-1190,0,0.0463307,"(2018) attempt to make the translation more robust towards noise from homophones. This type of noise is common in languages with non-phonetic writing systems and concerns words or phrases which are pronounced in the same way, but spelled differently. The authors of the paper train the word embeddings to capture the phonetic information which eventually leads not only to bigger robustness but also to improved translation quality in general. To our knowledge, the only work that specifically uses the MTNT dataset attempts to improve the system robustness by emulating the noise in the clean data (Vaibhav et al., 2019). They introduce two techniques for noise induction, one employing hand-crafted rules, and one based on back-translation. The techniques offer a similar translation quality gains as fine-tuning on MTNT data. 4 Corpus Mono in the LSTMs is 1,024 and the word embedding size is 512. The model that was used as a baseline for the Robustness Task was trained on the WMT15 parallel data. Additionally, simple fine-tuning using stochastic gradient descent on the MTNT data is shown to improve the translation quality by a large margin. The translation quality of the system is tabulated among our systems in"
W19-5364,W18-1819,0,0.0318768,"weights. We executed several fine-tuning runs with different learning rates and observed that learning rates smaller than 10−5 do not change the model outputs at all and learning rates larger than 10−4 cause the models to diverge immediately. The models in our final submission were fine-tuned with a learning rate of 10−4 . The CUNI Transformer model Our original plan was to train a system that would be robust by itself and would not require further fine-tuning on the MTNT dataset. As the baseline model, we use the Transformer “Big” model (Vaswani et al., 2017) as implemented in Tensor2Tensor (Vaswani et al., 2018). We train the model using the procedure 6 Results We evaluate the results on four datasets. The first one is neswtest2014 (Bojar et al., 2014), a standard WMT test set consisting of manually translated newspaper texts where one half is originally in English and the other half originally in French. 540 English-French French-English WMT14 WMT15 MTNT blind WMT14 WMT15 MTNT blind MTNT baseline + fine-tuning 33.5 — 33.0 — 21.8 29.7 22.1 — 28.9 — 30.8 — 23.3 30.3 25.6 — CUNI Transformer + fine-tuning 43.6 43.5 41.6 41.6 34.0 36.6 37.0 38.5 42.9 41.5 39.6 40.9 39.9 42.1 42.6 44.8 fr-en 41.4 38.5 36."
W19-5364,W15-3001,0,\N,Missing
W19-5364,W18-6401,0,\N,Missing
zeman-etal-2012-hamledt,zeman-2008-reusable,1,\N,Missing
zeman-etal-2012-hamledt,bosco-etal-2010-comparing,0,\N,Missing
zeman-etal-2012-hamledt,W08-2121,0,\N,Missing
zeman-etal-2012-hamledt,C00-2143,0,\N,Missing
zeman-etal-2012-hamledt,P06-1033,0,\N,Missing
zeman-etal-2012-hamledt,W08-0325,1,\N,Missing
zeman-etal-2012-hamledt,D11-1036,0,\N,Missing
zeman-etal-2012-hamledt,D11-1006,0,\N,Missing
zeman-etal-2012-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
zeman-etal-2012-hamledt,R09-1007,0,\N,Missing
zeman-etal-2012-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
zeman-etal-2012-hamledt,taule-etal-2008-ancora,0,\N,Missing
zeman-etal-2012-hamledt,afonso-etal-2002-floresta,0,\N,Missing
