2004.iwslt-evaluation.3,2002.jeptalnrecital-long.26,0,0.0377017,"Missing"
2004.iwslt-evaluation.3,takezawa-etal-2002-toward,0,0.0582757,"Missing"
2004.iwslt-evaluation.3,W03-2804,0,0.0423251,"Missing"
2004.iwslt-evaluation.3,W03-2800,0,0.178329,"Missing"
2004.iwslt-evaluation.3,2003.mtsummit-papers.51,0,0.0873221,"Missing"
2004.iwslt-evaluation.3,2003.mtsummit-papers.32,0,0.0823767,"Missing"
2004.iwslt-papers.1,P04-1077,0,0.0797235,"italized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by the each system. We found different use of case (“Tokyo”, vs. “tokyo”), punctuation (“juice, please” vs. “juice plea"
2004.iwslt-papers.1,C04-1072,0,0.133457,"and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by the each system. We found different use of case (“Tokyo”, vs. “tokyo”), punctuation (“juice, please” vs. “juice please”), digits (spelled-out vs. numerals), abbreviations (“OK” vs. “okay”), compound words (“duty-free” vs. “duty free”), sentence boundaries, and"
2004.iwslt-papers.1,N03-2021,0,0.0529929,"Missing"
2004.iwslt-papers.1,niessen-etal-2000-evaluation,0,0.0927176,"Missing"
2004.iwslt-papers.1,C92-2067,0,0.11536,"Missing"
2004.iwslt-papers.1,takezawa-etal-2002-toward,0,0.0345848,"racters. The MT outputs and the references translation were normalized. We observed differences of ±0.15 on the BLEU scores and ±1.8 on the NIST scores. Those differences are quite important. In the future, system outputs and references will be normalized for common evaluation. Under the secondary condition, the ranking is still inconsistent with BLEU and NIST. For both conditions, C-STAR III systems outperformed Systran systems available free of charge on the web. 4.2. A new evaluation framework from C-STAR III Nonetheless, we would like to promote comparative evaluation on the same data set [22] in the C - STAR III framework. This would enable us to tackle some of the questions raised above. 4.2.1. Pilot closed evaluation (2002) The C-STAR III partners ran a pilot evaluation experiment in year 2003 on our common BTEC corpus for two conditions. Development and test data were picked up for BTEC. For development purposes every kind of resources could be used. The test set consisted of 500 English sentences that had their translation into Italian, Japanese, Korean and Chinese within BTEC. Subjective evaluation followed the L inguistic D ata C onsortium evaluation guidelines for the DARPA"
2004.iwslt-papers.1,2003.mtsummit-papers.51,0,0.061178,"lian. This is mainly due to the characteristics of the 98 4/8 Free Systran® MT systems available on the web have been evaluated on the same test set. All systems scores were inferior to those of the C-STAR partners. However, nothing was done to tune them to that particular task, although that is possible on systranet: choice and order of dictionaries, handling of capitalized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with"
2004.iwslt-papers.1,2001.mtsummit-papers.3,0,0.0226989,"Missing"
2004.iwslt-papers.1,2003.mtsummit-papers.9,0,0.013347,"S4 2,59 (5) 2,30 (5) 0,2733 (5) 5,6830 (4) S5 3,21 (3) 3,74 (3) 0,5542 (3) 3,4013 (5) 4.2.3. 4.3. Problems and proposals 4.3.1. NIST [0..∞[ 0.5542 (1) 3.4013 (3) S2 0,3884 (2) 8.1383 (1) S3 0.2733 (3) 5.6830 (2) Table 9: results of the C-STAR III pilot evaluation under the secondary condition 1 Problems 4.3.1.1 Current metrics don&apos;t measure linguistic quality A first problem is that the figures produced with these techniques are not directly interpretable in terms of translation quality2. A lot of work has been done t o correlate objective evaluation results with subjective evaluation results [6, 7, 19], but the results are inconsistent. BLEU is said to correlate well with human judgments of quality, NIST is said to be better than BLEU for theoretical reasons, but BLEU and NIST give contradictory rankings (see above). Hence, if correlation with human judgments is a measure of the quality of a metrics, NIST cannot be better than BLEU… or the correlation is too weak to be meaningful. Another trouble is that, as reported in ACL-03, these metrics often give quite bad scores to high quality human translations. An experiment has been reported, where each of 15 (human) paraphrases had been tested,"
2004.iwslt-papers.1,2003.mtsummit-papers.32,0,0.207577,"Missing"
2004.iwslt-papers.1,W04-1013,0,0.0121128,"m to that particular task, although that is possible on systranet: choice and order of dictionaries, handling of capitalized and unknown words, polite and direct forms of address (please … / I would like you to…). between the translation produced by the system and a gold translation associated with a set of paraphrases. We can also cite the GMT [24] score which is the harmonic mean (F-measure) of a new proposed precision and recall measures based on a maximum match size between a candidate and a reference translation. Recently, the R O U G E (Recall-Oriented Understudy for Gisting Evaluation) [12] framework, proposed t o automatically determine the quality of summaries, has also been used for MT evaluation [13]. The same authors also proposed O RANGE (Oracle Ranking for Gisting Evaluation), for evaluating evaluation metrics for machine translation [14]. 4.2.2. For the primary condition, there is an inconsistent ranking for system 5 with BLEU (3rd) and NIST (5th). This system outputs are significantly shorter than the output of the other systems. It impact strongly on the results with a brevity penalty for NIST. A fourth parameter came to us when checking the actual strings produced by"
2004.iwslt-papers.1,P02-1040,0,\N,Missing
2004.jeptalnrecital-long.11,1994.bcs-1.22,1,0.778889,"Missing"
2004.jeptalnrecital-poster.24,J92-4003,0,0.00664423,"ur d’une classe, a la même probabilité d’occurence, dans ce cas une classe peut se réduire à une liste de mots. Lorsqu’on a définit des classes, on peut alors directement remplacer les mots du corpus d’apprentissage par leur classe avant l’apprentissage du modèle de langage. Afin d&apos;introduire des connaissances sémantiques dans le ML, nous proposons de regrouper certains mots dans des classes correspondant a des entités sémantiques de l&apos;IF. Par exemple, les mots « bien », « d&apos;accord » et « okay » seront des éléments de la classe « c:acknowledgment » tel que le spécifie l&apos;IF. Plusieurs articles [5,6] ont montré l’intérêt de l’utilisation de classes dans diverses tâches de Traitement Automatique de Langue Naturelle. La plupart des méthodes pour constituer automatiquement des classes utilisent des critères statistiques permettant, par exemple, de diminuer la perplexité d’un modèle de langage. Dans notre cas, notre critère de choix de classes est guidé par la définition du langage pivot et par les concepts les plus utilisés dans l’IF. Notre approche consiste en deux étapes : (1) la sélection des IFs les plus fréquentes à intégrer comme classes dans le nouveau modèle de langage (2) la calcul"
2004.jeptalnrecital-poster.24,C02-1170,1,0.779843,"nowledge, affirm, negate …). La figure 2 illustre comment cette sélection des IFs les plus fréquentes est réalisée automatiquement à partir d’un corpus textuel brut non annoté manuellement en IF. Dialogues NESPOLE Analyse auto. en IF délissasses 1 croquantes 42 emmènerais 9 emmènerait 26 badgé 19 badge 3439 faillirent 52 pentateuque 309 tabloïde 17 tabloïds 117 attendriraient Selection Ifs les plus freq. {c:affirm} oui, ouais, euh oui, … {c:acknowledge} okay, d’accord, … {c:thank} merci, merci beaucoup, … … Figure 2: Sélection des IFs les plus fréquentes L’analyseur automatique en IF du CLIPS [7] a été utilisé pour analyser un corpus qui comprend 46 transcriptions de dialogues collectés lors du projet NESPOLE [8]. Ce corpus représente des dialogues possibles entre un client et un agent de voyage concernant l’organisation de vacances, la réservation d’hôtels et les activités sportives ou culturelles, dans la région de Trente en Italie. L’analyseur transforme automatiquement tous ces dialogues en une représentation de langage IF. Nous avons par conséquent un Quang VU-MINH Laurent BESACIER Herve BLANCHON Brigitte BIGI corpus aligné français-IF. Bien sûr, ce corpus n’est pas parfait car l"
2004.jeptalnrecital-poster.5,2003.mtsummit-papers.9,0,0.0227624,"Missing"
2004.jeptalnrecital-poster.5,P02-1040,0,0.0726437,"Missing"
2004.jeptalnrecital-poster.5,2001.mtsummit-papers.62,0,0.0399823,"Missing"
2004.jeptalnrecital-poster.5,takezawa-etal-2002-toward,0,0.0659387,"Missing"
2004.jeptalnrecital-poster.5,2004.jeptalnrecital-poster.24,1,0.779833,"Missing"
2006.iwslt-evaluation.3,2004.iwslt-papers.1,1,0.892477,"Missing"
2006.iwslt-evaluation.3,2003.mtsummit-papers.32,0,0.0610239,"Missing"
2006.iwslt-evaluation.3,takezawa-etal-2002-toward,0,0.0825889,"Missing"
2006.iwslt-evaluation.3,W03-2804,0,0.0573321,"Missing"
2006.iwslt-evaluation.3,2003.mtsummit-papers.51,0,0.0787107,"Missing"
2006.iwslt-evaluation.3,E06-1032,0,\N,Missing
2009.iwslt-evaluation.9,J97-2003,0,0.0166598,"ented in Figure 3. The ASR lattice (marked as a “word lattice” in the figure, but it is more accurate to say that it is a lattice made up of “unknown” units) is decomposed according to the different sub-word sets (corresponding to different morphological segmentations). Then we create a new starting node S and a new ending node E for the common lattice. We link the node S with starting nodes of all subword lattices (n°1 and n°2) and link ending nodes of all lattices with E. After this step, all lattices are merged into a common lattice. This operation can also be seen as a “union” of lattices [8]. Finally, the obtained lattice is converted into a CN which will keep both ASR ambiguity and Arabic segmentation ambiguity. This latter CN is the input of the translation system which uses, as in section 3, multiple phrase tables corresponding to multiple Arabic segmenters. 5. 5.1. the IWSLT09 organizers and a few publicly available additional data. Multiple Segmentation Experiments Automatic Speech Recognition Word lattice Vocabulary extraction Lattice of Morphemes n°1 Lattice of Morphemes n°2 Generation of the confusion net (CN) Decoding Fig. 3. Multiple segmentation process for spoken lang"
2009.iwslt-evaluation.9,J03-1002,0,0.00225656,"t, made up of 500 sentences, which corresponds to the IWSLT07 evaluation data (we will refer, in the rest of the paper, to tst07 for this data set). The tuning of the MT model parameters (minimum error rate training) was systematically done on the dev06 subset. As additional data, we first used an Arabic / English bilingual dictionary of around 84k entries. This dictionary can be found online 6 . For English LM training, we also used out-of-domain corpora taken from the LDC’s Gigaword corpus7. Our baseline speech translation system was built using tools available in the MT community: - GIZA++ [9] was used for the alignments, -The moses 8 decoder (and the training / testing scripts associated) was used (2008-07-11 release), - SRILM [10] was used to train the LMs and to deal with ASR word graphs, - The Buckwalter morphological segmenter9 and ASVM (a free http://freedict.cvs.sourceforge.net/freedict/eng-ara/ http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC 2003T05 8 Moses open source project: http://www.statmt.org/moses 9 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC 2002L49 7 Since 2007, the LIG laboratory participates yearly to the IWSLT evaluation camp"
2009.iwslt-evaluation.9,P02-1040,0,0.0835329,"Missing"
2009.iwslt-evaluation.9,W07-0734,0,0.0312049,"Missing"
2009.mtsummit-btm.3,huynh-etal-2008-sectra,1,0.837123,"Missing"
2009.mtsummit-btm.3,2003.mtsummit-papers.30,0,0.0882,"Missing"
2009.mtsummit-btm.3,2005.mtsummit-papers.11,0,0.0177828,"Missing"
2009.mtsummit-btm.3,2009.jeptalnrecital-recital.6,0,0.0951026,"Missing"
2009.mtsummit-btm.3,takezawa-etal-2002-toward,0,0.0398942,"Missing"
2011.eamt-1.24,W05-0909,0,0.0260462,"tences using the test2006 dataset selected via oracle filtering mentioned previously. For each of these sentences, we compare the 1-best and oracle-best features and compute the mean value per feature. This is then used to compute two new sets of weights using the R ESCsum and R ESCprod rescoring strategies, described in the previous section. We implemented our rescoring strategies on the devset and then applied the 2 new sets of weights computed on the testset of n-bests. Evaluation is done at a system level for both the development and testsets using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We also evaluate how many sentences contain the oracle candidates in the top position (rank 1). This is shown in Table 3. The last row in each subsection labeled O R ACLE gives the upper bound on each system, i.e. performance if our algorithm was perfect and all the oracles were placed at position 1. We also perform a Top5-BLEU oracle evaluation (shown in Table 4). The difference between the evaluations in Tables 3 and 4 is that the lat173 ter evaluates on a list of top-5 hypotheses for each sentence instead of the usual comparison of a single translation hypothesis with the reference transl"
2011.eamt-1.24,P08-2010,0,0.0375043,"Missing"
2011.eamt-1.24,2009.mtsummit-papers.8,0,0.029929,"instead of the usual comparison of a single translation hypothesis with the reference translation. The sentences used in Table 3 are present in the top 1 position of sentences used in Table 4. This means that when BLEU and METEOR scores are evaluated at system-level, for each sentence, the translation (among 5) with the highest sBLEU score is selected as the translation for that sentence. This is similar to the post-editing scenario where human translators are shown n translations and are asked to either select the best or rank them. Some studies have used as many as 10 translations together (Koehn and Haddow, 2009). We only use 5 in our evaluation. We observe that overall the R ESCsum system shows a modest improvement over the baseline in terms of METEOR scores, but not BLEU scores. This trend is consistent across all the 3 n-best list sizes. We speculate that perhaps the reliance of METEOR on both precision and recall as opposed to precision-based BLEU is a factor for this disagreement between metrics. We also observe that the degree of improvement in the BLEU and METEOR scores of each system from top-1 (Table 3) to top-5 (Table 4) is more obvious in the rescored systems R ESCsum and R ESCprod compared"
2011.eamt-1.24,P06-1096,0,0.10707,"tion metric. We chose BLEU for our experiments, as despite shortcomings such as those pointed out by (Callison-Burch et al., 2006), it remains the most popular metric, and is most often used in MERT for optimizing the feature weights. Our rescoring experiments focus heavily on these weights. Note that BLEU as defined in (Papineni et al., 2002) is a geometric mean of precision n-grams (usually 4), and was not designed to work at the sentencelevel, as is our requirement for the oracle selection. Several sentence-level implementations known as smoothed BLEU have been proposed (Lin and Och, 2004; Liang et al., 2006). We use the one proposed in the latter, as shown in (2). S BLEU = 4 X B LEUi (cand, ref ) 24−i+1 (2) i=1 Figure 1 shows a sample of 10 candidate English translations from an n-best list for a French sentence. The first column gives the respective decoder cost (log-linear score) used to rank an nbest list and the third column displays the sBLEU (sentence-level BLEU score) for each candidate translation. The candidate in the first position in the figure is the 1-best according to the decoder. The 7th-ranked sentence is most similar to the reference translation and hence awarded the highest Deco"
2011.eamt-1.24,C04-1072,0,0.0162498,"an automatic evaluation metric. We chose BLEU for our experiments, as despite shortcomings such as those pointed out by (Callison-Burch et al., 2006), it remains the most popular metric, and is most often used in MERT for optimizing the feature weights. Our rescoring experiments focus heavily on these weights. Note that BLEU as defined in (Papineni et al., 2002) is a geometric mean of precision n-grams (usually 4), and was not designed to work at the sentencelevel, as is our requirement for the oracle selection. Several sentence-level implementations known as smoothed BLEU have been proposed (Lin and Och, 2004; Liang et al., 2006). We use the one proposed in the latter, as shown in (2). S BLEU = 4 X B LEUi (cand, ref ) 24−i+1 (2) i=1 Figure 1 shows a sample of 10 candidate English translations from an n-best list for a French sentence. The first column gives the respective decoder cost (log-linear score) used to rank an nbest list and the third column displays the sBLEU (sentence-level BLEU score) for each candidate translation. The candidate in the first position in the figure is the 1-best according to the decoder. The 7th-ranked sentence is most similar to the reference translation and hence awa"
2011.eamt-1.24,P02-1038,0,0.0631562,"ing two rescoring strategies to push the oracle up the n-best list. We observe modest improvements in METEOR scores over the baseline SMT system trained on French– English Europarl corpora. We present a detailed analysis of the oracle rankings to determine the source of model errors, which in turn has the potential to improve overall system performance. 1 Introduction Phrase-based Statistical Machine Translation (PBSMT) systems typically learn translation, reordering, and target-language features from a large number of parallel sentences. Such features are then combined in a log-linear model (Och and Ney, 2002), the coefficients of which are optimized on an objective function measuring translation quality such as the BLEU metric (Papineni et al., 2002), using Minimum Error Rate Training (MERT) as described in Och (2003). An SMT decoder non-exhaustively explores the exponential search space of translations for each source sentence, scoring each hypothesis using the c 2011 European Association for Machine Translation. formula (Och and Ney, 2002) in (1). P (e|f ) = exp( M X λi hi (e, f )) (1) i=1 The variable h denotes each of the M features (probabilities learned from language models, translation mode"
2011.eamt-1.24,N07-2015,0,0.0346952,"Missing"
2011.eamt-1.24,2009.mtsummit-posters.8,0,0.0251303,"Missing"
2011.eamt-1.24,N03-1017,0,0.0126584,"ystem The set of parallel sentences for all our experiments is extracted from the WMT 20091 Europarl (Koehn, 2005) dataset for the language pair French–English after filtering out sentences longer than 40 words (1,050,398 sentences for training and 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscontinuous orientation conditioned on both the source and target languages with respect to previous and next phrases. We use the Moses (Koehn et al., 2007) phrasebased beam-search decoder, setting the stack size to 500 and the distortion limit to 6, and switching on the n-best-list option. Thus, this baseline model uses 15 features, namely 7 distortion features (d1 through d7), 1 language model feature (lm), 5 translation model features (tm1 through tm5), 1 word penalty (w), and 1 unknown word penalty fea"
2011.eamt-1.24,2005.mtsummit-papers.11,0,0.01787,"e here ? is there not here a case of double standards ? sBLEU score. This sentence is the oracle translation for the given French sentence. Note that there may be ties where the oracle is concerned (the 7th and the 10th ranking sentence have the same sBLEU score). Such issues are discussed and dealt with in section 3.4. Oracle-best hypotheses are a good indicator of what could be achieved if our MT models were perfect, i.e. discriminated properly between good and bad hypotheses. 3.2 Baseline System The set of parallel sentences for all our experiments is extracted from the WMT 20091 Europarl (Koehn, 2005) dataset for the language pair French–English after filtering out sentences longer than 40 words (1,050,398 sentences for training and 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscont"
2011.eamt-1.24,P07-2045,0,0.00824134,"nd 2,000 sentences each for development (test2006 dataset) and testing (test2008 dataset)). We train a 5-gram language model using SRILM 2 with Kneser-Ney smoothing (Kneser and Ney , 1995). We train the translation model using GIZA++ 3 for word alignment in both directions followed by phrase-pair extraction using grow-diag-final heuristic described in Koehn et al., (2003). The reordering model is configured with a distance-based reordering and monotone-swapdiscontinuous orientation conditioned on both the source and target languages with respect to previous and next phrases. We use the Moses (Koehn et al., 2007) phrasebased beam-search decoder, setting the stack size to 500 and the distortion limit to 6, and switching on the n-best-list option. Thus, this baseline model uses 15 features, namely 7 distortion features (d1 through d7), 1 language model feature (lm), 5 translation model features (tm1 through tm5), 1 word penalty (w), and 1 unknown word penalty feature. Note that the unknown word fea1 http://www.statmt.org/wmt09/ http://www-speech.sri.com/projects/srilm/ 3 http://code.google.com/p/giza-pp/ 2 171 sBLEU 0.0188 0.147 0.0125 0.025 0.025 0.0677 0.563 0.0188 0.0190 0.563 Figure 1: Sample from a"
2011.eamt-1.24,P80-1000,0,0.876409,"Missing"
2011.eamt-1.24,P02-1040,0,0.0897841,"stem trained on French– English Europarl corpora. We present a detailed analysis of the oracle rankings to determine the source of model errors, which in turn has the potential to improve overall system performance. 1 Introduction Phrase-based Statistical Machine Translation (PBSMT) systems typically learn translation, reordering, and target-language features from a large number of parallel sentences. Such features are then combined in a log-linear model (Och and Ney, 2002), the coefficients of which are optimized on an objective function measuring translation quality such as the BLEU metric (Papineni et al., 2002), using Minimum Error Rate Training (MERT) as described in Och (2003). An SMT decoder non-exhaustively explores the exponential search space of translations for each source sentence, scoring each hypothesis using the c 2011 European Association for Machine Translation. formula (Och and Ney, 2002) in (1). P (e|f ) = exp( M X λi hi (e, f )) (1) i=1 The variable h denotes each of the M features (probabilities learned from language models, translation models, etc.) and λ denotes the associated feature weight (coefficient). The candidate translation (in the n-best list) having the highest decoder s"
2011.eamt-1.24,N04-1023,0,0.0284826,"e agree with this statement (cf. figure 2). However, we believe that there is scope for improvement on the baseline features (used in decoding) before extracting more complex features for reranking. 175 Role of oracles in boosting translation accuracy We believe oracle-based training to be a viable method. In future work, we intend to explore more features (especially those used in the reranking literature such as Och et al., (2004)) to help promote oracles. We believe that our oracle-based method can help select better features for reranking. We also plan to use a host of reranking features (Shen et al., 2004) and couple them with our R ESCsum rescoring strategy. We will also generate a feature based on our rescoring formula and use it as an additional feature in discriminative reranking frameworks. We have used here sentence-level BLEU as opposed to system-level BLEU as used in MERT for oracle identification. We plan to use metrics better suited for sentence-level like TER (Snover et al., 2006). 6 Conclusion We analyze the relative position of oracle translations in the n-best list of translation hypotheses to help reranking in a PB-SMT system. We propose two new rescoring strategies. In general,"
2011.eamt-1.24,2006.amta-papers.25,0,0.0377378,"ranking literature such as Och et al., (2004)) to help promote oracles. We believe that our oracle-based method can help select better features for reranking. We also plan to use a host of reranking features (Shen et al., 2004) and couple them with our R ESCsum rescoring strategy. We will also generate a feature based on our rescoring formula and use it as an additional feature in discriminative reranking frameworks. We have used here sentence-level BLEU as opposed to system-level BLEU as used in MERT for oracle identification. We plan to use metrics better suited for sentence-level like TER (Snover et al., 2006). 6 Conclusion We analyze the relative position of oracle translations in the n-best list of translation hypotheses to help reranking in a PB-SMT system. We propose two new rescoring strategies. In general, the improvements provided by reranking the n-best lists is dependent on the size of n and the type of translations produced in the n-best list. We see an improvement in METEOR scores. To conclude, oracles have much to contribute to the ranking of better translations and reducing the model errors. Acknowledgements This work is supported by Science Foundation Ireland (grant number: 07/CE/I114"
2011.eamt-1.24,E06-1032,0,\N,Missing
2011.eamt-1.24,N03-1031,0,\N,Missing
2011.eamt-1.24,N04-1021,0,\N,Missing
2011.eamt-1.24,P03-1021,0,\N,Missing
2011.iwslt-evaluation.8,P07-2045,0,0.00419654,"ng with interpolation. We interpolated a LM trained on the TED training data (47k sentences) with a LM trained on Europarl, News, UN and 68 News-mono (24M sentences in total). After a perplexity test to optimize the interpolation weight (on Dev2010), we chose an interpolation weight equal to 0.5. 2.4. Translation modeling and tuning For the translation model training, the uncased (but punctuated) corpus was word aligned and then, the pairs of source and corresponding target phrases were extracted from the word-aligned bilingual training corpus using the scripts provided with the Moses decoder [3]. The result is a phrasetable containing all the aligned phrases. This phrase-table, produced by the translation modeling, is used to extract several translations models. In the experiments reported here, only 8 features were used in the phrase-based models: 5 translation model scores, 1 distance-based reordering score, 1 LM score and 1 word penalty score. We used the Minimum Error Rate Training (MERT) method to tune the weights. MERT was applied on the TED Dev2010 corpus (934 sentences). Moreover, it is important to note that, during tuning, punctuation was systematically removed from the Nbe"
2011.iwslt-evaluation.8,D07-1103,0,0.0175508,"ut translation task for which decoding (and tuning) is also done without punctuation. +News+UN+Newsmono: 24M sentences in total). The punctuation was restored after translation using this LM and the hidden-ngram command from SRILM toolkit. After repunctuation, we used the SMT-based recaser presented earlier. For the SLT task, the final system submitted by LIG in 2010 was ranked among the best sites that participated to the TALK task last year. 3. Improvements of MT and SLT systems done for 2011 3.1. Iterative improvement of the MT system -apply phrase-table pruning with a technique similar to [4] (retuning with MERT needed after pruning). Table 2 summarizes the iterative improvements done this year over the LIG 2010 system. First, we evaluated the performance of a phrase-table trained on the TED 2011 bilingual data (107268 sentences in total) only with and without tuning (2,3). The target language model was also updated using the TED 2011 mono (111431 sentences) data (4), which slightly increased the performance. The results obtained show a reasonable performance of the PT trained on TED 2011 only, so we experimented multiple phrasetable decoding where translation options are collecte"
2011.iwslt-evaluation.8,P02-1040,0,0.0851348,"Missing"
2011.iwslt-evaluation.8,W11-2154,1,0.875821,"Missing"
2011.iwslt-evaluation.8,2010.iwslt-evaluation.1,0,\N,Missing
2012.iwslt-papers.19,N07-1064,0,0.163393,"mation-based” APE trained on 12 000 manually post-edited translations, to correct the raw output. There was a signiﬁcant improvement in translation quality with the use of a “transformation-based” APE. The increasing amount of raw MT translation (hypotheses) aligned with their manually post-edited good translations gave rise to the idea of automatic statistical post-edition. A statistical post-edition (SPE) system is developed as a monolingual statistical MT system using the original hypotheses as the source language and the human post-editions as the target language. In 2007, M. Simard & al. [5] were the ﬁrst to propose the use of a phrase-based statistical machine translation (PBMT) system for SPE purpose. In this framework, the PBMT aims to learn “correction rules” between initial MT hypotheses (PBMT source language) and their corrected version (PBMT target language). Such an approach makes SPE easy to learn and tune with new training data. In their work, they successfully showed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE syst"
2012.iwslt-papers.19,2007.mtsummit-papers.34,0,0.896952,"y showed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE system was trained using 35,000 manually post-edited sentences. Encouraged by these results, post-editing the outputs 2 http://www.jobbank.gc.ca 284 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimen"
2012.iwslt-papers.19,W07-0728,0,0.139022,"wed the efﬁciency of using an SPE system (built with the PBMT Portage) to improve the output of a commercial RBMT system. The experiments were done in a speciﬁc domain (a job offer Web site2 ) and the SPE system was trained using 35,000 manually post-edited sentences. Encouraged by these results, post-editing the outputs 2 http://www.jobbank.gc.ca 284 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental s"
2012.iwslt-papers.19,J03-1002,0,0.00364466,"ation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in total 1,638,440 aligned sentences: the fourth version of the Europarl corpus (data derived from transcriptions of European parliament proceedings) and news corpora (data extracted from various Websites). Both corpora were provided in the framework of WMT 2010. The PBMT decoding model is a log-linear combination of fourteen weighted feature functions extracted from the monolingual and bilingual training data: six distortion models; lexicon word-based and phrase-based trans"
2012.iwslt-papers.19,2006.amta-papers.25,0,0.0586335,"e SPE training corpus (in terms of size and domain) are the same in the two cases. 2.4. Evaluation metrics To our knowledge, there is no work that compares both approaches (real vs simulated PE) on the same source language data (post-edited MT hypotheses vs professional translations) to train an SPE. Considering the same source language data, we tried to ﬁnd out if a simulated PE corpus is as effective as a real PE corpus to train an SPE system. This is what we will try to ﬁnd out in the following experiment. Translation output quality has been evaluated using the Translation Error Rate (TER) [18] and the BLEU score [19]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU scores suggest better translation quality. To ensure that differences between scores are real, we estimated the statistical signiﬁcance of test results in terms of BLEU score, according to the bootstrap resampling method described in [20]. 3. Real vs Simulated post-edited corpus for SPE trai"
2012.iwslt-papers.19,W04-3250,0,0.0532129,"been evaluated using the Translation Error Rate (TER) [18] and the BLEU score [19]. The TER score reﬂects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU scores suggest better translation quality. To ensure that differences between scores are real, we estimated the statistical signiﬁcance of test results in terms of BLEU score, according to the bootstrap resampling method described in [20]. 3. Real vs Simulated post-edited corpus for SPE training 3.1. Previous work In order to build SPE systems, manually post-edited MT hypotheses are usually used as target translations instead of translations produced by professional translators. When preexisting human translations are used, we will speak of “simulated PE” in contrast to “real PE” when target translations are manually post-edited MT hypotheses. It is important to notice that the “real PE” setting corresponds to the workﬂows implemented in real-life situations (when users feedback is re-used to improve a given system) and “simul"
2012.iwslt-papers.19,2011.mtsummit-papers.35,0,0.892487,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for d"
2012.iwslt-papers.19,W07-0732,0,0.0923357,"ly post-edited MT hypotheses are usually used as target translations instead of translations produced by professional translators. When preexisting human translations are used, we will speak of “simulated PE” in contrast to “real PE” when target translations are manually post-edited MT hypotheses. It is important to notice that the “real PE” setting corresponds to the workﬂows implemented in real-life situations (when users feedback is re-used to improve a given system) and “simulated PE” setup will allow access to much more training data (use of pre-translated parallel corpus). Several works [21, 10, 22, 9] have attempted to show that SPE can be successfully trained on pre-existing human translations rather than on system-speciﬁc post-edited translations. Both simulated (MT system hypotheses aligned with 3.2. Experiment In order to build two comparable SPE using real vs simulated target corpus, we used in both cases the same training corpus on the source side (the one described in 2.2) and, for one system we used the PBMT post-edited hypotheses (“real” setting) on the target side and for the other system, we used the translations provided with the parallel corpus (“simulated” setting) as the tar"
2012.iwslt-papers.19,N09-2055,0,0.7581,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 of the PBMT system Portage was also tried but in this setting no improvements were observed. In the same way, the following studies described in [6], [7] and [8] have shown that a RBMT system that was automatically post-edited by a PBMT system performed signiﬁcantly better than each of the individual systems on their own. Quite a lot of studies have focused on pipeline architectures where SPE systems are successfully applied to RBMT systems outputs to improve translation quality. However, only few studies ([9, 8, 10]), have investigated the efﬁciency of SPE systems applied after PBMT systems. The goal of our study is to provide a better understanding of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for d"
2012.iwslt-papers.19,W10-1723,1,0.835519,"ing of SPE usefulness when pipelined to PBMT systems. We ﬁrst describe our baseline experimental settings (Section 2) and then we try to answer the following questions: is there a difference between a real and a simulated corpus for SPE training (Section 3)? Is SPE useful in improving a generic PBMT system and what explains the effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for domainadaptation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in"
2012.iwslt-papers.19,P07-2045,0,0.0129007,"he effectiveness of SPE on specialized domain (Section 4)? And, ﬁnally, is SPE really the simplest and most efﬁcient and effective way for domainadaptation purposes (Section 5)? 2. Experimental setting 2.1. Baseline PBMT Our baseline MT system (described in more detail in [11]) translates news stories (general domain) from French into English. It is a state-of-the-art phrase-based machine translation (PBMT) system presented at the international Workshop of Machine Translation (WMT3 ) evaluation campaign in july 2010. The system was built using free open source toolkits: we used standard Moses [12] system set-up, a 3-gram language model trained with SriLM [13] and Kneser-Ney smoothing, the GIZA++ implementation of IBM word alignment model 4 [14] and the phrase extraction heuristics described in [12]. The system has been trained on two parallel corpora, containing in total 1,638,440 aligned sentences: the fourth version of the Europarl corpus (data derived from transcriptions of European parliament proceedings) and news corpora (data extracted from various Websites). Both corpora were provided in the framework of WMT 2010. The PBMT decoding model is a log-linear combination of fourteen w"
2012.iwslt-papers.19,huynh-etal-2008-sectra,1,0.892995,"Missing"
2012.iwslt-papers.19,W09-0419,0,\N,Missing
2012.iwslt-papers.19,P02-1040,0,\N,Missing
2012.iwslt-papers.19,J11-2010,0,\N,Missing
2012.iwslt-papers.19,potet-etal-2012-collection,1,\N,Missing
2012.iwslt-papers.19,P03-1021,0,\N,Missing
2015.jeptalnrecital-long.8,P91-1034,0,0.416111,"Missing"
2015.jeptalnrecital-long.8,S07-1054,0,0.0594784,"Missing"
2015.jeptalnrecital-long.8,P02-1033,0,0.126341,"Missing"
2015.jeptalnrecital-long.8,W08-0510,0,0.0387488,"Missing"
2015.jeptalnrecital-long.8,W02-1006,0,0.143672,"Missing"
2015.jeptalnrecital-long.8,J93-2004,0,0.0568506,"Missing"
2015.jeptalnrecital-long.8,H93-1061,0,0.237275,"Missing"
2015.jeptalnrecital-long.8,P96-1006,0,0.386883,"Missing"
2015.jeptalnrecital-long.8,F14-1005,0,0.0292103,"Missing"
2015.jeptalnrecital-long.8,Q14-1005,0,0.0259831,"Missing"
2015.jeptalnrecital-long.8,N01-1026,0,0.103393,"Missing"
2015.jeptalnrecital-long.8,H01-1035,0,0.056196,"Missing"
2015.mtsummit-papers.7,W15-1006,0,0.321027,"abase. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction of the paraphrase database using only the parallel corpora used to develop the MT system (which was not the case in 2010). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 81 In order to prevent synonyms/paraphrases corresponding to different senses to be treated as semantically equivalent, Apidianaki and Marie (2015) proposed METEOR-WSD. The English references are further disambiguated and annotated using Babelfly (Moro et al., 2014) for several language pairs (French, Hindi, German, Czech and Russian to English). For their experiment, Apidianaki and Marie (2015) got a better segment-level Kendall’s τ correlation than METEOR for 4 language pairs when the paraphrase module was activated. 2.3. Lexical resources 2.3.1. WordNet WordNet is a large lexical database for English, developed by linguists of Princeton University (Fellbaum, 1998). Nowadays, it has become an important and a very useful resource for NL"
2015.mtsummit-papers.7,W05-0909,0,0.924239,"ranslation quality, such as adequacy, fluency, and intelligibility. However, subjective evaluation conducted a posteriori often costs too much (in term of human resources) and, thus, objective evaluation metrics (fast and cheap as long as references are available) are often preferred nowadays. One drawback with automatic evaluation metrics is that they compare the MT hypothesis with few (and sometimes only one) reference translations. This is definitely not enough to capture lexical variation in translation. For this reason, metrics which exploit synonymy or stem similarities, such as METEOR (Banerjee and Lavie, 2005), exhibit better correlation with human judgement. METEOR maps words with the same stem or the same synset using lexico-semantic resources. However, so far, the full potential of METEOR is only exploited when English is the target language (use of WordNet). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 80 Contribution This paper proposes an extension of METEOR for multiple target languages using a lexical resource called DBnary (Sérasset, 2015). DBnary is an extraction in RDF of the lexical data of multiple editions of Wiktionary. It has several mill"
2015.mtsummit-papers.7,W10-1751,0,0.137501,"produced by a three-leveled mapping approach between the hypothesis and the reference, using additional resources if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish"
2015.mtsummit-papers.7,N10-1031,0,0.316805,"produced by a three-leveled mapping approach between the hypothesis and the reference, using additional resources if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish"
2015.mtsummit-papers.7,W13-2202,0,0.0350395,"Missing"
2015.mtsummit-papers.7,W14-3336,0,0.110331,"or 2 systems picked up randomy from WMT14 data (French-English MT) In Table 3, METEOR-TTG shows a slight increase in the score, compared to METEORMorphy, because TreeTagger lemmatizes all categories (including Adverbs), whereas Morphy lemmatizes only three categories (Noun, Verb and Adjective). 4. Correlation with human judgements In order to evaluate the correlation of our proposed METEOR-DBnary with human judgements of machine translation outputs, we used the data from the WMT13 Metrics Shared Task (Machacek and Bojar, 2013) for English-to-Spanish MT, and from the WMT14 Metrics Shared Task (Machacek and Bojar, 2014) for French-English, English-French, EnglishGerman and English-Russian MT. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 84 We present the results in a similar fashion as in the WMT metrics task methodology using the following metrics. More details and formulas can be found in (Machacek and Bojar, 2013) or (Machacek and Bojar, 2014). • System-level using Pearson correlation coefficient between system ranking based on human judgments versus METEOR (we will use our augmented metric and compare it to the baseline METEOR). • Segment-level using Kendall’s"
2015.mtsummit-papers.7,Q14-1019,0,0.0709859,"ses for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction of the paraphrase database using only the parallel corpora used to develop the MT system (which was not the case in 2010). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 81 In order to prevent synonyms/paraphrases corresponding to different senses to be treated as semantically equivalent, Apidianaki and Marie (2015) proposed METEOR-WSD. The English references are further disambiguated and annotated using Babelfly (Moro et al., 2014) for several language pairs (French, Hindi, German, Czech and Russian to English). For their experiment, Apidianaki and Marie (2015) got a better segment-level Kendall’s τ correlation than METEOR for 4 language pairs when the paraphrase module was activated. 2.3. Lexical resources 2.3.1. WordNet WordNet is a large lexical database for English, developed by linguists of Princeton University (Fellbaum, 1998). Nowadays, it has become an important and a very useful resource for NLP applications, such as machine translation, word sense disambiguation, cross-lingual information retrieval etc. WordNe"
2015.mtsummit-papers.7,P02-1040,0,0.0938324,"Missing"
2015.mtsummit-papers.7,2006.amta-papers.25,0,0.0897145,"if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction o"
2015.mtsummit-papers.7,W09-0441,0,0.272425,"Missing"
2016.jeptalnrecital-long.23,W05-0909,0,0.356762,"Missing"
2016.jeptalnrecital-long.23,D10-1115,0,0.0873607,"Missing"
2016.jeptalnrecital-long.23,S15-2048,0,0.0407588,"Missing"
2016.jeptalnrecital-long.23,D12-1050,0,0.0648203,"Missing"
2016.jeptalnrecital-long.23,W14-3302,0,0.0614457,"Missing"
2016.jeptalnrecital-long.23,D14-1179,0,0.0134047,"Missing"
2016.jeptalnrecital-long.23,J81-4005,0,0.769578,"Missing"
2016.jeptalnrecital-long.23,N10-1031,0,0.0459707,"Missing"
2016.jeptalnrecital-long.23,W10-1751,0,0.0595858,"Missing"
2016.jeptalnrecital-long.23,W14-3348,0,0.101712,"Missing"
2016.jeptalnrecital-long.23,2015.mtsummit-papers.7,1,0.806899,"Missing"
2016.jeptalnrecital-long.23,W15-3047,0,0.0354392,"Missing"
2016.jeptalnrecital-long.23,P12-1092,0,0.109942,"Missing"
2016.jeptalnrecital-long.23,W15-1521,0,0.0485311,"Missing"
2016.jeptalnrecital-long.23,W14-3336,0,0.0360437,"Missing"
2016.jeptalnrecital-long.23,N13-1090,0,0.098669,"Missing"
2016.jeptalnrecital-long.23,Q14-1019,0,0.0309918,"Missing"
2016.jeptalnrecital-long.23,D15-1222,0,0.045271,"Missing"
2016.jeptalnrecital-long.23,P02-1040,0,0.107255,"Missing"
2016.jeptalnrecital-long.23,2006.amta-papers.25,0,0.162279,"Missing"
2016.jeptalnrecital-long.23,W09-0441,0,0.0556121,"Missing"
2016.jeptalnrecital-long.23,P10-1040,0,0.136217,"Missing"
2016.jeptalnrecital-long.23,W15-3051,0,0.035997,"Missing"
2016.jeptalnrecital-long.23,D13-1141,0,0.0792269,"Missing"
2016.jeptalnrecital-poster.1,2012.iwslt-evaluation.16,0,0.023813,"Missing"
2016.jeptalnrecital-poster.1,W08-0510,0,0.0243148,"Missing"
2016.jeptalnrecital-poster.1,P07-2045,0,0.00480078,"Missing"
2016.jeptalnrecital-poster.1,W14-1614,0,0.0542726,"Missing"
2018.jeptalnrecital-court.15,D07-1108,0,0.108092,"Missing"
2018.jeptalnrecital-court.15,S07-1054,0,0.0411897,"Missing"
2018.jeptalnrecital-court.15,P16-1085,0,0.0216973,"Missing"
2018.jeptalnrecital-court.15,P07-2045,0,0.0060887,"Missing"
2018.jeptalnrecital-court.15,2009.jeptalnrecital-long.19,0,0.0865709,"Missing"
2018.jeptalnrecital-court.15,2007.jeptalnrecital-long.9,0,0.0896225,"Missing"
2018.jeptalnrecital-court.15,S07-1047,0,0.0443775,"Missing"
2018.jeptalnrecital-court.15,2007.jeptalnrecital-long.37,0,0.0254618,"Missing"
2018.jeptalnrecital-demo.7,2009.jeptalnrecital-long.19,0,0.162179,"Missing"
2018.jeptalnrecital-demo.7,2007.jeptalnrecital-long.9,0,0.185196,"Missing"
2018.jeptalnrecital-demo.7,2007.jeptalnrecital-long.37,0,0.0839897,"Missing"
C02-1170,W00-0203,0,\N,Missing
C12-1146,C08-1009,0,0.122185,"ife scenarios as well as on analysis windows of more than a few words. Several approximation methods can be used in order to overcome the combinatorial explosion problem. On the one hand, complete approaches, try to reduce dimensionality using pruning techniques and sense selection heuristics. Some examples include: (Hirst and St-Onge, 1998), based on lexical chains that restrict the possible sense combinations by imposing constraints on the succession of relations in a taxonomy (e.g. WordNet); or (Gelbukh et al., 2005) that review general pruning techniques for Lesk-based algorithms; or yet (Brody and Lapata, 2008). On the other hand, incomplete approaches generally use stochastic sampling techniques to reach a local maximum by exploring as little as necessary of the search space. Our present work focuses on such approaches. Furthermore, we can distinguish two possible variants: • local neighbourhood-based approaches (new configurations are created from existing configurations) among which are some approaches from artificial intelligence such as genetic algorithms or optimization methods such as simulated annealing ; • constructive approaches (new configurations are generated by iteratively adding new e"
C12-1146,H92-1046,0,0.725909,"s a paragraph for instance, can not be handled. Thus, such approaches can not be used for applications where real time is a necessary constraint (image retrieval, machine translation, augmented reality). In order to overcome this problem and to perform WSD faster, we are interested in other methods. In this paper, we focus on three methods that globally propagate a local algorithm based on semantic relatedness to the span of a whole text. We consider two unsupervised algorithms from the state of the art, a Genetic Algorithm (GA) (Gelbukh et al., 2003) and a Simulated Annealing (SA) algorithm (Cowie et al., 1992), as well as an adaptation of an Ant Colony Algorithm (ACA) (Schwab et al., 2011). Our aim is to provide an empirical comparison of the ACA with the two other unsupervised algorithms, using the Semeval-2007, Task-7, Coarse grained corpus (Navigli et al., 2007) (both in terms of quality and execution time). Furthermore, we also evaluate the results after applying a majority vote strategy. After a brief review of the state-of-the-art of WSD, the algorithms are described. Subsequently, their implementations are discussed, as well as the estimation of the best parameters and the evaluation of the"
C12-1146,H92-1045,0,0.490009,"ods do for computational reasons). Indeed, in our opinion, using a context window smaller than that of the whole text raises two main issues: no guarantee on the consistency between two selected senses; contradictory sense assignments outside of the window range. For example in the following sentence, considering a window of 6 words: &quot;The two planes were parallel to each other. The pilot had parked them meticulously.&quot;, plane may be disambiguated wrongly due to pilot being outside the window of plane. Furthermore it can be detrimental to the semantic unity in the disambiguation, given that as (Gale et al., 1992) or (Hirst and St-Onge, 1998) pointed out, two words used several times in the same context tend to have the same sense. Therefore, some algorithms that are similar to our Ant Colony Algorithm but that use a context window have not been studied here (notably the adaptation (Mihalcea et al., 2004) of PageRank (Brin and Page, 1998) to WSD). Moreover, we are not interested in comparing these incomplete algorithm, which cannot pragmatically be used in a real-life context, to the optimal disambiguation (Brute Force). Even with a reduced windows of the context and weeks of execution time we were onl"
C12-1146,C04-1162,0,0.067379,"Missing"
C12-1146,S07-1006,0,0.464786,"aster, we are interested in other methods. In this paper, we focus on three methods that globally propagate a local algorithm based on semantic relatedness to the span of a whole text. We consider two unsupervised algorithms from the state of the art, a Genetic Algorithm (GA) (Gelbukh et al., 2003) and a Simulated Annealing (SA) algorithm (Cowie et al., 1992), as well as an adaptation of an Ant Colony Algorithm (ACA) (Schwab et al., 2011). Our aim is to provide an empirical comparison of the ACA with the two other unsupervised algorithms, using the Semeval-2007, Task-7, Coarse grained corpus (Navigli et al., 2007) (both in terms of quality and execution time). Furthermore, we also evaluate the results after applying a majority vote strategy. After a brief review of the state-of-the-art of WSD, the algorithms are described. Subsequently, their implementations are discussed, as well as the estimation of the best parameters and the evaluation of the tested algorithms. Finally, an analysis of the results is presented as well as a comparison to other systems on Semeval 2007 Task 7. Then, we conclude and propose some perspectives for future work. 2 Brief State of the art of Word Sense Disambiguation In simpl"
C12-1146,P10-1154,0,0.0407808,"Local and global WSD Algorithms 3.1 Our local algorithm : A variant of the Lesk Algorithm Our local algorithm is a variant of the Lesk Algorithm (Lesk, 1986). Proposed more than 25 years ago, it is simple, only requires a dictionary and no training. The score given to a sense pair is the number of common words (space separated strings) in the definition of the senses, without taking into account neither the word order in the definitions (bag-of-words approach), nor any syntactic or morphological information. Variants of this algorithm are still today among the best on English-language texts (Ponzetto and Navigli, 2010). Our local algorithm exploits the links provided by WordNet: it considers not only the definition of a sense but also the definitions of the linked senses (using all the semantic relations from WordNet) following (Banerjee and Pedersen, 2002), henceforth referred as E x t Lesk1 . Contrarily to Banerjee, however, we do not consider the sum of squared sub-string overlaps, but merely a bag-of-words overlap that allows us to generate a dictionary from WordNet, where each word contained in any of the word sense definitions is indexed by a unique integer and where each resulting definition is sorte"
C12-1146,J98-1001,0,\N,Missing
C16-1110,W05-0909,0,0.245811,"16, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1159–1168, Osaka, Japan, December 11-17 2016. Contributions: this article attempts to review the contribution of vector representations to measure sentence similarity. We compare them with similarity measures based on lexical resources such as WordNet or DBnary. Machine Translation (MT) evaluation was identified as a particularly interesting application to investigate, since MT evaluation is still an open problem nowadays. More precisely, we propose to augment a well known MT evaluation metric (METEOR (Banerjee and Lavie, 2005)) which allows an approximate matching (through synonymy or morphological similarity) between MT hypothesis and reference. The augmented versions of METEOR proposed (using word embeddings, lexical resources or both) allow us to objectively compare the contribution of each approach to measure sentence similarity. For this, correlations between METEOR and human judgements (of MT outputs) are measured within the framework of WMT 2014 Metrics task. The code of the augmented versions of METEOR is also provided on our Github page3 . Outline: in section 2 (Related Work), we quickly present METEOR, le"
C16-1110,D10-1115,0,0.0445168,"). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embeddings capture intrinsically synonymy or morphological closeness) or in two different languages (bilingual embeddings allow to directly compute a distance between two senten"
C16-1110,L16-1662,1,0.882216,"Missing"
C16-1110,J81-4005,0,0.745114,"Missing"
C16-1110,N10-1031,0,0.0201285,"n judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and"
C16-1110,W10-1751,0,0.0244647,"n judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and"
C16-1110,W14-3348,0,0.0579533,"d Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russ"
C16-1110,2015.mtsummit-papers.7,1,0.739833,"an, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russian and English. 2.2 Lexical resources 2.2.1 WordNet WordNet is a well known lexical resource for English. Created at the University of Princeton (Fellbaum, 1998), it is used in several NLP tasks such as Machine Translation, Word Sense Disambiguation, 3 https://github.com/cservan/METEOR-E 1160 Cross-lingual Information Retrieval, etc. WordNet links nouns, verbs, adjectives and adverbs to a set of synonyms called “synsets”. Each synset represents a s"
C16-1110,W15-3047,0,0.0410281,"Missing"
C16-1110,P12-1092,0,0.0360721,"erent target languages. Additionally, DBnary contains lexicosemantic relations (syno/anto-nyms, hypo/hypero-nyms, etc.). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embeddings capture intrinsically synonymy or morph"
C16-1110,W15-1521,0,0.0250285,"particular, many contributions were proposed after the work of (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) on training word embeddings. The main reasons for this strong interest are: the proposal of a simple and efficient neural architecture to learn word vector representations, the availability of an open source tool Word2Vec1 and the rapid structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monol"
C16-1110,W14-3336,0,0.118723,"Missing"
C16-1110,N13-1090,0,0.0327015,"n automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page. 1 Introduction Learning vector representations of words using neural networks has generated a strong enthusiasm in the NLP research community. In particular, many contributions were proposed after the work of (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) on training word embeddings. The main reasons for this strong interest are: the proposal of a simple and efficient neural architecture to learn word vector representations, the availability of an open source tool Word2Vec1 and the rapid structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between w"
C16-1110,Q14-1019,0,0.0221709,"ng based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (Sérasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russian and English. 2.2 Lexical resources 2.2.1 WordNet WordNet is a well known lexical resource for English. Created at the University of Princeton (Fellbau"
C16-1110,P10-1023,0,0.030229,"d structuring of a user community2 . Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close rep"
C16-1110,D15-1222,0,0.0790626,"Missing"
C16-1110,L16-1421,0,0.0197845,"es of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic). However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (Sérasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity,"
C16-1110,P02-1040,0,0.100993,"utline: in section 2 (Related Work), we quickly present METEOR, lexical resources and word embeddings. Section 3 presents our propositions to augment METEOR in order to conduct a fair comparison between lexical resources and vector representations respectively. Section 4 presents our experiments made within the framework of WMT 2014, as well as quantitative and qualitative analyses. Finally, section 5 concludes this work and gives some perpectives. 2 Related Work 2.1 An automatic metric for MT evaluation: METEOR 2.1.1 The origins METEOR was proposed to compensate BLEU’s and NIST’s weaknesses (Papineni et al., 2002; Doddington, 2002). In short, METEOR was created to better correlate with human judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be"
C16-1110,P15-1173,0,0.0250212,"initum on any monolingual or bilingual corpora. In short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity, sentiment analysis, finding of synonyms). More recently, Panchenko (2016) and Rothe and Schütze (2015) extended word embeddings to sense embeddings and tried to compare them to lexical synsets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://word2vec.googlecode.com/svn/trunk/ 2 https://groups.google.com/d/forum/word2vec-toolkit 1159 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1159–1168, Osaka, Japan, December 11-17 2016. Contributions: this article attempts to review the contribution of vector representations to meas"
C16-1110,2006.amta-papers.25,0,0.0961653,"ade according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR). One contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Mar"
C16-1110,W09-0441,0,0.0257369,"ose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on. 2.1.2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate – HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014). METEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR. Finally, to extend the use of Synonym module to target languages others than Eng"
C16-1110,P10-1040,0,0.0606763,"cted sources languages to more than 1500 different target languages. Additionally, DBnary contains lexicosemantic relations (syno/anto-nyms, hypo/hypero-nyms, etc.). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5 . Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets. 2.3 Monolingual and bilingual embeddings 2.3.1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity. Using word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embed"
C16-1110,W15-3051,0,0.245132,"Missing"
C16-1110,D13-1141,0,0.0832119,"Missing"
C16-1110,W15-1006,0,\N,Missing
C92-4198,C90-3008,0,0.238563,"Missing"
C92-4198,C90-3074,0,0.354677,"Missing"
C92-4198,C90-2045,0,0.232871,"Missing"
C92-4198,C90-1017,0,0.0213068,"Missing"
C92-4198,C88-2155,0,\N,Missing
C92-4198,C88-2160,0,\N,Missing
C92-4198,C90-3006,0,\N,Missing
C92-4198,C90-2006,0,\N,Missing
C92-4198,C82-1034,0,\N,Missing
C94-1017,C92-4198,1,\N,Missing
C94-1017,C92-3165,0,\N,Missing
huynh-etal-2008-sectra,2004.iwslt-papers.1,1,\N,Missing
huynh-etal-2008-sectra,2006.iwslt-evaluation.3,1,\N,Missing
potet-etal-2012-collection,J11-2010,0,\N,Missing
S13-2041,C08-1009,0,0.373973,"Missing"
S13-2041,S13-2040,0,0.184228,"Missing"
S13-2041,P10-1154,0,0.351195,"Missing"
S13-2041,C12-1146,1,0.913991,"Missing"
S13-2041,W12-6108,1,0.896185,"Missing"
S13-2041,P94-1019,0,\N,Missing
W10-1723,P02-1040,0,0.0769726,"Missing"
W10-1723,P03-1021,0,0.0650887,"Missing"
W10-1723,W04-3250,0,0.197288,"Missing"
W10-1723,W09-0427,0,0.0116719,"on quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs. 1 Introduction We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation (WMT 2010) for the FrenchEnglish language pair. The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system’s unknown words. Indeed, as shown in (Carpuat, 2009), (Habash, 2008) and (Niessen, 2004), handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is a way to improve translation quality. After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement. In the last part, we evaluate their impact on translation quality using automatic and human evaluations. We used the provided Europarl and News parallel corpora (total 1,638,440 sentences) to train the translation model and the Ne"
W10-1723,J04-2003,0,0.0808342,"Missing"
W10-1723,2009.mtsummit-btm.3,1,0.826755,"Missing"
W10-1723,P07-2045,0,\N,Missing
W10-1723,P08-2015,0,\N,Missing
W10-1723,D08-1076,0,\N,Missing
W10-4009,lavie-etal-2002-nespole,0,0.0174981,"uting the angular distance between two conceptual vectors (Schwab and Lafourcade, 2007). In our case, conceptual vectors are used for automatic disambiguation of texts. Using this method, we calculate confidence score for each UW hypothesis appearing in the Q-Graph. 5 Ontology driven content extraction The content extraction has to be leaded by a “knowledge base” containing the informations we want to retrieve. 5.1 Previous works in content extraction This approach has its roots in machine translation projects such as C-Star II (1993-1999) (Blanchon and Boitet, 2000) and Nespole! (2000-2002) (Metze et al., 2002), for on the fly translation of oral speech acts in the domain of tourism. In these projects, semantic transfer was achieved through an IF (Inter-exchange Format), that is a semantic pivot dedicated to the domain. This IF allows to store information extracted from texts but is although used to lead the content extraction process by giving a formal representation of the relevant informations to extract, according to the domain. The Nespole! IF consists of 123 concepts from the tourism domain, associated with several arguments and associable with speech acts markers. The extraction process is ba"
W11-2154,W08-0509,0,0.0344349,"of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga Engl"
W11-2154,2005.eamt-1.19,0,0.0365934,"l when citations included in sentences have to be translated. Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups. However, the measured gains were finally too marginal to include the method in the final system. 4.2 Parallel corpus subsampling As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best co"
W11-2154,W10-1713,1,0.771338,"in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best configuration tested in (Huet et al., 2010), we index the three out-of-domain corpora using L EMUR3 , and build queries from English news-s sentences where stop words are removed. The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected. The process is repeated until the training parallel corpus reaches a threshold over the number of re"
W11-2154,D07-1103,0,0.0637009,"ning on mono-news-c and news-s Training on news-c, euro and UN 5-gram models Training on 10 M sentence pairs selected in news-c, euro, UN and giga Translation model Phrase table filtering Use of -monotone-at-punctuation option Table 2: Distinct features between final configurations retained for the LIG and LIA systems 3.3 Translation model training Translation models were trained from the parallel corpora news-c, euro and UN. Data were aligned at the word-level and then used to build standard phrase-based translation models. We filtered the obtained phrase table using the method described in (Johnson et al., 2007). Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system. 3.4 Tuning For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability. We prevent phrase reordering over punctuation using the M OSES option -monotone-atpunctuation. As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus"
W11-2154,P07-2045,0,0.0138313,"10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga English Monolingual training News Commentary v6 mono-news-c Shuffled News Crawl corpus (from 2007 to 2011) news-s Europarl v6 mono"
W11-2154,J03-1002,0,0.00334518,"evoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCE"
W11-2154,P03-1021,0,0.0208191,"us (from 2007 to 2011) news-s Europarl v6 mono-euro 116 k 1.8 M 12 M 23 M 181 k 25 M 1.8 M Development newstest2008 newssyscomb2009 newstest2009 test08 testcomb09 test09 2,051 502 2,525 test10 2,489 Test newstest2010 Table 1: Used corpora processed in order to normalize a special French form (named euphonious “t”) as described in (Potet et al., 2010). • 5 translation model scores, • 1 distance-based reordering score, • 6 lexicalized reordering score, • 1 LM score and • 1 word penalty score. The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003). The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4. Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST. Table 2 summarizes the differences between the final configuration of the systems. 3 The LIG machine translation system LIG participated for the second time to the WMT shared news translation task for the French-English language pair. 3.1 Pre-processing Training data were first lowercased with the P ERL script provided for the"
