2020.acl-main.602,P15-1034,0,0.0366732,"Missing"
2020.acl-main.602,N19-1405,0,0.0494746,"o address this issue, the community has introduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to"
2020.acl-main.602,P19-1346,0,0.014447,"tpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs [1] , [2] , and [7] , however, only a sub"
2020.acl-main.602,D16-1264,0,0.167371,"Missing"
2020.acl-main.602,C18-1283,0,0.0154463,"also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs ["
2020.acl-main.602,Q18-1021,1,0.940297,"[7] Wood died only days before the scheduled release of the band’s debut album, “Apple”, thus ending the… Explanation Answer Supporting facts (SFs): Malfunkshun Output [1], [2], [4], [6], [7] R4C: Derivation [Malfunkshun] is [a rock band] [Andrew Wood] is lead singer of [Malfunkshun] [Andrew Wood] is a member of [Mother Love Bone] [Malfunkshun] is former of [Mother Love Bone] [Andrew Wood] died just before the release of [Apple] Introduction Reading comprehension (RC) has become a key benchmark for natural language understanding (NLU) systems, and a large number of datasets are now available (Welbl et al., 2018; Koˇcisk`y et al., 2018; Yang et al., 2018, i.a.). However, it has been established that these datasets suffer from annotation artifacts and other biases, which may allow systems to “cheat”: Instead of learning to read and comprehend texts in their entirety, systems learn to exploit these biases and find answers via simple heuristics, such as looking for an entity with a particular semantic type (Sugawara et al., 2018; Mudrakarta et al., 2018) (e.g. given a question starting with Who, a system finds a person entity found in a document). To address this issue, the community has introduced incr"
2020.acl-main.602,D18-1259,0,0.358136,"release of the band’s debut album, “Apple”, thus ending the… Explanation Answer Supporting facts (SFs): Malfunkshun Output [1], [2], [4], [6], [7] R4C: Derivation [Malfunkshun] is [a rock band] [Andrew Wood] is lead singer of [Malfunkshun] [Andrew Wood] is a member of [Mother Love Bone] [Malfunkshun] is former of [Mother Love Bone] [Andrew Wood] died just before the release of [Apple] Introduction Reading comprehension (RC) has become a key benchmark for natural language understanding (NLU) systems, and a large number of datasets are now available (Welbl et al., 2018; Koˇcisk`y et al., 2018; Yang et al., 2018, i.a.). However, it has been established that these datasets suffer from annotation artifacts and other biases, which may allow systems to “cheat”: Instead of learning to read and comprehend texts in their entirety, systems learn to exploit these biases and find answers via simple heuristics, such as looking for an entity with a particular semantic type (Sugawara et al., 2018; Mudrakarta et al., 2018) (e.g. given a question starting with Who, a system finds a person entity found in a document). To address this issue, the community has introduced increasingly more difficult Question Answering"
2020.acl-main.602,W18-1703,0,0.0229255,"o provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts”"
2020.acl-main.602,L18-1433,0,0.116796,"Missing"
2020.acl-main.602,P19-1261,0,0.0191287,"troduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify sup"
2020.acl-main.602,Q18-1023,0,0.0608472,"Missing"
2020.acl-main.602,P19-1416,0,0.0313211,"e community has introduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but"
2020.acl-main.602,P19-1487,0,0.0634591,"., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs [1] , [2] , and [7] , however, only a subset of SFs may contrib"
2020.emnlp-main.244,P17-1171,0,0.0235636,"layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset"
2020.emnlp-main.244,P18-1078,0,0.0198513,"iever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Reader Model For all our experiments, we fine-tune a pre-trained ALBERT model (Lan et al., 2020), consisting of 24 transformer layers and cross-layer parameter sharing. We do not use global normalisation (Clark and Gardner, 2018) in our implementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages an"
2020.emnlp-main.244,2020.emnlp-main.21,0,0.0135936,"able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et al. (2020) tune them on an held-out validation set via temperature calibration (Guo et al., 2017; Desai and Durrett, 2020), where a temperature T is tuned to adapt the softmax output probabilities at each layer. 3 Adaptive Computation in ODQA Our goal is to incrementally build up towers of transformer layers for all passages in Dq in a way that minimises unnecessary computation. Our algorithms maintain a state, or skyline, S = (H, A), consisting of current tower heights H = (h1 , . . . , hn ), indicating how many layers have been processed for each of the n towers, and the last representations A = (a1 , . . . , an ) computed for each of the towers. We want to build up the 1 In practice, Schwartz et al. (2020) cho"
2020.emnlp-main.244,N19-1423,0,0.0105078,"A) requires a system to answer questions using a large collection of documents as the information source. In contrast to context-based machine comprehension, where models are to extract answers from single paragraphs or documents, it poses a fundamental technical challenge in machine reading at scale (Chen et al., 2017) . Most ODQA systems consist of two-stage pipelines, where 1) a context retriever such as BM25 (Robertson, 2004) or DPR (Karpukhin et al., 2020) first selects a small subset of passages that are likely to contain the answer to the question, and 2) a machine reader such as BERT (Devlin et al., 2019) then examines the retrieved contexts to extract the answer. This two-stage process leads to a computational trade-off that is indicated in Fig. 1. We can run computationally expensive deep networks on a large number of passages to increase the probability that we find the right answer (“All Layers, All Passages”), or cut the number of passages and layers to reduce the computational footprint at the possible cost of missing an answer (“6 Layers, Top-2 Passages”). We hypothesise that a better accuracy-efficiency trade-off can be found if the computational budget is not allocated statically, but"
2020.emnlp-main.244,2020.emnlp-main.550,0,0.023734,"Missing"
2020.emnlp-main.244,J81-4005,0,0.673255,"Missing"
2020.emnlp-main.244,N19-4013,0,0.0736717,". (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training set, two development sets Size"
2020.emnlp-main.244,D18-1153,0,0.025201,"ut conditioned on the input. Elbayad et al. (2020) generalise universal transformers by also learning which layer to execute at each step. Schwartz et al. (2020); Liu et al. (2020) propose methods that can adaptively decide when to early stop the computation in sentence classification tasks. To the best of our knowledge, previous work has focused adaptive computation for a single input. We are the first to learn how to prioritise computation across instances in the context of ODQA. Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a tw"
2020.emnlp-main.244,2020.acl-main.537,0,0.0172027,"e selected tower contains an answer and r = 0 otherwise. c ∈ R+ is a penalty cost of taking a step. In our experiments, we set c = 0.1. 3032 4 Related Work SQuAD-Open Adaptive Computation One strategy to reduce a model’s complexity consists in dynamically deciding which layers to execute during inference (Bengio et al., 2015; Graves, 2016). Universal transformers (Dehghani et al., 2019) can learn after how many layers to emit an output conditioned on the input. Elbayad et al. (2020) generalise universal transformers by also learning which layer to execute at each step. Schwartz et al. (2020); Liu et al. (2020) propose methods that can adaptively decide when to early stop the computation in sentence classification tasks. To the best of our knowledge, previous work has focused adaptive computation for a single input. We are the first to learn how to prioritise computation across instances in the context of ODQA. Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer"
2020.emnlp-main.244,D19-1284,0,0.0113689,"his idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training"
2020.emnlp-main.244,2020.acl-main.593,0,0.106923,"ned height n and use an output layer to produces the final output, y = OutputLayer(hn ). In this work, due to efficiency reasons, we restrict ourselves to pre-trained ALBERT (Lan et al., 2020) models. One critical property of these models is parameter tying across layers: TransformerLayeri (h) = TransformerLayerj (h) for any i, j. 2.3 Adaptive Computation Our goal is to early-exit the iterative layer-by-layer process in order to save computation. We assume this can be happening adaptively, based on the input, since some passages might require less computation to produce an answer than others. Schwartz et al. (2020) show how this can be achieved for classification tasks. They first require internal layers to be able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et a"
2020.emnlp-main.244,D19-1599,0,0.050169,"dels. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 Experiments Dataset SQuAD-Open (Chen et al., 2017) is a popular open-domain question answering dataset based on SQuAD. We partition the dataset into four subsets: training set, two development sets Size Hits@30 train dev0 dev1 test 78,839 71."
2020.emnlp-main.692,D13-1170,0,\N,Missing
2020.emnlp-main.692,W19-5034,0,\N,Missing
2020.emnlp-main.692,P19-1513,0,\N,Missing
2020.emnlp-main.692,N19-1423,0,\N,Missing
2020.emnlp-main.692,2020.acl-main.398,0,\N,Missing
2020.findings-emnlp.103,P18-2006,0,0.19252,"recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier unde"
2020.findings-emnlp.103,W17-5401,0,0.0397192,"Missing"
2020.findings-emnlp.103,D18-1407,0,0.0273755,"Missing"
2020.findings-emnlp.103,N18-2017,0,0.0587184,"Missing"
2020.findings-emnlp.103,N18-1170,0,0.349247,"non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier undersensitivity in computer vision. Niu and Bansal (2018) investiga"
2020.findings-emnlp.103,D17-1215,0,0.569717,"acked on a substantial proportion of samples. The observed undersensitivity correlates negatively with in-distribution test set performance metrics (EM/F1 ), suggesting that this phenomenon – where present – is indeed a reflection of a model’s lack of question comprehension. When training models to defend against undersensitivity attacks with data augmentation and adversarial training, we observe that they can generalise their robustness to held out evaluation data without sacrificing in-distribution test set performance. Furthermore, the models improve on the adversarial datasets proposed by Jia and Liang (2017), and behave more robustly in a learning scenario that has dataset bias with a train / evaluation distribution mismatch, increasing performance by up to 10.9%F1 . List of Contributions: i) We propose a new type of adversarial attack exploiting the undersensitivity of neural RC models to input changes, and show that contemporary models are vulnerable to it; ii) We compare data augmentation and adversarial training as defences, and show their effectiveness at reducing undersensitivity errors on both held-out data and held-out perturbations without sacrificing nominal test performance; iii) We de"
2020.findings-emnlp.103,2021.ccl-1.108,0,0.144103,"Missing"
2020.findings-emnlp.103,K18-1047,0,0.0205783,"araphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Undersensitivity Jacobsen et al. (2019) demonstrated classifier undersensitivity in computer vision. Niu and Bansal (2018) investigated undersensitivity in dialogue models and addressed the problem with a max-margin training approach. Ribeiro 1153 et al. (2018a) describe a general model diagnosis tool to identify minimal feature sets that are sufficient for a model to form high-confidence predictions. Feng et al. (2018) showed that it is possible to reduce inputs to minimal input word sequences without changing a model’s predictions. Welbl et al. (2020) investigated formal verification against undersensitivity to text deletions. We see our work as a continuation of these lines of inquiry, with a particular focus"
2020.findings-emnlp.103,P18-2124,0,0.0322545,"ion that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve pe"
2020.findings-emnlp.103,P18-1079,0,0.355555,") We demonstrate that the resulting models generalise better on the adversarial datasets of Jia and Liang (2017), and in the biased data setting of Lewis and Fan (2019). 2 Related Work Adversarial Attacks in NLP Adversarial examples have been studied extensively in NLP – see Zhang et al. (2019) for a recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can"
2020.findings-emnlp.103,2020.acl-main.442,0,0.0113561,"odels learning shallow but successful heuristics, and propose counterfactual data annotation paradigms as prevention. The perturbations used in this work define such counterfactual samples. Their composition does not require additional annotation efforts, and we furthermore adapt an adversarial perspective on the choice of such samples. Finally, one of the methods we evaluate for defending against undersensitivity attacks is a form of data augmentation that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable que"
2020.findings-emnlp.103,P19-1485,0,0.132099,"Missing"
2020.findings-emnlp.103,N18-2003,0,0.0268061,"ght it. Gardner et al. (2020) and Kaushik et al. (2020) also recognise the problem of models learning shallow but successful heuristics, and propose counterfactual data annotation paradigms as prevention. The perturbations used in this work define such counterfactual samples. Their composition does not require additional annotation efforts, and we furthermore adapt an adversarial perspective on the choice of such samples. Finally, one of the methods we evaluate for defending against undersensitivity attacks is a form of data augmentation that has similarly been used for de-biasing NLP models (Zhao et al., 2018; Lu et al., 2018). Concurrent work on model C HECK L IST evaluation (Ribeiro et al., 2020) includes an invariance test which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43"
2020.findings-emnlp.103,P19-1415,0,0.0208967,"dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve performance in SQuAD2.0 (Zhu et al., 2019; Alberti et al., 2019). In contrast to prior work, we demonstrate that despite improving performance on test sets that include unanswerable questions, the problem persists when adversarially choosing from a larger space of questions. 3 Methodology Problem Overview Consider a discriminative model fθ , parameterised by a collection of vectors θ, which transforms an input x into a prediction yˆ = fθ (x). In our task, x = (t, q) is a given text t paired with a question q about this text. The label y is the answer to q where it exists, or a NoAnswer label where it cannot be answered.1 In an RC set"
2020.findings-emnlp.103,W17-2623,0,0.0426761,"which also examines model undersensitivity. In contrast to C HECK L IST, our work focuses with more detail on the analysis of the invariance phenomenon, the automatic generation of probing samples, an investigation of concrete methods to overcome undesirably invariant model behaviour, and shows that adherence to invariance tests leads to more robust model generalisation. Unanswerable Questions in RC Rajpurkar et al. (2018) proposed the SQuAD2.0 dataset, which includes over 43,000 human-curated unanswerable questions. NewsQA is a second dataset with unanswerable questions, in the news domain (Trischler et al., 2017). Training on these datasets should conceivably result in models with an ability to tell whether questions are answerable or not; we will however see that this does not extend to adversarially chosen unanswerable questions. Hu et al. (2019) address unanswerability of questions from a given text using additional verification steps. Other approaches have shown the benefit of synthetic data to improve performance in SQuAD2.0 (Zhu et al., 2019; Alberti et al., 2019). In contrast to prior work, we demonstrate that despite improving performance on test sets that include unanswerable questions, the p"
2020.findings-emnlp.103,N18-2091,0,0.0621991,"ave been studied extensively in NLP – see Zhang et al. (2019) for a recent survey. Yet automatically generating adversarial inputs is non-trivial, as altering a single word can change the semantics of an instance or render it incoherent. Prior work typically considers semantic-invariant input transformations to which neural models are oversensitive. For instance, Ribeiro et al. (2018b) use a set of simple perturbations such as replacing Who is with Who’s. Other semantics-preserving perturbations include typos (Hosseini et al., 2017), the addition of distracting sentences (Jia and Liang, 2017; Wang and Bansal, 2018), character-level adversarial perturbations (Ebrahimi et al., 2018; Belinkov and Bisk, 2018), and paraphrasing (Iyyer et al., 2018b). In this work, we focus on the complementary problem of undersensitivity of neural RC models to semantic perturbations of the input. Our method is based on the idea that modifying, for instance, the named entities in a question can completely change its meaning and, as a consequence, the question should become unanswerable given the context. Our approach does not assume white-box access to the model, as do e.g. Ebrahimi et al. (2018) and Wallace et al. (2019). Un"
2020.sustainlp-1.9,P17-1171,0,0.0476768,"ementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs"
2020.sustainlp-1.9,2020.emnlp-main.21,0,0.0136063,"able to produce outputs too, yielding an anytime algorithm. 1 This can be achieved with a suitable training objective. Next, for each candidate layer i, they calculate the exit probability given its hidden state hi , and use them for taking an early-exit decision: if the highest exit probability is above a global threshold τ , they return OutputLayer(hi ) otherwise they continue with the following layers. The output layer probabilities are not calibrated for exit decisions, and hence Schwartz et al. (2020) tune them on an held-out validation set via temperature calibration (Guo et al., 2017; Desai and Durrett, 2020), where a temperature T is tuned to adapt the softmax output probabilities at each layer. 3 Early Exit with Local Exit Probabilities Adaptive Computation in ODQA Our goal is to incrementally build up towers of transformer layers for all passages in Dq in a way that minimises unnecessary computation. Our algorithms maintain a state, or skyline, S = (H, A), consisting of current tower heights H = (h1 , . . . , hn ), indicating how many layers have been processed for each of the n towers, and the last representations A = (a1 , . . . , an ) computed for each of the towers. We want to build up the"
2020.sustainlp-1.9,N19-1423,0,0.026663,"Missing"
2020.sustainlp-1.9,2020.emnlp-main.550,0,0.0358545,"me reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs predictions at the final layer, 2) the efficient baseline that always exits at a given intermediate layer for all passages, and is optimised to do so, 3) the top-k baseline that only reads the k top ranked passages an"
2020.sustainlp-1.9,J81-4005,0,0.681512,"Missing"
2020.sustainlp-1.9,N19-4013,0,0.245448,"v1 ), and test set, and their details are summarised in Table 1. Experimental Setup We follow the preprocessing approached proposed by Wang et al. (2019) and split passages into 100-word long chunks with 50-word long strides. We use a BM25 retriever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These m"
2020.sustainlp-1.9,D18-1153,0,0.024694,") and split passages into 100-word long chunks with 50-word long strides. We use a BM25 retriever to retrieve the top n passages for each question as inputs to the reader and the Wikipedia dump provided by Chen et al. (2017) as source corpus. Following Wang et al. (2019), we set n = 5 for training and n = 30 for test evaluations. Table 1 shows the Hits@30 results of our BM25 retriever on the dataset and they are comparable with previous works (Yang et al., 2019; Wang et al., 2019). Smaller Networks Another strategy consists in training smaller and more efficient models. In layer-wise dropout (Liu et al., 2018), during training, layers are randomly removed, making the model robust to layer removal operations. This idea was expanded Fan et al. (2020) to modern Transformer-based models. Other methods include Distillation (Hinton et al., 2015) of a teacher model into a student model, Pruning of architectures after training (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Reader Model For all our experiments, we fine-tune a pre-trained A"
2020.sustainlp-1.9,2020.acl-main.537,0,0.138834,"Missing"
2020.sustainlp-1.9,D19-1284,0,0.0749616,"tem (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2020). As observed by Chen et al. (2017); Yang et al. (2019); Karpukhin et al. (2020); Wang et al. (2019), the accuracy of such two-stage models increases with more passages retrieved. But it remains a challenge to efficiently read a large number of passages as the reader models are usually quite computationally costly. 5 train Baselines Following Schwartz et al. (2020), we use three types of baselines: 1) the standard baseline that reads all passages and outputs predictions at the final la"
2020.sustainlp-1.9,2020.acl-main.593,0,0.482009,"non-adaptive Transformer-based models, we incrementally build a tower—a composition of Transformer layers—until we reach some pre-defined height n and use an output layer to produces the final output, y = OutputLayer(hn ). In this work, due to efficiency reasons, we restrict ourselves to pre-trained ALBERT (Lan et al., 2020) models. One critical property of these models is parameter tying across layers: TransformerLayeri (h) = TransformerLayerj (h) for any i, j. 2.3 skyline so that we reach an accurate solution fast and then stop processing. 3.1 Our first proposal is to extend the method from Schwartz et al. (2020) in order to build up the skyline S. In particular, we will process each passage xi ∈ Dq in isolation, building up height hi and representation ai until an exit probability reaches a threshold. For Schwartz et al. (2020) the exit probability is set to be the probability of the most likely class. While ODQA is not a classification problem per se, it requires solving one as a sub-step, either explicitly or implicitly: deciding whether a passage contains the answer. In turn, our first method T OWER B UILDER, uses the probability 1 − HasAnswer(ai ) of the passage not containing the answer to calcu"
2020.sustainlp-1.9,D19-1599,0,0.0967528,"raining (LeCun et al., 1989) and Quantisation of the parameter space (Wr´obel et al., 2018; Shen et al., 2019; Zafrir et al., 2019). These methods are not adaptive, but could be used in concert with the methods proposed here. Reader Model For all our experiments, we fine-tune a pre-trained ALBERT model (Lan et al., 2020), consisting of 24 transformer layers and cross-layer parameter sharing. We do not use global normalisation (Clark and Gardner, 2018) in our implementation, but our full system (without adaptive computation) achieves an EM score of 52.6 and is comparable to Multi-passage BERT (Wang et al., 2019) which uses global normalisation. Training Pipeline The anytime reader models are first trained on training set and validated on dev0 . Then we conduct temperature calibration on dev0 . For S KYLINE B UILDER, the scheduler model is trained on dev0 with the calibrated anytime model, and validated with dev1 . Open Domain Question Answering Most modern ODQA systems adopt a two-stage approach that consists of a retriever and a reader, such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019), BERTserini (Yang et al., 2019), Multipassage BERT (Wang et al., 2019), and PathRetriever (Asai et al., 2"
2020.tacl-1.43,D15-1075,0,0.148803,"Missing"
2020.tacl-1.43,P16-1223,0,0.0608875,"Missing"
2020.tacl-1.43,D18-1241,0,0.0205586,"models in the loop. Compared to training on SQuAD, training on adversarially composed questions leads to a similar degree of generalization to non-adversarially written questions, both for SQuAD and NaturalQuestions (Kwiatkowski et al., 2019). It furthermore leads 663 find challenging, and for which natural language understanding is a requisite for generalization. Attempts to achieve this non-trivial aim have typically revolved around extensions to the SQuAD dataset annotation methodology. They include unanswerable questions (Trischler et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019; Choi et al., 2018), adding the option of ‘‘Yes’’ or ‘‘No’’ answers (Dua et al., 2019; Kwiatkowski et al., 2019), questions requiring reasoning over multiple sentences or documents (Welbl et al., 2018; Yang et al., 2018a), questions requiring rule interpretation or context awareness (Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2019), limiting annotator passage exposure by sourcing questions first (Kwiatkowski et al., 2019), controlling answer types by including options for dates, numbers, or spans from the question (Dua et al., 2019), as well as questions with free-form answers (Nguyen et al., 2016; Ko"
2020.tacl-1.43,D19-1606,0,0.140911,"Missing"
2020.tacl-1.43,W17-5401,0,0.0679325,"Missing"
2020.tacl-1.43,N19-1423,0,0.0721366,"Missing"
2020.tacl-1.43,D19-1461,0,0.0490981,"ne in a separate stage of the process, usually after data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator can probe the model directly to explore its weaknesses and formulate targeted adversarial attacks. Although Dua et al. (2019) and Dasigi et al. (2019) make use of adversarial annotations for RC, both annotation setups limit the reach of the model-in-the-loop: In DROP, primarily due to the imposition of specific answer types, and in Quoref by focusing on coreference, which is already a known RC model weakne"
2020.tacl-1.43,W18-2501,0,0.0460236,"Missing"
2020.tacl-1.43,N18-2017,0,0.0313858,"al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and AdversarialNLI (Nie et al., 2019).1 This approach probes mod"
2020.tacl-1.43,D17-1215,0,0.0525677,"al., 2009; Bowman et al., 2015; Rajpurkar et al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and Adversarial"
2020.tacl-1.43,P17-1147,0,0.0288267,"Investigating Adversarial Human Annotation for Reading Comprehension Max Bartolo Alastair Roberts Johannes Welbl Sebastian Riedel Pontus Stenetorp Department of Computer Science University College London {m.bartolo,a.roberts,j.welbl,s.riedel,p.stenetorp}@cs.ucl.ac.uk Abstract which they can arguably be seen as co-responsible (Deng et al., 2009; Bowman et al., 2015; Rajpurkar et al., 2016). Annotation approaches include expert annotation, for example, relying on trained linguists (Marcus et al., 1993), crowd-sourcing by non-experts (Snow et al., 2008), distant supervision (Mintz et al., 2009; Joshi et al., 2017), and leveraging document structure (Hermann et al., 2015). The concrete data collection paradigm chosen dictates the degree of scalability, annotation cost, precise task structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrong"
2020.tacl-1.43,D18-1546,0,0.0639995,"Missing"
2020.tacl-1.43,N19-1225,0,0.019189,"55.10.6 29.20.8 62.20.7 65.10.7 18.30.6 43.01.1 41.61.0 27.40.7 54.21.0 52.71.0 Table 8: Training models on SQuAD combined with all the adversarially created datasets DBiDAF , DBERT , and DRoBERTa . Results underlined indicate the best result per model. We report the mean and standard deviation (subscript) over 10 runs with different random seeds. addition of the original SQuAD1.1 training data, but unlike in Table 6, this comes without any noticeable decline in performance on DSQuAD , suggesting that the adversarially constructed datasets expose inherent model weaknesses, as investigated by Liu et al. (2019a). vice versa, suggesting that there may exist weaknesses inherent to each model class. 4.3 Generalization to Non-Adversarial Data Compared with standard annotation, the modelin-the-loop approach generally results in new question distributions. Consequently, models trained on adversarially composed questions might not be able to generalize to standard (‘‘easy’’) questions, thus limiting the practical usefulness of the resulting data. To what extent do models trained on model-in-the-loop questions generalize differently to standard (‘‘easy’’) questions, compared with models trained on standard"
2020.tacl-1.43,J93-2004,0,0.070458,"Missing"
2020.tacl-1.43,D16-1264,0,0.370844,"and following the same annotation protocol, we investigate the annotation setup where an annotator has to compose questions for which the model predicts the wrong answer. As a result, only samples that the model fails to predict correctly are retained in the dataset—see Figure 1 for an example. 2 Related Work Constructing Challenging Datasets Recent efforts in dataset construction have driven considerable progress in RC, yet datasets are structurally diverse and annotation methodologies vary. With its large size and combination of freeform questions with answers as extracted spans, SQuAD1.1 (Rajpurkar et al., 2016) has become an established benchmark that has inspired the construction of a series of similarly structured datasets. However, mounting evidence suggests that models can achieve strong generalization performance merely by relying on superficial cues—such as lexical overlap, term frequencies, or entity type matching (Chen et al., 2016; Weissenborn et al., 2017; Sugawara et al., 2018). It has thus become an increasingly important consideration to construct datasets that RC models We apply this annotation strategy with three distinct models in the loop, resulting in datasets with 12,000 samples e"
2020.tacl-1.43,D08-1027,0,0.166111,"Missing"
2020.tacl-1.43,D18-1233,1,0.90295,"Missing"
2020.tacl-1.43,Q19-1029,0,0.158623,"fter data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator can probe the model directly to explore its weaknesses and formulate targeted adversarial attacks. Although Dua et al. (2019) and Dasigi et al. (2019) make use of adversarial annotations for RC, both annotation setups limit the reach of the model-in-the-loop: In DROP, primarily due to the imposition of specific answer types, and in Quoref by focusing on coreference, which is already a known RC model weakness. In contrast, we investigate a scenario where a"
2020.tacl-1.43,Q18-1000,0,0.181939,"Missing"
2020.tacl-1.43,D18-1009,0,0.216213,"k structure (often arising as a compromise of the above) and difficulty, domain coverage, as well as resulting dataset biases and model blind spots (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018). A recently emerging trend in NLP dataset creation is the use of a model-in-the-loop when composing samples: A contemporary model is used either as a filter or directly during annotation, to identify samples wrongly predicted by the model. Examples of this method are realized in Build It Break It, The Language Edition (Ettinger et al., 2017), HotpotQA (Yang et al., 2018a), SWAG (Zellers et al., 2018), Mechanical Turker Descent (Yang et al., 2018b), DROP (Dua et al., 2019), CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), and AdversarialNLI (Nie et al., 2019).1 This approach probes model robustness and ensures that the resulting datasets pose a challenge to current models, which drives research to tackle new sets of problems. We study this approach in the context of Reading Comprehension (RC), and investigate its robustness in the face of continuously progressing models—do adversarially constructed datasets quickly become outdated in their usefulness as models grow stronger? Innova"
2020.tacl-1.43,K17-1028,0,0.111655,"in dataset construction have driven considerable progress in RC, yet datasets are structurally diverse and annotation methodologies vary. With its large size and combination of freeform questions with answers as extracted spans, SQuAD1.1 (Rajpurkar et al., 2016) has become an established benchmark that has inspired the construction of a series of similarly structured datasets. However, mounting evidence suggests that models can achieve strong generalization performance merely by relying on superficial cues—such as lexical overlap, term frequencies, or entity type matching (Chen et al., 2016; Weissenborn et al., 2017; Sugawara et al., 2018). It has thus become an increasingly important consideration to construct datasets that RC models We apply this annotation strategy with three distinct models in the loop, resulting in datasets with 12,000 samples each. We then study the reproducibility of the adversarial effect when retraining the models with the same data, as well as the generalization ability of models trained using datasets produced with and without a model adversary. Models can, to a considerable degree, learn to generalize to more challenging questions, based on training sets collected with both s"
2020.tacl-1.43,Q19-1000,0,0.181978,"Missing"
2020.tacl-1.43,P19-1472,0,0.0772758,"l Annotation One recently adopted approach to constructing challenging datasets involves the use of an adversarial model to select examples that it does not perform well on, an approach which superficially is akin to active learning (Lewis and Gale, 1994). Here, we make a distinction between two sub-categories of adversarial annotation: i) adversarial filtering, where the adversarial model is applied offline in a separate stage of the process, usually after data generation; examples include SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), HotpotQA (Yang et al., 2018a), and HellaSWAG (Zellers et al., 2019); ii) model-in-the-loop adversarial annotation, where the annotator can directly interact with the adversary during the annotation process and uses the feedback to further inform the generation process; examples include CODAH (Chen et al., 2019), Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), FEVER2.0 (Thorne et al., 2019), AdversarialNLI (Nie et al., 2019), as well as work by Dinan et al. (2019), Kaushik et al. (2020), and Wallace et al. (2019) for the Quizbowl task. We are primarily interested in the latter category, as this feedback loop creates an environment where the annotator ca"
2020.tacl-1.43,P09-1113,0,\N,Missing
2020.tacl-1.43,D13-1020,0,\N,Missing
2020.tacl-1.43,W17-2623,0,\N,Missing
2020.tacl-1.43,D18-1453,0,\N,Missing
2020.tacl-1.43,Q19-1026,0,\N,Missing
2020.tacl-1.43,N19-1246,0,\N,Missing
2020.tacl-1.43,Q18-1023,0,\N,Missing
2020.tacl-1.43,Q19-1016,0,\N,Missing
2021.acl-short.57,P17-1171,0,0.0998819,"eart model on two datasets, and is also more accurate than previous AC methods due to the stronger base ODQA model. All source code and datasets are available at https:// github.com/uclnlp/APE. 1 Figure 1: Overview of our approach. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. Ho"
2021.acl-short.57,P18-1078,0,0.0733427,"1: Overview of our approach. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most resea"
2021.acl-short.57,P17-1147,0,0.027952,"hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the proportion of questions whose predicted answer matches exactly with the ground truth. We also include the unrestricted setting to compare"
2021.acl-short.57,2020.emnlp-main.550,0,0.0489704,"ted Work Open Domain Question Answering ODQA is a task that aims to answer a factoid question given a document corpus. Most works in this domain follow a retriever-reader design first proposed by Chen et al. (2017). The retriever collects a set of relevant passages, then the reader comprehends and aggregates the information from multiple passages to produce the answer. Depending on the design of the reader model, these systems could be further categorised into extractive models and generative models. Extractive models (Min et al., 2019; Yang et al., 2019; Wang et al., 2019; Asai et al., 2020; Karpukhin et al., 2020) exploit an answer extraction model to predict the probabilities of answer spans, and use global normalisation (Clark and Gardner, 2018) to aggregate the answer probabilities across multiple passages. However, thanks to recent advances in sequenceto-sequence pretrained language models (Raffel et al., 2020; Lewis et al., 2020a), generative ODQA models (Min et al., 2020; Lewis et al., 2020b; Izacard and Grave, 2020b) achieve significant improvement upon extractive models, demonstrating stronger capability in combining evidence from multiple passages. We focus on generative models in this work. P"
2021.acl-short.57,Q19-1026,0,0.0116324,"pdates its priorities l0 qn with its new hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the proportion of questions whose predicted answer matches exactly with the ground truth. We also include th"
2021.acl-short.57,P19-1612,0,0.065349,"a passage with the maximum priority, forward one encoder layer for it ln0 = ln + 1, and updates its priorities l0 qn with its new hidden representation hnn and hasl0 answer probability pnn . This process will iterate for B (budget) steps, and only k passages with the most layers computed are retained in the end. 3.3 NaturalQuestions TriviaQA Train Validation Test 79,168 78,785 8,757 8,837 3,610 11,313 Table 1: Number of samples of the evaluated datasets. calibration of the HasAnswer model, unlike the method proposed by Wu et al. (2020). 4 4.1 Experiments Experimental Setup Datasets Following (Lee et al., 2019; Izacard and Grave, 2020b), we evaluate our method on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) whose statistics are shown in Table 1. Evaluation Metrics Following Wu et al. (2020), we conduct the evaluation under different computational costs at inference time. Since the number of passages k is almost linearly correlated with memory consumption and number of operations, we evaluate the performances with various number of passages k ∈ {5, 10, 20}. To evaluate the end performance of ODQA models, we use the standard Exact Match (EM) score, which is the propor"
2021.acl-short.57,2020.acl-main.703,0,0.431065,"computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficienc"
2021.acl-short.57,2020.acl-main.537,0,0.027718,"(2019) rely on BM25 for ranking passages (Robertson, 2004). Recently, Karpukhin et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020a) achieved substantial increase in retrieval performance using dense representations. Our work is based on the retrieval results from a dense retriever (Izacard and Grave, 2020b), but we show that the proposed method can still improve the quality of the support passages despite the strong retrieval performance. Adaptive Computation Adaptive computation allows the model to condition the computation cost on the input. For example, Schwartz et al. (2020b); Liu et al. (2020); Xin et al. (2020) propose models that can dynamically decide to early exit at intermediate layers when the confidence at the layer exceeds a threshold. They show that adaptively early exiting can significantly reduce the computational cost for various sequence classification tasks. Closest to our work, Wu et al. (2020) introduced adaptive computation for extractive ODQA models. We extend adaptive computation to generative ODQA models, and our approach can be incorporated in existing generative ODQA models without finetuning the base model. 3 Method In this section, we will introduce the base"
2021.acl-short.57,D19-1284,0,0.0484143,"ides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show t"
2021.acl-short.57,2020.emnlp-main.466,0,0.0563004,"er with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly i"
2021.acl-short.57,2020.acl-main.593,0,0.121795,"t al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficiency of extractive ODQA models at inference time. However, it requires fine-tuning all model parameters with a multitask learning objective, making it computationally challenging to apply this method to current state-of-the-art models. In this work, we explore an efficient approach to apply adaptive computation to large generative ODQA models. We introduce the Adaptive Passage Encoder (APE), a module that can be added to the encoder of an existing ODQA model, which has the following features: 1) it eff"
2021.acl-short.57,D19-1599,0,0.0489334,"ssage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et"
2021.acl-short.57,2020.sustainlp-1.9,1,0.705695,"2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et al., 2020a). Wu et al. (2020) show that Adaptive Computation (AC) can significantly improve the efficiency of extractive ODQA models at inference time. However, it requires fine-tuning all model parameters with a multitask learning objective, making it computationally challenging to apply this method to current state-of-the-art models. In this work, we explore an efficient approach to apply adaptive computation to large generative ODQA models. We introduce the Adaptive Passage Encoder (APE), a module that can be added to the encoder of an existing ODQA model, which has the following features: 1) it efficiently reuses the"
2021.acl-short.57,2020.acl-main.204,0,0.0272795,"5 for ranking passages (Robertson, 2004). Recently, Karpukhin et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020a) achieved substantial increase in retrieval performance using dense representations. Our work is based on the retrieval results from a dense retriever (Izacard and Grave, 2020b), but we show that the proposed method can still improve the quality of the support passages despite the strong retrieval performance. Adaptive Computation Adaptive computation allows the model to condition the computation cost on the input. For example, Schwartz et al. (2020b); Liu et al. (2020); Xin et al. (2020) propose models that can dynamically decide to early exit at intermediate layers when the confidence at the layer exceeds a threshold. They show that adaptively early exiting can significantly reduce the computational cost for various sequence classification tasks. Closest to our work, Wu et al. (2020) introduced adaptive computation for extractive ODQA models. We extend adaptive computation to generative ODQA models, and our approach can be incorporated in existing generative ODQA models without finetuning the base model. 3 Method In this section, we will introduce the base model and how our"
2021.acl-short.57,N19-4013,0,0.0619369,"ch. The adaptive passage encoder overrides the layer-by-layer computation of the encoder with an adaptive computation policy (indicated in blue dash arrows). Introduction Open-Domain Question Answering (ODQA) requires finding relevant information for a given question and aggregating the information to produce an answer. The retriever-reader architecture, popularised by Chen et al. (2017), has shown great success in this task. The retriever acquires a set of documents from external sources (e.g., Wikipedia) and the reader extracts the answer spans from these documents (Clark and Gardner, 2018; Yang et al., 2019; Wang et al., 2019; Min et al., 2019; Asai et al., 2020). Recently, Min et al. (2020); Lewis et al. (2020b); Izacard and Grave (2020b) showed that generative reader models that exploit an encoder-decoder architecture can significantly outperform previous extractive models, thanks to their better capability in aggregating and combining evidence from multiple passages. However, these generative models are much more computationally expensive than extractive models, and often need to be trained with a large number of passages, making it hard to train these models for most researchers (Schwartz et"
2021.eacl-main.13,N19-1423,0,0.0547219,"Missing"
2021.eacl-main.13,P18-2006,0,0.041831,"ally discovered for computer vision tasks, natural language processing (NLP) models have also been shown to be oversensitive to adversarial input perturbations for a variety of tasks (Papernot et al., 2016; Jia and Liang, 2017; Belinkov and Bisk, 2018; Glockner et al., 2018; Iyyer et al., 2018). Here we focus on highly successful synonym substitution attacks (Alzantot et al., 2018; Ren et al., 2019; Zang et al., 2020), in which individual words are replaced with semantically similar ones. Existing defense methods against these attacks mainly focus on adversarial training (Jia and Liang, 2017; Ebrahimi et al., 2018; Ribeiro et al., 2018; Ren et al., 2019; Jin et al., 2019) and hence typically require a priori attack knowledge and models to be retrained from scratch to increase their robustness. Recent work by Zhou et al. (2019) instead proposes DISP (learning to discriminate perturbations), a perturbation discrimination framework that exploits pre-trained contextualized word representations to detect and correct word-level adversarial substitutions without having to retrain the attacked model. In this paper, we show that we can achieve an improved performance for the detection and correction of adversar"
2021.eacl-main.13,P18-2103,0,0.0129721,"ons (bold, black) using the G E NETIC (Alzantot et al., 2018) and PWWS (Ren et al., 2019) attacks on SST-2 (Socher et al., 2013). Introduction Artificial neural networks are vulnerable to adversarial examples—carefully crafted perturbations of input data that lead a learning model into making false predictions (Szegedy et al., 2014). While initially discovered for computer vision tasks, natural language processing (NLP) models have also been shown to be oversensitive to adversarial input perturbations for a variety of tasks (Papernot et al., 2016; Jia and Liang, 2017; Belinkov and Bisk, 2018; Glockner et al., 2018; Iyyer et al., 2018). Here we focus on highly successful synonym substitution attacks (Alzantot et al., 2018; Ren et al., 2019; Zang et al., 2020), in which individual words are replaced with semantically similar ones. Existing defense methods against these attacks mainly focus on adversarial training (Jia and Liang, 2017; Ebrahimi et al., 2018; Ribeiro et al., 2018; Ren et al., 2019; Jin et al., 2019) and hence typically require a priori attack knowledge and models to be retrained from scratch to increase their robustness. Recent work by Zhou et al. (2019) instead proposes DISP (learning to"
2021.eacl-main.13,P82-1020,0,0.454704,"Missing"
2021.eacl-main.13,P16-1162,0,0.0545428,"Missing"
2021.eacl-main.13,D13-1170,0,0.00531053,"astly, we analyze the probability weighted word saliency (PWWS) algorithm (Ren et al., 2019). For each word in an input sequence, PWWS selects a set of synonym replacements from W ORD N ET and chooses the synonym yielding the highest difference in prediction confidence for the true class label after replacement. The algorithm furthermore computes the word saliency (Li et al., 2016a,b) for each input word and ranks word replacements based on these two indicators. Datasets and models. We perform experiments on two binary sentiment classification datasets, the Stanford Sentiment Treebank (SST-2, Socher et al., 2013) and the IMDb reviews dataset (Maas et al., 2011), both of which are widely used in related work focusing on adversarial examples in NLP (Jia et al., 2019; Ren et al., 2019; Zhou et al., 2019). Dataset details can be found in Appendix A. Adhering to Zhou et al. (2019), we attack a pretrained model based on the Transformer architecture (Vaswani et al., 2017). Zhou et al. (2019) use BERT (Devlin et al., 2019) in their experiments, but we found that RoBERTa (Liu et al., 2019) represents a stronger model for the specified tasks. We additionally experiment with both a CNN (Kim, 2014) and an LSTM (H"
2021.eacl-main.13,W19-4824,0,0.059858,"Missing"
2021.eacl-main.13,2020.acl-main.540,0,0.168588,"ial neural networks are vulnerable to adversarial examples—carefully crafted perturbations of input data that lead a learning model into making false predictions (Szegedy et al., 2014). While initially discovered for computer vision tasks, natural language processing (NLP) models have also been shown to be oversensitive to adversarial input perturbations for a variety of tasks (Papernot et al., 2016; Jia and Liang, 2017; Belinkov and Bisk, 2018; Glockner et al., 2018; Iyyer et al., 2018). Here we focus on highly successful synonym substitution attacks (Alzantot et al., 2018; Ren et al., 2019; Zang et al., 2020), in which individual words are replaced with semantically similar ones. Existing defense methods against these attacks mainly focus on adversarial training (Jia and Liang, 2017; Ebrahimi et al., 2018; Ribeiro et al., 2018; Ren et al., 2019; Jin et al., 2019) and hence typically require a priori attack knowledge and models to be retrained from scratch to increase their robustness. Recent work by Zhou et al. (2019) instead proposes DISP (learning to discriminate perturbations), a perturbation discrimination framework that exploits pre-trained contextualized word representations to detect and co"
2021.eacl-main.13,P19-1559,0,0.0318526,"Missing"
2021.eacl-main.137,2020.acl-main.463,0,0.149914,"achieve humanlevel understanding. For example, Jia and Liang (2017) use manually crafted adversarial examples to show that successful systems are easily distracted. Sugawara et al. (2020) show that a significant part of already solved questions is solvable even after shuffling the words in a sentence or dropping content words. These studies demonstrate that we cannot explain what type of understanding is required by the datasets and is actually acquired by models. Although benchmarking MRC is related to the intent behind questions and is critical to test hypotheses from a top-down viewpoint (Bender and Koller, 2020), its theoretical foundation is poorly investigated in the literature. In this position paper, we examine the prerequisites for benchmarking MRC based on the following two questions: (i) What does reading comprehension involve? (ii) How can we evaluate it? Our motivation is to provide a theoretical basis for the creation of MRC datasets. As Gilpin et al. (2018) indicate, interpreting the internals of a system is closely related to only the system’s architecture and is insufficient for explaining how the task is accomplished. This is because even if the internals of models can be interpreted, w"
2021.eacl-main.137,bentivogli-etal-2010-building,0,0.045129,"ortance of the requirement of various skills in MRC, which can serve as the units for the explanation of reading comprehension. Therefore, our motivation is to provide an overview of the skills as a hierarchical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency par"
2021.eacl-main.137,2020.emnlp-main.703,0,0.144309,"Missing"
2021.eacl-main.137,2020.emnlp-main.85,0,0.0380571,"Missing"
2021.eacl-main.137,W18-2607,0,0.0224664,"hical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu"
2021.eacl-main.137,D15-1075,0,0.0278114,"these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu et al., 2019), they are hardly required in NLU tasks including MRC. In the natural language inference task, McCoy et al. (2019) indicate that existing datasets (e.g., Bowman et al. (2015)) may fail to elucidate the syntactic understanding of given sentences. Although it is not obvious that these basic tasks should be included in MRC and it is not easy to circumscribe linguistic knowledge from concrete and abstract knowledge (Zaenen et al., 2005; Manning, 2006), we should always care about the capabilities of basic tasks (e.g., use of checklists 1595 Construct the global structure of propositions. Skills: creating a coherent representation and grounding it to other media. Situation model Textbase Surface structure Construct the local relations of propositions. Skills: recognizi"
2021.eacl-main.137,P17-1171,0,0.0184303,"a passage of text if, for any question regarding that text that can be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question. We overview various aspects of the task along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al."
2021.eacl-main.137,N19-1405,0,0.0377633,"Missing"
2021.eacl-main.137,W19-2008,0,0.0332615,"Missing"
2021.eacl-main.137,N18-2017,0,0.0647421,"Missing"
2021.eacl-main.137,N18-1175,0,0.0664065,"Missing"
2021.eacl-main.137,W18-2605,0,0.053332,"Missing"
2021.eacl-main.137,P16-1145,0,0.0714057,"Missing"
2021.eacl-main.137,D19-1243,0,0.146481,"Missing"
2021.eacl-main.137,D19-1259,0,0.040119,"Missing"
2021.eacl-main.137,P17-1147,0,0.0746664,"Missing"
2021.eacl-main.137,N18-1023,0,0.0136785,"E (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi et al., 2019)), and logical reasoning (ReClor (Yu et al., 2020)). 2.2 Benchmarking Issues In some datasets, the performance of machines has already reached human-level performance. However, Jia and Liang (2017) indicate that models can easily be fooled by manual injection of distracting sentences. Their study revealed that questions simply gathered by crowdsourcing without careful guideline"
2021.eacl-main.137,Q18-1023,0,0.0719978,"Missing"
2021.eacl-main.137,Q19-1026,0,0.039576,"y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et"
2021.eacl-main.137,P18-1077,0,0.0651417,"Missing"
2021.eacl-main.137,D17-1082,0,0.0330346,"on et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 201"
2021.eacl-main.137,D16-1062,0,0.0225099,"hat of other NLU tasks and measurements. 6. Consequential Value implications of score interpretation as a basis for the consequences of test use, especially regarding the sources of invalidity related to issues of bias, fairness, and distributive justice. Considering the model vulnerabilities to adversarial attacks and social biases of models and datasets to ensure the fairness of model outputs. Table 2: Aspects of the construct validity in psychometrics and corresponding features in MRC. may be useful if such ambiguity can be reflected in the evaluation (e.g., using the item response theory (Lalor et al., 2016)). As for model predictions, an issue may be the reproducibility of results (Bouthillier et al., 2019), which implies that the reimplementation of a system generates statistically similar predictions. For the reproducibility of models, Dror et al. (2018) emphasize statistical testing methods to evaluate models. For the reproducibility of findings, Bouthillier et al. (2019) stress it as the transferability of findings in a dataset/task to another dataset/task. In open-domain question answering, Lewis et al. (2021) point out that successful models might only memorize dataset-specific knowledge."
2021.eacl-main.137,2021.eacl-main.86,1,0.747166,"biguity can be reflected in the evaluation (e.g., using the item response theory (Lalor et al., 2016)). As for model predictions, an issue may be the reproducibility of results (Bouthillier et al., 2019), which implies that the reimplementation of a system generates statistically similar predictions. For the reproducibility of models, Dror et al. (2018) emphasize statistical testing methods to evaluate models. For the reproducibility of findings, Bouthillier et al. (2019) stress it as the transferability of findings in a dataset/task to another dataset/task. In open-domain question answering, Lewis et al. (2021) point out that successful models might only memorize dataset-specific knowledge. To facilitate this transferability, we need to have units of explanation that can be used in different datasets (Doshi-Velez and Kim, 2018). External Aspect This aspect refers to the relationship between a model’s scores on different tasks. Yogatama et al. (2019) point out that current models struggle to transfer their ability from a task originally trained on (e.g., MRC) to different unseen tasks (e.g., SRL). To develop a general NLU model, one would expect that a successful MRC model should show sufficient perf"
2021.eacl-main.137,D19-5808,0,0.0326017,"Missing"
2021.eacl-main.137,2020.acl-main.465,0,0.0196021,"ng that the questions comprehensively specify and evaluate textbase-level skills. • Evaluating the capability of the situation model in which propositions are coherently organized 4.2 and are grounded to non-textual information. Construct Validity in MRC Should MRC models mimic human text comprehension? In this paper, we do not argue that MRC models should mimic human text comprehension. However, when we design an NLU task and create datasets for testing human-like linguistic generalization, we can refer to the aforementioned features to frame the intended behavior to evaluate in the task. As Linzen (2020) discusses, the task design is orthogonal to how the intended behavior is realized at the implementation level (Marr, 1982). Table 2 also raises MRC features corresponding to the six aspects of construct validity. In what follows, we elaborate on these correspondings and discuss the missing aspects that are needed to achieve the construct validity of the current MRC. 4 Substantive Aspect This aspect appraises the evidence for the consistency of model behavior. We consider that this is the most important aspect for explaining reading comprehension, a process that subsumes various implicit and c"
2021.eacl-main.137,N19-1112,0,0.0176302,"18) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu et al., 2019), they are hardly required in NLU tasks including MRC. In the natural language inference task, McCoy et al. (2019) indicate that existing datasets (e.g., Bowman et al. (2015)) may fail to elucidate the syntactic understanding of given sentences. Although it is not obvious that these basic tasks should be included in MRC and it is not easy to circumscribe linguistic knowledge from concrete and abstract knowledge (Zaenen et al., 2005; Manning, 2006), we should always care about the capabilities of basic tasks (e.g., use of checklists 1595 Construct the global structure of propositions. Skills: c"
2021.eacl-main.137,P11-2057,0,0.0118386,"MRC, which can serve as the units for the explanation of reading comprehension. Therefore, our motivation is to provide an overview of the skills as a hierarchical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity re"
2021.eacl-main.137,D18-1260,0,0.040328,"Missing"
2021.eacl-main.137,P19-1416,0,0.0384813,"Missing"
2021.eacl-main.137,D18-1233,0,0.0538666,"Missing"
2021.eacl-main.137,P17-1075,1,0.843473,"eful for organizing such terms. We ground existing NLP technologies and tasks to different representation levels in the next section. 3.2 knowledge and reasoning for scientific questions in MRC (Clark et al., 2018). A limitation of both these studies is that the proposed sets of knowledge and inference are limited to the domain of elementary-level science. Although some existing datasets for MRC have their own classifications of skills, they are coarse and only cover a limited extent of typical NLP tasks (e.g., word matching and paraphrasing). In contrast, for a more generalizable definition, Sugawara et al. (2017) propose a set of 13 skills for MRC. Rogers et al. (2020) pursue this direction by proposing a set of questions with eight question types. In addition, Schlegel et al. (2020) propose an annotation schema to investigate requisite knowledge and reasoning. Dunietz et al. (2020) propose a template of understanding that consists of spatial, temporal, causal, and motivational questions to evaluate precise understanding of narratives with reference to human text comprehension. In what follows, we describe the three representation levels that basically follow the three representations of the CI model"
2021.eacl-main.137,Q19-1014,0,0.0123835,"urkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi et al., 2019)), and logical reasoning (ReClor (Yu et al., 2020)). 2.2 Benchmarking Issues In some datasets, the performance of machines has already reached human-level performance. However, Jia and Liang (2017) indicate that models can easily be fooled by manual injection of distracting sentences. Their study revealed that quest"
2021.eacl-main.137,N18-1140,0,0.0150031,"). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi"
2021.eacl-main.137,N19-1421,0,0.0396611,"Missing"
2021.eacl-main.137,W17-2623,0,0.029479,"es (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsour"
2021.eacl-main.137,D19-1221,0,0.0224497,"ent unseen tasks (e.g., SRL). To develop a general NLU model, one would expect that a successful MRC model should show sufficient performance on other NLU tasks as well. To this end, Wang et al. (2019) propose an evaluation framework with ten different NLU tasks in the same format. Consequential Aspect This aspect refers to the actual and potential consequences of test use. In MRC, this refers to the use of a successful model in practical situations other than tasks, where we need to ensure the robustness of a model to adversarial attacks and the accountability for unintended model behaviors. Wallace et al. (2019) highlight this aspect by showing that existing NLP models are vulnerable to adversarial examples, thereby generating egregious outputs. Summary: Design of Rubric Given the validity aspects, our suggestion is to design a rubric (scoring guide used in education) of what reading comprehension we expect is evaluated in a dataset; this helps to inspect detailed strengths and weaknesses of models that cannot be obtained only by simple accuracy. The rubric should not only cover various linguistic phenomena (the content aspect) but also involve different levels of intermediate evaluation in the readi"
2021.eacl-main.137,Q18-1021,1,0.930753,"ing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). L"
2021.eacl-main.137,D18-1257,0,0.0204048,"k along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are"
2021.eacl-main.137,D18-1259,0,0.341169,". Burges (2013) provides a general definition of MRC, i.e., a machine comprehends a passage of text if, for any question regarding that text that can be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question. We overview various aspects of the task along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) c"
2021.eacl-main.137,D18-1009,0,0.0563267,"aset biases embedded by annotators. If machine learning models exploit such biases for answering questions, we cannot evaluate the precise NLU of models. Following Geirhos et al. (2020), we define shortcut-proof questions as ones that prevent models from exploiting dataset biases and learning decision rules (shortcuts) that perform well only on i.i.d. test examples with regard to its training examples. Gardner et al. (2019) also point out the importance of mitigating shortcuts in MRC. In this section, we view two different approaches for this challenge. Removing Unintended Biases by Filtering Zellers et al. (2018) propose a model-based adversarial filtering method that iteratively trains an ensemble of stylistic classifiers and uses them to filter out the questions. Sakaguchi et al. (2020) also propose filtering methods based on both machines and humans to alleviate dataset-specific and word-association biases. However, a major issue is the inability to discern knowledge from bias in a closed domain. When the domain is equal to a dataset, patterns that are valid only in the domain are called dataset-specific biases (or annotation artifacts in the labeled data). When the domain covers larger corpora, th"
2021.eacl-main.86,D13-1160,0,0.487792,"o new heights (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017) and Open NaturalQuestions (Kwiatkowski et al., 2019; Lee et al., 2019). We identify three types of desired behaviour of a trained ODQA system, in increasing order of difficulty: 1) to recall the answer to a question that the model has seen at training time. 2) to answer novel questions at test time and choose an answer from the set of answers it has seen during training. 3) to answer novel questions which have answers which are not contained in the training data. It is not clear to what extent our current ODQA datasets measure each of these three behaviours. To"
2021.eacl-main.86,P16-1223,0,0.0833455,"Missing"
2021.eacl-main.86,P17-1147,0,0.289487,"Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017) and Open NaturalQuestions (Kwiatkowski et al., 2019; Lee et al., 2019). We identify three types of desired behaviour of a trained ODQA system, in increasing order of difficulty: 1) to recall the answer to a question that the model has seen at training time. 2) to answer novel questions at test time and choose an answer from the set of answers it has seen during training. 3) to answer novel questions which have answers which are not contained in the training data. It is not clear to what extent our current ODQA datasets measure each of these three behaviours. To address this, we stratify the t"
2021.eacl-main.86,D18-1546,0,0.0173117,"etrieval model outperforms the BART closed-book model on NaturalQuestions and TriviaQA. Further, the dense nearest neighbor model also outperforms the significantly more complex DPR open-book model on TriviaQA and WebQuestions on the question overlap subset. 5 Related Work Examining what behaviours are learnt by models has received attention in language understanding tasks, such as GLUE (Wang et al., 2018), which includes tools for probing for different reasoning types. There has also been critical and careful analysis of QA systems and datasets. Chen et al. (2016), Sugawara et al. (2020) and Kaushik and Lipton (2018) analyse the difficulty of various machine reading datasets, and Manjunatha et al. (2018) show that visual QA models memorize common question-answer relationships in training data. F´evry et al. (2020) analyse various closed-book models’ TriviaQA predictions. Kwiatkowski et al. (2019) note that the machine reading NaturalQuestions dataset has train-test overlap of Wikipedia titles, and provide baselines for “long-answer” QA. Verga et al. (2020) observe answer overlap effects in a related modality (knowledgebase QA), but no not consider question overlap. 6 Conclusion We performed an analysis of"
2021.eacl-main.86,Q19-1026,0,0.20212,"Missing"
2021.eacl-main.86,P19-1612,0,0.314496,"ation plays in these benchmarks. 1 Introduction Open-domain Question Answering (ODQA) is a task that examines the ability of models to produce answers to natural language factoid questions drawn from an open set of domains. ODQA has received significant attention for its potential practical applications, and more recently as a popular method to analyse how well NLP systems can capture and recall factual knowledge. This interest in ODQA as a challenging “knowledge-intensive” task has led to a flurry of recent works that have driven test-set performance on standard ODQA datasets to new heights (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018; Kaushik and Lipton, 2018; Sugawara et al., 2018, 2020), however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing. In this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), TriviaQA"
2021.eacl-main.86,D19-1284,0,0.047626,"by crowdworkers. The ODQA task consists of predicting the name of the Freebase entity. We use the standard train/test splits from Berant et al. (2013) and the development split from Karpukhin et al. (2020), which was randomly split from the train set. TriviaQA consists of 78,785 train, 8,837 development and 11,313 test instances obtained by scraping trivia websites. Answers are Wikipedia entities, and any alias for the answer entity is considered a correct answer. We use the ODQA splits, which correspond to the unfiltered-train and unfiltereddev reading comprehension splits (Lee et al., 2019; Min et al., 2019, 2020b; Karpukhin et al., 2020). Open-NaturalQuestions consists of search engine questions with crowdsourced answer spans in Wikipedia articles. The ODQA version consists of question-answer pairs from NaturalQuestions which have short answer spans less than 6 tokens in length. We use the standard open-domain splits in our experiments, consisting of 79,168 train, 8,757 development and 3,610 question answer pairs. For all three datasets, the canonical train, development and test splits were obtained by randomly splitting the question-answer pairs, and there are no exact duplicate questions in a"
2021.eacl-main.86,D16-1264,0,0.0543447,"est-Train Overlaps We explore two ways of examining the test sets based on overlaps between training and test data. Consider a question-answer pair (q, a) from the test set Dtest where the answer consists of at least one answer reference a = {s1 ..sn }. We can consider answer overlap where there exists at least one (q 0 , a0 ) ∈ Dtrain which shares at least one answer reference with (q, a). We can also consider question overlap, where there exists some (q 00 , a00 ) ∈ Dtrain where q 00 is a duplicate of q, such that q and q 00 are paraphrases and have the same answer. Answer Overlap Following Rajpurkar et al. (2016), we apply answer normalization (lowercasing, stripping punctuation, removing articles and normalizing whitespace) on answer references 1001 Open NaturalQuestions Overlapping Non-overlapping Overlapping Phil Simms Brian Johnson 8 the Indians the 1830s David Bowie Battle of camlann Heligoland Henry VII Niagra Falls Cloves Matt Monro 1,020 – 1,080 kg Hermann Ebbinghaus Matt Flinders TriviaQA Non-overlapping Death in the afternoon Clash of the Titans ice-cream sundae Camshaft Cumberland WebQuestions Overlapping Non-overlapping Harvard Alderaan India 2011 Zeus Queen Victoria Bras´ılia Paddington T"
2021.eacl-main.86,2020.emnlp-main.437,0,0.292708,"Missing"
2021.eacl-main.86,W18-5446,0,0.0752755,"Missing"
2021.emnlp-main.651,D18-1316,0,0.38799,"stion by incorporating human judgments into the adversarial example generation process. Specifically, we report on a series of data collection efforts in which we task humans to generate adversarial examples from existing movie reviews, while instructed to strictly adhere to a set of validity constraints. In 1 Introduction contrast to previous work (e.g., Bartolo et al., 2020; The vulnerability of natural language processing Potts et al., 2020), and in an attempt to replicate (NLP) models to adversarial examples has received a word-level attack’s mode of operation, human widespread attention (Alzantot et al., 2018; Iyyer participants were only able to substitute individual et al., 2018; Ren et al., 2019). Text processing words, and were not allowed to delete or insert new models have been shown to be susceptible to ad- words into the sequence. This represents a blackversarial input perturbations across tasks, includ- box attack scenario, since human participants do ing question answering and text classification (Jia not have access to information about the model’s and Liang, 2017; Jin et al., 2019). The concept parameters or gradients. Participants worked in of adversarial examples originated in comput"
2021.emnlp-main.651,2020.tacl-1.43,1,0.934887,"acks might represent an overestimation of their true capabilities. This, in turn, raises the question of whether valid word-level adversarial examples are routinely possible against trained NLP models. In this work, we aim to address this question by incorporating human judgments into the adversarial example generation process. Specifically, we report on a series of data collection efforts in which we task humans to generate adversarial examples from existing movie reviews, while instructed to strictly adhere to a set of validity constraints. In 1 Introduction contrast to previous work (e.g., Bartolo et al., 2020; The vulnerability of natural language processing Potts et al., 2020), and in an attempt to replicate (NLP) models to adversarial examples has received a word-level attack’s mode of operation, human widespread attention (Alzantot et al., 2018; Iyyer participants were only able to substitute individual et al., 2018; Ren et al., 2019). Text processing words, and were not allowed to delete or insert new models have been shown to be susceptible to ad- words into the sequence. This represents a blackversarial input perturbations across tasks, includ- box attack scenario, since human participants d"
2021.emnlp-main.651,K18-1011,0,0.0198459,"difications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scores (i.e., black-box) (Yuan et al., 2019). Word-level attacks have been explored for NLP tasks such as question answering (Blohm et al., 2018; Welbl et al., 2020), natural language inference (Jin et al., 2019), and text classification (Papernot et al., 2016; Jin et al., 2019). A range of methodologies has been explored for finding optimal synonym substitutions, including population-based gradient-free optimization via genetic algorithms (Alzantot et al., 2018), word saliency probability weighting (Ren et al., 2019), similarity and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg an"
2021.emnlp-main.651,D18-2029,0,0.0611264,"Missing"
2021.emnlp-main.651,N19-1246,0,0.0330189,"Missing"
2021.emnlp-main.651,C18-1055,0,0.0178013,"data might aid researchers in identifying human-inspired, more efficient ways of conducting adversarial word substitutions against neural text classification models. sequences with respect to their preservation of semantics and naturalness. This is followed by the analysis reported in Section 4, and a discussion of our findings and future work in Section 5. Finally, we conclude our paper in Section 6. 2 Related work Adversarial attacks for NLP. Adversarial attacks have been increasingly applied to NLP, with a diverse set of attack types being investigated, ranging from character-level edits (Ebrahimi et al., 2018b,a), word-level replacements (Alzantot et al., 2018), adding text to the input (Jia and Liang, 2017), paraphrase-level modifications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scor"
2021.emnlp-main.651,P18-2006,0,0.0195221,"data might aid researchers in identifying human-inspired, more efficient ways of conducting adversarial word substitutions against neural text classification models. sequences with respect to their preservation of semantics and naturalness. This is followed by the analysis reported in Section 4, and a discussion of our findings and future work in Section 5. Finally, we conclude our paper in Section 6. 2 Related work Adversarial attacks for NLP. Adversarial attacks have been increasingly applied to NLP, with a diverse set of attack types being investigated, ranging from character-level edits (Ebrahimi et al., 2018b,a), word-level replacements (Alzantot et al., 2018), adding text to the input (Jia and Liang, 2017), paraphrase-level modifications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scor"
2021.emnlp-main.651,2020.emnlp-main.498,0,0.0311192,"Missing"
2021.emnlp-main.651,N18-1170,0,0.023578,"on models. sequences with respect to their preservation of semantics and naturalness. This is followed by the analysis reported in Section 4, and a discussion of our findings and future work in Section 5. Finally, we conclude our paper in Section 6. 2 Related work Adversarial attacks for NLP. Adversarial attacks have been increasingly applied to NLP, with a diverse set of attack types being investigated, ranging from character-level edits (Ebrahimi et al., 2018b,a), word-level replacements (Alzantot et al., 2018), adding text to the input (Jia and Liang, 2017), paraphrase-level modifications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scores (i.e., black-box) (Yuan et al., 2019). Word-level attacks have been explored for NLP tasks such as question answering (Blohm et al., 2018; Welbl et al"
2021.emnlp-main.651,D17-1215,0,0.0318281,"al word substitutions against neural text classification models. sequences with respect to their preservation of semantics and naturalness. This is followed by the analysis reported in Section 4, and a discussion of our findings and future work in Section 5. Finally, we conclude our paper in Section 6. 2 Related work Adversarial attacks for NLP. Adversarial attacks have been increasingly applied to NLP, with a diverse set of attack types being investigated, ranging from character-level edits (Ebrahimi et al., 2018b,a), word-level replacements (Alzantot et al., 2018), adding text to the input (Jia and Liang, 2017), paraphrase-level modifications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scores (i.e., black-box) (Yuan et al., 2019). Word-level attacks have been explored for NLP tasks such as"
2021.emnlp-main.651,D19-1423,0,0.146124,"ethodologies has been explored for finding optimal synonym substitutions, including population-based gradient-free optimization via genetic algorithms (Alzantot et al., 2018), word saliency probability weighting (Ren et al., 2019), similarity and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg and Ramakrishnan, 2020). Word-level perturbations have also been used as part of data augmentation strategies to certifiably improve model robustness (Jia et al., 2019). Existing efforts to detect and defend against word-level adversarial examples resort to adversarial data augmentation (e.g., Ren et al., 2019; Jin et al., 2019) as well as rule-based (Mozes et al., 2021) and learning-based (Zhou et al., 2019) approaches to identify adversarially perturbed inputs. The remainder of this paper is structured as follows. Section 2 discusses previous work related to our research. Section 3 describes both phases of our data collection approach, i.e., the human generation of word-level adversarial examples and the subse- Evaluating word-level attacks. Of particular"
2021.emnlp-main.651,2020.emnlp-main.12,0,0.0420372,"maticality, the edit distance between original and perturbed text, and non-suspicion (Morris et al., 2020a). However, to the best of our knowledge, such evaluation efforts are limited to automated attacks and how humans perform at creating word-level adversarial examples across these dimensions remains unexplored. Human-in-the-loop adversarial examples. When the task is unconstrained, human crowdworkers have been shown to be capable of creating high quality adversarial examples for a variety of NLP tasks such as question answering (Wallace et al., 2019; Dua et al., 2019; Bartolo et al., 2020; Khashabi et al., 2020), natural language inference, (Nie et al., 2019), and sentiment analysis (Potts et al., 2020). We extend this line of work and investigate whether human capabilities for creating adversarial examples persist when the examples are constrained to arise from word-level perturbations, which have been shown to be highly effective (Alzantot et al., 2018; Jin et al., 2019). 3 Method Our data collection process has two stages: first, we ask human annotators to perform a word-level adversarial attack for given input sequences. To this end, we prepared an online interface that lets participants perturb"
2021.emnlp-main.651,N16-1082,0,0.0772196,"Missing"
2021.emnlp-main.651,2021.ccl-1.108,0,0.0407967,"Missing"
2021.emnlp-main.651,P11-1015,0,0.261107,"Missing"
2021.emnlp-main.651,2020.findings-emnlp.341,0,0.279779,"ural-reading, sentimentpreserving examples, though they do so by being much more computationally efficient. and in that domain defines perturbations of input data to neural networks that are barely perceptible to the human viewer. Due to the discrete nature of text, however, that definition is less applicable in an NLP context, since every perturbation to the input tokens is unavoidably perceptible. Consequently, recent work aims to perturb textual inputs while preserving the sequence’s naturalness and semantics (i.e., rendering changes imperceptible on these dimensions). However, as shown by Morris et al. (2020a), achieving these desiderata is challenging because even small perturbations can render a text meaningless, grammatically incorrect or unnatural, and furthermore several proposed adversarial attacks fail routinely to achieve them. If the algorithms are modified to ensure that they do achieve the desiderata then their rate of generating successful examples greatly diminishes, suggesting that the reported success rates of recently proposed attacks might represent an overestimation of their true capabilities. This, in turn, raises the question of whether valid word-level adversarial examples ar"
2021.emnlp-main.651,2020.emnlp-demos.16,0,0.0295314,"Missing"
2021.emnlp-main.651,2021.eacl-main.13,1,0.758013,"eighting (Ren et al., 2019), similarity and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg and Ramakrishnan, 2020). Word-level perturbations have also been used as part of data augmentation strategies to certifiably improve model robustness (Jia et al., 2019). Existing efforts to detect and defend against word-level adversarial examples resort to adversarial data augmentation (e.g., Ren et al., 2019; Jin et al., 2019) as well as rule-based (Mozes et al., 2021) and learning-based (Zhou et al., 2019) approaches to identify adversarially perturbed inputs. The remainder of this paper is structured as follows. Section 2 discusses previous work related to our research. Section 3 describes both phases of our data collection approach, i.e., the human generation of word-level adversarial examples and the subse- Evaluating word-level attacks. Of particular quent validation of human- and machine-generated importance for this paper is how adversarial at8259 tacks can be evaluated against various dimensions. Adversarial attack performance has been shown to vary"
2021.emnlp-main.651,P19-1103,0,0.0460123,"ally, we report on a series of data collection efforts in which we task humans to generate adversarial examples from existing movie reviews, while instructed to strictly adhere to a set of validity constraints. In 1 Introduction contrast to previous work (e.g., Bartolo et al., 2020; The vulnerability of natural language processing Potts et al., 2020), and in an attempt to replicate (NLP) models to adversarial examples has received a word-level attack’s mode of operation, human widespread attention (Alzantot et al., 2018; Iyyer participants were only able to substitute individual et al., 2018; Ren et al., 2019). Text processing words, and were not allowed to delete or insert new models have been shown to be susceptible to ad- words into the sequence. This represents a blackversarial input perturbations across tasks, includ- box attack scenario, since human participants do ing question answering and text classification (Jia not have access to information about the model’s and Liang, 2017; Jin et al., 2019). The concept parameters or gradients. Participants worked in of adversarial examples originated in computer vi- a web interface (Figure 1) that allowed them to sion (Szegedy et al., 2013; Goodfello"
2021.emnlp-main.651,2020.findings-emnlp.103,1,0.733128,"t al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scores (i.e., black-box) (Yuan et al., 2019). Word-level attacks have been explored for NLP tasks such as question answering (Blohm et al., 2018; Welbl et al., 2020), natural language inference (Jin et al., 2019), and text classification (Papernot et al., 2016; Jin et al., 2019). A range of methodologies has been explored for finding optimal synonym substitutions, including population-based gradient-free optimization via genetic algorithms (Alzantot et al., 2018), word saliency probability weighting (Ren et al., 2019), similarity and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg and Ramakrishnan, 2020)"
2021.emnlp-main.651,2020.acl-main.540,0,0.0330171,"tacks have been explored for NLP tasks such as question answering (Blohm et al., 2018; Welbl et al., 2020), natural language inference (Jin et al., 2019), and text classification (Papernot et al., 2016; Jin et al., 2019). A range of methodologies has been explored for finding optimal synonym substitutions, including population-based gradient-free optimization via genetic algorithms (Alzantot et al., 2018), word saliency probability weighting (Ren et al., 2019), similarity and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg and Ramakrishnan, 2020). Word-level perturbations have also been used as part of data augmentation strategies to certifiably improve model robustness (Jia et al., 2019). Existing efforts to detect and defend against word-level adversarial examples resort to adversarial data augmentation (e.g., Ren et al., 2019; Jin et al., 2019) as well as rule-based (Mozes et al., 2021) and learning-based (Zhou et al., 2019) approaches to identify adversarially perturbed inputs. The remainder of this paper is structured as follows. Section 2 dis"
2021.emnlp-main.651,D19-1496,0,0.0130245,"and consistency filtering (Jin et al., 2019), sememe-based word substitution and particle swarm optimization-based search (Zang et al., 2020), and contextual perturbations from masked language models (Garg and Ramakrishnan, 2020). Word-level perturbations have also been used as part of data augmentation strategies to certifiably improve model robustness (Jia et al., 2019). Existing efforts to detect and defend against word-level adversarial examples resort to adversarial data augmentation (e.g., Ren et al., 2019; Jin et al., 2019) as well as rule-based (Mozes et al., 2021) and learning-based (Zhou et al., 2019) approaches to identify adversarially perturbed inputs. The remainder of this paper is structured as follows. Section 2 discusses previous work related to our research. Section 3 describes both phases of our data collection approach, i.e., the human generation of word-level adversarial examples and the subse- Evaluating word-level attacks. Of particular quent validation of human- and machine-generated importance for this paper is how adversarial at8259 tacks can be evaluated against various dimensions. Adversarial attack performance has been shown to vary across evaluation dimensions including"
2021.emnlp-main.651,P18-1079,0,0.0195463,"with respect to their preservation of semantics and naturalness. This is followed by the analysis reported in Section 4, and a discussion of our findings and future work in Section 5. Finally, we conclude our paper in Section 6. 2 Related work Adversarial attacks for NLP. Adversarial attacks have been increasingly applied to NLP, with a diverse set of attack types being investigated, ranging from character-level edits (Ebrahimi et al., 2018b,a), word-level replacements (Alzantot et al., 2018), adding text to the input (Jia and Liang, 2017), paraphrase-level modifications (Iyyer et al., 2018; Ribeiro et al., 2018), to creating adversarial examples from scratch (Bartolo et al., 2020; Nie et al., 2019). This work focuses on word-level attacks. Word-level attacks. Our work builds on existing efforts on word-level adversarial attacks. Attacks of this type can be further distinguished by whether the adversary has access to the model parameters (i.e., white-box) or is restricted to accessing only the predicted labels or confidence scores (i.e., black-box) (Yuan et al., 2019). Word-level attacks have been explored for NLP tasks such as question answering (Blohm et al., 2018; Welbl et al., 2020), natural langu"
2021.emnlp-main.651,Q19-1029,0,0.0156705,"20), as well as linguistic constraints such as semantics, grammaticality, the edit distance between original and perturbed text, and non-suspicion (Morris et al., 2020a). However, to the best of our knowledge, such evaluation efforts are limited to automated attacks and how humans perform at creating word-level adversarial examples across these dimensions remains unexplored. Human-in-the-loop adversarial examples. When the task is unconstrained, human crowdworkers have been shown to be capable of creating high quality adversarial examples for a variety of NLP tasks such as question answering (Wallace et al., 2019; Dua et al., 2019; Bartolo et al., 2020; Khashabi et al., 2020), natural language inference, (Nie et al., 2019), and sentiment analysis (Potts et al., 2020). We extend this line of work and investigate whether human capabilities for creating adversarial examples persist when the examples are constrained to arise from word-level perturbations, which have been shown to be highly effective (Alzantot et al., 2018; Jin et al., 2019). 3 Method Our data collection process has two stages: first, we ask human annotators to perform a word-level adversarial attack for given input sequences. To this end,"
2021.emnlp-main.696,P19-1620,0,0.278606,"ction (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that attempt to maximise diversity; we find that"
2021.emnlp-main.696,2020.tacl-1.43,1,0.809231,"century (iv) Q: When did Old English begin to be used? A: 5th century Figure 1: The Synthetic Adversarial Data Generation Pipeline showing: (i) passage selection from Wikipedia; (ii) answer candidate selection and filtering by model confidence (an example retained answer shown in green, and a dropped answer candidate in red); (iii) question generation using BARTLarge ; and (iv) answer re-labelling using self-training. The generated synthetic data is then used as part of the training data for a downstream Reading Comprehension model. A recently proposed alternative is dynamic data collection (Bartolo et al., 2020; Nie et al., 2020), Large-scale labelled datasets like SQuAD (Ra- where data is collected with both humans and modjpurkar et al., 2016) and SNLI (Bowman et al., els in the annotation loop. Usually, these humans 2015) have been driving forces in natural language are instructed to ask adversarial questions that fool processing research. Over the past few years, how- existing models. Dynamic adversarial data colever, such “statically collected” datasets have been lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art mode"
2021.emnlp-main.696,D15-1075,0,0.0867811,"Missing"
2021.emnlp-main.696,E06-1032,0,0.0453541,"g. 2.3 Self-training In self-training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate se"
2021.emnlp-main.696,P18-1177,0,0.0471497,"Missing"
2021.emnlp-main.696,P17-1123,0,0.0217455,"llace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples usi"
2021.emnlp-main.696,D19-5801,1,0.897458,"Missing"
2021.emnlp-main.696,D19-1107,0,0.0399652,"Missing"
2021.emnlp-main.696,N18-2017,0,0.045407,"Missing"
2021.emnlp-main.696,J15-4004,0,0.046805,"training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate selection, question g"
2021.emnlp-main.696,D17-1215,1,0.769976,"Missing"
2021.emnlp-main.696,2020.acl-main.441,1,0.791342,"Missing"
2021.emnlp-main.696,2021.acl-long.186,1,0.733505,"part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lew"
2021.emnlp-main.696,2020.acl-main.703,0,0.172923,"Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer only, as we find that this setting provides better control of answer diversity. We use the same range of k ∈ {1, 5, 10, 15, 20} for both settings. 3.1.1 Passage Selection The text passages we use are sourced from SQuAD (further details can be found in Appendix A). We Self-Attention Labelling (SAL) We propose a also experiment with using passages external to multi-label classification head to jointly model canSQuAD, which also sourced from Wikipedia. To didate start and end tokens, and provide a bin"
2021.emnlp-main.696,P19-1484,1,0.842616,"ends, with improved performance. Since SQuAD and the AdversarialQA datasets use the same passages partitioned into the same data splits, we align the annotated answers to create representative answer selection training, validation and test sets. Dataset statistics (see Appendix C), highlight the high percentage of overlapping answers suggesting that existing answer tagging methods (Zhou et al., 2017; Zhao et al., 2018) might struggle, and models should ideally be capable of handling span overlap. Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer on"
2021.emnlp-main.696,2021.eacl-main.86,1,0.886215,"een lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art models, but it can also lar, they often exhibit inadvertent spurious statisti- create higher-quality training data (Bartolo et al., cal patterns that models learn to exploit, leading to 2020; Nie et al., 2020) due to the added incentive poor model robustness and generalisation (Jia and for crowdworkers to provide challenging examples. Liang, 2017; Gururangan et al., 2018; Geva et al., It can also reduce the prevalence of dataset biases 2019; McCoy et al., 2019; Lewis et al., 2021a). and annotator artefacts over time (Bartolo et al., ∗ 2020; Nie et al., 2020), since such phenomena can Most of this work was carried out while MB was at Facebook AI Research. be subverted by model-fooling examples collected 8830 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8830–8848 c November 7–11, 2021. 2021 Association for Computational Linguistics in subsequent rounds. However, dynamic data collection can be more expensive than its static predecessor as creating examples that elicit a certain model response (i.e., fooling"
2021.emnlp-main.696,D16-1264,0,0.115275,"Missing"
2021.emnlp-main.696,2020.acl-main.442,0,0.0352994,"Missing"
2021.emnlp-main.696,2021.acl-long.132,1,0.719606,"he Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri"
2021.emnlp-main.696,Q19-1029,0,0.0183855,"t baseline. The collected dataset will form part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du"
2021.emnlp-main.696,2020.findings-emnlp.90,0,0.0320765,"Missing"
2021.emnlp-main.696,D18-1424,0,0.110175,"s (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that"
2021.findings-acl.454,D18-1444,0,0.0927742,"ory into a string, ending with a special token, “TL;DR”. Take Figure 1 as an example, the summary sketch is “1 what 2 abstain ’s just one of ... square garden 8 why 9 abstain TL;DR”. We train our model first to generate this summary sketch and then generate the final summary in an autoregressive way. We use TL;DR token to distinguish sketch and final summary during inference time. 2.3 Controllability Due to the success of controllable language modeling (Keskar et al., 2019), the ability to control text summarization in the News domain has gradually been attracting attention (Fan et al., 2018; Liu et al., 2018) The high-level intuition for our solution is that if we can control a generative model only to generate one sentence as output for a partiallyhighlighted input, we can control the number of output sentences by choosing how to highlight the input. We highlight each dialogue split using the special token &lt; hl >. For example, in Figure 1, we generate the first summary sentence for the first segment from turn one to four, and the second and third from turn five to seven and turn eight to nine, respectively (separated by the dashed lines). This way, we can not only gain the summary controllability"
2021.findings-acl.454,P14-1115,0,0.031826,"ffectiveness with reinforcement learning. Recently, Liu and Lapata, 2019 apply BERT on text summarization and propose a general framework for both extractive and abstractive models. Zhang et al., 2019c pre-train hierarchical document encoder for extractive summarization. Lewis et al., 2019 introduces BART, a denoising autoencoder for pretraining sequence-tosequence models. BART significantly outperforms the best previous work in terms of ROUGE metrics. Dialogue Summarization Regarding to the datasets in dialogue summarization, initial abstractive dialogue summarization work (Oya et al., 2014; Mehdad et al., 2014; Banerjee et al., 2015) are conducted on the AMI meeting corpus (McCowan et al., 2005), with only 141 summaries. Goo and Chen, 2018 propose to use the topic descriptions (high-level goals of meetings) in AMI as reference summaries and use dialogue acts as training signals. Pan et al., 2018 build the Dial2Desc dataset by reversing a visual dialogue task, aligning image dialogues with the image caption as a summary. Liu et al., 2019 collect their dataset from the logs in the DiDi customer service center. It is restricted to taskoriented scenario, where one speaker is the user and the other is t"
2021.findings-acl.454,K16-1028,0,0.0543081,"to produce an abridged version of the input text by distilling its most critical information. In particular, abstractive – as opposed to extractive – summarization requires generative models with a high level of semantic understanding, as the output words do not necessarily appear in the source text. While it is more challenging, it gives more flexibility to a summary compared to extractive summarization models (Zhang et al., 2018). Significant research efforts have been focused on summarization of singlespeaker documents such as text documents (Liao et al., 2018), News (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) or scientific ∗ Equal contribution. Work mainly done when Linqing Liu was an intern at Salesforce Research. publications (Qazvinian and Radev, 2008; Nikolov et al., 2018). However, dialogue summarization has not received much attention despite the prevalence of dialogues (text messages, email, social media, etc.) and the vast application potential of dialogue summarization systems. Since dialogue language is inherently different from written text, it poses a unique set of challenges (Zechner, 2001): 1) Distributed information across multiple speakers. The most important inf"
2021.findings-acl.454,J81-4005,0,0.601068,"Missing"
2021.findings-acl.454,2020.emnlp-main.66,1,0.806206,"language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attr"
2021.findings-acl.454,2020.findings-emnlp.400,1,0.737408,"language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attr"
2021.findings-acl.454,P19-1078,1,0.850855,"ell-deserved break” and the first summary sentence “Suzanne is at work and is having a break now.” The entire model is trained using cross-entropy loss for the generated tokens. During inference, we first use the trained binary classifier to predict cutting points. Then, we use the predicted segmentation to add highlighting tokens into a dialogue. Finally, after generating multiple summary sentences separately, we concatenate them to be the final summary. 5111 Longest-3* Pointer Generator (See et al., 2017)* Fast Abs RL (Chen and Bansal, 2018)* Transformer (Vaswani et al., 2017)* DynamicConv (Wu et al., 2019b)* DynamicConv + GPT-2 emb* D-HGN (Feng et al., 2020) TGDGA (Zhao et al., 2020) DialoGPT (Zhang et al., 2019d) UniLM (Dong et al., 2019) PEGASUS (Zhang et al., 2019a) BART-xsum (Lewis et al., 2019) BART-xsum + Sketch (Ours) BART-xsum + Ctrl (Ours) CODS (Ours) ROUGE-1 32.46 37.27 41.03 42.37 41.07 45.41 42.03 43.11 39.77 47.85 50.50 51.74 51.79 52.84 52.65 ROUGE-2 10.27 14.42 16.93 18.44 17.11 20.65 18.07 19.15 16.58 24.23 27.23 26.46 26.85 27.35 27.84 ROUGE-L 29.92 34.36 39.05 39.27 37.27 41.45 39.56 40.49 38.42 46.67 49.32 48.72 49.15 50.29 50.79 Table 1: Dialogue summarization ROUGE evaluat"
2021.findings-acl.454,2020.emnlp-main.409,1,0.720469,"ve been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attribute for the conversa"
2021.findings-acl.454,D18-1088,0,0.0137233,"UGE-L score. In addition, we conduct a case study and show competitive human evaluation results and controllability to humanannotated summaries. 1 Introduction Text summarization aims to produce an abridged version of the input text by distilling its most critical information. In particular, abstractive – as opposed to extractive – summarization requires generative models with a high level of semantic understanding, as the output words do not necessarily appear in the source text. While it is more challenging, it gives more flexibility to a summary compared to extractive summarization models (Zhang et al., 2018). Significant research efforts have been focused on summarization of singlespeaker documents such as text documents (Liao et al., 2018), News (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) or scientific ∗ Equal contribution. Work mainly done when Linqing Liu was an intern at Salesforce Research. publications (Qazvinian and Radev, 2008; Nikolov et al., 2018). However, dialogue summarization has not received much attention despite the prevalence of dialogues (text messages, email, social media, etc.) and the vast application potential of dialogue summarization systems. Since di"
2021.findings-acl.454,P19-1499,0,0.171025,"sentence dialogue summary Y = {Y1 , . . . , YM } that is suppose to be briefer than the overall dialogue history. 2.1 Generative Pre-trained Language Models As a first, our model needs transform a conversational history input into a dialogue summary. Re5109 cently, self-supervised pretrained language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Re"
2021.findings-acl.454,2020.coling-main.39,0,0.0671581,"Missing"
2021.findings-acl.454,D19-1053,0,0.038759,"Missing"
2021.naacl-main.324,2020.emnlp-main.393,0,0.0658177,"hoice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likeli"
2021.naacl-main.324,2020.tacl-1.3,0,0.0240418,"med “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,W17-5401,0,0.025796,", 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah a"
2021.naacl-main.324,2020.acl-main.465,0,0.0806274,"solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al.,"
2021.naacl-main.324,2020.insights-1.13,0,0.0263233,"er, e.g., the multiple iterations of SemEval or WMT datasets over the years, we’ve already been handling this quite well—we accept that a model’s BLEU score on WMT16 is not comparable to WMT14. That is, it is perfectly natural for benchmark datasets to evolve as the community makes progress. The only thing Dynabench does differently is that it anticipates dataset saturation and embraces the loop so that we can make faster and more sustained progress. ever, it has also been found that model-in-the-loop counterfactually-augmented training data does not necessarily lead to better generalization (Huang et al., 2020). Given the distributional shift induced by adversarial settings, it would probably be wisest to combine adversarially collected data with nonadversarial data during training (ANLI takes this approach), and to also test models in both scenarios. To get the most useful training and testing data, it seems the focus should be on collecting adversarial data with the best available model(s), preferably with a wide range of expertise, as that will likely be beneficial to future models also. That said, we expect this to be both task and model dependent. Much more research is required, and we encourag"
2021.naacl-main.324,2020.acl-main.768,1,0.846016,"e the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle t"
2021.naacl-main.324,N19-1225,0,0.0738848,"bstantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and that encour- introduce Dynabench, an open-source, web-based age apples-to-apples model comparisons. Bench- research platform for dynamic data collection and marks provide a north star goal for researchers, and model benchmarking. The guiding hypothesis be4110 Proceedings of the 2021 Con"
2021.naacl-main.324,D17-1215,1,0.795486,"ard et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et a"
2021.naacl-main.324,2021.ccl-1.108,0,0.0774911,"Missing"
2021.naacl-main.324,D15-1166,0,0.00793057,"rk tasks, that milestone is now rou- cording to the narrow criteria used to define human performance) nonetheless fail on simple chaltinely reached within just a few years for newer lenge examples and falter in real-world scenarios. datasets (see Figure 1). As with the rest of AI, NLP A substantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and"
2021.naacl-main.324,2020.emnlp-main.154,0,0.0295127,"uperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring"
2021.naacl-main.324,J93-2004,0,0.0749322,"humans? This reveals the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang"
2021.naacl-main.324,C10-1091,0,0.0606954,"Missing"
2021.naacl-main.324,N19-1063,0,0.0241688,"d its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not s"
2021.naacl-main.324,P19-1334,0,0.0218097,"ang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al.,"
2021.naacl-main.324,K18-1007,1,0.843089,"for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing tar"
2021.naacl-main.324,W10-0719,1,0.726488,"collaborative effort, the platform is meant to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community. In the current iteration, the platform is set up for dynamic adversarial data collection, where humans can attempt to find modelfooling examples. This design choice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics."
2021.naacl-main.324,C18-1198,0,0.0239103,"ard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Mo"
2021.naacl-main.324,2020.acl-main.441,1,0.878791,"our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of resource collection and architectural improvements. Similar to Dynabench, recent work seeks to embrace this phenomenon, addressing many of the previously mentioned issues through an iterative human-and-model-in-the-loop annotation process (Yang et al., 2017; Dinan et al., 2019; Chen et al., 2019; Bartolo et al., 2020; Nie et al., 2020), to find “unknown unknowns” (Attenberg et al., 2015) or in a never-ending or life-long learning setting (Silver et al., 2013; Mitchell et al., 2018). The Adversarial NLI (ANLI) dataset (Nie et al., 2020), for example, was collected with an adversarial setting over multiple rounds to yield “a ‘moving post’ dynamic target for NLU systems, rather than a static benchmark that will eventually saturate”. In its few-shot learning mode, GPT-3 barely shows “signs of life” (Brown et al., 2020) (i.e., it is barely above random) on ANLI, which is evidence that we are still far away from human performance"
2021.naacl-main.324,S18-2023,0,0.0556136,"Missing"
2021.naacl-main.324,W12-4501,0,0.0439542,"the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However"
2021.naacl-main.324,P18-2124,1,0.869954,"Missing"
2021.naacl-main.324,D16-1264,0,0.230428,"n use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its lead"
2021.naacl-main.324,2020.acl-main.442,0,0.230761,"performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuri"
2021.naacl-main.324,K19-1019,0,0.019609,"al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models"
2021.naacl-main.324,N18-2002,0,0.0355102,"Missing"
2021.naacl-main.324,2020.emnlp-main.661,1,0.822242,"Missing"
2021.naacl-main.324,P19-1004,0,0.0188742,"put character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally"
2021.naacl-main.324,2020.acl-main.479,0,0.0170779,"ead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Etting"
2021.naacl-main.324,2020.acl-main.222,0,0.0281863,"19) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likelihood ilarly, the paradigm is perfectly compatible with training on i.i.d. train/test splits and human lan- collaborative settings that utilize human feedback, guage (Linzen, 2020; Stiennon et al., 2020). or even negotiation. The crucial aspect of this proposal is the fact that models and humans interact We think there is widespread agreement that something has to change about our standard eval- live “in the loop” for evaluation and data collection. uation paradigm and that we nee"
2021.naacl-main.324,D08-1027,0,0.352191,"Missing"
2021.naacl-main.324,D13-1170,1,0.0131607,"t data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather tha"
2021.naacl-main.324,2020.emnlp-main.746,0,0.0673253,"Missing"
2021.naacl-main.324,N18-1074,0,0.0416359,"Missing"
2021.naacl-main.324,L18-1239,0,0.0544033,"Missing"
2021.naacl-main.324,W19-3509,1,0.836306,"Missing"
2021.naacl-main.324,D19-1221,0,0.0180989,"2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of r"
2021.naacl-main.324,W18-5446,1,0.794698,"Missing"
2021.naacl-main.324,D19-1286,0,0.0188342,"ive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressiv"
2021.naacl-main.324,2020.tacl-1.25,0,0.0303378,"enging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzi"
2021.naacl-main.324,W17-3012,1,0.867269,"Missing"
2021.naacl-main.324,D18-1501,0,0.0349494,"Missing"
2021.naacl-main.324,N18-1101,1,0.774917,"he background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive bod"
2021.naacl-main.324,D18-1259,0,0.0459996,"Missing"
2021.naacl-main.324,2020.emnlp-main.397,0,0.0238252,"was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,2020.emnlp-main.659,1,0.819658,"Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by sim"
D14-1163,D10-1115,0,0.605812,"phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS"
D14-1163,D12-1050,0,0.219803,"nsional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011"
D14-1163,D08-1094,0,0.231181,"Missing"
D14-1163,W13-3203,0,0.0157278,"s jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operato"
D14-1163,D11-1129,0,0.0465968,"Missing"
D14-1163,D13-1137,1,0.512175,"The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using"
D14-1163,P13-1088,0,0.278818,"ith these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structur"
D14-1163,P12-1092,0,0.347535,"ationship to previous work. If we omit the the category-specific weight vectors hci in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-LBLM, we use a scoring function to evaluate how well the target word wt fits the given context: s(wt , p(wt )) = v˜(wt )T p(wt ), (4) where v˜(wt ) ∈ Rd×1 is the scoring weight v"
D14-1163,D13-1166,0,0.282877,"Missing"
D14-1163,W13-3513,0,0.0612916,"nexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5 1550 https://code.google.com/p/word2vec/ Model PAS-CLBLM (Addl ) PAS-CLBLM (Addnl ) PAS-CLBLM (Waddl ) PAS-CLBLM (Waddnl ) PAS-LBLM word2vec Grefenstette and Sadrzadeh (2011) Tsubaki et al. (2013) Van de Cruys et al. (2013) Human agreement Corpus BNC BNC BNC ukWaC ukWaC Averaged SVO-SVO SVO-V 0.29 0.34 0.27 0.32 0.25 0.26 0.42 0.50 0.21 0.06 0.12 0.32 n/a n/a n/a 0.47 n/a n/a 0.75 Non-averaged SVO-SVO SVO-V 0.24 0.28 0.24 0.28 0.21 0.23 0.34 0.41 0.18 0.08 0.12 0.28 0.21 n/a n/a n/a 0.32 0.37 0.62 Table 4: Spearman’s rank correlation scores ρ for the SVO task. Ave"
D14-1163,P14-2050,0,0.418227,"o, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs). Our model jointly learns word representations and their composition functions using bagof-words and dependency-based contexts. Unlike previous word-sequencebased models, our PAS-base"
D14-1163,P08-1028,0,0.788883,"ver, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) i"
D14-1163,J08-1002,0,0.0501574,"sis of our model. We then introduce a Log-Bilinear Language Model using Predicate-Argument Structures (PAS-LBLM) to learn word representations using both bag-ofwords and dependency-based contexts. Finally, we propose integrating compositions of words into the model. Figure 1 (b) shows the overview of the proposed model. 3.1 Predicate-Argument Structures Due to advances in deep parsing technologies, syntactic parsers that can produce predicateargument structures are becoming accurate and fast enough to be used for practical applications. In this work, we use the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008) to obtain the predicate-argument structures of individual sentences. In its grammar, each word in a sentence is treated as a predicate of a certain category with zero or more arguments. Table 1 shows some exCategory adj arg1 noun arg1 verb arg12 prep arg12 predicate heavy car cause at arg1 rain accident rain eat arg2 accident restaurant Table 1: Examples of predicates of different categories from the grammar of the Enju parser. arg1 and arg2 denote the first and second arguments. amples of predicates of different categories.1 For example, a predicate of the category verb arg12 expresses a ver"
D14-1163,P14-1009,0,0.0244756,"o composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al."
D14-1163,D12-1110,0,0.862473,"presentations and Composition Functions Using Predicate-Argument Structures Kazuma Hashimoto† , Pontus Stenetorp† , Makoto Miwa‡ , and Yoshimasa Tsuruoka† †The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs)."
D14-1163,P13-1045,0,0.255766,"ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in"
D14-1163,D13-1170,0,0.21026,"ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in"
D14-1163,P10-1097,0,0.0689392,"Missing"
D14-1163,D13-1014,0,0.122155,"azuma Hashimoto† , Pontus Stenetorp† , Makoto Miwa‡ , and Yoshimasa Tsuruoka† †The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs). Our model jointly learns word representations and their composition functions"
D14-1163,P10-1040,0,0.0613101,"Missing"
D14-1163,N13-1134,0,0.281979,"Missing"
D14-1163,Q14-1017,0,\N,Missing
D16-1167,D14-1179,0,0.0214089,"Missing"
D16-1167,P16-1004,0,0.0165459,"ble y ∈ Y representing the output. The 1609 goal is to find this predictive distribution by learning it from examples D := {(xi , yi )}ni=1 . Building on the current success in the application of deep learning to NLP, we assume that there exists a good model family {fθ , θ ∈ Θ} to predict y given x, where θ is an element of the parameter space Θ. For example, the stacked LSTM encoder-decoder is a general purpose model that has helped to improve results on relatively complex tasks, such as machine translation (Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2014), semantic parsing (Dong and Lapata, 2016) and textual entailment (Rockt¨aschel et al., 2016). For many applications, the amount of training data is too small or too costly to acquire. We hence look for alternative ways to regularize the model so that we can achieve good performance using few data points. Let pθ (y|x) be the target prediction model. Given the training dataset D, the penalized maximum likelihood estimator is obtained by minθ∈Θ L(θ) where: L(θ) := `(θ) + λΩ(θ) . (1) P where `(θ) := − n1 ni=1 log pθ (yi |xi ) = EPˆ [log pθ (y|x)] is the negative log-likelihood. Here, Ω(θ) is a regularizer that prevents over-fitting, λ ∈"
D16-1167,W09-0613,0,0.0414668,"r that can be set by cross-validation, and Pˆ is the empirical distribution. Instead of using a standard regularizer Ω – such as the squared norm or the Lasso penalty which are domain-agnostic, – in this paper we propose to use a generative model to regularize the estimator. Domain knowledge A natural way to inject background knowledge is to define a generative model that simulates the way the data is generated. In text understanding applications, such generative models are common and include probabilistic contextfree grammars (PCFG) and natural language generation frameworks (e.g. SimpleNLG (Gatt and Reiter, 2009)). Let Pγ (x, y) be such a generative model parametrized by a continuous parameter vector γ ∈ Γ, such as the concatenation of all the parameters of the production rules in a PCFG. One important difference between the discriminative and the generative probability distributions is that the inference problem of y given x might be intractable1 for the generative model, even if the joint model can be computed efficiently. In this work, we use the following regularizer:    Pγ (y|x) Ω(θ) := min EPγ (x,y) log .(2) γ∈Γ pθ (y|x) This regularizer makes intuitive sense as it corresponds to the smalles"
D16-1167,D14-1058,0,0.0324269,"n are Recurrent Neural Nets (RNN) with nonlinear transitions between states. Treated as a translation problem, math word problem solving should be simpler than developing a machine translation model between two human languages, as the output vocabulary (the math symbols) is significantly smaller than any human vocabulary. However, machine translation can be learned on millions of pairs of already translated sentences, and such massive training datasets dwarf all previously introduced math exam datasets. We used standard benchmark data from the literature. The first one, AI2, was introduced by Hosseini et al. (2014) and covers addition and subtraction of one or two variables or two additions scraped from two web pages. The second (IL), introduced by Roy et al. (2015), contains single operator questions but covers addition, subtraction, multiplication, and division, and was also obtained from two, although different from AI2, web pages. The last data set (CC) was introduced by Roy and Roth (2015) to cover combinations of different operators and was obtained from a fifth web page. An overview of the equation patterns in the data is shown in Table 1. It should be noted that there are sometimes numbers menti"
D16-1167,D15-1202,0,0.0455588,"rom her older sister. If she only ate 9 pieces a day, how long would the candy last her? The answer is given by the following equation: X = (66 + 15)/9 . Note that similarly to real world school exams, giving the final answer of (9 in this case) is not considered enough for the response to be correct. The only publicly available word problem datasets we are aware of contain between 400 and 600 problems (see Table 2), which is not enough to properly train sufficiently rich models that capture the link between the words and the quantities involved in the problem. 5 From the Common Core dataset (Roy and Roth, 2015) Figure 1: Test loss vs. fraction of real data used in G ENE R E on the text-to-equation experiment. Sequence-to-sequence learning is the task of predicting an output sequence of symbols based on a sequence of input symbols. It is tempting to cast the problem of answering math exams as a sequenceto-sequence problem: given the sequence of words from the problem description, we can predict the sequence of symbols for the equation as output. Currently, the most successful models for sequence prediction are Recurrent Neural Nets (RNN) with nonlinear transitions between states. Treated as a transla"
D16-1167,Q15-1001,0,0.0173547,"developing a machine translation model between two human languages, as the output vocabulary (the math symbols) is significantly smaller than any human vocabulary. However, machine translation can be learned on millions of pairs of already translated sentences, and such massive training datasets dwarf all previously introduced math exam datasets. We used standard benchmark data from the literature. The first one, AI2, was introduced by Hosseini et al. (2014) and covers addition and subtraction of one or two variables or two additions scraped from two web pages. The second (IL), introduced by Roy et al. (2015), contains single operator questions but covers addition, subtraction, multiplication, and division, and was also obtained from two, although different from AI2, web pages. The last data set (CC) was introduced by Roy and Roth (2015) to cover combinations of different operators and was obtained from a fifth web page. An overview of the equation patterns in the data is shown in Table 1. It should be noted that there are sometimes numbers mentioned in the problem 1613 AI2 X+Y X+Y+Z X−Y IL X+Y X−Y X∗Y X/Y CC X+Y−Z X ∗ (Y + Z) X ∗ (Y − Z) (X + Y)/Z (X − Y)/Z Table 1: Patterns of the equations seen"
D18-1541,D17-1151,0,0.0364251,"In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4977–4983 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ware such as a phrase-table generator, decoder, and part-of-speech tagger. We created multiple synthetic datasets from in-domain and outof-domain sources, and found that stochastic token sampling, and pruning redundant and lowlikelihood sentences, were helpful in generating meaningful corruptions. Using the artificial samples thus generated, we imp"
D18-1541,P06-1032,0,0.161866,"ictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Give"
D18-1541,E14-3013,0,0.0949662,"idence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4977–4983 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ware such as a phrase-table generator, decoder, and part-of-speech tagger. We created mult"
D18-1541,W09-2112,0,0.222768,"Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Given an input sequence, the encoder generates context vectors for each token. Then, the attention mechanism and the decoder work in tandem to emit a distribution over the target vocabulary. At"
D18-1541,P12-2076,0,0.123753,"ng and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Recently, Rei et al. (2017) corrupted error-free text using SMT to create training instances for error detection. 3 Neural error generation To learn to introduce errors, we use an off-theshelf attentive sequence-to-sequence neural network (Bahdanau et al., 2014). Given an input sequence, the encoder generates context vectors for each token. Then, the attention mechanism and the decoder work in tandem to emit a distribution over the target vocabulary. At every decoder timestep, the encoder context vec"
D18-1541,P15-1162,0,0.091247,"Missing"
D18-1541,W14-1703,0,0.0497629,"ifficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Lang"
D18-1541,D16-1161,0,0.123137,"Missing"
D18-1541,1994.amta-1.18,0,0.0393408,"(2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen,"
D18-1541,N06-1020,0,0.019391,"ty annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et al., 2014) could be used for this purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead lea"
D18-1541,W17-5039,0,0.0279333,"their learning is a crucial, labour-intensive endeavour. Part of this process is identifying and correcting grammatical errors, and several computational techniques have been developed to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et"
D18-1541,W14-1701,0,0.0923693,"it is not aligned to the last word of the source sentence, then ‘i’, as a human would realise that this sentence ends abruptly, Else, ‘c’. These token-labelled corrupted sentences now form an artificial dataset for training an error detector. Duplicate instances and corrupted sentences with more than 5 errors were dropped to remove noise from the downstream training. 4 Experiments We evaluated our approach on the First Certificate of English (FCE) error detection dataset (Rei and Yannakoudakis, 2016), as well as on two humanannotated test sets (CoNLL1, CoNLL2) from the CoNLL 2014 shared task (Ng et al., 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our NMT with a single-layered encoder and decoder with cell size 256, on the parallel corpus version of FCE (Yannako"
D18-1541,P17-1194,0,0.0939786,"ation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leveraging linguistic information to identify error patterns and transfer them onto grammatically correct text (Foster and Andersen, 2009; Yuan and Felice, 2013). Imamura et al. (2012) investigated methods to generate pseudo-erroneous sentences for error correction in Japanese. Rec"
D18-1541,C16-1030,0,0.113597,"Missing"
D18-1541,W17-5032,0,0.240647,"veloped to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated. Generative Adversarial Networks (Goodfellow et al., 2014) could be used for this purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt sel"
D18-1541,P16-1112,0,0.122199,"at this point a human reader would notice that there is a word missing in the sentence. Else, if it is the last word, but it is not aligned to the last word of the source sentence, then ‘i’, as a human would realise that this sentence ends abruptly, Else, ‘c’. These token-labelled corrupted sentences now form an artificial dataset for training an error detector. Duplicate instances and corrupted sentences with more than 5 errors were dropped to remove noise from the downstream training. 4 Experiments We evaluated our approach on the First Certificate of English (FCE) error detection dataset (Rei and Yannakoudakis, 2016), as well as on two humanannotated test sets (CoNLL1, CoNLL2) from the CoNLL 2014 shared task (Ng et al., 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our N"
D18-1541,A88-1027,0,0.0605404,"belled instances output by existing state-of-theart parsers as ground-truth labels, and improved syntactic parsing performance. Sennrich et al. (2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight et al., 1994) and spelling correction (Golding and Roth, 1996). Most recently, architectural innovations in neural sequence labelling (Rei et al., 2016; Rei, 2017) raised error detection performance through improved ability to process unknown words and jointly learning a language model. Early efforts for artificial error generation included generating specific types of errors, such as mass noun errors (Brockett et al., 2006) and article errors (Rozovskaya and Roth, 2010), and leve"
D18-1541,P11-1019,0,0.249998,", 2014). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation procedures (AM, TS, BS) and contrasted the downstream performance of a bidirectional LSTM with an elaborate sequence labeller. 4.1 Implementation details NMT training and corruption: We minimally modified the open source implementation1 of Britz et al. (2017) to implement TS and BS.2 We trained our NMT with a single-layered encoder and decoder with cell size 256, on the parallel corpus version of FCE (Yannakoudakis et al., 2011), with early stopping after the FCE development set score dropped consistently for 20 epochs. We introduced errors into three datasets: FCE itself (450K tokens), the English Vocabulary Profile or EVP (270K tokens) and a subset of Simple Wikipedia or SW (8.4M tokens); of these, FCE and EVP were both used in artificial error generation via SMT and pattern extraction (PAT) by Rei et al. (2017), enabling us to make a fair experimental comparison. Ten corrupted versions using each of AM, TS (τ = 0.05) and BS were sampled for FCE and EVP corruptions, while one sufficed for SW. The theoretical time c"
D18-1541,D17-1297,0,0.135977,"g them with quick feedback to facilitate their learning is a crucial, labour-intensive endeavour. Part of this process is identifying and correcting grammatical errors, and several computational techniques have been developed to automate it (Rozovskaya and Roth, 2014; Junczys-Dowmunt and Grundkiewicz, 2016). For example, given an erroneous sentence “I wanted to goes to the beach”, the grammatical error correction task is to output the valid sentence “I wanted to go to the beach”. The task can be cast as a two-stage process, detection and correction, which can either be performed sequentially (Yannakoudakis et al., 2017), or jointly (Napoles and Callison-Burch, 2017). Automated error correction performance is arguably still too low for practical consideration, perhaps limited by the amount of training data (Rei et al., 2017). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncomfortable granting access to their data. Instead, one could attempt to supplement existing manual annotations with synthetic instances. Such artificial samples are beneficial only when they share structure with the true distribution from which human errors are generated."
D18-1541,N16-1042,0,0.0585727,"ollection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 49"
D18-1541,W13-3607,0,0.45933,"purpose, but they are difficult to train, and require a large collection of sentences that are incorrect. One might attempt self-training (McClosky et al., 2006), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely available text such as Wikipedia articles as they strive towards correctness. In place of using machine translation (MT) to correct grammatical mistakes (Yuan and Felice, 2013; Junczys-Dowmunt and Grundkiewicz, 2014; Yuan and Briscoe, 2016), one might consider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset (Felice and Yuan, 2014). Recently, Rei et al. (2017) used a statistical MT (SMT) system to induce errors into error-free text. Building on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf attentive sequence-to-sequence model (Britz et al., 2017), eliminating the need of specialised soft4977 Proceedings of the 2018 Confere"
D18-1541,Q14-1033,0,0.0611825,"Missing"
D18-1541,P16-1009,0,0.0570945,"computer vision, images are blurred, rotated, or otherwise deformed inexpensively to create new training instances (Wang and Perez, 2017), because such manipulation does not significantly alter the image semantics. Similar coarse processes do not work in NLP since mutating even a single letter or a word can change a sentence’s meaning, or render it nonsensical. Nonetheless, Vinyals et al. (2015) employed a kind of selftraining where they use noisy predictions for unlabelled instances output by existing state-of-theart parsers as ground-truth labels, and improved syntactic parsing performance. Sennrich et al. (2016) synthesised training instances by roundtrip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to improve the translation. Bouchard et al. (2016) developed an efficient algorithm to blend generated and true data for improving generalisation. Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers (Macdonald, 1983) and dictionarybased linguistic analysis engines (Richardson and Braden-Harder, 1988). Later systems used statistical approaches, addressing specific kinds of errors such as article insertion (Knight"
D18-1541,D16-1167,1,\N,Missing
E12-2021,W05-0620,0,0.0614673,"Missing"
E12-2021,doddington-etal-2004-automatic,0,0.0628567,"an be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). F"
E12-2021,J02-3001,0,0.0249721,", allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and mult"
E12-2021,W11-1801,1,0.354037,"Missing"
E12-2021,W03-3017,0,0.0113928,"dheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and multilingual dependency parsing (Buchholz and Marsi, 2006). BRAT also imple"
E12-2021,N06-4006,0,0.00738235,"the semantic class disambiguation component (Stenetorp et al., 2011a). Although further research is needed to establish the benefits of this approach in various annotation tasks, we view the results of this initial experiment as promising regarding the potential of our approach to using machine learning to support annotation efforts. 5 informed by experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepa"
E12-2021,W11-1816,1,0.207231,"Missing"
E12-2021,X96-1048,0,0.137156,"lly configurable and can be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003)"
E12-2021,W08-0605,1,0.44524,"experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepage at http://brat.nlplab.org Acknowledgements The authors would like to thank early adopters of BRAT who have provided us with extensive feedback and feature suggestions. This work was supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan), the UK Biotechnology and Biological Sciences Research Council (BBSRC) under project Automated Biologic"
E12-2021,W06-2920,0,\N,Missing
E12-2021,W03-0419,0,\N,Missing
E12-2021,P05-1013,0,\N,Missing
E17-1119,J92-4003,0,0.322738,"development data. 3.1 Sparse Feature Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly avai"
E17-1119,D15-1103,0,0.462101,"eir respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-graine"
E17-1119,P16-1059,0,0.0215587,"– into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and"
E17-1119,Q15-1023,0,0.0194509,"n comparable set of features. What we find is that the performance drop is very dramatic, 9.85 points of loose micro score. Given that the training data for the previously introduced model is not publicly available, we hesi1277 (a) Figure 3: PCA projections of the label embeddings learnt from the OntoNotes dataset where subtypes share the same color as their parent type. Sub-figure (a) uses the non-hierarchical encoding, while sub-figure (b) uses the hierarchical encoding. tate to speculate as to exactly why this drop is so dramatic, but similar observations have been made for entity linking (Ling et al., 2015). This clearly underlines how essential it is to compare models on an equal footing using the same training data. PCA visualisation of label embeddings By visualising the learnt label embeddings (Figure 3) and comparing the non-hierarchical and hierarchical label encodings, we can observe that the hierarchical encoding forms clear distinct clusters. 4.7 Parent Before After Frequent Words /location /organization /art/film /music /award /event 0.319 0.324 0.207 0.259 0.583 0.310 0.228 0.178 0.429 0.116 0.292 0.188 0.070 0.119 0.021 0.018 0.083 0.089 in, at, born at, the, by film, films, in album"
E17-1119,P09-1113,0,0.0916087,"on. Their end goal was to use the resulting types in a question answering system and they developed a conditional random field model that they trained and evaluated on a manually annotated Korean dataset to detect and classify entity mentions. Other early work include Sekine (2008), that emphasised the need for having access to a large set of entity types for several NLP applications. The work primarily discussed design issues for fine-grained set of entity types and served as a basis for much of the future work on fine-grained entity classification. The first work to use distant supervision (Mintz et al., 2009) to induce a large – but noisy – training set and manually label a significantly smaller dataset to evaluate their fine-grained entity classification system, was Ling and Weld (2012) who introduced both a training and evaluation dataset F IGER (GOLD). Arguing that fine-grained sets of types must be organised in a very fine-grained hierarchical taxonomy, Yosef et al. (2012) introduced such a taxonomy covering 505 distinct types. This new set of types lead to improvements on F IGER (GOLD), and they also demonstrated that the fine-grained labels could be used as features to improve coarse-grained"
E17-1119,D14-1162,0,0.10334,"73.94 Attentive Attentive + Hand-crafted 54.53 59.68 74.76 78.97 71.58 75.36 F IGER (Ling and Weld, 2012) F IGER (Ren et al., 2016) 52.30 47.4 69.90 69.2 69.30 65.5 Table 3: Performance on F IGER (GOLD) for models using the same W2M training data. Pre-trained Word Embeddings We use pre-trained word embeddings that were not updated during training to help the model generalise to words not appearing in the training set (Rockt¨aschel et al., 2015). For this purpose, we used the freely available 300-dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For words not present in the pretrained word embeddings, we use the embedding of the “unk” token. Model Model Data Acc. Macro Micro Attentive + Hand-crafted Attentive (Shimaoka et al., 2016) W2M W2.6M 59.68 58.97 78.97 77.96 75.36 74.94 F IGER + PLE (Ren et al., 2016) HYENA + PLE (Ren et al., 2016) W2M+D W2M+D 59.9 54.2 76.3 69.5 74.9 68.1 K-WASABIE (Yogatama et al., 2015) GN2 n/a n/a 72.25 Table 4: Performance on F IGER (GOLD) for models using different training data. used dropout (Hinton et al., 2012) with probability 0.5 applied to the mention representation and sparse feature representat"
E17-1119,N13-1071,0,0.0177729,"Missing"
E17-1119,sekine-2008-extended,0,0.0444578,"match series against New Zealand is held on Monday”. 1 Introduction Entity type classification aims to label entity mentions in their context with their respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction ("
E17-1119,W16-1313,1,0.661596,"fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they compl"
E17-1119,P08-1086,0,0.0123156,"e Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly available. corresponding word embeddings. Those"
E17-1119,N16-1174,0,0.00639303,"es (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they complement each other. Ad"
E17-1119,P15-2048,0,0.59274,"xt-dependent fine-grained entity type classification where the types of a mention is constrained to what can be deduced from its context and introduced a new OntoNotes-derived manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Building upon the noise reduction aspects of this work, Ren et al. (2016) introduced a method to reduce label noise even further, leading to significant performance gains on both the evaluation dataset of Ling and Weld (2012) and Gillick et al. (2014). Yogatama et al. (2015) proposed to map handcrafted features and labels to embeddings in or1272 der to facilitate information sharing between both related types and features. A pure feature learning approach was proposed by Dong et al. (2015). They defined 22 types and used a two-part neural classifier that used a recurrent neural network to obtain a vector representation of each entity mention and in its second part used a fixed-size window to capture the context of a mention. A recent workshop paper (Shimaoka et al., 2016) introduced an attentive neural model that unlike previous work obtained vector representatio"
E17-1119,C12-2133,0,0.11954,"Missing"
K15-1027,P14-2131,0,0.316157,"paku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp words. For example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for"
K15-1027,D10-1115,0,0.0394017,"ings. To discriminate between words in n from those in win , wbef , and waf t , we have two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training samp"
K15-1027,D14-1199,0,0.136415,"Missing"
K15-1027,H05-1091,0,0.706306,"el features and no external annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused"
K15-1027,C14-1078,0,0.0277905,"Missing"
K15-1027,D14-1163,1,0.870336,"two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training sample, whose target word is w, √ is discarded with the probability t Pd (w) = 1 − p(w"
K15-1027,W06-1670,0,0.155868,"Missing"
K15-1027,P15-1061,0,0.28635,"RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBest . (a) Financial [stress]E1 is one of the main causes of [divorce]E2 (b) The [burst]E1 has been caused by water hammer [pressure]E2 Training example (a) is classified as CauseEffect(E1 , E2 ) which denotes that E2 is an effect caused by E1 , while training example (b) is cla"
K15-1027,N15-1133,0,0.0136109,"bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. noun pairs in their contexts. The dataset, containing 8,000 training and 2,717 test samples, defines nine classes (Cause-Effect, Entity-Origin, etc.) for ordered relations and one class (Other) for other relations. Thus, the task can be treated as a 19class classification task. Two examples from the training set are shown below. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not re"
K15-1027,D13-1166,0,0.0424757,"Missing"
K15-1027,S07-1003,0,0.0433618,"Missing"
K15-1027,D11-1129,0,0.0721086,"Missing"
K15-1027,D14-1012,0,0.0246398,"r example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are"
K15-1027,D13-1137,1,0.921942,"ef , and waf t , we have two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training sample, whose target word is w, √ is discarded with the probab"
K15-1027,P06-1104,0,0.0440265,"l annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this li"
K15-1027,P14-2012,0,0.142311,"c1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are used to train a relation cl"
K15-1027,S10-1057,0,0.278814,"use syntactic information or manually constructed external resources. 1 Introduction Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010). A traditional approach to relation classification is to train classifiers using various kinds of features with class labels annotated by humans. Carefully crafted features derived from lexical, syntactic, and semantic resources play a significant role in achieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between 1 https://code.google.com/p/word2vec/. 268 Proceedings of the 19th Conference on Computational Language Learning, pages 268–278, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Figure 1: The overview of our system (a) and the embedding learning method (b). In the example sentence, each of are, caused, and by is treate"
K15-1027,D12-1110,0,0.706942,"ty of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL . Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 4.2 Initialization and Optimization We initialized the embedding matrices N and W with zero-mean gaussian noise with a variance of 1 ˜ d . W and b were zero-initialized. The model parameters were optimized by maximizing the objective function in Eq. (3) using stochastic gradient ascent. The learning rate was set to α"
K15-1027,P14-1146,0,0.0362761,"target word to be predicted during training. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction. Another kind of task-specific word embeddings was proposed by Tang et al. (2014), which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks. However, such an approach is only feasible when a large amount of labeled data is available. cal level features and no external annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-wo"
K15-1027,P10-1040,0,0.066139,"rsity College London, London, United Kingdom pontus@stenetorp.se §Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp words. For example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-sp"
K15-1027,P12-2018,0,0.0413567,"in our method, and 1.6 × 107 in CRCNNOther assuming N is 10. dos Santos et al. (2015) also boosted the score of CR-CNNOther by omitting the noisy class “Other” by a rankingbased classifier, and achieved the best score (CRCNNBest ). Our results may also be improved by using the same technique, but the technique is dataset-dependent, so we did not incorporate the technique. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these baselines. 5.3.2 Comparison with SVM-Based Systems RelEmb performs much better than the bag-ofwords-based SVM. This is not surprising given that we use a large unannotated corpus and embeddings with a large number of parameters. RelEmb also outperforms the SVM system of Rink and Harabagiu (2010), which demonstrates the effectiveness of our task-specific word embeddings, despite our only requirement being a large unannotated corpus and a POS tagger. 5.3.3 Comparison with Neural Network Models Re"
K15-1027,C14-1220,0,0.0554248,"w. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBest . (a) Financial [stress]E1 is one of the main causes of [divorce]E2 (b) The"
K15-1027,J08-1002,0,\N,Missing
N15-3019,P14-1023,0,0.0218795,"and phrases to retrieve English sentences that are similar to a given phrase or word in a different language (query). These vector representations not only allow for efficient crosslingual lookups in databases consisting of millions of sentences, but can also be employed to visualize intralingual and interlingual semantic relationships between phrases. 2 Related Work Various types of neural network models have been proposed to induce distributed word representations and leveraging these word embeddings as features has proven viable in achieving state-of-the-art results for a variety of tasks (Baroni et al., 2014; Collobert and Weston, 2008). Recently, methods that attempt to compose embeddings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resou"
N15-3019,W04-2209,0,0.0437915,"es in the word/phrase vector space as a measure of semantic relatedness. Currently, our system supports the lookup of Japanese and English queries in English text. Our system encourages refining retrieved results and viewing relations in different contexts by supporting multiple queries. All queries and their corresponding results are visualized together to aid a better understanding of their relationships. To illustrate the differences to phrase vector-based sentence retrieval, we also offer a retrieval option based on direct word-to-text matching using the EDICT Japanese-English dictionary (Breen, 2004) and Apache Lucene1 for sentence retrieval. To the best of our knowledge, our system is the first to provide writing assistance using vector representations of words and phrases. 3.1 Inducing Crosslingually Constrained Word Representations We employ the approach presented in Soyer et al. (2015) to learn bilingually constrained representations of Japanese and English words. The method draws from sentence-parallel bilingual text to constrain word vectors crosslingually, handles text on a phrase level ensuring the compositionality of the induced word embeddings, and is agnostic to how phrase repr"
N15-3019,P14-1006,0,0.0209294,"dings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resources to constrain vector representations crosslingually. (Soyer et al., 2015; Hermann and Blunsom, 2014; Cho et al., 2014; Chandar A P et al., 2014). Embeddings learned using these methods not only possess meaningful properties within a language, but also interlingually. 3 Crosslingual Vector-Based Writing Assistance (CroVeWA) Our system harnesses crosslingually constrained word and phrase representations to retrieve and visualize sentences related to given queries, using distances in the word/phrase vector space as a measure of semantic relatedness. Currently, our system supports the lookup of Japanese and English queries in English text. Our system encourages refining retrieved results and vi"
N15-3019,D11-1014,0,0.0268484,"efficient crosslingual lookups in databases consisting of millions of sentences, but can also be employed to visualize intralingual and interlingual semantic relationships between phrases. 2 Related Work Various types of neural network models have been proposed to induce distributed word representations and leveraging these word embeddings as features has proven viable in achieving state-of-the-art results for a variety of tasks (Baroni et al., 2014; Collobert and Weston, 2008). Recently, methods that attempt to compose embeddings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resources to constrain vector representations crosslingually. (Soyer et al., 2015; Hermann and Blunsom, 2014; Cho et al., 2014; Chandar A P et al., 2014). Embeddings learn"
N15-3019,D14-1179,0,\N,Missing
P09-2008,J94-2001,0,0.0106901,"l, confidence and threshold estimation, and output optimization. The following sections will explain the steps in detail. Confidence and Threshold Estimation T = f (C) 2.1 Baseline Word Segmentation Model We use the tri-gram Hidden Markov Model (HMM) of (Lee et al., 2007) as the baseline WS model; however, we adopt the Maximum Likelihood (ML) decoding strategy to independently find the best word spacing states. ML-decoding allows us to directly compare each output to the threshold. There is little discrepancy in accuracy when using ML-decoding, as compared to Viterbidecoding, as mentioned in (Merialdo, 1994).1 Let o1,n be a sequence of n-character user input without WBMs, xt be the best word spacing state for ot where 1 ≤ t ≤ n. Assume that xt is either 1 (space after ot ) or 0 (no space after ot ). Then each best word spacing state x ˆt for all t can be found by using Equation 1. xˆt = argmax P (xt = i|o1,n ) i∈(0,1) = argmax P (o1,n , xt = i) = argmax i∈(0,1) i∈(0,1) × X X Then, we define the confidence as is done in Equation 5. Because calculating such a variable is impossible, we estimate the value by substituting the word spacing states produced by the S baseline WS model, xW 1,n , with the"
P09-2008,C04-1067,0,0.0753508,"Missing"
P15-4016,H94-1020,0,0.545178,"DF serializations: JSON-LD, N-Triples and N-Quads, Notation3, RDF/XML, TriG, TriX, and Turtle. With the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of"
P15-4016,N06-4006,0,0.0351016,"ith the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of the available libraries that implement the full JSON-LD syntax and API specifications.6"
P15-4016,W12-3610,0,0.0188254,"format (Sporny et al., 2014) and is the recommended serialization of OA. Every JSON-LD document is both a JSON document and a representation of RDF data. Figure 2 shows an example of a simple annotation using the OA JSON-LD representation.2 ""@id"": ""@type"": ""target"": ""body"": Action Read annotation Read all annotations Update annotation Delete annotation Create annotation Table 1: HTTP verbs, resources, and actions. Read-only services support only the two GET requests. is an RDF-based graph representation compatible with linguistic annotation formalisms such as LAF/GrAF (Ide and Suderman, 2007; Verspoor and Livingston, 2012). At its most basic level, the OA model differentiates between three key components: annotation, body, and target, where the annotation expresses that the body is related to the target of the annotation (Figure 1). The body can carry arbitrarily complex embedded data. { Resource Annotation Collection Annotation Annotation Collection 3.1 OA Store The OA Store is a reference implementation of persistent, server-side annotation storage that allows clients to create, read, update and delete annotations using the API. The store uses MongoDB, which is well suited to the task as it is a documentorien"
P15-4016,wright-2014-restful,0,0.0306169,"o all four databases. They query a PostgreSQL back-end for text and annotations, which are formatted as OA JSON-LD using the standard Python json module. 4.4 5 Related work Our approach builds directly on the OA data model (Bradshaw et al., 2013), which harmonizes the earlier Open Annotation Collaboration (Haslhofer et al., 2011) and Annotation Ontology Initiative (Ciccarese et al., 2011) efforts and is currently developed further under the auspices of the W3C Web Annotation WG.8 Approaches building on RESTful architectures and JSON-LD are also being pursued by the Linguistic Data Consortium (Wright, 2014) and the Language Application Grid (Ide et al., 2014), among others. A number of annotation stores following similar protocols have also been released recently, including Lorestore (Hunter and Gerber, 2012), PubAnnotation (Kim and Wang, 2012), the Annotator.js store9 , and NYU annotations10 . 6 Conclusions and future work We have proposed to share annotations using a minimal RESTful interface for Open Annotation data in JSON-LD. We introduced reference implementations of a server, client, validation and conversion tools, and demonstrated the integration of several independently developed annot"
P15-4016,W07-1501,0,\N,Missing
P15-4016,W12-2425,0,\N,Missing
P15-4016,ide-etal-2014-language,0,\N,Missing
P15-4016,E12-2021,1,\N,Missing
P18-4005,D16-1244,0,0.221957,"ther popular MR task is Natural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al.,"
P18-4005,D16-1084,1,0.852145,"o quickly set up, load and run the existing systems for QA and NLI. The model training notebook demonstrates training, testing, evaluating and saving QA and NLI models programmatically. However, normally the user will simply use the provided training script from command line. The model implementation notebook delves deeper into implementing new models from scratch by writing all modules for a custom model. 7 Natural Language Inference. For NLI, we report results for our implementations of conditional BiLSTMs (cBiLSTM) (Rocktäschel et al., 2016), the bidirectional version of conditional LSTMs (Augenstein et al., 2016), the Decomposable Attention Model (DAM, Parikh et al., 2016) and Enhanced LSTM (ESIM, Chen et al., 2017). ESIM was entirely implemented as a modular NLI model, i.e. its architecture was purely defined in a configuration file – see Appendix A for more details. Our models or training configurations contain slight modifications from the original which we found to perform better than the original setup. Our results are slightly differ from those reported, since we did not always perform an exhaustive hyper-parameter search. Conclusion We presented Jack the Reader (JACK), a shared framework for Ma"
P18-4005,D16-1264,0,0.300836,"efore, existing input- and output modules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or"
P18-4005,D15-1075,0,0.0435049,"yping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: ea"
P18-4005,W15-4007,0,0.0320485,"e that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, represented as a relationship of type p between entities s and o, such as: (L ONDON, CAPITAL O F, UK). Realworld Knowledge Graphs, such as Freebase (Bo"
P18-4005,P17-1152,0,0.157395,"ural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various dataset"
P18-4005,W17-2623,0,0.0840343,"Missing"
P18-4005,P02-1022,0,0.0771673,"I N PUT , M ODEL and O UTPUT modules that compose a JTR EADER instance. On the right, the data format that is used to interact with a JTR EADER (dotted lines indicate that the component is optional). Related Work Machine Reading requires a tight integration of Natural Language Processing and Machine Learning models. General NLP frameworks include C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), O PEN NLP6 and SPAC Y. All these frameworks offer pre-built models for standard NLP preprocessing tasks, such as tokenisation, sentence splitting, named entity recognition and parsing. GATE (Cunningham et al., 2002) and UIMA (Ferrucci and Lally, 2004) are toolkits that allow quick assembly of baseline NLP pipelines, and visualisation and annotation via a Graphical User Interface. GATE can utilise NLTK and C ORE NLP models and additionally enable development of rule-based methods using a dedicated pattern language. UIMA offers a text analysis pipeline which, unlike GATE, also includes retrieving information, but does not offer its own rule-based language. It is further worth mentioning the Information Retrieval frameworks A PACHE L UCENE and A PACHE S OLR which can be used for building simple, keyword-bas"
P18-4005,K17-1028,1,0.844327,"63 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various datasets. Results are otlined in Table 3. Table 2: Accuracy on the SNLI test set achieved by cBiLSTM, DAM, and ESIM. Question Answering. For the Question Answering (QA) experiments we report results for our implementations of FastQA (Weissenborn et al., 2017), BiDAF (Seo et al., 2016) and, in addition, our own JackQA implementations. With JackQA we aim to provide a fast and accurate QA model. Both BiDAF and JackQA are realised using high-level architecture descriptions, that is, their architectures are purely defined within their respective configuration files. Results of our models on the SQuAD (Rajpurkar et al., 2016) development set along with additional run-time and parameter metrics are presented in Table 1. Apart from SQuAD, JACK supports the more recent NewsQA (Trischler et al., 2017) and TriviaQA (Joshi et al., 2017) datasets too. 6 Demo W"
P18-4005,N18-1101,0,0.0218508,"lthough we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, re"
P18-4005,P17-1147,0,0.188943,"odules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Kno"
P18-4005,P14-5010,0,0.00961028,"Missing"
Q18-1021,D13-1160,0,0.223537,"Missing"
Q18-1021,W12-0705,0,0.023093,"taset for the domain of molecular biology – a field that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction incre"
Q18-1021,D14-1067,0,0.024069,"). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer an"
Q18-1021,D12-1118,0,0.0265152,"Missing"
Q18-1021,P16-1223,0,0.0689703,"Missing"
Q18-1021,E17-1013,0,0.0228804,"nefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediat"
Q18-1021,Q15-1015,0,0.0505894,"ion (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work aims to evaluate whether end-to-end multi-step RC models can indeed operate on raw text documents only – while performing the kind of inference most commonly associated with logical inference methods operating on structured knowledge. Text-Based Multi-Step Reading Comprehension Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneficial for reranking answers in open-domain non-factoid QA. Jansen et al. (2017) chain textual background resources for science exam QA and provide multisentence answer explanations. Beyond, a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on syntheti"
Q18-1021,D13-1080,0,0.0231409,"ude Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al"
Q18-1021,P16-1145,0,0.0987845,"Missing"
Q18-1021,N16-2016,0,0.0398816,"circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our"
Q18-1021,J17-2005,0,0.030324,"standing approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work aims to evaluate whether end-to-end multi-step RC models can indeed operate on raw text documents only – while performing the kind of inference most commonly associated with logical inference methods operating on structured knowledge. Text-Based Multi-Step Reading Comprehension Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneficial for reranking answers in open-domain non-factoid QA. Jansen et al. (2017) chain textual background resources for science exam QA and provide multisentence answer explanations. Beyond, a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2016). One common characteristic of neural multi-hop models 298 is their rich structure that enables matching and interaction between question, c"
Q18-1021,D17-1215,0,0.0359633,"ument set Sq . That is, Sq comprises chains of documents leading not only from the query subject to the correct answer candidate, but also to type-consistent false answer candidates. With this methodology, relevant textual evidence for (q, a∗ ) will be spread across documents along the chain connecting s and a∗ – ensuring that multihop reasoning goes beyond resolving co-reference within a single document. Note that including other type-consistent candidates alongside a∗ as end points in the graph traversal – and thus into the support documents – renders the task considerably more challenging (Jia and Liang, 2017). Models could otherwise identify a∗ in the documents by simply relying on type-consistency heuristics. It is worth pointing out that by introducing alternative candidates we counterbalance a type-consistency bias, in contrast to Hermann et al. (2015) and Hill et al. (2016) who instead rely on entity masking. 2 To determine entities which are type-consistent for a query q, we consider all entities which are observed as object in a fact with r as relation type – including the correct answer. 289 Entities s Documents KB (s, r, o) (s, r, o0 ) (s0 , r, o00 ) o o00 o0 Figure 2: A bipartite graph co"
Q18-1021,P17-1147,0,0.272977,"integrating cross-document information. 7 Related Work Related Datasets End-to-end text-based QA has witnessed a surge in interest with the advent of largescale datasets, which have been assembled based on F REEBASE (Berant et al., 2013; Bordes et al., 2015), W IKIPEDIA (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2016; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datasets are confined to single documents, and RC typically does not require a combination of multiple independent facts. In contrast, W IKI H OP and M ED H OP are specifically designed for cross-document RC and multistep inference. There exist other multi-hop RC resources, but they are either very limited in size, such as the FraCaS test suite, or based on synthetic language (Weston et al., 2016). TriviaQA partly involves multi-step reasoning, but the complexity largely stems from parsing compositional questions. Our datasets center around compositional inference from comparativel"
Q18-1021,P16-1086,0,0.04437,"Missing"
Q18-1021,W11-1802,0,0.0180635,"e total count of how often d co-occurs with c in a sample where c is also the correct answer. We use this statistic to filter the dataset, by discarding samples with at least one document-candidate pair (d, c) for which cooccurrence(d, c) &gt; 20. 4 M ED H OP Following the same general methodology, we next construct a second dataset for the domain of molecular biology – a field that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that h"
Q18-1021,D11-1049,0,0.025664,"nowledge resources which formulate facts using first-order logic. KB inference methods include Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the ou"
Q18-1021,K17-1034,0,0.0377532,"Missing"
Q18-1021,E17-1001,0,0.0177183,"a rich collection of neural models tailored towards multi-step RC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) define a model class that iteratively attends over textual memory items, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2016). One common characteristic of neural multi-hop models 298 is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2017; Liu and Perez, 2017), which is often iterated over several times (Sordoni et al., 2016; Neumann et al., 2016; Seo et al., 2017b; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira a"
Q18-1021,P09-1113,0,0.224026,"Missing"
Q18-1021,D16-1199,0,0.0313126,"Missing"
Q18-1021,D16-1261,0,0.0184707,"2017b; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2018). While related, this work ultimately aims at reformulating queries to better acquire evidence documents, and not at answering queries through combining facts. 8 Conclusions and Future Work We have introduced a new cross-document multihop RC task, devised a generic dataset derivation strategy and applied it to two separate domains. The resulting datasets test RC methods in their ability to perform composite reasoning – something thus far limited to models operating on structured knowledge resources. In our experiments we found that contemporary RC mo"
Q18-1021,P15-1016,0,0.0125112,"rom limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-defined schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sides"
Q18-1021,D17-1061,0,0.0242707,"and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2018). While related, this work ultimately aims at reformulating queries to better acquire evidence documents, and not at answering queries through combining facts. 8 Conclusions and Future Work We have introduced a new cross-document multihop RC task, devised a generic dataset derivation strategy and applied it to two separate domains. The resulting datasets test RC methods in their ability to perform composite reasoning – something thus far limited to models operating on structured knowledge resources. In our experiments we found that contemporary RC models can leverage cross-"
Q18-1021,D16-1241,0,0.0816821,"Missing"
Q18-1021,P16-1144,0,0.0609488,"Missing"
Q18-1021,Q17-1008,0,0.0208216,"t supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separate pieces of evidence. The promise of multi-hop methods is finding and combining individual observations that can suggest previously unobserved DDIs, aiding the process of making scientific discoveries, yet not directly from experiments, but by inferring them from established public knowledge (Swanso"
Q18-1021,D14-1162,0,0.0805393,"Missing"
Q18-1021,D16-1264,0,0.302893,"(Weissenborn et al., 2017), which have shown a robust performance across several datasets. These models predict an answer span within a single document. We adapt them to a multidocument setting by sequentially concatenating all d ∈ Sq in random order into a superdocument, adding document separator tokens. During training, the first answer mention in the concatenated document serves as the gold span.4 At test time, we measured accuracy based on the exact match between the prediction and answer, both lowercased, after removing articles, trailing white spaces and punctuation, in the same way as Rajpurkar et al. (2016). To rule out any signal stemming from the order of documents in the superdocument, this order is randomized both at training and test time. In a preliminary experiment we also trained models using different random document order permutations, but found that performance did not change significantly. 4 We also tested assigning the gold span randomly to any one of the mention of the answer, with insignificant changes. For BiDAF, the default hyperparameters from the implementation of Seo et al. (2017a) are used, with pretrained GloVe (Pennington et al., 2014) embeddings. However, we restrict the"
Q18-1021,D08-1009,0,0.09811,"Missing"
Q18-1021,D10-1106,0,0.0317284,"erence goes beyond resolving co-reference. Compositional Knowledge Base Inference Combining multiple facts is common for structured knowledge resources which formulate facts using first-order logic. KB inference methods include Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoenmackers et al., 2008). These approaches suffer from limited coverage and inefficient inference, though efforts to circumvent sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). All of these previous approaches center around learning how to combine facts from a"
Q18-1021,K17-1004,0,0.0227403,"Missing"
Q18-1021,S13-2056,0,0.0216875,"Missing"
Q18-1021,W11-1816,1,0.851112,"Missing"
Q18-1021,K17-1028,0,0.295469,"n (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separate pieces of evidence. The promise of multi-hop methods is finding and combining individual observations that can suggest previously unobserved DDIs, aiding the process of making scientific discoveries, yet not directly from experiments, but by inferring them from established public knowledge (Swanso"
Q18-1021,W17-4413,1,0.919328,"that the number of candidates differs between samples. Max-mention Predicts the most frequently mentioned candidate in the support documents Sq of a sample – randomly breaking ties. Majority-candidate-per-query-type Predicts the candidate c ∈ Cq that was most frequently observed as the true answer in the training set, given the query type of q. For W IKI H OP, the query type is the property p of the query; for M ED H OP there is only the single query type – interacts with. TF-IDF Retrieval-based models are known to be strong QA baselines if candidate answers are provided (Clark et al., 2016; Welbl et al., 2017). They 294 search for individual documents based on keywords in the question, but typically do not combine information across documents. The purpose of this baseline is to see if it is possible to identify the correct answer from a single document alone through lexical correlations. The model forms its prediction as follows: For each candidate c, the concatenation of the query q with c is fed as an OR query into the whoosh text retrieval engine. It then predicts the candidate with the highest TF-IDF similarity score: arg max[max(TF-IDF(q + c, s))] c∈Cq s∈Sq (1) Document-cue During dataset cons"
Q18-1021,W17-2309,0,0.011721,"an et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017). A task that has received significant attention is detecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly – but rather need to be inferred from separa"
Q18-1021,D15-1237,0,0.112307,"Missing"
W11-0218,W10-1904,1,0.871169,"Missing"
W11-0218,W10-1911,0,0.033661,"Missing"
W11-0218,W04-1213,0,0.0888074,"Missing"
W11-0218,W11-1802,0,0.0476946,"Missing"
W11-0218,W11-1803,1,0.872374,"Missing"
W11-0218,C10-1096,1,0.81728,"elated lexical resource. For example, if we observe the text “Carbonic anhydrase IV” marked as P ROTEIN and have an entry for “Carbonic anhydrase 4” in a lexical resource, a machine learning method can learn to associate the resource with the P ROTEIN category (at specific similarity thresholds) despite syntactic differences. In this study, we aim to construct such a system and to demonstrate that it outperforms strict string matching approaches. We refer to our system as SimSem, as in “Similarity” and “Semantic”. 2.2 SimString SimString1 is a software library utilising the CPMerge algorithm (Okazaki and Tsujii, 2010) to enable fast approximate string matching. The software makes it possible to find matches in a collection with over ten million entries using cosine similarity and a similarity threshold of 0.7 in approximately 1 millisecond with modest modern hardware. This makes it useful for querying a large collection of strings to 1 http://www.chokkan.org/software/simstring/ 137 find entries which may differ from the query string only superficially and may still be members of the same semantic category. As an example, if we construct a SimString database using an American English wordlist2 and query it"
W11-0218,W11-1804,1,0.849299,"Missing"
W11-0218,W09-1119,0,0.022841,"nce. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but inst"
W11-0218,W08-0609,0,0.020694,"troduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performing multi-class biomedical NER system, this motivates our desire to include multiple semantic categories. 136 Proceedings of the 2011 Worksho"
W11-0218,W03-1306,1,0.875157,"rch directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performi"
W11-0218,W09-1401,1,\N,Missing
W11-1816,I05-2038,1,\N,Missing
W11-1816,W11-1825,0,\N,Missing
W11-1816,de-marneffe-etal-2006-generating,0,\N,Missing
W11-1816,W11-1802,1,\N,Missing
W11-1816,W11-1803,1,\N,Missing
W11-1816,J93-2004,0,\N,Missing
W11-1816,W11-1812,1,\N,Missing
W11-1816,D10-1096,0,\N,Missing
W11-1816,N10-1123,0,\N,Missing
W11-1816,J08-1002,1,\N,Missing
W11-1816,J04-4004,0,\N,Missing
W11-1816,W06-2920,0,\N,Missing
W11-1816,W09-1401,1,\N,Missing
W11-1816,P96-1025,0,\N,Missing
W11-1816,D07-1111,1,\N,Missing
W11-1816,P05-1022,0,\N,Missing
W11-1816,P06-1055,0,\N,Missing
W11-1816,P08-1006,1,\N,Missing
W11-1816,W10-3006,0,\N,Missing
W11-1816,P04-1014,0,\N,Missing
W11-1816,W07-2416,0,\N,Missing
W11-1816,C10-1088,1,\N,Missing
W11-1816,W11-1809,0,\N,Missing
W11-1816,W11-1824,0,\N,Missing
W11-1816,W11-1810,0,\N,Missing
W11-1816,W11-1801,1,\N,Missing
W11-1816,W11-1804,1,\N,Missing
W12-2412,W11-1828,0,0.0929443,"Missing"
W12-2412,W11-1820,0,0.0684248,"Missing"
W12-2412,P11-1098,0,0.0165262,"c span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise fol"
W12-2412,doddington-etal-2004-automatic,0,0.282956,"IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the requirement to match trigger spans 2. only require entity texts, not spans, to match The first alternative criterion has also been previously consider"
W12-2412,W11-1824,0,0.0264993,"Missing"
W12-2412,P10-1160,0,0.0661515,"Missing"
W12-2412,W11-1827,0,0.568696,"Missing"
W12-2412,W11-1801,1,0.941981,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1822,0,0.0679042,"Missing"
W12-2412,W11-1826,0,0.0323279,"Missing"
W12-2412,P11-1163,0,0.0329128,"l only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation and each event extracted by a system is considered in"
W12-2412,P09-1113,0,0.0205423,"tly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and event annotation is associated with a specific span of text. Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating"
W12-2412,W11-1803,1,0.935394,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,D07-1075,0,0.0343243,"IE systems to identify a specific span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode"
W12-2412,W11-1804,1,0.922475,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1812,1,0.828716,"ferring to the real-world entities in text, the overall task is “text-bound” in the sense of requiring not only the extraction of targeted statements from text, but also the identification of specific regions of text expressing each piece of extracted information. Events can further be marked with modifiers identifying additional features such as being explicitly negated or stated in a speculative context. Figure 1 shows an illustration of event annotations. This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al., 2011c), ID (Pyysalo et al., 2011a) and EPI (Ohta et al., 2011) tasks. A variant of this representation that omits event triggers was applied in the BioNLP ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3"
W12-2412,W11-1825,0,0.02855,"P ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3.1 Following the BioNLP ST 2011, the MSR-NLP group called for the release of outputs from various participating systems (Quirk et al., 2011) and made analyses of their system available.2 Despite the obvious benefits of the availability of these resources, we are not aware of other groups following this example prior to the time of this publication. To create the combined resource, we approached each group that participated in the three targeted BioNLP ST 2011 main tasks to ask for their support to the creation of a dataset including analyses from their event extraction systems. This suggestion met with the support of all but a few groups that were approached.3 The groups providing analyses from their systems into this merged resou"
W12-2412,D11-1001,0,0.0233845,"alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation"
W12-2412,W11-1808,0,0.31655,"Missing"
W12-2412,W11-1816,1,0.901463,"Missing"
W12-2412,H91-1059,0,0.261268,"on systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the re"
W12-2412,W10-1921,1,0.866603,"Missing"
W12-2412,W11-1821,0,0.0439833,"Missing"
W12-2412,W11-0204,0,0.0629482,"Missing"
W12-2412,W11-1805,0,0.0284448,"Missing"
W12-2412,W11-1802,1,\N,Missing
W12-2412,W11-1809,0,\N,Missing
W12-2412,W11-1811,1,\N,Missing
W12-2412,W11-1810,0,\N,Missing
W12-3806,W11-1828,0,0.0482957,"Missing"
W12-3806,W10-3001,0,0.0645895,"Missing"
W12-3806,W10-3010,0,0.161775,"sk, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine how cue-and-scope recognition systems can be used to produce a state-of-the-art negation/speculation detection system for the EE task. 2 Resources Several existing resources can support the inv"
W12-3806,W11-1827,0,0.0114523,"us used to train the CLiPSNESP system, the GE test set does not, and thus test set results are not expected to be overfit. We noted when performing development set experiments that training machine learning-based methods on the negation/speculation annotations of the event-annotated corpora was problematic due to the sparseness of these flags in the annotation. To address this issue, we merge the training data of the three corpora in all experiments with machine learning methods. 5.2 Baseline methods We use the event analyses created by the UTurku (Bj¨orne and Salakoski, 2011) and UConcordia (Kilicoglu and Bergler, 2011) systems for the BioNLP 2011, the only systems that included negation and speculation analyses. To investigate the impact on a system that did not include a negation/speculation component, we further consider analyses created Negation (R/P/F) EPI GE ID H HR 29.23/31.67/30.40 27.69/32.73/30.00 53.92/52.84/53.38 53.24/71.89/61.18 44.00/31.88/36.97 44.00/37.93/40.74 M ME MC MCE 47.69/20.00/28.18 60.00/66.10/62.90 40.00/74.29/52.00 58.46/73.08/64.96 43.00/25.25/31.82 58.36/70.08/63.69 58.36/76.34/66.15 61.77/83.03/70.84 46.00/26.74/33.82 54.00/69.23/60.67 52.00/61.90/56.52 58.00/70.73/63.74 Table"
W12-3806,W09-1401,1,0.94131,"ion/Speculation Annotations: A Bridge Not Too Far Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3 Sophia Ananiadou2,3 and Jun’ichi Tsujii2,3,4 1 Department of Computer Science, University of Tokyo, Tokyo, Japan 2 School of Computer Science, University of Manchester, Manchester, United Kingdom 3 National Centre for Text Mining, University of Manchester, Manchester, United Kingdom 4 Microsoft Research Asia, Beijing, People’s Republic of China {pontus,smp,okap}@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk jtsujii@microsoft.com Abstract some marking of certainty and polarity (LDC, 2005; Kim et al., 2009; Saur and Pustejovsky, 2009; Kim et al., 2011a; Thompson et al., 2011). We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feas"
W12-3806,W11-1801,1,0.855017,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1802,0,0.0691394,"Missing"
W12-3806,W11-1806,0,0.142886,"2.43 48.08/51.02/49.50 25.65/10.84/15.24 22.08/42.24/29.00 27.27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data."
W12-3806,W09-1304,0,0.0270192,"icallyoriented and task-oriented perspectives on negation/speculation detection. In this study, we make use of the following resources. First, we study the three BioNLP 2011 Shared Task corpora that include annotation for negation and speculation: the GE, EPI and ID main task corpora (Table 1). Second, we make use of supporting analyses provided for these corpora in response to a call sent by the BioNLP Shared Task organisers to the developers of third-party systems (Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking t"
W12-3806,W10-3006,0,0.158592,"Missing"
W12-3806,W11-1803,1,0.674875,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1804,1,0.674411,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W12-2412,1,0.850965,"(Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking them for negation and speculation. The above three resources present us with many opportunities to relate scope-based annotations to three highly relevant event-based corpora containing negation/speculation annotations. 3 Manual Analysis To gain deeper insight into the data and the challenges in combining the cue-and-scope and eventoriented perspectives, we performed a manual analysis of the corpus annotations using the manually Name Negated Events Speculated Ev"
W12-3806,W11-1807,0,0.0123498,"27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data. We use the training, development and test set partition provided"
W12-3806,W11-1808,0,0.183789,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1816,1,0.908546,"Missing"
W12-3806,W08-0606,0,0.206797,"ons of text statements have explicitly included Although extra-propositional aspects are recognised as important, there is no clear consensus on how to address their annotation and extraction from text. Some comparatively early efforts focused on the detection of negation cue phrases associated with specific (previously detected) terms through regular expression-based rules (Chapman et al., 2001). A number of later efforts identified the scope of negation cues with phrases in constituency analyses in sentence structure (Huang and Lowe, 2007). Drawing in part on this work, the BioScope corpus (Vincze et al., 2008) applied a representation where both cues and their associated scopes are marked as contiguous spans of text (Figure 1 bottom). This approach was also applied in the CoNLL-2010 Shared Task (Farkas et al., 2010), in which 13 participating groups proposed approaches for Task 2, which required the identification of uncertainty cues and their associated scopes in text. In the following, we will term this task-independent, linguisticallymotivated approach as the cue-and-scope representation (please see Vincze et al. (2008) for details regarding the representation). For IE efforts, more task-oriente"
W13-2013,D10-1096,0,0.0159182,"This pre-processing was identical to that applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL"
W13-2013,P05-1022,0,0.00694797,"deep parser based on the Head-Driven Phrase Structure Grammar (HPSG) formalism. Enju analyses its input in terms of phrase structure trees with predicate-argument structure links, represented in a specialised XML-format. To make the analyses of the parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Parser (Charniak and Johnson, 2005), also variously known as the Charniak parser, the Charniak-Johnson parser, or the Brown reranking parser, has been applied in numerous biomedical domain NLP efforts, frequently using the self-trained biomedical model of McClosky (2010) (i.e. the McClosky-Charniak-Johnson or McCCJ parser). The BLLIP Parser is a constituency (phrase structure) parser and the applied model produces PTB analyses as its native output. These analyses were made available to participants without modification. 5 Pre-processing and Conversions Results and Discussion Just like in previous years the supporting resources"
W13-2013,W07-2416,0,0.0161951,"like tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the Stanford Dependencies. Table 2: Parsers used for the syntactic analyses. and format conversions applied to their output. The applied parsers are listed in Table 2. 4.1 Syntactic Parsers Enju Enju (Miyao and Tsujii, 2008) is a deep parser based on the Head-Driven Phrase S"
W13-2013,W09-1401,0,0.12787,"valuation can improve understanding of the applicability and benefits of specific tools and representations. The supporting resources described in this paper will continue to be publicly available from the shared task homepage http://2013.bionlp-st.org/ 1 Introduction The BioNLP Shared Task (ST), first organised in 2009, is an ongoing series of events focusing on novel challenges in biomedical domain information extraction. In the first BioNLP ST, the organisers provided the participants with automatically generated syntactic analyses from a variety of Natural Language Processing (NLP) tools (Kim et al., 2009) and similar syntactic analyses have since then been a key component of the best performing systems participating in the shared tasks. This initial work was followed up by a similar effort in the second event in the series (Kim et al., 2011), extended by the inclusion of software tools and contributions from the broader BioNLP com2 Organisation Following the practice established in the BioNLP ST 2011, the organisers issued an open call for supporting resources, welcoming contributions relevant to the task from all authors of NLP tools. In the call it was mentioned that points such as availabil"
W13-2013,W11-1802,0,0.0137379,"onlp-st.org/ 1 Introduction The BioNLP Shared Task (ST), first organised in 2009, is an ongoing series of events focusing on novel challenges in biomedical domain information extraction. In the first BioNLP ST, the organisers provided the participants with automatically generated syntactic analyses from a variety of Natural Language Processing (NLP) tools (Kim et al., 2009) and similar syntactic analyses have since then been a key component of the best performing systems participating in the shared tasks. This initial work was followed up by a similar effort in the second event in the series (Kim et al., 2011), extended by the inclusion of software tools and contributions from the broader BioNLP com2 Organisation Following the practice established in the BioNLP ST 2011, the organisers issued an open call for supporting resources, welcoming contributions relevant to the task from all authors of NLP tools. In the call it was mentioned that points such as availability for research purposes, support for well-established formats and access 99 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99–103, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics Name Annotation"
W13-2013,W12-3806,1,0.841552,"their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different representations for negation and speculation annotation included in the data. These research contributions and the overall positive reception of the supporting resources prompted us to continue to provide supporting resources for the BioNLP Shared Task 2013. This paper presents the details of this technical contribution. This paper describes the technical contribution of the supporting resources provided for the BioNLP Shared Task 2013. Following the tradition of the previous two BioNLP Shared Task events, the task organisers and several"
W13-2013,W11-1806,0,0.0125311,"or Biotechnology Information, National Library of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibili"
W13-2013,N10-1004,0,0.0373258,"e parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Parser (Charniak and Johnson, 2005), also variously known as the Charniak parser, the Charniak-Johnson parser, or the Brown reranking parser, has been applied in numerous biomedical domain NLP efforts, frequently using the self-trained biomedical model of McClosky (2010) (i.e. the McClosky-Charniak-Johnson or McCCJ parser). The BLLIP Parser is a constituency (phrase structure) parser and the applied model produces PTB analyses as its native output. These analyses were made available to participants without modification. 5 Pre-processing and Conversions Results and Discussion Just like in previous years the supporting resources were well-received by the shared task participants and as many as five participating teams mentioned utilising the supporting resources in their initial submissions (at the time of writing, the cameraready versions were not yet availabl"
W13-2013,W11-1818,0,0.0123952,"ation, National Library of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different re"
W13-2013,C10-1088,0,0.0168273,"as identical to that applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom tex"
W13-2013,J08-1002,0,0.0281123,"LL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the Stanford Dependencies. Table 2: Parsers used for the syntactic analyses. and format conversions applied to their output. The applied parsers are listed in Table 2. 4.1 Syntactic Parsers Enju Enju (Miyao and Tsujii, 2008) is a deep parser based on the Head-Driven Phrase Structure Grammar (HPSG) formalism. Enju analyses its input in terms of phrase structure trees with predicate-argument structure links, represented in a specialised XML-format. To make the analyses of the parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Par"
W13-2013,W11-1814,0,0.0254302,"y of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different representations for negation"
W13-2013,W11-1825,0,0.0141209,"t applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the"
W13-2013,W11-1816,1,0.851328,"Missing"
W13-2013,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2013,W11-1824,0,\N,Missing
W13-2013,W11-1805,0,\N,Missing
W16-1313,D15-1103,0,0.215392,"population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold: 1. A first model for fi"
W16-1313,C96-1079,0,0.231995,"for the mention “New York” in the sentence “She got a Ph.D from New York in Feb. 1995.”. Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its contex"
W16-1313,P09-1113,0,0.151619,"owledge, Lee et al. (2006) were the first to address the task of fine-grained entity type classification. They defined 147 finegrained entity types and evaluated a conditional random fields-based model on a manually annotated Korean dataset. Sekine (2008) advocated the necessity of a large set of types for entity type classification and defined 200 types which served as a basis for future work on fine-grained entity type classification. Ling and Weld (2012) defined a set of 112 types based on Freebase and created a training dataset from Wikipedia using a distant supervision method inspired by Mintz et al. (2009). For evaluation, they created a small manually annotated dataset of newspaper articles and also demonstrated that their system, FIGER, could improve the performance of a relation extraction system by providing fine-grained entity type predictions as features. Yosef et al. (2012) organised 505 types in a hierarchical taxonomy, with several hundreds of types at different levels. Based on this taxonomy they developed a multi-label hierarchical classification system. In Yogatama et al. (2015) the authors proposed to use label embeddings to allow information sharing between related labels. This ap"
W16-1313,D14-1162,0,0.0832247,"i=1 • loose micro Dataset Pre-trained Word Embeddings Evaluation Criteria Following Ling and Weld (2012), we evaluate the model performances by strict, loose macro, and 72 PN ˆ i=1 |Ti ∩ Ti | P N ˆ i=1 |Ti | PN ˆ i=1 |Ti ∩ Ti | Recall = P N i=1 |Ti | P recision = Experiment The only features used by our model are pretrained word embeddings that were not updated during training to help the model generalize for words not appearing in the training set. Specifically, we used the freely available 300 dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). As embeddings for out-ofvocabulary words, we used the embedding of the “unk” token from the pre-trained embeddings. 4.3 P recision = Recall = (10) To train and evaluate our model we use the publicly available FIGER dataset with 112 fine-grained types from Ling and Weld (2012). The sizes of our datasets are 2, 600, 000 for training, 90, 000 for development, and 563 for testing. Note that the train and development sets were created from Wikipedia, whereas the test set is a manually annotated dataset of newspaper articles. 4.2 • strict (9) The equations for computing eri , a ˜ri , and ari were"
W16-1313,sekine-2008-extended,0,0.304162,"Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recurs"
W16-1313,P15-2048,0,0.230963,", 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold"
W16-1313,C12-2133,0,0.387061,"nswering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main con"
W16-2522,D12-1118,0,0.0735175,"Missing"
W16-2522,N16-1162,0,0.0568602,"Missing"
W16-2522,P09-2053,0,0.0808319,"Missing"
W16-2522,Q14-1035,0,0.235239,"Missing"
W18-1005,D15-1075,0,0.0894992,"Missing"
W18-1005,D16-1244,0,0.0639563,"Missing"
W18-1005,D14-1162,0,0.107282,"Missing"
W18-1005,W14-1618,0,0.0787905,"Missing"
W18-5515,P17-1152,0,0.0569157,"mentioned in the claim (e.g., “Watchmen” vs “Watchmen (film)”). The model is trained on a balanced set of positive and negative examples drawn from the training set, and the top-ranked articles are then passed on to the sentence retrieval component. This process is related to, but goes substantially beyond entity recognition and linking (Mendes et al., 2017). These processes attempt to identify 2.3 Natural Language Inference (NLI) In this component, an NLI model predicts a label for each pair of claim and retrieved evidence sentence. We adopted the Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) as NLI model. ESIM employs a bidirectional LSTM (BiLSTM) to encode premise and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on th"
W18-5515,D16-1244,0,0.162336,"Missing"
W18-5515,D14-1162,0,0.0850075,"and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al., 2018) simply concatenates all evidence sentences and feeds them into the NLI model, regardless of their evidence type. In contra"
W18-5515,P18-1196,1,0.863792,"Missing"
W18-5515,N18-1074,0,0.227007,"et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al., 2018) simply concatenates all evidence sentences and feeds them into the NLI model, regardless of their evidence type. In contrast, we generate NLI predictions individually for each predicted evidence, thus processing them in parallel. Furthermore, we observed that evidence sentences often include a pronoun referring to the subject of the article without explicitly mentioning it. This co-reference is opaque to the NLI model without further information. To resolve this, we prepend the corresponding title of the article to the sentence, along with a separator as described in Figure 1. We also experim"
W18-5515,D15-1075,0,0.0311149,"uential Inference Model (ESIM) (Chen et al., 2017) as NLI model. ESIM employs a bidirectional LSTM (BiLSTM) to encode premise and hypothesis, and also encodes local inference information so that the model can effectively exploit sequential information. We also experimented with the Decomposable Attention Model (DAM) (Parikh et al., 2016) — as used in the baseline model, however ESIM consistently performed better. The Jack the Reader (Weissenborn et al., 2018) framework was used for both DAM and ESIM. We first pre-trained the ESIM model on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and then fine-tuned 98 on the FEVER dataset. We used 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). As training input, we used gold evidence sentences for SUPPORTS and REFUTES samples, and retrieved evidence sentences for NOT ENOUGH INFO. It is worth noting that there are two kinds of evidences in this task. The first is a complete set of evidence, which can support/refute a claim, and can consist of multiple sentences. The second is incomplete evidence, which can support or refute the claim only when paired with other evidence. The baseline model (Thorne et al."
