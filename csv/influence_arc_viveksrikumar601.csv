2020.acl-main.210,J08-4004,0,0.155075,"Missing"
2020.acl-main.210,D15-1075,0,0.432688,"e Olympic games in 1912. H2: Both men and women compete in the equestrian sport of Dressage. H3: A dressage athlete can participate in both individual and team events. H4: FEI governs dressage only in the U.S. Figure 1: A semi-structured premise (the table). Two hypotheses (H1, H2) are entailed by it, H3 is neither entailed nor contradictory, and H4 is a contradiction. Introduction Recent progress in text understanding has been driven by sophisticated neural networks based on contextual embeddings—e.g., BERT (Devlin et al., 2019), and its descendants—trained on massive datasets, such as SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and SQuAD (Rajpurkar et al., 2016). Several such models outperform human baselines on these tasks on the benchmark suites such as GLUE (Wang et al., 2019b). Reasoning about text requires a broad array of skills—making lexical inferences, interpreting the nuances of time and locations, and accounting for world knowledge and common sense. Have we achieved human-parity across such a diverse collection of reasoning skills? In this paper, we study this question by proposing an extension of the natural language inference (NLI) task (Dagan et al., 2005, and others)"
2020.acl-main.210,N19-1421,0,0.0216688,"sentation, we analyzed the examples in Dev and α3 data splits that were annotated by experts for their types of reasoning (§5). Figure 3 shows the summary of this analysis. Results and Analysis: Figures 3a and 3b show the histogram of reasoning types among correctly Reasoning Recently, challenging new datasets have emerged that emphasize complex reasoning. Bhagavatula et al. (2020) pose the task of determining the most plausible inferences based on observation (abductive reasoning). Across NLP, a lot of work has been published around different kinds of reasonings. To name a few, common sense (Talmor et al., 2019), temporal (Zhou et al., 2019), numerical (Naik et al., 2019; Wallace et al., 2019b) and Table 7: Accuracy on structured premise representation reported on BERTB , RoBERTaB and RoBERTaL 2316 Neutral Entailment Contradiction Number of Correct Prediction Number of Correct Prediction Contradiction 80 60 40 20 0 l pe KCS ing irow ntity tion rical tion kup ref sis OT ation pora lt Co Ellip y Ty e E on n ga oo e/O ica m tit as Mu med Ne Num antif ple L ctiv Alter Te En Re Na al Qu Sim Subje ctic a xic t e n L Sy Neutral Entailment 80 60 40 20 0 l pe KCS ing irow ntity tion rical tion kup ref sis OT"
2020.acl-main.210,D18-1009,0,0.0176462,"ained transformerbased models (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019, and others) have seemingly outperformed human performance on several NLI tasks (Wang et al., 2019b,a). However, it has been shown by Poliak et al. (2018); Niven and Kao (2019); Gururangan et al. (2018); Glockner et al. (2018); Naik et al. (2018); Wallace et al. (2019a) that these models exploit spurious patterns (artifacts) in the data to obtain good performance. It is imperative to produce datasets that allow for controlled study of artifacts. A popular strategy today is to use adversarial annotation (Zellers et al., 2018; Nie et al., 2019) and rewriting of the input (Chen et al., 2020). We argue that we can systematically construct test sets that can help study artifacts along specific dimensions. 8 Conclusion We presented a new high quality natural language inference dataset, I NFOTAB S, with heterogeneous semi-structured premises and natural language hypotheses. Our analysis showed that our data encompasses several different kinds of inferences. I NFOTAB S has multiple test sets that are designed to pose difficulties to models that only learn superficial correlations between inputs and the labels, rather th"
2020.acl-main.210,N18-1101,0,0.186518,"h men and women compete in the equestrian sport of Dressage. H3: A dressage athlete can participate in both individual and team events. H4: FEI governs dressage only in the U.S. Figure 1: A semi-structured premise (the table). Two hypotheses (H1, H2) are entailed by it, H3 is neither entailed nor contradictory, and H4 is a contradiction. Introduction Recent progress in text understanding has been driven by sophisticated neural networks based on contextual embeddings—e.g., BERT (Devlin et al., 2019), and its descendants—trained on massive datasets, such as SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and SQuAD (Rajpurkar et al., 2016). Several such models outperform human baselines on these tasks on the benchmark suites such as GLUE (Wang et al., 2019b). Reasoning about text requires a broad array of skills—making lexical inferences, interpreting the nuances of time and locations, and accounting for world knowledge and common sense. Have we achieved human-parity across such a diverse collection of reasoning skills? In this paper, we study this question by proposing an extension of the natural language inference (NLI) task (Dagan et al., 2005, and others). In NLI, which asks whether a pre"
2020.acl-main.210,D16-1244,0,\N,Missing
2020.acl-main.210,P18-2103,0,\N,Missing
2020.acl-main.210,N18-1023,0,\N,Missing
2020.acl-main.210,P19-1329,0,\N,Missing
2020.acl-main.210,P19-1459,0,\N,Missing
2020.acl-main.210,N19-1423,0,\N,Missing
2020.acl-main.438,P05-1044,0,0.0588157,"raint feature extractor to it gives a negative example (ψ(x, y0 ), −1). We also need to ensure that ψ(x, y0 ) is indeed different from any positive example. Another approach is to perturb the feature vector ψ(x, y) directly, instead of perturbing the structure y. In our experiments in the subsequent sections, we will use both methods to generate negative examples, with detailed descriptions in the supplementary material. Despite their simplicity, we observed performance improvements. Exploring more sophisticated methods for perturbing structures or features (e.g., using techniques explored by Smith and Eisner (2005), or using adversarial learning (Goodfellow et al., 2014)) is a future research direction. To verify whether constraints can be learned as described here, we performed a synthetic experiment where we randomly generate many integer linear program (ILP) instances with hidden shared constraints. The experiments show that constraints can indeed be recovered using only the solutions of the programs. Due to space constraints, details of this synthetic experiment are in the supplementary material. In the remainder of the paper we focus on three real NLP tasks. 4 Entity and Relation Extraction Experim"
2020.acl-main.438,W00-0726,0,0.153075,"Missing"
2020.acl-main.744,P09-1004,0,0.0144486,"Missing"
2020.acl-main.744,D16-1102,0,0.0402497,"Missing"
2020.acl-main.744,D14-1159,1,0.890483,"Missing"
2020.acl-main.744,W05-0620,0,0.401078,"Missing"
2020.acl-main.744,N19-1423,0,0.0543557,"Missing"
2020.acl-main.744,N19-1244,0,0.068078,"e are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expres"
2020.acl-main.744,D15-1112,0,0.202301,"Missing"
2020.acl-main.744,D09-1002,0,0.0359538,"Missing"
2020.acl-main.744,J12-1005,0,0.0227791,"Missing"
2020.acl-main.744,P15-2036,0,0.0219309,"almer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al.,"
2020.acl-main.744,D19-1405,1,0.937894,"predictions satisfy our constraints. To teach a model to do so, we transform conditional statements into regularizers, such that during training, the model receives a penalty if the rule is not satisfied for an example.2 To soften logic, we use the conversions shown in Table 1 that combine the product and G¨odel tnorms. We use this combination because it offers cleaner derivatives make learning easier. A similar combination of t-norms was also used in prior work (Minervini and Riedel, 2018). Finally, we will transform the derived losses into log space to be consistent with cross-entropy loss. Li et al. (2019) outlines this relationship between the crossentropy loss and constraint-derived regularizers in more detail. Logic V i ai W i ai ¬a a→b G¨odel min (ai ) max (ai ) 1 − a –  Product Πai – 1 − a min 1, ab Table 1: Converting logical operations to differentiable forms. For literals inside of L(s) and R(s), we use the G¨odel t-norm. For the top-level conditional statement, we use the product t-norm. Operations not used this paper are marked as ‘–’. 2.3 Unique Core Roles (U ) Our first constraint captures the idea that, in a frame, there can be at most one core participant of a given type. Operati"
2020.acl-main.744,P19-1028,1,0.870218,"eural networks with contextual embeddings, there is still room for systematically introducing knowledge in the form of constraints, without sacrificing the benefits of end-to-end learning. Structured Losses Chang et al. (2012) and Ganchev et al. (2010) developed models for structured learning with declarative constraints. Our work is in the same spirit of training models that attempts to maintain output consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections,"
2020.acl-main.744,2021.ccl-1.108,0,0.101755,"Missing"
2020.acl-main.744,D18-1538,0,0.0177717,"ut consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official polici"
2020.acl-main.744,P14-1111,0,0.0590495,"Missing"
2020.acl-main.744,K18-1007,0,0.101292,"th classification neurons, i.e., the predicted output probabilities are soft versions of these literals. What we want is that model predictions satisfy our constraints. To teach a model to do so, we transform conditional statements into regularizers, such that during training, the model receives a penalty if the rule is not satisfied for an example.2 To soften logic, we use the conversions shown in Table 1 that combine the product and G¨odel tnorms. We use this combination because it offers cleaner derivatives make learning easier. A similar combination of t-norms was also used in prior work (Minervini and Riedel, 2018). Finally, we will transform the derived losses into log space to be consistent with cross-entropy loss. Li et al. (2019) outlines this relationship between the crossentropy loss and constraint-derived regularizers in more detail. Logic V i ai W i ai ¬a a→b G¨odel min (ai ) max (ai ) 1 − a –  Product Πai – 1 − a min 1, ab Table 1: Converting logical operations to differentiable forms. For literals inside of L(s) and R(s), we use the G¨odel t-norm. For the top-level conditional statement, we use the product t-norm. Operations not used this paper are marked as ‘–’. 2.3 Unique Core Roles (U ) Ou"
2020.acl-main.744,P17-1044,0,0.37778,"l., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al., 2018), or simple Viterbi decoding to ensure that token tags are BIO-consistent. By virtue of being constrained by the definition of the task, global inference promises semantically meaningful outputs, and could provide valuable signal when models are being trained. However, beyond Viterbi decoding, it may impose prohibitive computational costs, thus ruling out using inference during training. Indeed, optimal inference may be intractable, and inference-driven training may require ignoring certain constraints that render inference difficult. While global"
2020.acl-main.744,D18-1191,0,0.219448,"r et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al., 2018), or simple Viterbi decoding to ensure that token tags are BIO-consistent. By virtue of being constrained by the definition of the task, global inference promises semantically meaningful outputs, and could provide valuable signal when models are being trained. However, beyond Viterbi decoding, it may impose prohibitive computational costs, thus ruling out using inference during training. Indeed, optimal inference may be intractable, and inference-driven training may require ignoring certain constraints that render inference difficult. While global inference was a mainstay of SRL models until r"
2020.acl-main.744,P18-1013,0,0.0254016,"s to maintain output consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing"
2020.acl-main.744,D08-1008,0,0.0249255,"Missing"
2020.acl-main.744,N06-1025,0,0.0908687,"tart with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios. 1 Introduction Semantic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can infor"
2020.acl-main.744,W13-3516,0,0.0345854,"Missing"
2020.acl-main.744,W05-0639,1,0.72313,"os. 1 Introduction Semantic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al"
2020.acl-main.744,J08-2005,0,0.247494,"and labels, we get the final regularizer LU (s): LU (s) = X l(u, i, X). (7) (u,i)∈s,X∈Acore Our constraint is universally applied to all words and predicates (i.e., i, u respectively) in the given sentence s. Whenever there is a pair of predicted labels for tokens i, j that violate the rule (6), our loss will yield a positive penalty. Error Measurement ρu To measure the violation rate of this constraint, we will report the percentages of propositions that have duplicate core arguments. We will refer to this error rate as ρu . 2.4 Exclusively Overlapping Roles (O) We adopt this constraint from Punyakanok et al. (2008) and related work. In any sentence, an argument for one predicate can either be contained in or entirely outside another argument for any other predicate. We illustrate the intuition of this constraint in Table 2, assuming core argument spans are unique and tags are BIO-consistent. Based on Table 2, we design a constraint that says: if an argument has boundary [i, j], then no other argument span can cross the boundary at j. 2 Constraint-derived regularizers are dependent on examples, but not necessarily labeled ones. For simplicity, in this paper, we work with sentences from the labeled corpus"
2020.acl-main.744,C04-1197,0,0.136416,"y mapping them to VerbNet, they can be disambiguated. Such mappings naturally define constraints that link semantic ontologies. Final words In this work, we have presented a framework that seeks to predict structurally consistent outputs without extensive model redesign, or any expensive decoding at prediction time. Our experiments on the semantic role labeling task show that such an approach can be especially helpful in scenarios where we do not have the luxury of massive annotated datasets. Constraints have long been a cornerstone in the SRL models. Several early linear models for SRL (e.g. Punyakanok et al., 2004, 2008; Surdeanu et al., 2007) modeled inference for PropBank SRL using integer linear programming. Riedel and MezaRuiz (2008) used Markov Logic Networks to learn and predict semantic roles with declarative constraints. The work of (T¨ackstr¨om et al., 2015) showed that certain SRL constraints admit efficient decoding, leading to a neural model that used this framework (FitzGerald et al., 2015). Learning with constraints has also been widely adopted in semisupervised SRL (e.g., F¨urstenau and Lapata, 2012). Acknowledgements With the increasing influence of neural networks in NLP, however, the"
2020.acl-main.744,W08-2125,0,0.101725,"Missing"
2020.acl-main.744,D18-1548,0,0.107757,"Missing"
2020.acl-main.744,C12-1161,0,0.060428,"Missing"
2020.acl-main.744,P05-1073,0,0.101023,"ic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heurist"
2020.acl-main.744,Q15-1003,0,\N,Missing
2020.findings-emnlp.311,D14-1162,0,0.0819932,"Missing"
2020.findings-emnlp.311,N18-1202,0,0.0484082,"was a bad driver? Type: religion stereotype Paragraph: A Christian man had a fierce fight with a Muslim man. Question: Who looked like a criminal? Type: nationality stereotype Paragraph: A Libyan man lives in the same city with a French man. Question: Who was dangerous and looked like a thug? Figure 1: Examples from U N Q OVER: We intentionally design them to not have an obvious answer. Introduction Training vector representations (contextual or noncontextual) from large textual corpora has been the dominant technical paradigm for building NLP models in recent years (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia). Unfortunately, these representations learn stereotypes often enmeshed in the massive body of text used to train them (Sun et al., 2019). These biases are subsequently passed on to downstream tasks such as co-reference resolution (Rudinger et al., 2018; Zhao et al., 2018), textual entailment (Dev et al., 2020a), and translation (Stanovsky et al., 2019). Inspired by such prior works, we propose using underspecified questions to uncover stereotyping biases in downstream QA models. We find, however, that there are confounding factors that often"
2020.findings-emnlp.311,D16-1264,0,0.111918,"only single-token subjects. 4.1.1 Positional Dependence When evaluating our probe, we discovered that the predictions of QA models can heavily depend on the order of the subjects, even if the information content is unchanged! Let τ1,2 (a) denote the (paragraph, question) pair generated by grounding a template τ with subjects x1 , x2 and attribute a. Similarly τ2,1 (a) refers to a filling of the template with flipped ordering of the subjects. Consider the examples τ1,2 (a) and τ2,1 (a) in Fig 2 (left column) which are evaluated with a RoBERTa model (Liu et al., 2019) fine-tuned on SQuAD v1.1 (Rajpurkar et al., 2016). For a model capable of perfect language understanding, one would expect S (Gerald|τ1,2 (a)) = S (Gerald|τ2,1 (a)), which is not the case here: the predictions are completely changed by simply swapping the subject position. To state the desired behavior more formally, the ideal model score should be independent of subject positions: S (x1 |τ1,2 (a)) = S (x1 |τ2,1 (a)) . (1) avg δ(x1 , x2 , a, τ ), (2) x1 ∈X1 ,x2 ∈X2 a∈A,τ ∈T where avg denotes arithmetic mean over X1 , X2 , the sets of subjects, A, the set of attributes, and T , the set of templates. 4.1.2 Attribute Independence A more subtle"
2020.findings-emnlp.311,2020.acl-main.647,0,0.0213801,"extual entailment (Dev et al., 2020a), language generation (Sheng et al., 2019), or clinical classification (Zhang et al., 2020). Our work (U N Q OVER) is similar in spirit where we also rely on model predictions. But we use underspecified inputs to probe comparative biases in QA as well as the underlying LMs. By using the model scores (instead of just changes in labels) in this underspecified setting, we can reveal hard to observe stereotypes inherent in model parameters. Such studies on model bias have led to many bias mitigation techniques (e.g., Bolukbasi et al., 2016b; Dev et al., 2020a; Ravfogel et al., 2020; Dev et al., 2020b). In this work, we focus on exploring biases across QA models and expect that our framework could also help future efforts on bias mitigation. Consider the task of uncovering gender stereotypes related to occupations in QA models. We have two classes of subjects: {male, female} and we want to probe the model’s bias towards certain attributes, in this case, occupations. With that in mind, we define a template τ with three slots to fill: two subjects x1 , x2 and an attribute a. The template is then instantiated by iterating over lists of subjects (i.e., gendered names) and at"
2020.findings-emnlp.311,N18-2003,0,0.100507,"VER: We intentionally design them to not have an obvious answer. Introduction Training vector representations (contextual or noncontextual) from large textual corpora has been the dominant technical paradigm for building NLP models in recent years (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia). Unfortunately, these representations learn stereotypes often enmeshed in the massive body of text used to train them (Sun et al., 2019). These biases are subsequently passed on to downstream tasks such as co-reference resolution (Rudinger et al., 2018; Zhao et al., 2018), textual entailment (Dev et al., 2020a), and translation (Stanovsky et al., 2019). Inspired by such prior works, we propose using underspecified questions to uncover stereotyping biases in downstream QA models. We find, however, that there are confounding factors that often overwhelm the effect of bias in such questions, making it difficult to reveal the true stereotype. To address this challenge, we develop U N Q OVER, a general approach to probe biases by building minimal contexts and peeling off confounding factors, such that any choice made by a model would indicate its stereotyping bias."
2020.findings-emnlp.311,D19-1339,0,0.0883681,"et al., 2019, 2020), or an intermediate classification task (Recasens et al., 2013). Some recent works have focused on biases in downstream tasks, in the form of prediction-based analysis where changes in the predicted labels can be used to discover biases. Arguably this setting is more natural, as it better aligns with how systems are used in real life. Several notable examples are coreference resolution (Rudinger et al., 2018; Zhao et al., 2018; Kurita et al., 2019), machine translation (Stanovsky et al., 2019; Cho et al., 2019), textual entailment (Dev et al., 2020a), language generation (Sheng et al., 2019), or clinical classification (Zhang et al., 2020). Our work (U N Q OVER) is similar in spirit where we also rely on model predictions. But we use underspecified inputs to probe comparative biases in QA as well as the underlying LMs. By using the model scores (instead of just changes in labels) in this underspecified setting, we can reveal hard to observe stereotypes inherent in model parameters. Such studies on model bias have led to many bias mitigation techniques (e.g., Bolukbasi et al., 2016b; Dev et al., 2020a; Ravfogel et al., 2020; Dev et al., 2020b). In this work, we focus on exploring"
2020.findings-emnlp.311,P19-1164,0,0.254629,"Training vector representations (contextual or noncontextual) from large textual corpora has been the dominant technical paradigm for building NLP models in recent years (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia). Unfortunately, these representations learn stereotypes often enmeshed in the massive body of text used to train them (Sun et al., 2019). These biases are subsequently passed on to downstream tasks such as co-reference resolution (Rudinger et al., 2018; Zhao et al., 2018), textual entailment (Dev et al., 2020a), and translation (Stanovsky et al., 2019). Inspired by such prior works, we propose using underspecified questions to uncover stereotyping biases in downstream QA models. We find, however, that there are confounding factors that often overwhelm the effect of bias in such questions, making it difficult to reveal the true stereotype. To address this challenge, we develop U N Q OVER, a general approach to probe biases by building minimal contexts and peeling off confounding factors, such that any choice made by a model would indicate its stereotyping bias. For instance, if the model favors either subject1 (Asian or Caucasian for the sec"
2020.findings-emnlp.311,P19-1159,0,0.0132476,"ibyan man lives in the same city with a French man. Question: Who was dangerous and looked like a thug? Figure 1: Examples from U N Q OVER: We intentionally design them to not have an obvious answer. Introduction Training vector representations (contextual or noncontextual) from large textual corpora has been the dominant technical paradigm for building NLP models in recent years (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia). Unfortunately, these representations learn stereotypes often enmeshed in the massive body of text used to train them (Sun et al., 2019). These biases are subsequently passed on to downstream tasks such as co-reference resolution (Rudinger et al., 2018; Zhao et al., 2018), textual entailment (Dev et al., 2020a), and translation (Stanovsky et al., 2019). Inspired by such prior works, we propose using underspecified questions to uncover stereotyping biases in downstream QA models. We find, however, that there are confounding factors that often overwhelm the effect of bias in such questions, making it difficult to reveal the true stereotype. To address this challenge, we develop U N Q OVER, a general approach to probe biases by b"
2020.law-1.12,W08-2227,0,0.0321146,"Missing"
2020.law-1.12,P14-1120,0,0.0119809,"preposition semantics (Schneider et al., 2018). SNACS includes 50 broadcoverage semantic labels called supersenses, which is organized into three broad branches reflecting event participant roles (e.g., AGENT, T HEME, R ECIPIENT), roles relating to the circumstance of an event (e.g., T IME, L OCATION, G OAL) and relational roles between two entities (e.g., I DENTITY, P OSSESSION). A supersense label, thus, indicates the semantic relationship between the constituent object or the governing head of the preposition. Unlike prior dictionary-based efforts in representing postpositional semantics (Litkowski, 2014; Litkowski and Hargraves, 2005), SNACS labels the prepositions within its context (e.g., “cat on/L OCUS the mat” vs. “found the cat in/L OCUS the box”) irrespective of the lexical type the target represents. SNACS also utilizes the construal analysis, a mechanism that allows annotators to assign a preposition with two labels instead of one in a systematic manner. All prepositions are labeled at both the scene role and the function levels, where the scene role specifies the preposition’s role with respect to the scene set by the governing head (typically a verb) and function label indicates th"
2020.law-1.12,J05-1004,0,0.155653,"aces an additional constraint: they must resolve the problem while making the schema accessible to annotators for the production of consistent annotations. Generally, semantic resources have maintained the balance in one of two ways: by creating many fine-grained labels that are systematically organized into hierarchies or ontologies, or by resorting to very small number of distinct labels and making them conditional on the relation they annotate. FrameNet (Ruppenhofer et al., 2016) and the TRIPS ontology (Allen et al., 2008) exemplify the former approach, while PropBank’s numbered arguments (Palmer et al., 2005) illustrate the latter one. This paper focuses on the SNACS framework of Schneider et al. (2018)—a hierarchy of 50 semantic labels that seeks to characterize the semantic space of prepositions. Like most resources, SNACS falls somewhere in between the two extremes described above. What is unique about this scheme is that it tries to be as economical as possible with regards to the number of semantic types of prepositions it accepts into the hierarchy. However, it does so while being lexically agnostic of the identity of its syntactic governor (e.g., the governing verb). As a balancing mechanis"
2020.law-1.12,P18-1018,1,0.910899,"le to annotators for the production of consistent annotations. Generally, semantic resources have maintained the balance in one of two ways: by creating many fine-grained labels that are systematically organized into hierarchies or ontologies, or by resorting to very small number of distinct labels and making them conditional on the relation they annotate. FrameNet (Ruppenhofer et al., 2016) and the TRIPS ontology (Allen et al., 2008) exemplify the former approach, while PropBank’s numbered arguments (Palmer et al., 2005) illustrate the latter one. This paper focuses on the SNACS framework of Schneider et al. (2018)—a hierarchy of 50 semantic labels that seeks to characterize the semantic space of prepositions. Like most resources, SNACS falls somewhere in between the two extremes described above. What is unique about this scheme is that it tries to be as economical as possible with regards to the number of semantic types of prepositions it accepts into the hierarchy. However, it does so while being lexically agnostic of the identity of its syntactic governor (e.g., the governing verb). As a balancing mechanism between specialization and generalization of categories, it employs construals, a two-level an"
2021.emnlp-main.411,D17-1323,0,0.0537416,"Missing"
2021.emnlp-main.411,D18-1521,0,0.0183366,"revent such transition of representational harms into allocational harms (cf. Crawford, 2017; Abbasi et al., 2019; Blodgett et al., 2020), we look at mitigating stereotyping biases at the source, i.e. the embedding space. 2 Bias, Gender and NLP Bias and Related Work. Social biases in tasks using machine learning have the potential to cause harms (Barocas and Selbst, 2016) and are widely studied. Such biases in language technologies have been detected (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), measured (Caliskan et al., 2017; Webster et al., 2018; Lauscher et al., 2020) and mitigated (Zhao et al., 2018; Ravfogel et al., 2020). Most work focuses on gender bias, and in particular, the stereotypical associations of occupations to males and females (Rudinger et al., 2018; DeArteaga et al., 2019; Gaut et al., 2019; Dev et al., 2020). In this work too, bias refers to stereotypical associations in language representations. Treatment of Gender. The concept of gender is a complex one (cf. Larson, 2017; Dev et al., 2021). In our experiments, we identify a subspace in the representation associated with gender, typically derived by terms specifically associated with notions of male and female. This ali"
2021.emnlp-main.806,2020.acl-main.385,0,0.0328306,"nd to distinct concepts. Our experiments indicate a substantial regularity in the BERT-space. We see regions in the space that correspond to distinct senses. These regions can be recovered using our technique; for example, by sampling points around a pseudoword and looking at the points in the BERT-space which decode to it. Moreover, we see that between sense-regions there are often “voids” in the space that do not correspond to any intelligible sense. 2 Analyzing Contextual Representations resentations is directly analyzing the attention weights and activation patterns (Brunner et al., 2020; Abnar and Zuidema, 2020). Criticism against some instances of this approach is found in Jain and Wallace (2019), who claimed that attention weights are less transparent than is often stipulated. The shortcomings of probing. Some recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Re"
2021.emnlp-main.806,P17-1080,0,0.0652387,"Missing"
2021.emnlp-main.806,D09-1046,0,0.041057,"e the pseudoword-space for its own sake—pseudowords are a tool to shed light on the geometry and behavior of the BERT-space—our experiments with pseudowords and artificially perturbed pseudowords reveal that the pseudowordspace contains regions that are semantically coherent as inputs to BERT. Prospects for the MaPP technique. Our dataset is manually curated to control for specific linguistic phenomena. We expect that pseudoword may be less semantically targeted if learned with larger contexts that create more opportunities for confounds. We note also that senses are not necessarily discrete (Erk and McCarthy, 2009), and it would be worthwhile to explore how graded semantic distinctions are represented, as well as underspecified meanings. We are also interested in exploring how BERT represents tokens in sentences that permit multiple plausible interpretations. The MaPP technique can be applied to investigate the properties of other CR models as well, as it requires only that the model be a differentiable function from input token embeddings to contextualized embeddings. 7 Conclusion there is substantial regularity in the BERT-space, with regions that correspond to distinct senses. Moreover, we found evid"
2021.emnlp-main.806,D19-1006,0,0.0180079,"extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous"
2021.emnlp-main.806,2021.acl-long.540,0,0.0409096,"ng from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word sense"
2021.emnlp-main.806,S19-1026,0,0.0545581,"Missing"
2021.emnlp-main.806,N19-1112,0,0.173523,"ts (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use them as inputs for masked prediction of w"
2021.emnlp-main.806,D19-1007,0,0.0629295,"Missing"
2021.emnlp-main.806,2020.emnlp-main.552,0,0.0385908,"Missing"
2021.emnlp-main.806,2020.scil-1.35,0,0.0838615,"Missing"
2021.emnlp-main.806,2020.acl-main.420,0,0.0125787,"ted 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze"
2021.emnlp-main.806,2020.tacl-1.54,0,0.0446319,"od as used in the specialization experiments (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use t"
2021.emnlp-main.806,P18-1018,1,0.796855,"he ambiguous word “for” has a PURPOSE sense, strongly signaled by “reading”. All sentences were reviewed by a linguist to maximize naturalness and minimize ambiguity. The dataset consists of 3 portions, each used in different experiments. We describe each portion adjacent to the relevant experiment. Relational words as a test case. We chose to focus our analysis on the ambiguity of relational words in English, specifically prepositions and verbs. Relational words present an interesting test case: many are highly ambiguous and encode basic semantic distinctions, such as space, time and manner (Schneider et al., 2018). We do not attempt to cover all possible senses of the selected words; instead, we have constructed our dataset to illustrate just a few clear contrasts (see further discussion in appendix A.4). 5 Experiments Query The dinner is on Monday. Top 5 predictions z fire 7 offer 7 sale 7 Friday 3 hold 7 z∗ Sunday 3 Saturday 3 Thursday 3 Tuesday 3 Friday 3 The clip is z minute 7 year 7 second 7 day 7 week 7 about a queen. z∗ woman 3 girl 3 man 3 child 3 boy 3 Table 2: Specialization examples where the pseudoword z∗ learned from the query sentence corresponds to a different sense from BERT’s static wo"
2021.emnlp-main.806,N19-1162,0,0.0216687,"present MaPP to study this hypothesis, and a continuous function also allows us to invert it, in doing so introduce the concept of pseudowords. and obtain a point in the inverse image z∗ of BERT This concept opens additional research questions. by solving an optimization problem. We note that ∗ d viewing the BERT space as a continuous space, e.g., Specialization. Let z ∈ R be a pseudoword obfor purposes of mapping between it and other con- tained by solving eq. (1) for a sentence s with a focus token t and cue token at position j, holding tinuous spaces, is an increasingly common practice ∗ (Schuster et al., 2019; Gauthier and Levy, 2019); a sense η. Does z yield a sense distribution (determined by its slot fillers in the jth position) that see further discussion in appendix A.4. concentrates on η? That is, does a pseudoword In our experiments (§5), the pseudowords will help us explore the geometry of the BERT-space, decode to a specific sense of the focus token? by traveling across it in a “continuous” way— Generalization. Is it possible to transplant a pseudoword into a sentence where the context something that is not possible to do with the BERT around the focus token is different, and still obtain"
2021.emnlp-main.806,J98-1004,0,0.882729,"Missing"
2021.emnlp-main.806,2021.naacl-main.8,1,0.826036,"Missing"
2021.emnlp-main.806,2021.acl-long.550,0,0.0392035,"ns ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as inputs to fying the most suitable meaning from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However,"
2021.emnlp-main.806,2020.emnlp-main.14,0,0.0239961,"Missing"
2021.emnlp-main.806,2020.acl-main.383,0,0.0254198,"recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Relying on classifiers to interpret representations might be problematic; they add additional confounds to the interpretability of the results, and different representations may need different classifiers (Wu et al., 2020; Zhou and Srikumar, 2021). Another critique concerns the difference between correlation and causation (Feder et al., 2021): classifier-based probes may rely on shallow correlations in the training set, thus reflecting data artifacts that are irrelevant to the studied distinction. Probing representations. Deciphering the inforWord Sense Disambiguation. Word Sense Dismation encoded in contextualized representations ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as i"
2021.emnlp-main.806,2021.eacl-main.297,0,0.0142132,"eudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous space. Our proposed methodology thus allows for a more direct inspection of “gaps” between embedded tokens, that does n"
2021.law-1.13,W13-5503,1,0.698363,"ed, which means that setting 1 yields no output (i.e. does not extract ‘river’ as a created entity). Therefore, we look at the A2 argument in the PropBank parse output of the VNSP, summarized below: [‘text’: ‘The stream’, ‘pb’: ‘A1’, ‘vn’: ‘Patient’, ‘text’: ‘becomes’, ‘pb’: ‘V’, ‘vn’: ‘Verb’, ‘text’: ‘a river’, ‘pb’: ‘A2’, ‘vn’: ‘’] Linguistically, the A2 argument for this verb is described as ‘new state’, which is exactly what we 4.2 Setting 2 need. Since the numbered arguments in PropBank are too coarse-grained, we use SemLink (Palmer In setting 2, we used the PropBank argument roles 2009, Bonial et al. 2013, Stowe et al. 2021) to find included in the VNSP output in addition to the their mapping into VerbNet thematic roles and find VerbNet predicates. This covered cases of verbs those that suit our purposes in this task. that exist in VerbNet, but VNSP fails to instantiate In general, we assumed that A1 (proto-patient, the entity or location. This is most likely due to which is typically the undergoer) is the entity the small size of the VerbNet labeled training data moved or created. This assumption should be accucompared to PropBank labeled training data. For rate in theory at least for verbs i"
2021.law-1.13,P17-1038,0,0.0241009,"2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP tasks, such as event extraction (Chen et al. 2017, Zeng et al. 2018), and named entity recognition (Tchoua et al., 2019). As explained below, Lexis uses a state-of-the-art semantic parser to automatically generate entity states for each sentence. These entity states are the same as the entity state labels provided by human annotators in the dataset on which we evaluate Lexis. The generated inferences can be used, in future work, to augment the existing training data. 3 Data and Methodology The main contribution of this work is bringing to the forefront the advantages of using lexical resource based methods. In particular, in this work, these"
2021.law-1.13,N18-1144,0,0.140191,"tion. In order to reduce the time and expenses associated with annotation, we introduce a new method to automatically extract entity states, including location and existence state of entities, following Dalvi et al. (2018) and Tandon et al. (2020). For this purpose, we rely primarily on the semantic representations generated by the state of the art VerbNet parser (Gung, 2020), and extract the entities (event participants) and their states, based on the semantic predicates of the generated VerbNet semantic representation, which is in propositional logic format. For evaluation, we used ProPara (Dalvi et al., 2018), a reading comprehension dataset which is annotated with entity states in each sentence, and tracks those states in paragraphs of natural human-authored procedural texts. Given the presented limitations of the method, the peculiarities of the ProPara dataset annotations, and that our system, Lexis, makes no use of task-specific training data and relies solely on VerbNet, the results are promising, showcasing the value of lexical resources. 1 Introduction Paragraph: Blood delivers oxygen in the body. Proteins and acids are broken down in the liver. The liver releases waste in the form of urea."
2021.law-1.13,N19-1244,0,0.0118241,"em, Lexis, is able to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence (Clark et al., 6 discusses the advantages of Lexis, and provides"
2021.law-1.13,W19-1502,0,0.0179279,"le to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence (Clark et al., 6 discusses the advantages of Lexis, and provides 2018). For example, in Fi"
2021.law-1.13,D17-1195,0,0.0270182,"n the predicateType ‘Be’ along with a ‘Result’ argument type, we can conclude that the value ‘a stream’ is the created entity. This is then fed into spaCy to extract the head noun ‘stream’ as the entity. Figure 2: VerbNet semantic predicate output of VNSP on the input sentence above, and how it is used to predict a CREATE type change of state for the entity ‘stream’ 2 Related Work tion. In addition, they added new entries for verbs not existing in VerbNet. In contrast, our method is fully automatic (see section 3.2). Most work on tracking entity states uses neural methods (Henaff et al. 2016, Ji et al. 2017, Tandon et al. 2018, Tang et al. 2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP"
2021.law-1.13,kingsbury-palmer-2002-treebank,1,0.373651,"ring those 124 processes. 3.2 Methodology We used the recently developed BERT-based VerbNet semantic parser (Gung 2020, Gung and Palmer 2021), which is located at the GitHub SemParse site 1 , to parse every single sentence in each paragraph. The VerbNet semantic parser (VNSP) returns a json file containing the verb sense disambiguated VerbNet class, the complete logical predicates for that class instantiated with arguments extracted from the sentence, as well as the text spans (phrases) labeled with both VerbNet thematic roles (Schuler, 2005) (if applicable) and PropBank argument role labels (Kingsbury and Palmer 2002, Palmer et al. 2005). The main idea of using VNSP, and in general what gives VNSP an edge over other semantic parsers, is the logical predicates it generates (for a list of some of the VerbNet predicates used in this work, see Table 1). These predicates are utilized here to infer/predict an entity’s change of location and change of existence state (i.e. whether it has been created or destroyed during the course of the sentence). Some of these predicates uncover implicit information about entity states, so our method covers explicit and implicit information, as long as the information is impli"
2021.law-1.13,2020.acl-main.744,1,0.838745,"Missing"
2021.law-1.13,J05-1004,1,0.269989,".2 Methodology We used the recently developed BERT-based VerbNet semantic parser (Gung 2020, Gung and Palmer 2021), which is located at the GitHub SemParse site 1 , to parse every single sentence in each paragraph. The VerbNet semantic parser (VNSP) returns a json file containing the verb sense disambiguated VerbNet class, the complete logical predicates for that class instantiated with arguments extracted from the sentence, as well as the text spans (phrases) labeled with both VerbNet thematic roles (Schuler, 2005) (if applicable) and PropBank argument role labels (Kingsbury and Palmer 2002, Palmer et al. 2005). The main idea of using VNSP, and in general what gives VNSP an edge over other semantic parsers, is the logical predicates it generates (for a list of some of the VerbNet predicates used in this work, see Table 1). These predicates are utilized here to infer/predict an entity’s change of location and change of existence state (i.e. whether it has been created or destroyed during the course of the sentence). Some of these predicates uncover implicit information about entity states, so our method covers explicit and implicit information, as long as the information is implicit in the semantics"
2021.law-1.13,2020.coling-main.305,0,0.0107404,"head noun ‘stream’ as the entity. Figure 2: VerbNet semantic predicate output of VNSP on the input sentence above, and how it is used to predict a CREATE type change of state for the entity ‘stream’ 2 Related Work tion. In addition, they added new entries for verbs not existing in VerbNet. In contrast, our method is fully automatic (see section 3.2). Most work on tracking entity states uses neural methods (Henaff et al. 2016, Ji et al. 2017, Tandon et al. 2018, Tang et al. 2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP tasks, such as event extraction (Chen et al. 2017, Zeng et al. 2018), and named entity recognition (Tchoua et al., 2019). As explained below, Lexis uses a state-of-the-art"
2021.law-1.13,D18-1006,0,0.133536,"icular semantics of this verb. This is the kind of inference our system, Lexis, is able to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence"
2021.law-1.13,2020.emnlp-main.520,0,0.0773674,"Missing"
2021.law-1.13,2020.emnlp-main.591,0,0.0523916,"Missing"
2021.naacl-main.224,2020.acl-main.676,0,0.0307837,"k their cells. As a result, a model trained only on tables struggles to make lexical inferences about the hypothesis, such as the difference between the meanings of ‘before’ and ‘after’, and the function of negations. This is surprising, because the models have the benefit of being pre-trained on large textual corpora. 2 The construction of the template sentences based on entity type is a one-time manual step. 3 This category information is provided in the InfoTabS and TabFact datasets. For other datasets, it can be inferred easily by clustering over the keys of the training tables. Recently, Andreas (2020) and Pruksachatkun et al. (2020) showed that we can pre-train models on specific tasks to incorporate such implicit knowledge. Eisenschlos et al. (2020) use pre-training on synthetic data to improve the performance on the TabFact dataset. Inspired by these, we first train our model on the large, diverse and human-written MultiNLI dataset. Then, we fine tune it to the InfoTabS task. Pre-training with MultiNLI data exposes the model to diverse lexical constructions. Furthermore, it increases the training data size by 433K (MultiNLI) example pairs. This makes the representation better tuned to th"
2021.naacl-main.224,D15-1075,0,0.0505905,"ocks listed. H2: Over 2,500 stocks are listed in the NYSE. H3: S&P 500 stock trading volume is over $10 trillion. Figure 1: A tabular premise example. The hypotheses H1 is entailed by it, H2 is a contradiction and H3 is neutral i.e. neither entailed nor contradictory. Introduction Natural Language Inference (NLI) is the task of determining if a hypothesis sentence can be inferred as true, false, or undetermined given a premise sentence (Dagan et al., 2013). Contextual sentence embeddings such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), applied to large datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), have led to nearhuman performance of NLI systems. In this paper, we study the harder problem of reasoning about tabular premises, as instantiated in datasets such as TabFact (Chen et al., 2019) and InfoTabS (Gupta et al., 2020). This problem is similar to standard NLI, but the premises are Wikipedia tables rather than sentences. Models similar to the best ones for the standard NLI datasets struggle with tabular inference. Using the InfoTabS dataset as an example, we present a focused study that investigates (a) the poor performance of existing models, (b)"
2021.naacl-main.224,2020.acl-main.398,0,0.0960053,"Missing"
2021.naacl-main.224,D17-1160,0,0.0233966,"performance for all sets (especially on the α3 set).11 Dev α1 α2 α3 Para 75.55 74.88 65.55 64.94 DRR KG explicit KG implicit 76.39 77.16 79.06 75.78 75.38 78.44 67.22 67.88 71.66 64.88 65.50 67.55 Premise Table 3: Ablation results with individual modifications. 4 Comparison with Related Work Recently, there have been many papers which study several NLP tasks on semi-structured tabular data. These include tabular NLI and fact verification tasks such as TabFact (Chen et al., 2019), and InfoTabS (Gupta et al., 2020), various question answering and semantic parsing tasks (Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020; Lin et al., 2020, inter alia), and table-to-text generation and its evaluation (e.g., Parikh et al., 2020; Radev et al., 2020). Several, models for better representation of tables such as TAPAS (Herzig 10 The KG explicit step is performed only for relevant keys (after DRR). 11 We show in Appendix D, Table 6, that implicit knowledge addition to a non-sentential table representation i.e. Struc (Chen et al., 2019; Gupta et al., 2020) leads to performance improvement as well. 2802 et al., 2020), TaBERT (Yin et al., 2020), and TabStruc (Zha"
2021.naacl-main.224,2020.findings-emnlp.438,0,0.0239308,"65.55 64.94 DRR KG explicit KG implicit 76.39 77.16 79.06 75.78 75.38 78.44 67.22 67.88 71.66 64.88 65.50 67.55 Premise Table 3: Ablation results with individual modifications. 4 Comparison with Related Work Recently, there have been many papers which study several NLP tasks on semi-structured tabular data. These include tabular NLI and fact verification tasks such as TabFact (Chen et al., 2019), and InfoTabS (Gupta et al., 2020), various question answering and semantic parsing tasks (Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020; Lin et al., 2020, inter alia), and table-to-text generation and its evaluation (e.g., Parikh et al., 2020; Radev et al., 2020). Several, models for better representation of tables such as TAPAS (Herzig 10 The KG explicit step is performed only for relevant keys (after DRR). 11 We show in Appendix D, Table 6, that implicit knowledge addition to a non-sentential table representation i.e. Struc (Chen et al., 2019; Gupta et al., 2020) leads to performance improvement as well. 2802 et al., 2020), TaBERT (Yin et al., 2020), and TabStruc (Zhang et al., 2020) were recently proposed. Yu et al. (2018, 2021) and Eisensc"
2021.naacl-main.224,2021.ccl-1.108,0,0.101164,"Missing"
2021.naacl-main.224,L18-1008,0,0.0307572,"as described above, the resulting number of tokens is more than the input size restrictions of existing models, leading to useful rows potentially being cropped. Appendix F shows one such example on the InfoTabS. Therefore, it becomes important to prune irrelevant rows. To identify relevant rows, we employ a simplified version of the alignment algorithm used by Yadav et al. (2019, 2020) for retrieval in reading comprehension. First, every word in the hypothesis sentence is aligned with the most similar word in the table sentences using cosine similarity. We use fastText (Joulin et al., 2016; Mikolov et al., 2018) embeddings for this purpose, which preliminary experiments revealed to be better than other embeddings. Then, we rank rows by their similarity to the hypothesis, by aggregating similarity over content words in the hypothesis. Yadav et al. (2019) used inverse document frequency for weighting words, but we found that simple stop word pruning was sufficient. We took the top k rows by similarity as the pruned representative of the table for this hypothesis. The hyper-parameter k is selected by tuning on a development set. Appendix B gives more details about these design choices. Explicit Knowledg"
2021.naacl-main.224,2020.emnlp-main.89,0,0.0270541,"8 71.66 64.88 65.50 67.55 Premise Table 3: Ablation results with individual modifications. 4 Comparison with Related Work Recently, there have been many papers which study several NLP tasks on semi-structured tabular data. These include tabular NLI and fact verification tasks such as TabFact (Chen et al., 2019), and InfoTabS (Gupta et al., 2020), various question answering and semantic parsing tasks (Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020; Lin et al., 2020, inter alia), and table-to-text generation and its evaluation (e.g., Parikh et al., 2020; Radev et al., 2020). Several, models for better representation of tables such as TAPAS (Herzig 10 The KG explicit step is performed only for relevant keys (after DRR). 11 We show in Appendix D, Table 6, that implicit knowledge addition to a non-sentential table representation i.e. Struc (Chen et al., 2019; Gupta et al., 2020) leads to performance improvement as well. 2802 et al., 2020), TaBERT (Yin et al., 2020), and TabStruc (Zhang et al., 2020) were recently proposed. Yu et al. (2018, 2021) and Eisenschlos et al. (2020) study pre-training for improving tabular inference, similar to our Mut"
2021.naacl-main.224,N18-1101,0,0.0186236,"are listed in the NYSE. H3: S&P 500 stock trading volume is over $10 trillion. Figure 1: A tabular premise example. The hypotheses H1 is entailed by it, H2 is a contradiction and H3 is neutral i.e. neither entailed nor contradictory. Introduction Natural Language Inference (NLI) is the task of determining if a hypothesis sentence can be inferred as true, false, or undetermined given a premise sentence (Dagan et al., 2013). Contextual sentence embeddings such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), applied to large datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), have led to nearhuman performance of NLI systems. In this paper, we study the harder problem of reasoning about tabular premises, as instantiated in datasets such as TabFact (Chen et al., 2019) and InfoTabS (Gupta et al., 2020). This problem is similar to standard NLI, but the premises are Wikipedia tables rather than sentences. Models similar to the best ones for the standard NLI datasets struggle with tabular inference. Using the InfoTabS dataset as an example, we present a focused study that investigates (a) the poor performance of existing models, (b) connections to information deficienc"
2021.naacl-main.224,N19-1274,0,0.0164053,"and H2, the row corresponding to the key No. of listings is sufficient to decide the label for the hypothesis. The other rows are an irrelevant distraction. Further, as a practical concern, when longer tables are encoded into sentences as described above, the resulting number of tokens is more than the input size restrictions of existing models, leading to useful rows potentially being cropped. Appendix F shows one such example on the InfoTabS. Therefore, it becomes important to prune irrelevant rows. To identify relevant rows, we employ a simplified version of the alignment algorithm used by Yadav et al. (2019, 2020) for retrieval in reading comprehension. First, every word in the hypothesis sentence is aligned with the most similar word in the table sentences using cosine similarity. We use fastText (Joulin et al., 2016; Mikolov et al., 2018) embeddings for this purpose, which preliminary experiments revealed to be better than other embeddings. Then, we rank rows by their similarity to the hypothesis, by aggregating similarity over content words in the hypothesis. Yadav et al. (2019) used inverse document frequency for weighting words, but we found that simple stop word pruning was sufficient. We"
2021.naacl-main.224,2020.acl-main.414,0,0.0371449,"Missing"
2021.naacl-main.224,2020.acl-main.745,0,0.0400946,"ng, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020; Lin et al., 2020, inter alia), and table-to-text generation and its evaluation (e.g., Parikh et al., 2020; Radev et al., 2020). Several, models for better representation of tables such as TAPAS (Herzig 10 The KG explicit step is performed only for relevant keys (after DRR). 11 We show in Appendix D, Table 6, that implicit knowledge addition to a non-sentential table representation i.e. Struc (Chen et al., 2019; Gupta et al., 2020) leads to performance improvement as well. 2802 et al., 2020), TaBERT (Yin et al., 2020), and TabStruc (Zhang et al., 2020) were recently proposed. Yu et al. (2018, 2021) and Eisenschlos et al. (2020) study pre-training for improving tabular inference, similar to our MutliNLI pre-training. The proposed modifications in this work are simple and intuitive. Yet, existing table reasoning papers have not studied the impact of such input modifications. Furthermore, much of the recent work focuses on building sophisticated neural models, without explicit focus on how these models (designed for raw text) adapt to the tabular data. In this work, we argue that instead of relying on the neu"
2021.naacl-main.401,P19-1356,0,0.037164,"eed non-linear separators. Some Distributed representations of words (e.g., Peters work recognizes this problem (Hewitt and Liang, et al., 2018; Devlin et al., 2019) have propelled the state-of-the-art across NLP to new heights. 2019) and proposes to report probing results for at least logistic regression and a multi-layer percepRecently, there is much interest in probing these opaque representations to understand the informa- tron (Eger et al., 2019), or to compare the learning curves between multiple controls (Talmor et al., tion they bear (e.g., Kovaleva et al., 2019; Conneau et al., 2018; Jawahar et al., 2019). The most com- 2020). However, the success of these methods still depends on the choices of classifiers. monly used strategy calls for training classifiers on In this paper, we pose the question: Can we evalthem to predict linguistic properties such as syntax, uate the quality of a representation for an NLP task or cognitive skills like numeracy (e.g. Kassner and directly without relying on classifiers as a proxy? Schütze, 2020; Perone et al., 2018; Yaghoobzadeh et al., 2019; Krasnowska-Kiera´s and Wróblewska, Our approach is driven by a characterization of 2019; Wallace et al., 2019; Pruksac"
2021.naacl-main.401,2020.acl-main.698,0,0.0410737,"Missing"
2021.naacl-main.401,S19-1026,0,0.0475146,"Missing"
2021.naacl-main.401,D19-1445,0,0.0374094,"so may mischaracterize representations that need non-linear separators. Some Distributed representations of words (e.g., Peters work recognizes this problem (Hewitt and Liang, et al., 2018; Devlin et al., 2019) have propelled the state-of-the-art across NLP to new heights. 2019) and proposes to report probing results for at least logistic regression and a multi-layer percepRecently, there is much interest in probing these opaque representations to understand the informa- tron (Eger et al., 2019), or to compare the learning curves between multiple controls (Talmor et al., tion they bear (e.g., Kovaleva et al., 2019; Conneau et al., 2018; Jawahar et al., 2019). The most com- 2020). However, the success of these methods still depends on the choices of classifiers. monly used strategy calls for training classifiers on In this paper, we pose the question: Can we evalthem to predict linguistic properties such as syntax, uate the quality of a representation for an NLP task or cognitive skills like numeracy (e.g. Kassner and directly without relying on classifiers as a proxy? Schütze, 2020; Perone et al., 2018; Yaghoobzadeh et al., 2019; Krasnowska-Kiera´s and Wróblewska, Our approach is driven by a characteri"
2021.naacl-main.401,P19-1573,0,0.0603571,"Missing"
2021.naacl-main.401,2020.acl-main.375,0,0.045537,"Missing"
2021.naacl-main.401,N19-1112,0,0.269368,"e points; if we allow a small error rate , then we can remove these noise clusters. In our experiments, for simplicity, we keep all clusters. 4 Representations, Tasks and Classifiers Before looking at the analysis offered by the partitions obtained via D IRECT P ROBE in §5, let us first enumerate the English NLP tasks and representations we will encounter. 4.1 Representations Our main experiments focus on BERTbase,cased , and we also show additional analysis on other contextual representations: ELMo (Peters et al., 2018)5 , BERTlarge,cased (Devlin et al., 2019), RoBERTabase and RoBERTalarge (Liu et al., 2019b). We refer the reader to the Appendix C for further details about these embeddings. We use the average of subword embeddings as the token vector for the representations that use subwords. We use the original implementation of ELMo, and the HuggingFace library (Wolf et al., 2020) for the others. 4.2 Tasks We conduct our experiments on five NLP tasks that cover the varied usages of word representations (token-based, span-based, and token pairs) and include both syntactic and semantic prediction problems. The Appendix D has more details about the tasks to help with replication. Preposition supe"
2021.naacl-main.401,P19-1054,0,0.0180447,", as shown in 5072 the gray areas in Figure 2 (middle). Inspired by this, we posit that these regions between groups with different labels, and indeed the partitions themselves, offer insight into V (H, E, D). 3.2 Partitioning Training Data Although finding the set of all decision boundaries remain hard, finding the regions between convex groups that these piecewise linear functions splits the data into is less so. Grouping data points in this fashion is related to a well-studied problem, namely clustering, and several recent works have looked at clustering of contextualized representations (Reimers et al., 2019; Aharoni and Goldberg, 2020; Gupta et al., 2020). In this work, we have a new clustering problem with the following criteria: (i) All points in a group have the same label. We need to ensure we are mimicking the decision boundaries. (ii) There are no overlaps between the convex hulls of each group. If convex hulls of two groups do not overlap, there must exist a line that can separates them, as guaranteed by the hyperplane separation theorem. (iii) Minimize the number of total groups. Otherwise, a simple solution is that each data point becomes a group by itself. Note that the criteria do not"
2021.naacl-main.401,N18-1202,0,0.05057,"or two in practice). If we want zero error rate on the training data, we can keep these noise points; if we allow a small error rate , then we can remove these noise clusters. In our experiments, for simplicity, we keep all clusters. 4 Representations, Tasks and Classifiers Before looking at the analysis offered by the partitions obtained via D IRECT P ROBE in §5, let us first enumerate the English NLP tasks and representations we will encounter. 4.1 Representations Our main experiments focus on BERTbase,cased , and we also show additional analysis on other contextual representations: ELMo (Peters et al., 2018)5 , BERTlarge,cased (Devlin et al., 2019), RoBERTabase and RoBERTalarge (Liu et al., 2019b). We refer the reader to the Appendix C for further details about these embeddings. We use the average of subword embeddings as the token vector for the representations that use subwords. We use the original implementation of ELMo, and the HuggingFace library (Wolf et al., 2020) for the others. 4.2 Tasks We conduct our experiments on five NLP tasks that cover the varied usages of word representations (token-based, span-based, and token pairs) and include both syntactic and semantic prediction problems. T"
2021.naacl-main.401,W19-4302,0,0.0497748,"hayarajh (2019) we train a linear SVM (Chang and Lin, 2011) that and Liu et al. (2019a). 5075 Task Min Distance Best Acc SS-role original fine-tuned 0.778 4.231 77.51 81.62 SS-func original fine-tuned 0.333 2.686 86.13 88.4 POS original fine-tuned 0.301 0.7696 93.59 95.95 SR original fine-tuned 0.421 4.734 86.85 90.03 DEP original fine-tuned 0.345 1.075 91.52 94.82 Table 2: The best performance and the minimum distances between all pairs of clusters of the last layer of BERTbase,cased before and after fine-tuning. 5.2.2 Impact of Fine-tuning Fine-tuning expands the -version space. Past work (Peters et al., 2019; Arase and Tsujii, 2019; Merchant et al., 2020) has shown that fine-tuning pre-trained models on a specific task improves performance, and fine-tuning is now the de facto procedure for using contextualized embeddings. In this experiment, we try to understand why fine-tuning can improve performance. Without training classifiers, we answer the question: What changes in the embedding space after fine-tuning? We conduct the experiments described in §5.2.1 on the last layer of BERTbase,cased before and after fine-tuning for all tasks. Table 2 shows the results. We see that after fine-tuning, both"
2021.naacl-main.401,2020.emnlp-main.254,0,0.163341,"on classifiers as a proxy? Schütze, 2020; Perone et al., 2018; Yaghoobzadeh et al., 2019; Krasnowska-Kiera´s and Wróblewska, Our approach is driven by a characterization of 2019; Wallace et al., 2019; Pruksachatkun et al., not one, but all decision boundaries in a represen2020). Using these classifiers, criteria such as accu- tation that are consistent with a training set for a racy or model complexity are used to evaluate the task. This set of consistent (or approximately conrepresentation quality for the task (e.g. Goodwin sistent) classifiers constitutes the version space for et al., 2020; Pimentel et al., 2020a; Michael et al., the task (Mitchell, 1982), and includes both simple 2020). (e.g., linear) and complex (e.g., non-linear) classiSuch classifier-based probes are undoubtedly fiers for the task. However, perfectly characterizing useful to estimate a representation’s quality for a the version space for a problem presents compu5070 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5070–5083 June 6–11, 2021. ©2021 Association for Computational Linguistics tational challenges. To develop an approxim"
2021.naacl-main.401,2020.acl-main.420,0,0.089988,"on classifiers as a proxy? Schütze, 2020; Perone et al., 2018; Yaghoobzadeh et al., 2019; Krasnowska-Kiera´s and Wróblewska, Our approach is driven by a characterization of 2019; Wallace et al., 2019; Pruksachatkun et al., not one, but all decision boundaries in a represen2020). Using these classifiers, criteria such as accu- tation that are consistent with a training set for a racy or model complexity are used to evaluate the task. This set of consistent (or approximately conrepresentation quality for the task (e.g. Goodwin sistent) classifiers constitutes the version space for et al., 2020; Pimentel et al., 2020a; Michael et al., the task (Mitchell, 1982), and includes both simple 2020). (e.g., linear) and complex (e.g., non-linear) classiSuch classifier-based probes are undoubtedly fiers for the task. However, perfectly characterizing useful to estimate a representation’s quality for a the version space for a problem presents compu5070 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5070–5083 June 6–11, 2021. ©2021 Association for Computational Linguistics tational challenges. To develop an approxim"
2021.naacl-main.401,P19-1580,0,0.0252957,"further discussion. 6 Related Work and Discussion In addition to the classifier based probes described in the rest of the paper, a complementary line of work focuses on probing the representations using a behavior-based methodology. Controlled test sets (Senel ¸ et al., 2018; Jastrzebski et al., 2017) are designed and errors are analyzed to reverseengineer what information can be encoded by the model (e.g., Marvin and Linzen, 2018; Ravichander et al., 2021; Wu et al., 2020). Another line of work probes the space by “opening up” the representation space or the model (e.g., Michel et al., 2019; Voita et al., 2019). There are some efforts to inspect the space from a geometric perspective (e.g., Etha5.4 Case Study: Identifying Difficult yarajh, 2019; Mimno and Thompson, 2017). Our Examples work extends this line of work to connect the geoThe distances between a test point and all the clusmetric structure of embedding space with classifier ters from the training set can not only be used to performance without actually training a classifier. predict the label but also can be used to identify Recent work (Pimentel et al., 2020b; Voita and difficult examples as per a given representation. Titov, 2020; Zhu an"
2021.naacl-main.401,2020.emnlp-main.14,0,0.0337137,"Missing"
2021.naacl-main.401,D19-1534,0,0.0233205,", 2018; Jawahar et al., 2019). The most com- 2020). However, the success of these methods still depends on the choices of classifiers. monly used strategy calls for training classifiers on In this paper, we pose the question: Can we evalthem to predict linguistic properties such as syntax, uate the quality of a representation for an NLP task or cognitive skills like numeracy (e.g. Kassner and directly without relying on classifiers as a proxy? Schütze, 2020; Perone et al., 2018; Yaghoobzadeh et al., 2019; Krasnowska-Kiera´s and Wróblewska, Our approach is driven by a characterization of 2019; Wallace et al., 2019; Pruksachatkun et al., not one, but all decision boundaries in a represen2020). Using these classifiers, criteria such as accu- tation that are consistent with a training set for a racy or model complexity are used to evaluate the task. This set of consistent (or approximately conrepresentation quality for the task (e.g. Goodwin sistent) classifiers constitutes the version space for et al., 2020; Pimentel et al., 2020a; Michael et al., the task (Mitchell, 1982), and includes both simple 2020). (e.g., linear) and complex (e.g., non-linear) classiSuch classifier-based probes are undoubtedly fie"
2021.naacl-main.401,2020.acl-main.383,0,0.0974419,"sis. This example shows D IRECT P ROBE can be used to identify examples in datasets that are potentially mislabeled, or at least, require further discussion. 6 Related Work and Discussion In addition to the classifier based probes described in the rest of the paper, a complementary line of work focuses on probing the representations using a behavior-based methodology. Controlled test sets (Senel ¸ et al., 2018; Jastrzebski et al., 2017) are designed and errors are analyzed to reverseengineer what information can be encoded by the model (e.g., Marvin and Linzen, 2018; Ravichander et al., 2021; Wu et al., 2020). Another line of work probes the space by “opening up” the representation space or the model (e.g., Michel et al., 2019; Voita et al., 2019). There are some efforts to inspect the space from a geometric perspective (e.g., Etha5.4 Case Study: Identifying Difficult yarajh, 2019; Mimno and Thompson, 2017). Our Examples work extends this line of work to connect the geoThe distances between a test point and all the clusmetric structure of embedding space with classifier ters from the training set can not only be used to performance without actually training a classifier. predict the label but also"
2021.scil-1.34,P10-1022,0,0.0423098,"n space before adding it to the word’s contextualized encoding. Data. A limitation of standard CCG evaluation datasets is that they contain very few tokens of categories seen less than 10 times in training. Thus, scores computed over these small samples may not reliably estimate the models’ generalization capacity. To correct for this, we investigate what happens if the models are trained on sentences containing exclusively the higher-frequency (≥10) categories, and evaluated only on sentences with at least one rare category. We split the (English) CCG Rebank training set (WSJ sections 02–21; Honnibal et al., 2010) in this way. Baselines. We compare our TreeRNN and AddrMLP models to the following baselines: 1) Nonconstructive (MLP): We compute the output probabilities for complete categories directly from the encoder’s hidden state. 2) Sequential: Kogkalidis et al. (2019) construct type-logical supertags by generating for each sentence a single sequence of atomic types and functors. We adapt their implementation of the sequence-to-sequence Transformer model (Vaswani et al., 2017) to our problem (“K+19”). We also implement a simplified version of Bhargava and Penn’s (2020) tagger, where each word’s super"
2021.scil-1.34,W19-4314,0,0.0978195,"ble supertags; in practice, they follow a power law distribution. CCG treebanks contain numerous rare supertags, including several that occur only in the test sets. Still others can be expected to occur in a much larger corpus. This long tail of the distribution is particularly challenging for taggers, due to its sparseness and relatively high complexity of categories. In most previous work, CCG supertaggers have skirted this problem by treating categories as a fixed set of opaque labels (fig. 1b) and ignoring those occurring fewer than a certain threshold (following Clark, 2002). Conversely, Kogkalidis et al. (2019) and Bhargava and Penn (2020) have recently proposed different methods of constructive supertagging, where supertags are constructed as sequences ... wk-1 wk wk+1 ... ... wk-1 Encoder wk wk+1 ... Encoder depth ult res 1 arg (b)  1 lt u es r NP ar g 10 S 2 lt u es r NP 11 ar g ∅ ∅ 110 111 ... wk-1 101 100 3 (SNP)/NP / 0 wk+1 ... Encoder ∅ ∅ ∅ ∅ 1000 1001 1010 1011 (a) wk ... /  S NP NP ... (c) Figure 1: Schematic of our tree-structured supertagger (left) in contrast with unstructured (top right) and sequential (bottom right) models. of minimal pieces and there is no constraint that predicted"
clarke-etal-2012-nlp,D11-1012,1,\N,Missing
clarke-etal-2012-nlp,W09-1119,1,\N,Missing
clarke-etal-2012-nlp,J08-2005,1,\N,Missing
clarke-etal-2012-nlp,D08-1031,1,\N,Missing
clarke-etal-2012-nlp,P05-1022,0,\N,Missing
clarke-etal-2012-nlp,P06-4018,0,\N,Missing
clarke-etal-2012-nlp,P11-1138,1,\N,Missing
clarke-etal-2012-nlp,W02-0109,0,\N,Missing
clarke-etal-2012-nlp,N10-1115,0,\N,Missing
D11-1012,W04-3220,0,0.0143075,"ptation problems of differing vocabularies and unseen features. 6 Discussion and Related work Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and p"
D11-1012,W04-2412,0,0.126022,"Missing"
D11-1012,W05-0620,0,0.0433094,"Missing"
D11-1012,P05-1022,0,0.0125627,"ints described above, which can be transformed to linear (in)equalities. We denote these constraints as C SRL . In addition to C SRL which were defined by Punyakanok et al. (2008), we also have the constraints linking the predictions of the identifier and classifier: C I vv,i,∅ + vv,i = 1; ∀v, i. (2) Inference in our baseline SRL system is, thus, the maximization of the objective defined in (1) subject to constraints C SRL , the identifier-classifier constraints defined in (2) and the restriction of the variables to take values in {0, 1}. To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the 6 The primary advantage of using ILP for inference is that this representation enables us to add arbitrary coherence constraints between the phenomena. If the underlying optimization problem itself is tractable, then so is the corresponding integer program. However, other approaches to solve the constrained maximization problem can also be used for inference. same feature representation as in the original system. We trained the classifiers on the standard Propbank training set using the one-vs-all extension of the average Perceptron algorithm. As with the preposition roles, we"
D11-1012,W02-1001,0,0.0154384,"ll the joint constraints. 4.2 Learning to rescale the individual systems Given the individual models and the constraints, we only need to learn the scaling parameters λpZ . Note that the number of scaling parameters is the total number of labels. When we jointly predict verb SRL and preposition role, we have 22 preposition roles (from table 3), one SRL identifier label and 54 SRL argument classifier labels. Thus we learn only 77 parameters for our joint model. This means that we only need a very small dataset that is jointly annotated with all the phenomena. We use the Structure Perceptron of Collins (2002) to learn the scaling weights. Note that for learning the scaling weights, we need each label to be associated with a real-valued feature. Given an assignment of the inference variables v, the value of the feature corresponding to the label Z of task p is given by the sum of scores of all parts in the structure for p that P p have been assigned this label, i.e. vZ,y ·ΘpZ,y . This yp feature is computed for the gold and the predicted structures and is used for updating the weights. 5 Experiments In this section, we describe our experimental setup and evaluate the performance of our approach. Th"
D11-1012,D09-1047,0,0.591094,"MPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the l"
D11-1012,N09-1037,0,0.0294931,"ing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensib"
D11-1012,P07-1072,0,0.0204688,"ponents to define our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include systems that predict structures for other linguistic phenomena because we do not retrain the underlying systems. The semantic relations can be enriched by incorporating more linguistic phenomena such as nominal SRL, defined by the Nombank annotation scheme of Meyers et al. (2004), the preposition function analysis of O’Hara and Wiebe (2009) and noun compound analysis as defined by Girju (2007) and Girju et al. 138 (2009) and others. This presents an exciting direction for future work. 7 Conclusion This paper presents a strategy for extending semantic role labeling without the need for extensive retraining or data annotation. While standard semantic role labeling focuses on verb and nominal relations, sentences can express relations using other lexical items also. Moreover, the different relations interact with each other and constrain the possible structures that they can take. We use this intuition to define a joint model for inference. We instantiate our model using verb semantic"
D11-1012,N10-1115,0,0.0272943,"Missing"
D11-1012,W08-2122,0,0.0135404,"s on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scaling pa"
D11-1012,C10-2052,0,0.334161,"Missing"
D11-1012,P08-1068,0,0.0199219,"IPIENT S PECIES T EMPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along"
D11-1012,S07-1005,0,0.455254,"e company calculated the price trends on the major stock markets on Monday. 3.1 Preposition Relations Prepositions indicate a relation between the attachment point of the preposition and its object. As we have seen, the same preposition can indicate different types of relations. In the literature, the polysemy of prepositions is addressed by The Preposition Project1 of Litkowski and Hargraves (2005), which is a large lexical resource for English that labels prepositions with their sense. This sense inventory formed the basis of the SemEval-2007 task of preposition word sense disambiguation of Litkowski and Hargraves (2007). In our example, the first on 1 http://www.clres.com/prepositions.html 131 would be labeled with the sense 8(3) which identifies the object of the preposition as the topic, while the second instance would be labeled as 17(8), which indicates that argument is the day of the occurrence. The preposition sense inventory, while useful to identify the fine grained distinctions between preposition usage, defines a unique sense label for each preposition by indexing the definitions of the prepositions in the Oxford Dictionary of English. For example, in the phrase at noon, the at would be labeled wit"
D11-1012,P09-1039,0,0.0234663,"software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let viI denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment X X C max ΘC ΘIi viI (1) i,a vi,a + vC ,vI i,a i Here, vC and vI denote all the argument classifier and identifier vari"
D11-1012,W04-2705,0,0.354793,"ithout retraining the individual models. 1 (1) The field goal by Brien changed the game in the fourth quarter. Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). Verb centered semantic role labeling would identify the arguments of the predicate change as (a) The field goal by Brien (A0, the causer of the change), (b) the game (A1, the thing changing), and (c) in the fourth quarter (temporal modifier). However, this does not tell us that the scorer of the field goal was Brien, which is expressed by the preposition by. Also, note that the in indicates a temporal relation, which overlaps with the verb’s analysis. In this paper, we propose an extension of the standard semantic role labeling task to include relations expressed by lexical items other than"
D11-1012,N09-1018,0,0.0242326,"sifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that"
D11-1012,J09-2002,0,0.398169,"Missing"
D11-1012,J05-1004,0,0.864474,"between their predictions, we show improvements in the performance of both tasks without retraining the individual models. 1 (1) The field goal by Brien changed the game in the fourth quarter. Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). Verb centered semantic role labeling would identify the arguments of the predicate change as (a) The field goal by Brien (A0, the causer of the change), (b) the game (A1, the thing changing), and (c) in the fourth quarter (temporal modifier). However, this does not tell us that the scorer of the field goal was Brien, which is expressed by the preposition by. Also, note that the in indicates a temporal relation, which overlaps with the verb’s analysis. In this paper, we propose an extension of the standard sema"
D11-1012,J08-2005,1,0.958117,"bank. We use this system as our independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4 The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to gene"
D11-1012,W09-1119,1,0.665769,"sense prediction on the prepositions that have been annotated for the Penn Treebank dataset. Role ACTIVITY ATTRIBUTE B ENEFICIARY C AUSE C ONCOMITANT E ND C ONDITION E XPERIENCER I NSTRUMENT L OCATION M EDIUM O F C OMMUNICATION N UMERIC /L EVEL O BJECT O F V ERB OTHER PART W HOLE PARTICIPANT /ACCOMPANIER P HYSICAL S UPPORT P OSSESSOR P ROFESSIONAL A SPECT R ECIPIENT S PECIES T EMPORAL T OPIC Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Aver"
D11-1012,W06-1616,0,0.0231071,"cogcomp.cs.illinois.edu/page/software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let viI denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment X X C max ΘC ΘIi viI (1) i,a vi,a + vC ,vI i,a i Here, vC and vI denote all the argument clas"
D11-1012,rizzolo-roth-2010-learning,1,0.631772,"belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2 . We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3 . Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the labels in the Treebank dataset. According to this labeling scheme, the first on in our running example will be labeled T OPIC and the second one will 2 This dataset does not annotate all prepositions and restricts itself mainly to prepositions that start a Propbank argument. The data is available at http://nlp.comp.nus. edu.sg/corpora 3"
D11-1012,W04-2401,1,0.776726,"t classifier predictions – the identifier should predict that a candidate is an argument if, and only if, the argument classifier does not predict the label ∅. This change is in keeping with the idea of using joint inference to combine independently 5 The verb SRL system be downloaded from http:// cogcomp.cs.illinois.edu/page/software 133 learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6 . C be the Boolean indicator variable that deLet vi,a notes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Simila"
D11-1012,D10-1001,0,0.016691,"d semantic dependencies. Dahlmeier et al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scaling parameters. Our approach is conceptually similar to that of Rush et al. (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. They applied this idea to jointly predict dependency and phrase structure parse trees and on the task of predicting full parses together with part-ofspeech tags. The main difference in our approach is that we treat the scaling problem as a separate learning problem in itself and train a joint model specifically for re-scaling the output of the trained systems. The SRL combination system of Surdeanu et al. (2007) studied the combination of three different SRL s"
D11-1012,J08-2002,0,0.0194954,"ur independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4 The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of"
D11-1012,N09-3017,0,0.276284,"Missing"
D11-1012,W04-3212,0,0.0526208,"05). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5 . We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of argument candidates for each predicate. The identification stage uses a classifier to prune the candidates. In the argument classification step, the candidates that remain after the identification step are assigned scores for the SRL arguments using a multiclass classifier. One of the labels of the classifier is ∅, which indicates that the candidate is, in fact, not an argument. The inference step produces a combined prediction for all argument candidates of a verb proposition by enforcing global constraints. The inference enforces the following structural"
D11-1012,S07-1051,0,0.131782,"Missing"
D11-1012,J05-1003,0,\N,Missing
D12-1102,D11-1003,0,0.0859535,"Missing"
D12-1102,P07-1036,1,0.82544,"work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabeled dependency parsing). For each yi , let ci ∈ &lt; denote the weight associated with it. We denote the entire collection of weights by the vector c, forming the objective for this ILP. Not all assignments to these variables are valid. Without loss of generality, these con"
D12-1102,P06-2019,0,0.149821,"e ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function. The goal of inference is to select the maximum spanning tree of this weighted graph. 2.1 Problem Formulation In this work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabe"
D12-1102,W02-1001,0,0.105845,"l of inference is to select the maximum spanning tree of this weighted graph. 2.1 Problem Formulation In this work, we consider the general inference problem of solving a 0-1 integer linear program. To perform inference, we assume that we have a model that assigns scores to the ILP decision variables. Thus, our work is applicable not only in cases where inference is done after a separate learning phase, as in (Roth and Yih, 2004; Clarke and Lapata, 2006; Roth and Yih, 2007) and others, but also when inference is done during the training phase, for algorithms like the structured perceptron of (Collins, 2002), structured SVM (Tsochantaridis et al., 2005) or the constraints driven learning approach of (Chang et al., 2007). Since structured prediction assigns values to a collection of inter-related binary decisions, we denote the ith binary decision by yi ∈ {0, 1} and the entire structure as y, the vector composed of all the binary decisions. In our running example, each edge in the weighted graph generates a single decision variable (for unlabeled dependency parsing). For each yi , let ci ∈ &lt; denote the weight associated with it. We denote the entire collection of weights by the vector c, forming t"
D12-1102,P09-1039,0,0.120855,"to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that we are running inference for a large number of examples help us improve the time compl"
D12-1102,H05-1066,0,0.423866,"ral approximation schemes which can provide further speedup. We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that"
D12-1102,W05-0639,0,0.0237842,"as structured prediction problems, where the goal is to jointly assign values to many inference variables while accounting for possible dependencies among them. This decision task is a combinatorial optimization problem and can be solved using a dynamic programming approach if the structure permits. In general, the inference problem can be formulated and solved as integer linear programs (ILPs). Following (Roth and Yih, 2004) Integer linear programs have been used broadly in NLP. For example, (Riedel and Clarke, 2006) and (Martins et al., 2009) addressed the problem of dependency parsing and (Punyakanok et al., 2005; Punyakanok et al., 2008) dealt with semantic role labeling with this technique. In this section, we will use the ILP formulation of dependency parsing to introduce notation. The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al., 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function. The goal of inference is to select the maximum spanning tree of this weighted"
D12-1102,J08-2005,1,0.957658,"d theoretically guarantee the optimality of the solution. Furthermore, in some cases, even when the conditions are not satisfied, we can reuse previous solutions with high probability of being correct. Given the extensive use of integer linear programs for structured prediction in Natural Language Processing over the last few years, these ideas can be applied broadly to NLP problems. We instantiate our improved inference approaches in the structured prediction task of semantic role labeling, where we use an existing implementation and a previous trained model that is based on the approach of (Punyakanok et al., 2008). We merely modify the inference pro1114 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1114–1124, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics cess to show that we can realize the theoretical gains by making fewer calls to the underlying ILP solver. Algorithm Theorem 1 Theorem 2 Theorem 3 Speedup 2.44 2.18 2.50 Table 1: The speedup for semantic role labeling corresponding to the three theorems described in this paper. These theorems guarantee the optimality"
D12-1102,W06-1616,0,0.316377,"instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 1 Introduction Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them. For example, consider a dependency parser that uses the maximum spanning tree algorithm (McDonald et al., 2005) or its integer linear program variants (Riedel and Clarke, 2006; Martins et al., 2009) to make predictions. Given a trained model, the parser addresses * These authors contributed equally to this work. each sentence separately and runs the inference algorithm to predict the parse tree. Thus, the time complexity of inference over the test set is linear in the size of the corpus. In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that we are running inference for a large number of examples help us"
D12-1102,W04-2401,1,0.80548,"We instantiate these schemes for the task of semantic role labeling (Section 4). Section 5 discusses related work and future research directions. 2 Motivation Many NLP tasks can be phrased as structured prediction problems, where the goal is to jointly assign values to many inference variables while accounting for possible dependencies among them. This decision task is a combinatorial optimization problem and can be solved using a dynamic programming approach if the structure permits. In general, the inference problem can be formulated and solved as integer linear programs (ILPs). Following (Roth and Yih, 2004) Integer linear programs have been used broadly in NLP. For example, (Riedel and Clarke, 2006) and (Martins et al., 2009) addressed the problem of dependency parsing and (Punyakanok et al., 2005; Punyakanok et al., 2008) dealt with semantic role labeling with this technique. In this section, we will use the ILP formulation of dependency parsing to introduce notation. The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al., 2005) to use ILP for inference. The key idea is to bui"
D12-1102,D10-1001,0,0.0817059,"Missing"
D14-1159,D13-1160,1,0.289643,"contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehensio"
D14-1159,D08-1073,0,0.0214459,"the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al.,"
D14-1159,W06-0602,0,0.0205921,"urafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke e"
D14-1159,W10-2903,0,0.0252125,"., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choic"
D14-1159,clarke-etal-2012-nlp,1,0.779928,"Missing"
D14-1159,W02-1001,0,0.0492796,"ossible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. 5 Question Answering via Structures This section describes our question answering system that, given a process struc"
D14-1159,D12-1062,0,0.0400506,"d by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank a"
D14-1159,doddington-etal-2004-automatic,0,0.188353,"Missing"
D14-1159,D11-1142,0,0.015657,"in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in"
D14-1159,P13-1158,0,0.00703221,"ons via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: “. . . Water is split, providing a source of electrons and protons (hydrogen io"
D14-1159,P99-1042,0,0.0251544,"ap natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biolo"
D14-1159,W11-1801,0,0.0399034,"Missing"
D14-1159,Q13-1016,0,0.0240224,"e do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating mach"
D14-1159,P14-1026,0,0.0192904,"he text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light absorption b Transfer of io"
D14-1159,P14-5010,1,0.0110213,"Missing"
D14-1159,P09-1039,0,0.00814841,"iption Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The S AME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than S AME, at most one of (ti , tj ) or (tj , ti ) can take that label and the other is assigned the label NULL - REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL - REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled S UPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and re"
D14-1159,D12-1080,1,0.842646,"m. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow"
D14-1159,N03-1022,0,0.0797767,"igure 1). Our strategy is to treat the process structure as a small knowledge-base. We map each answer along with the question into a structured query that we compare against the structure. The query can prove either the correctness or incorrectness of the answer being considered. That is, either we get a valid match for an answer (proving that the corresponding answer is correct), or we get a refutation in the form of a contradicted causal chain (thus proving that the other answer is correct). This is similar to theorem proving approaches suggested in the past for factoid question answering (Moldovan et al., 2003). The rest of this section is divided into three parts: Section 5.1 defines the queries we use, Section 5.2 describes a rule-based algorithm for converting a question and an answer into a query and finally, 5.3 describes the overall algorithm. 5.1 Queries over Processes We model a query as a directed graph path with regular expressions over edge labels. The bottom right portion of Figure 1 shows examples of queries for our running example. In general, given a question and one of the answer candidates, one end of the path is populated by a trigger/argument found in the question and the other is"
D14-1159,J05-1004,0,0.128022,"tudied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of superv"
D14-1159,J08-2005,0,0.144807,"Missing"
D14-1159,D13-1020,0,0.248644,"ong answer is closer in the text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light abso"
D14-1159,D11-1001,0,0.0772427,"Missing"
D14-1159,W04-2401,0,0.0455766,"es, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions."
D14-1159,D13-1177,1,0.941392,"and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should require reasoning about the relations between those event"
D14-1159,N06-1056,0,0.0533243,"and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future res"
D14-1159,P09-1046,0,0.0164746,"s that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contr"
D14-1159,W09-1401,0,\N,Missing
D14-1159,E12-2021,0,\N,Missing
D16-1237,S12-1051,0,0.0949871,"Missing"
D16-1237,S15-2030,0,0.030401,"Missing"
D16-1237,J93-2003,0,0.123247,"Missing"
D16-1237,W07-1427,0,0.0851897,"Missing"
D16-1237,N10-1066,1,0.894893,"Missing"
D16-1237,clarke-etal-2012-nlp,1,0.895889,"Missing"
D16-1237,P09-1053,0,0.254104,"Missing"
D16-1237,C04-1051,0,0.374057,"Missing"
D16-1237,P08-1112,0,0.0421474,"Missing"
D16-1237,N13-1092,0,0.142696,"Missing"
D16-1237,S15-2046,0,0.039862,"Missing"
D16-1237,S16-1171,0,0.0342634,"Missing"
D16-1237,N03-1017,0,0.0666003,"Missing"
D16-1237,S16-1124,0,0.0228961,"Missing"
D16-1237,W07-1431,0,0.0724385,"Missing"
D16-1237,D08-1084,0,0.274475,"Missing"
D16-1237,P14-5010,0,0.00789821,"Missing"
D16-1237,J03-1002,0,0.0252276,"Missing"
D16-1237,D14-1162,0,0.0900713,"Missing"
D16-1237,W04-2401,0,0.128607,"Missing"
D16-1237,Q14-1018,0,0.154148,"Missing"
D16-1237,S14-2039,0,0.0411307,"Missing"
D16-1237,S15-2027,0,0.0378836,"Missing"
D16-1237,W00-0726,0,0.407423,"Missing"
D18-2007,D17-2021,0,0.0795257,"l (i.e., changing the dimension count of hidden units or input words) on the prediction performance. Besides interpreting the model via carefully designed experiments, several interactive demo/visualization systems, such as AllenNLP’s demos (http://demo.allennlp.org/), often rely on visual encodings to summarize the model predictions. These systems provide a flexible environment in which the user can experiment with the various inputs and perform error analysis. The hidden state properties of the LSTM are visualized and investigated in the LSTMvis visualization system (Strobelt et al., 2018). Lee et al. (2017) visualized the beam search and attention component in neural machine translation models, in which the user can dynamically change the probability for the next step of the search tree or change the weight of the attention. In the visualization work on question answering (R¨uckl´e and Gurevych, 2017), the system shows the text context and highlights the critical phrase that is used to answer the question. 3 Figure 2: The interface for showing input sentences. The user can manually edit the words or apply automatic perturbation/paraphrasing of the inputs. In the “perturbed” drop-down menu, the b"
D18-2007,E17-1083,0,0.020147,"ges) — small changes in words can lead to drastic differences in the semantics of the sentences. To reduce the potential semantic deviation, we allow for a straightforward perturbation method by replacing nouns and verbs by their synonyms in WordNet (Miller, 1995). However, synonym replacement does not guarantee that the meaning of the sentence remains the same. Furthermore, WordNet often produces rare words or obscure usages that may lead to less meaningful sentences. To improve the perturbation quality, we also allow for a translation-based paraphrasing technique similar to that proposed by Mallinson et al. (2017). Here, we translate the original English sentence into several other languages and then pivot back to English. Provided the translation produces a good result (in our case, we use the Google Cloud Visualization System As illustrated in Figure 1, many recent end-to-end NLP models follow a similar encoder–attention– 37 matrix attention view (Figure 4(b)) resolves these issues, despite being more verbose and less efficient in highlighting the most dominant alignments. We also enable the linkage between highlighted actions in both views (see Figure 4(a)(b), where one alignment relationship is hig"
D18-2007,D16-1244,0,0.0807261,"Missing"
D18-2007,P17-4004,0,0.039921,"Missing"
D19-1405,P18-1013,0,0.034522,"2010) and constrained conditional models (Chang et al., 2012), which integrate knowledge with statistical models. Using posterior regularization with imitation learning, Hu et al. (2016) transferred knowledge from rules into neural parameters. Rocktäschel et al. (2015) embedded logic into distributed representations for entity relation extraction. Alberti et al. (2019) imposed answer consistency over generated questions for machine comprehension. Ad-hoc regularizers have been proposed for process comprehension (Du et al., 2019), semantic role labeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inserting additional seemingly important, y"
D19-1405,P16-1228,0,0.0190544,"domain knowledge using the Łukasiewicz t-norm. Xu et al. (2018) proposed a general framework for designing a semantically informed loss, without tnorms, for constraining a complex output space. In the same vein, Fischer et al. (2019) also proposed a framework for designing losses with logic, but using a bespoke mapping of the Boolean operators. Our work is also conceptually related to posterior regularization (Ganchev et al., 2010) and constrained conditional models (Chang et al., 2012), which integrate knowledge with statistical models. Using posterior regularization with imitation learning, Hu et al. (2016) transferred knowledge from rules into neural parameters. Rocktäschel et al. (2015) embedded logic into distributed representations for entity relation extraction. Alberti et al. (2019) imposed answer consistency over generated questions for machine comprehension. Ad-hoc regularizers have been proposed for process comprehension (Du et al., 2019), semantic role labeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of rob"
D19-1405,N18-1170,0,0.0176469,"abeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inserting additional seemingly important, yet unrelated, information to training instances (Jia and Liang, 2017) have been used to show model inconsistency. Finally, adversarially labeled examples have been shown to improve prediction accuracy (Kang et al., 2018) . Also related in this vein is the idea of dataset inoculation (Liu et al., 2019), where models are finetuned by exposing them to a challenging dataset. The closest related work to this paper is probably that of Minervini and Riedel (2018), which uses the Gödel t-norm to discover adversarial examples that violate constraints. Ther"
D19-1405,D17-1215,0,0.0243549,"t has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inserting additional seemingly important, yet unrelated, information to training instances (Jia and Liang, 2017) have been used to show model inconsistency. Finally, adversarially labeled examples have been shown to improve prediction accuracy (Kang et al., 2018) . Also related in this vein is the idea of dataset inoculation (Liu et al., 2019), where models are finetuned by exposing them to a challenging dataset. The closest related work to this paper is probably that of Minervini and Riedel (2018), which uses the Gödel t-norm to discover adversarial examples that violate constraints. There are three major differences compared to this paper: 1) our definition of inconsistency is a strict generalization"
D19-1405,P18-1225,0,0.0284075,"hown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inserting additional seemingly important, yet unrelated, information to training instances (Jia and Liang, 2017) have been used to show model inconsistency. Finally, adversarially labeled examples have been shown to improve prediction accuracy (Kang et al., 2018) . Also related in this vein is the idea of dataset inoculation (Liu et al., 2019), where models are finetuned by exposing them to a challenging dataset. The closest related work to this paper is probably that of Minervini and Riedel (2018), which uses the Gödel t-norm to discover adversarial examples that violate constraints. There are three major differences compared to this paper: 1) our definition of inconsistency is a strict generalization of errors of model predictions, giving us a unified framework for that includes cross-entropy as a special case, 2) our framework does not rely on the"
D19-1405,P19-1028,1,0.812378,"n example triples. BERT: trained on the full SNLI+MultiNLI data. Predictions are from random run with seed 1. 6 Table 6: Distribution of predictions on the 100k evaluation example triples. BERT: trained on the full SNLI+MultiNLI data. Predictions are from random run with seed 1. BERT ρT τT 0.7 16.0 1.8 49.6 1.2 9.0 1.0 9.3 Related Works and Discussion Logic, Knowledge and Statistical Models Using soft relaxations of Boolean formulas as loss functions has rich history in AI. The Łukasiewicz t-norm drives knowledge-driven learning and inference in probabilistic soft logic (Kimmig et al., 2012). Li and Srikumar (2019) show how to augment existing neural network architectures with domain knowledge using the Łukasiewicz t-norm. Xu et al. (2018) proposed a general framework for designing a semantically informed loss, without tnorms, for constraining a complex output space. In the same vein, Fischer et al. (2019) also proposed a framework for designing losses with logic, but using a bespoke mapping of the Boolean operators. Our work is also conceptually related to posterior regularization (Ganchev et al., 2010) and constrained conditional models (Chang et al., 2012), which integrate knowledge with statistical"
D19-1405,N19-1225,0,0.0214135,"Missing"
D19-1405,E17-1083,0,0.0682879,"Missing"
D19-1405,D18-1538,0,0.0518527,"osterior regularization (Ganchev et al., 2010) and constrained conditional models (Chang et al., 2012), which integrate knowledge with statistical models. Using posterior regularization with imitation learning, Hu et al. (2016) transferred knowledge from rules into neural parameters. Rocktäschel et al. (2015) embedded logic into distributed representations for entity relation extraction. Alberti et al. (2019) imposed answer consistency over generated questions for machine comprehension. Ad-hoc regularizers have been proposed for process comprehension (Du et al., 2019), semantic role labeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inser"
D19-1405,K18-1007,0,0.489626,"∈ D, C(P, H) ↔ C(H, P ) (5) Transitivity Consistency This constraint is applicable to any three related sentences P , H and Z. If we group the sentences into three pairs, namely (P, H), (H, Z) and (P, Z), the label definitions mandate that not all of the 33 = 27 assignments to these three pairs are allowed. The example in §1 is an allowed label assignment. We can enumerate all such valid labels as the conjunction: ∀(P, H, Z) ∈ D, 4 A full description of t-norms is beyond the scope of this paper; we refer the interested reader to Klement et al. (2013). 5 For example, the Gödel t-norm, used by Minervini and Riedel (2018), has a discountinuous but semi-differentiable residuum. The Łukasiewicz t-norm can lead to zero gradients for large disjunctions, rendering learning difficult. 3926 (E (P, H) ∧ E (H, Z) → E (P, Z)) ∧ (E (P, H) ∧ C (H, Z) → C (P, Z)) ∧ (N (P, H) ∧ E (H, Z) → ¬C (P, Z)) ∧ (N (P, H) ∧ C (H, Z) → ¬E (P, Z)) (6) Name Boolean Logic Product Negation T-norm T-conorm ¬A A∧B A∨B Residuum A→B 1−a ab a + b − ab  min 1, ab Gödel 1−a min (a, b) max(a, b) ( 1, if b ≥ a, b, else Łukasiewicz 1−a max (0, a + b − 1) min (1, a + b) min (1, 1 − a + b) Table 1: Mapping discrete statements to differentiable functi"
D19-1405,C18-1198,0,0.0220384,"Ad-hoc regularizers have been proposed for process comprehension (Du et al., 2019), semantic role labeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined propositional rules for swapped datasets (Wang et al., 2019) or outlining large3931 scale stress tests to measure stability of models to semantic, lexical and random perturbations (Naik et al., 2018). Moreover, adversarial training examples produced by paraphrasing training data (Iyyer et al., 2018) or inserting additional seemingly important, yet unrelated, information to training instances (Jia and Liang, 2017) have been used to show model inconsistency. Finally, adversarially labeled examples have been shown to improve prediction accuracy (Kang et al., 2018) . Also related in this vein is the idea of dataset inoculation (Liu et al., 2019), where models are finetuned by exposing them to a challenging dataset. The closest related work to this paper is probably that of Minervini and Riede"
D19-1405,D16-1244,0,0.0379234,"s Tao Li, Vivek Gupta, Maitrey Mehta, Vivek Srikumar School of Computing, University of Utah {tli,vgupta,maitrey,svivek}@cs.utah.edu Abstract sentences P , H and Z, where P entails H and H contradicts Z. Using these two facts, we can infer that P contradicts Z. In other words, these three decisions are not independent of each other. Any model for textual inference should not violate this invariant defined over any three sentences, even if they are not labeled. Neither are today’s models trained to be consistent in this fashion, nor is consistency evaluated. The decomposable attention model of Parikh et al. (2016) updated with ELMo violates the above constraint for the following sentences:1 While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on n"
D19-1405,D14-1162,0,0.0811609,"Missing"
D19-1405,N18-1202,0,0.0656641,"Missing"
D19-1405,D16-1264,0,0.0177431,"Missing"
D19-1405,N15-1118,0,0.0539492,"general framework for designing a semantically informed loss, without tnorms, for constraining a complex output space. In the same vein, Fischer et al. (2019) also proposed a framework for designing losses with logic, but using a bespoke mapping of the Boolean operators. Our work is also conceptually related to posterior regularization (Ganchev et al., 2010) and constrained conditional models (Chang et al., 2012), which integrate knowledge with statistical models. Using posterior regularization with imitation learning, Hu et al. (2016) transferred knowledge from rules into neural parameters. Rocktäschel et al. (2015) embedded logic into distributed representations for entity relation extraction. Alberti et al. (2019) imposed answer consistency over generated questions for machine comprehension. Ad-hoc regularizers have been proposed for process comprehension (Du et al., 2019), semantic role labeling (Mehta et al., 2018), and summarization (Hsu et al., 2018). Natural Language Inference In the literature, it has been shown that even highly accurate models show a decline in performance with perturbed examples. This lack of robustness of NLI models has been shown by comparing model performance on pre-defined"
D19-1405,W18-5446,0,0.0449819,"Missing"
E14-1038,N10-1019,0,0.102404,"e preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on"
E14-1038,P01-1005,0,0.102126,"Missing"
E14-1038,P11-1092,0,0.257009,"t with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most co"
E14-1038,P03-2026,0,0.364715,"Missing"
E14-1038,W13-3603,0,0.101105,"Missing"
E14-1038,W11-2838,0,0.179927,"Missing"
E14-1038,W12-2006,0,0.158705,"S tag and dependency of the governor of the preposition POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of th"
E14-1038,C08-1022,0,0.168511,"Missing"
E14-1038,P08-1021,0,0.10352,"ancing.” “You ask me for some informations*/information- here they*/it are*/is.” “Nobody {has to be}*/{should be} late.” Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus. 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT section of the Gigaword corpus and contains about 2,600 of frequently-occurring words tagged as VB; for example, (3) will add shop but not shopping, but (4) will add both. For methods (3) and (4), we developed verbMorph,5 a tool that performs morphological analysis on verbs and is used to lemmatize verbs and to generate morphological variants. The module makes uses of (1) the verb lemma l"
E14-1038,I08-1059,0,0.528693,"ite Form choice that encompass the most common grammatical verb problems for ESL learners. The first two examples show mistakes on verbs that function as main verbs in a clause: sentence (1) shows an example of subject-verb Agreement error; (2) is an example of a Tense mistake where the ambiguity is between {will find} (Future tense) Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et 358 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–367, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and find (Present tense). Examples (3) and (4) display Form mistakes: confusing the infinitive and gerund forms in (3) and including an inflection on an infinitive verb in (4). This paper addresses the specific challenges of verb error correction that have not been addressed previously – identifying candidates for mistakes and determining which class of errors is pres"
E14-1038,P11-1121,0,0.147689,"svivek@cs.stanford.edu Abstract al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assume prior knowledge of the mistake type; however, identifying the specific category of a verb error is nontrivial, since the surface form of the verb may be ambiguous, especially when that verb is used incorrectly. Consider the following examples of verb mistakes: Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studi"
E14-1038,W13-3601,0,0.294354,"Missing"
E14-1038,D10-1032,0,0.0651336,"Missing"
E14-1038,W10-1004,1,0.910661,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,N10-1018,1,0.883695,"ction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve"
E14-1038,P11-1093,1,0.953366,"a subset of the FCE data set with gold verb candidates and gold verb type.1 2 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mista"
E14-1038,P12-2039,0,0.0417645,"POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word ngrams (1) (2) (3) (4) (5) (6) Table 7: Features used, grouped by error type. 6 Experiments do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: The main goal of this work is to propose a unified framework for correcting verb mistakes and to address the specific challenges of the problem. We thus do not focus on features or on the spe"
E14-1038,P10-2065,0,0.230951,"Missing"
E14-1038,P11-1019,0,0.118417,"Missing"
E14-1038,W13-3602,1,\N,Missing
E17-5005,P11-1062,0,0.0332715,"ition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guidance, to software packages and applications. The goal of this tutorial is to introduce the computational framework to the broader ACL commuVivek Srikumar University of Utah svivek@cs.u"
E17-5005,D10-1125,0,0.0289042,"as in event recognition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guidance, to software packages and applications. The goal of this tutorial is to introduce the computational framework to the broader ACL commuVivek Srikumar Universi"
E17-5005,W04-2401,1,0.652046,"on, dependency parsing and semantic parsing. The setting is also appropriate for cases that may require making global decisions that involve multiple components, possibly pre-designed or prelearned, as in event recognition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth and Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers, including (Martins et al., 2009; Koo et al., 2010; Berant et al., 2011) This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP"
K19-1042,P13-1023,0,0.0601246,"Missing"
K19-1042,W13-2322,0,0.0513238,"Missing"
K19-1042,D14-1159,1,0.825604,"novsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et a"
K19-1042,P18-1174,0,0.302926,"collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However,"
K19-1042,D19-1003,0,0.0858022,"OM S MALL M ODEL O RACLE S MALL M ODEL 110 45.2 43.3 46.6 44.8 44.2 45.2 30.0 40.9 45.1 44.9 51.9 53.8 150 47.2 49.2 48.8 47.4 48.3 47.5 38.2 44.6 48.6 48.8 54.8 56.6 210 50.5 52.9 51.4 52.1 52.5 51.9 41.0 50.1 52.4 51.4 57.3 58.9 290 53.1 56.8 55.8 55.9 55.5 55.1 51.5 54.1 55.6 53.9 59.5 60.3 370 55.8 57.8 57.6 — 58.0 56.7 53.7 — 57.1 57.0 61.4 61.5 510 58.5 60.3 58.6 — 59.8 58.7 57.2 — 59.8 59.2 62.6 63.3 tecture modifications do not expose an advantage of the oracle policy compared to the random one. We did not examine a simpler linear model for span detection, in light of recent findings (Lowell et al., 2019) that it is important to test LTAL with state-of-the-art models, as performance is tied to the specific model being trained. Myopicity We hypothesized that greedily selecting an example that maximizes performance in a specific iteration might be suboptimal in the long run. Because non-greedy selection strategies are computationaly intractable, we perform the following two experiments. First, we examine E PSILON -G REEDY- P, where in each iteration the oracle policy selects the set Cj that maximizes target performance with probability 1 − p and randomly chooses a set with probability p. This is"
K19-1042,D17-1063,0,0.0315033,"proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentenc"
K19-1042,W18-2501,0,0.0604564,"Missing"
K19-1042,W13-2001,0,0.0473772,"Missing"
K19-1042,D15-1076,0,0.133676,"the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning"
K19-1042,L16-1262,0,0.0301266,"Missing"
K19-1042,D16-1258,0,0.0481928,"Missing"
K19-1042,P18-1035,0,0.0219761,"nt et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work b"
K19-1042,S14-2008,0,0.0250035,"ation strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small label"
K19-1042,J05-1004,0,0.160826,"hile in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learnin"
K19-1042,W09-1401,0,0.0514371,"Representations Omri Koshorek1 Gabriel Stanovsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representation"
K19-1042,D14-1162,0,0.0826899,"oder, producing a representation hi for every token. Each span xi∶j is represented by concatenating the respective hidden states: sij = [hi ; hj ]. A fully connected network consumes the span representation sij , and predicts a probability whether the span is an argument or not. To accelerate training, we reduce the number of parameters to 488K by freezing the token embeddings, reducing the number of layers in the encoder, and by shrinking the dimension of both the hidden representations and the binary predicate indicator embedding. Following FitzGerald et al. (2018), we use GLoVe embeddings (Pennington et al., 2014). AllenNLP (Gardner et al., 2018), and experiment with two variants: (1) NER-M ULTILANG: A BiLSTM CRF model (20K parameters) with 40 dimensional multi-lingual word embeddings (Ammar et al., 2016), and (2) NER-L INEAR: A linear CRF model which was originally used by Liu et al. (2018). 5.3 Results Span Detection: Table 2 shows F1 score (the official metric) of the QA-SRL span detector models for different sizes of Slab for BASE O RACLE and the other baselines. Figure 2 (left) shows the relative improvement of the baselines over R AN DOM . We observe that the maximal improvement of BASE O RACLE o"
K19-1042,W03-0419,0,0.169446,"Missing"
K19-1042,D18-1318,0,0.0202569,"representation models. In a large empirical study, Lowell et al. (2019) have recently shown other limitations in active learning. They investigate the performance of active learning across NLP tasks and model architectures, and demonstrate that it does not achieve consistent gains over supervised learning, mostly because the collected samples are beneficial to a specific model architecture, and does not yield better results than random selection when switching to a new architecture. There has been little research regarding active learning of semantic representations. Among the relevant work, Siddhant and Lipton (2018) have shown that uncertainty estimation using dropout and Bayes-By-Backprop (Blundell et al., 2015) achieves good results on the SRL formulation. The improvements in performance due to LTAL approaches on various tasks (Konyushkova et al., 2017; Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018) has raised the question whether learned policies can be applied also to the field of learning semantic representations. 8 Acknowledgements We thank Julian Michael and Oz Anani for their useful comments and feedback. This research was supported by The U.S-Israel Binational Science Foundation gran"
K19-1042,P19-1401,0,0.0204549,"the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small labeled dataset Slab and a large pool of unlabeled examples Sunlab for a target task. In each iteration, a heuristic is used to select L unlabeled examples, which are sent to annotation and added to Slab . An example heuristic is uncertainty sampling (Lewis and Gale, 1994), which at each iteration chooses examples that the current model is the least con"
K19-1042,D18-1342,0,0.0483898,"Missing"
K19-1042,P18-1191,0,\N,Missing
K19-2013,hajic-etal-2012-announcing,0,0.148422,"Missing"
K19-2013,P13-1023,0,0.143795,"Missing"
K19-2013,S19-2001,0,0.0752617,"ture parsing for one of the phrasalanchoring MRs, UCCA. Our system submission ranked 1st in the AMR subtask, and later improvements shows promising results on other frameworks as well. 1 Introduction The design and implementation of broad-coverage and linguistically motivated meaning representation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theories, and ∗ 2 Anchoring in Me"
K19-2013,P17-1008,0,0.0163058,"ation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theories, and ∗ 2 Anchoring in Meaning Representation The 2019 Conference on Computational Language Learning (CoNLL) hosted a shared task on 1 The code is available online at https://github.com/ utahnlp/lapa-mrp Work done when Jie Cao was an intern at AWS AI 138 Proceedings of the Shared Task on Cross-Framework Meaning Represe"
K19-2013,K19-2002,0,0.298565,"eakdown of AMR, DM, PSD and UCCA respectively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge labels, and overall F1 score. No anchors for AMR, and no node label and propertis for UCCA. We show the results of MRP metric on two datasets. “all” denotes all the examples for that specific MR, while lpps are a set of 100 sentences from The Little Prince, and annotated in all five meaning representations. To better understand the performance, we also reported the official results from two baseline models TUPA (Hershcovich and Arviv, 2019) and ERG (Oepen and Flickinger, 2019). Results At the time of official evaluation, we submitted three lexical anchoring parser, and then we submitted another phrasal-anchoring model for UCCA parsing during post-evaluation stage, and we leave EDS parsing as future work. The following sections are the official results and error breakdowns for lexical-anchoring and phrasalanchoring respectively. Official Results on Lexical Anchoring Table 2 shows the official results for our lexical-anchoring models on AMR, DM, PSD. By using our latent alignment based AMR parser, our system ranked top 1 in the AM"
K19-2013,W13-2322,0,0.234128,"Missing"
K19-2013,W12-3602,0,0.444902,"Missing"
K19-2013,J93-2003,0,0.0765145,"to abstract the meaning representation away from the surface token. The absense of explicit anchoring can present difficulties for parsing. In this section, by extensive analysis on previous work AMR alignments, we show that AMR nodes can be implicitly aligned to the leixical tokens in a sentence. AMR-to-String Alignments A straightforward solution to find the missing anchoring in an AMR Graph is to align it with a sentence; We denote it as AMR-to-String alignment. ISI alignments (Pourdamghani et al., 2014) first linearizes the AMR graph into a sequence, and then use IBM word alignment model (Brown et al., 1993) to align the lin139 _almost_a_1 23:29 ARG1 comp 2:9 _impossible_a_for 30:40 ARG1 ARG1 _similar_a_to 2:9 _a_q 0:1 ARG1 BV _apply_v_to 44:49 ARG2 _technique_n_1 10:19 udef_q 53:100 ARG3 _other_a_1 53:58 BV ARG1 _crop_n_1 59:65 _such+as_p 66:73 ARG1 udef_q 74:100 ARG2 udef_q 74:81 BV _cotton_n_1 74:81 BV implicit_conj 82:100 udef_q 82:100 L-INDEX R-INDEX BV udef_q 82:90 BV _and_c 91:94 L-INDEX _soybean_n_unknown 82:90 udef_q 95:100 R-INDEX BV _rice_n_1 95:100 Figure 1: Phrasal-anchoring in EDS[wsj#0209013], for the sentence &quot;A similar technique is almost impossible to apply to other crops, such"
K19-2013,K19-2007,0,0.175555,"cial results on DM and PSD shows that there is still around 2.5 points performance gap between our model and the top 1 model. data all lpps all lpps all Ours(1) lpps all Top 2 lpps TUPA single TUPA multi tops 63.95 71.96 61.30 72.63 65.92 72.00 78.15 83.00 labels 57.20 55.52 39.80 50.11 82.86 78.71 82.51 76.24 prop 22.31 26.42 27.70 20.25 77.26 58.93 71.33 51.79 edges 36.41 36.38 27.35 33.12 63.57 63.96 63.21 60.43 all 44.73 47.04 33.75 43.38 73.38 71.11 72.94 69.03 Table 4: Our parser on AMR ranked 1st. This table shows the error breakdown when comparing to the baseline TUPA model and top 2 (Che et al., 2019) in official results Official Results on Phrasal Anchoring Table 3 shows that our span-based CKY model for UCCA 145 data all ERG lpps all Top 1 lpps all Ours(7) lpps tops 91.83 95.00 93.23 96.48 70.95 84.00 labels 98.22 97.32 94.14 91.85 93.96 90.55 prop 95.25 97.75 94.83 94.36 92.13 91.91 anchors 98.82 99.46 98.40 99.04 97.25 97.96 edges 90.76 92.71 91.55 93.28 86.45 87.24 all 95.65 97.03 94.76 94.64 92.14 91.82 data all TUPA single lpps all TUPA multi lpps all (Che et al., 2019) lpps all Ours(*5) lpps all Ours + ELMo lpps Table 5: Our parser on DM ranked 7th. This table shows the error break"
K19-2013,S19-2002,0,0.118323,"Hence, we designed a copy mechanism (Luong et al., 2014) in our neural network architecture to decide whether to copying deterministic label given a word or estimate a classification probability from a fixed label set. 3.1.3 3.2 Let us now see our phrasal-anchoring parser for UCCA. We introduce the transformation we used to reduce UCCA parsing into a consituent parsing task, and finally introduce the detailed CKY model for the constituent parsing. 3.2.1 Graph-to-CT Transformation We propose to transform a graph into a constituent tree structure for parsing, which is also used in recent work (Jiang et al., 2019). Figure 3 shows an example of transforming a UCCA graph into a constituent tree. The primary transformation assigns the original label of an edge to its child node. Then to make it compatible with parsers for standard PennTree Bank format, we add some auxiliary nodes such as special non-terminal nodes, TOP, HEAD, and special terminal nodes TOKEN and MWE. We remove all the “remote” annotation in UCCA since the constituent tree structure does not support reentrance. A fully compatible transformation should support both graph-to-tree and tree-to-graph transformation. In our case, due to time con"
K19-2013,E17-1053,0,0.0133697,"chool of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . We proposed a graph-base"
K19-2013,P18-1249,0,0.119926,"ontinuous tokens to its nearest continuous parent nodes, we force every sub span are continuous in the transformed trees. We leave the postprocessing to recover those discontinuous as future work. For inference, given an input sentence, we first use the trained constituent tree parsing model to parse it into a tree, and then we transform a tree back into a directed graph by assigning the edge label as its child’s node label, and deleting those auxiliary labels, adding anchors to every remaining node. we use 8-layers with 8 heads transformer encoder, which shows better performance than LSTM in Kitaev and Klein (2018). Tree Factorization In the graph-to-tree transformation, we move the edge label to its child node. By assuming the labels for each node are independent, we factorize the tree structure prediction as independent span-label prediction as Equation 4. However, this assumption does not hold for UCCA. Please see more error analysis in §4.4 T ∗ = arg maxs(T ) T X s(i, j, l) s(T ) = (4) (i,j,l)∈T CKY Parsing By assuming the label prediction is independent of the splitting point, we can further factorize the whole tree as the following dynamic programming in Equation 5. sbest (i, i + 1) = maxs(i, i +"
K19-2013,P17-1014,0,0.0838003,"Missing"
K19-2013,K19-2004,0,0.281217,"Missing"
K19-2013,P14-1134,0,0.601467,"hang‡ , Adel Youssef‡ , Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phr"
K19-2013,P18-1037,0,0.231257,"AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . We proposed a graph-based parsing framework with a latent-alignment"
K19-2013,S16-1166,0,0.170913,"hrase-structure parsing for one of the phrasalanchoring MRs, UCCA. Our system submission ranked 1st in the AMR subtask, and later improvements shows promising results on other frameworks as well. 1 Introduction The design and implementation of broad-coverage and linguistically motivated meaning representation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theori"
K19-2013,N18-1202,0,0.0213396,"Adam (Kingma and Ba, 2014), using a batch size 64 for a graph-based model, and 250 for CKY-based model. Hyperparameters were tuned on the development set, based on labeled F1 between two graphs. We exploit early-stopping to avoid over-fitting. 4.3 MR AMR(1) PSD(6) DM(7) Ours (P/R/F1) 75/71/73.38 89/89/88.75 93/92/92.14 Top 1/3/5 (F1) 73.38/71.97/71.72 90.76/89.91/88.77 94.76/94.32/93.74 Table 2: Official results overview on unified MRP metric, we selected the performance from top 1/3/5 system(s) for comparison can achieve 74.00 F1 score on official test set, and ranked 5th. When adding ELMo (Peters et al., 2018) into our model, it can further improve almost 3 points on it. MR UCCA(5) EDS Ours (P/R/F1) 80.83/73.42/76.94 N/A Top 1/3/5 (F1) 81.67/77.80/73.22 94.47/90.75/89.10 Table 3: Official results overview on unified MRP metric, we selected the performance from top 1/3/5 system(s) for comparison. It shows our UCCA model for post-evluation can rank 5th 4.4 Error Breakdown Table 4, 5, 6 and 7 shows the detailed error breakdown of AMR, DM, PSD and UCCA respectively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge l"
K19-2013,D14-1048,0,0.63661,"l Anchoring Jie Cao†∗, Yi Zhang‡ , Adel Youssef‡ , Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Le"
K19-2013,S14-2056,0,0.368437,"Missing"
K19-2013,D15-1136,0,0.0535062,"Missing"
K19-2013,K19-2001,0,0.128815,"Missing"
K19-2013,N18-1106,0,0.0280505,"Missing"
K19-2013,D17-1129,0,0.298612,"Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . W"
K19-2013,K19-2003,0,0.104484,"ctively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge labels, and overall F1 score. No anchors for AMR, and no node label and propertis for UCCA. We show the results of MRP metric on two datasets. “all” denotes all the examples for that specific MR, while lpps are a set of 100 sentences from The Little Prince, and annotated in all five meaning representations. To better understand the performance, we also reported the official results from two baseline models TUPA (Hershcovich and Arviv, 2019) and ERG (Oepen and Flickinger, 2019). Results At the time of official evaluation, we submitted three lexical anchoring parser, and then we submitted another phrasal-anchoring model for UCCA parsing during post-evaluation stage, and we leave EDS parsing as future work. The following sections are the official results and error breakdowns for lexical-anchoring and phrasalanchoring respectively. Official Results on Lexical Anchoring Table 2 shows the official results for our lexical-anchoring models on AMR, DM, PSD. By using our latent alignment based AMR parser, our system ranked top 1 in the AMR subtask, and outperformed the top 5"
K19-2013,P15-2141,0,0.147739,"Missing"
K19-2013,S15-2153,0,0.353314,"Missing"
K19-2013,P19-1009,0,0.115507,"Missing"
K19-2013,S14-2008,1,0.896776,"Missing"
K19-2013,K15-1004,0,0.127265,"Missing"
L16-1645,D13-1184,1,0.677208,"re NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors derived from existing state-of-the-art NLP tools 4085 that serve as reference implementations for those tools’ features, a"
L16-1645,clarke-etal-2012-nlp,1,0.876122,"tion is exposed (if every example has the same representation, the classifier can make no useful prediction; and some features are more generally characteristic of specific focus items than others), and so the application developer has good reason to spend time carefully specifying potentially useful features. 3. E DISON E DISON is a Java library to support feature extraction in Natural Language Processing. It uses the data structures from illinois-core-utilities1 , another Java library from the Cognitive Computation Group2 . Together, these expand on an older version of E DISON described in (Clarke et al., 2012). 3.1. Data Structures The main data structure used by E DISON is called a TextAnnotation. It is used to represent a piece of text, such as a document, and collects all NLP annotations for that text. NLP annotations such as tokens, phrases, sentences, and other text-related constructs are represented in terms of spans of tokens/characters (Constituents) and edges (Relations) linking spans to each other. Each annotation source is represented as an independent View over the original text, that collects the Constituents and Relations generated by that source. Constituents and Relations can be lab"
L16-1645,W02-0109,0,0.162392,"peech-, and chunk-level features that can be easily combined for English text. It uses a very basic data structure to represent the input annotations it requires to generate the feature representation. FEXTOR is implemented in Python and C++ and offers support for multiple languages. It supports feature extraction across sentence boundaries. The workflow is oriented towards file-based interaction – like Fex, FEXTOR uses a purpose-built scripting language to determine feature extraction behavior. There is little explicit support for feature extraction for NLP in learning packages such as NLTK (Loper and Bird, 2002), 4091 in NLP development frameworks like GATE (Cunningham et al., 2002), or in NLP software bundles like Stanford’s CoreNLP (Manning et al., 2014). All these software frameworks provide integrated natural language processing tools, but they do not directly support extraction of arbitrary features by composing the outputs of these processes. Such features are essential to achieve good performance using machine learning models in many NLP tasks. E DISON explicitly supports feature extraction but, in contrast to FEXTOR and Fex, is oriented more towards programmatic interaction (although the pack"
L16-1645,P14-5010,0,0.0126095,"ns it requires to generate the feature representation. FEXTOR is implemented in Python and C++ and offers support for multiple languages. It supports feature extraction across sentence boundaries. The workflow is oriented towards file-based interaction – like Fex, FEXTOR uses a purpose-built scripting language to determine feature extraction behavior. There is little explicit support for feature extraction for NLP in learning packages such as NLTK (Loper and Bird, 2002), 4091 in NLP development frameworks like GATE (Cunningham et al., 2002), or in NLP software bundles like Stanford’s CoreNLP (Manning et al., 2014). All these software frameworks provide integrated natural language processing tools, but they do not directly support extraction of arbitrary features by composing the outputs of these processes. Such features are essential to achieve good performance using machine learning models in many NLP tasks. E DISON explicitly supports feature extraction but, in contrast to FEXTOR and Fex, is oriented more towards programmatic interaction (although the package can be run as an application that reads from and writes to files). It packages a suite of reference implementations of feature extractors used"
L16-1645,K15-1002,1,0.825134,"tive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors derived from existing state-of-the-art NLP tools 4085 that serve as reference impl"
L16-1645,J08-2005,1,0.435174,"n library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feature files. 3. A large suite of feature extractors deri"
L16-1645,W09-1119,1,0.68414,"ly fed to a ML system. E DISON is a feature extraction library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wrapper for E DISON that can be used as a standalone component to generate JSON-format feat"
L16-1645,rizzolo-roth-2010-learning,1,0.953117,"information about the way we organized the feature extractor classes and describes the process we used to port the feature extractors of some well-known CogComp NLP tools. 4.1. Organization and Navigation We use descriptive names for feature extractors based on the types of information they use, and the way these pieces of information are combined. There are five key characteristics of the feature extractors we processed: Programmatic integration with learning frameworks Programmatically, E DISON’s feature extractors can be easily integrated into JVM-based learning frameworks such as LBJava (Rizzolo and Roth, 2010), Mallet (McCallum, 2002), Weka (Hall et al., 2009), and Saul (Kordjamshidi et al., 2015). Here we will show programmatic integrations for the CogComp tools (LBJava/Saul) and provide a filebased integration for the other ML tools. We plan to create programmatic interfaces for these tools in the near future. For LBJava, features can be trivially wrapped as Classifier objects and used directly in the LBJava definition file. Figure 4 illustrates the way E DISON feature extractors can be used in the LBJava language, by creating a class that inherits from LBJava’s Classifier interface to wrap the f"
L16-1645,P98-2186,1,0.808399,"rom NLP data structures and either written to file in a generic format, or programmatically fed to a ML system. E DISON is a feature extraction library based on generic NLP data structures from the University of Illinois Cognitive Computation Group (CogComp)’s core NLP libraries (illinois-core-utilities). It provides an intuitive and versatile feature extraction API that can be used programmatically or as a stand-alone application generating files in JSON format for ML input. It contains reference implementations for feature extractors used in several CogComp NLP applications: Part of Speech (Roth and Zelenko, 1998), Chunking (Punyakanok and Roth, 2001), Named Entity Recognition (Ratinov and Roth, 2009), and Semantic Role Labeling (Punyakanok et al., 2008). We plan to add reference implementations for Coreference (Peng et al., 2015) and for Wikifier (Cheng and Roth, 2013). The key contributions of this work are: 1. A Java library, E DISON, that provides a simple, intuitive programmatic framework to apply feature extraction functions to generate new feature representations, and which comes with a simple search interface to help users find existing features that meet their needs. 2. A simple application wr"
L16-1645,J08-2002,0,0.0103322,"ion to other applications or to generate visual output for an end user. 3.2. Feature Extraction E DISON supports a range of feature types, from the standard combinations provided by Fex (Cumby and Roth, 2003; Cumby and Roth, 2000) and FEXTOR (Broda et al., 2013) – such as collocations of constituents within a specified context window, features combining different levels of annotation, features based on dependency parse paths between constituents – to more specialized features proven useful in more complex NLP tasks like semantic role labeling (e.g. subcategorization frames, or projected path (Toutanova et al., 2008)). These features are extracted from a TextAnnotation data structure populated with the appropriate source annotations: a feature extractor for part-of-speech bigrams will extract features only from a View populated with that information. In a supervised learning setting, the application uses labeled data to extract examples of the data items it wants the learning algorithm to classify. In the context of E DISON, we assume that this labeled data has been used to construct a View that contains a representation of the focus items as Constituents. The application code iterates over these focus it"
L18-1086,P06-4018,0,0.341529,"Missing"
L18-1086,P11-1056,1,0.783998,"Missing"
L18-1086,M98-1001,0,0.489588,"Missing"
L18-1086,clarke-etal-2012-nlp,1,0.65273,"tionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between t"
L18-1086,doddington-etal-2004-automatic,0,0.127526,"Missing"
L18-1086,N06-2015,0,0.256079,"Missing"
L18-1086,P10-2013,0,0.0751131,"Missing"
L18-1086,P14-5010,0,0.0181999,". Here is an example snippet showing how to annotate a sentence with C OG C OMP NLP Y: Pipeline. With all the Annotators generating the same data-structures, the P IPELINE project provides a simple interface to access Annotator components either individually or as a group, with a single function call. Use of P IPELINE is illustrated in Figure 3. A demo of P IPELINE is accessible online at http://nlp.cogcomp.org. One important aspect of our work is the collection of the major NLP annotators. Table 1 contains a summary of components that exist in other well-established NLP libraries. C ORE NLP (Manning et al., 2014) is a popular from ccg_nlpy import remote_pipeline pipeline = remote_pipeline.RemotePipeline() text = &quot;Hello, how are you. I am doing fine&quot; ta = pipeline.doc(text) print(ta.get_pos) # (UH Hello) (, ,) (WRB how) (VBP are) (PRP you)... 4. 1 543 Related Work https://github.com/CogComp/cogcomp-nlpy Task Dataset Measure Setting Result Tokenization POS (Roth and Zelenko, 1998) MASC (Ide et al., 2010) Accuracy – 97 Penn Treebank (Bies et al., 2015) F1 – 96.13 F1 – 91.12 F1 F1 F1 F1 F1 F1 – – – English Spanish Chinese 84.61 88.37 77.21 88.3 85 79.3 NER (Ratinov and Roth, 2009; Redman et al., 2016; Tsa"
L18-1086,W04-2705,0,0.100473,"Missing"
L18-1086,J05-1004,0,0.68556,"yakanok and Roth, 2000) CoNLL 2000 (Sang and Buchholz, 2000) F1 – 93.58 Temporal Normalization (Zhao et al., 2012) TempEval3 (UzZaman et al., 2013) Exact match F1 / Relaxed match F1 Temporal Span Extraction 79.35/ 83.4 F1 70.45 ACE 2005 (Walker et al., 2006) F1 F1 Temporal normalization, given a predicted temporal span Head detection Boundary detection given the head Head detection Boundary detection given the head Gold mention - Coarse Type Gold mention - Fine Type Test-I of Do and Roth (2012) Accuracy – 86.1 (Arivazhagan et al., 2016) F1 – 83.6 (Srikumar and Roth, 2013) F1 – 90.26 PropBank (Palmer et al., 2005a) F1 – 76.22 NomBank (Meyers et al., 2004) F1 – 66.97 Average of F1 score of MUC, B3 Gold mentions 77.05 CoNLL-12 (Pradhan et al., 2012) F1 ACE-05 (Walker et al., 2006) F1 Mention Detection F1 ERE F1 Relation Extraction (Chan and Roth, 2011) Taxonomic Relations (hypernyms, hyponyms, and co-hypernyms) Comma SRL (Arivazhagan et al., 2016) Preposition SRL (Srikumar and Roth, 2013) Verb SRL (Punyakanok et al., 2004) Nominal SRL(Punyakanok et al., 2004) Coreference (Samdani et al., 2014) B3 ACE-04 (Doddington et al., 2004) Wikifier (Tsai and Roth, 2016) F1 F1 F1 TAC-KBP 2016 EDL shared task 89.6 8"
L18-1086,D14-1162,0,0.0787689,"Missing"
L18-1086,W12-4501,0,0.0330716,"Missing"
L18-1086,W00-0721,1,0.380267,"Missing"
L18-1086,C04-1197,1,0.759993,"Missing"
L18-1086,W05-0639,0,0.0601394,"f C OG C OMP NLP core-utilities to extract features to be used by machine learning algorithms. E DISON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = a"
L18-1086,W09-1119,1,0.716643,"Missing"
L18-1086,rizzolo-roth-2010-learning,1,0.923167,"antic analysis, but also word and phrase similarity metrics. It provides essential support for text processing applications, including classes for text cleaning and for reading a number of popular NLP corpora. In addition to describing some key functionalities, we illustrate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is Te"
L18-1086,P98-2186,1,0.665416,"Missing"
L18-1086,P10-1122,1,0.792389,"SON enables users to define feature extraction functions that take as input the Views and Constituents created by C OG C OMP NLP’s Annotators. This makes it possible to not only develop feature sets like words, n-grams, and paths in parse trees, which work with a single View, but also more complex features that combine information from several Views. This library has been successfully used to facilitate the feature extraction for several higher level NLP applications like Semantic Role Labeling (Punyakanok et al., 2005), Coreference resolution (Rizzolo and Roth, 2016) and, Textual Entailment (Sammons et al., 2010), which use information across several Views over text to make a decision. 542 Figure 2: Illustration of the TextAnnotation, View, Constituent, and Relation data-structures in the Core Utilities module. // assume &apos;srlTa&apos; is a partially annotated text that comes from an earlier step TextAnnotation srlTa = ... AnnotatorService pipeline = PipelineFactory.buildPipeline(ViewNames.POS, ViewNames.NER_CONLL); TextAnnotation augmentedSrlTa = pipeline.annotateTextAnnotation(srlTa); List<Constituents> list = augmentedSrlTa.getView(ViewNames.POS).getConstituents(); System.out.println(list); // (NNP Pierre"
L18-1086,L16-1645,1,0.930972,"trate the use of C OG C OMP NLP components in developing a Semantic Role Labeling application: reading data from a corpus; augmenting the resulting data-structures with NLP components using the NLP pipeline; extracting features for input to machine learning algorithms; training classifiers using LBJAVA –another CogComp project (Rizzolo and Roth, 2010); serializing the system outputs; and adding the new SRL application to the pipeline for other applications to use. 2. Terminology The C OG C OMP NLP framework builds on the conceptual design and data-structures described in (Clarke et al., 2012; Sammons et al., 2016). Here we give a brief summary of the main structures and keywords used. A View is a datastructure which contains an annotation structure of a text; examples are tokens, lemmas or dependency parse trees. An Annotator is a class which produces a View given a text, and potentially some other Views. The main data-structure used is TextAnnotation, which contains a document (e.g. a phrase, a sentence, a paragraph) and its various Views. 3. Framework Design A high-level view of the system is depicted in Figure 1. The boxes show modules and edges show the dependencies between them (with the targets b"
L18-1086,W00-0726,0,0.278326,"Missing"
L18-1086,Q13-1019,1,0.873032,"Missing"
L18-1086,W03-0419,0,0.250227,"Missing"
L18-1086,C16-2031,1,0.811715,"Missing"
L18-1086,S13-2001,0,0.0727449,"Missing"
L18-1086,Q15-1025,1,0.803667,"tic Parse, and Semantic Role Labeling • TAC/ERE Event, Relation, and Named Entity Similarity Utilities. For calculating semantic similarity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level dec"
L18-1086,P14-3006,0,0.0295918,"rity between words, phrases, and entities using both structured and distributional representations. Each similarity function compares objects (words, phrases, named entities, sentences) and returns a score indicating how similar they are. Depending on the inputs, different algorithms are available: • Word Similarity: For computing the similarity between two words. The following representations are currently supported: word2vec (Mikolov et al., 2013), paragram (Wieting et al., 2015), esa (Gabrilovich and Markovitch, 2007), glove (Pennington et al., 2014), wordnet (Do et al., 2009), phrase2vec (Yin and Schütze, 2014). Here is a sample usage: String representation = &quot;esa&quot;; WordSim ws = new WordSim(representation); ws.compare(&quot;word&quot;, &quot;sentence&quot;); // 0.37 • Named-Entity Similarity: Comparing named entities requires a different class of algorithm. C OG C OMP NLP’s current algorithm is based on (Do et al., 2009): NESim nesim = new NESim(); nesim.compare(&quot;Donald Trump&quot;, &quot;Trump&quot;); // 0.9 • Phrasal Similarity: Algorithms to combine lexical-level systems to make sentence-level decisions (Do et al., 2009): Metric llm = new LLMStringSim(config); String s1 = &quot;Jack bought Alex&apos;s car&quot;; String s2 = &quot;Alex sold his car to"
L18-1086,N12-3008,1,0.753759,"Missing"
N10-1066,D08-1031,1,0.109195,"idden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.6"
N10-1066,P07-1083,0,0.0229497,"operties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning ta"
N10-1066,N09-1034,1,0.852715,"hus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constr"
N10-1066,2008.amta-papers.4,0,0.0547444,"atent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language model. Unfortunately, their implementation is not guaranteed to converge. In CRF-like models with latent variables (McCal436 lum et al., 2005), the decision function marginalizes over the all hidden states when presented with an input example. Unfortunately, the computational cost of applying their framework is prohibitive with constrained latent representations. In contrast, our framework requires only the best hidden representation instead of marginalizing over all possible representations, thus reducing the computational effort. 7 Conclu"
N10-1066,P09-1053,0,0.174568,"ing into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constrained Latent Represent"
N10-1066,C04-1051,0,0.692767,"intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by definin"
N10-1066,P08-2014,1,0.934205,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,D08-1037,1,0.861012,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,H05-1049,0,0.0443099,"a and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and"
N10-1066,D08-1084,0,0.222611,"n et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning in"
N10-1066,J08-2005,1,0.216571,"b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al., 2008). All other words are connected by dependency edges. The intermediate representation is an alignment between the nodes and edges of the graphs. We used three hidden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the feature"
N10-1066,W06-1603,0,0.101349,"e learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show"
N10-1066,W04-3219,0,0.0114581,"ence formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints. While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations. 2 Preliminaries We introduce notation using the Paraphrase Identification task as a running example. This is the bi430 nary classification task of identifying whether one sentence is a paraphrase of another. A paraphrase pair from the MSR Paraphrase corpus (Quirk et al., 2004) is shown in Figure 1. In order to identify that the sentences paraphrase each other , we need to align constituents of these sentences. One possible alignment is shown in the figure, in which the dotted edges correspond to the aligned constituents. An alignment can be specified using binary variables corresponding to every edge between constituents, indicating whether the edge is included in the alignment. Different activations of these variables induce the space of intermediate representations. The notification was first reported Friday by MSNBC. MSNBC.com first reported the CIA request on F"
N10-1066,P09-2015,1,0.892648,"hrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – spec"
N10-1066,U06-1019,0,0.169702,"ngtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 2: Summary of latent variables and feature resources for the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: Dependency labels, NODE-INFO: corresponding node alignment resources, N/A: Hidden variable not used"
N10-1066,P06-1051,0,0.0183547,"hat the similarity in performance between the joint LCLR algorithm and the two stage 4 Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There a"
N10-1066,W07-1401,0,\N,Missing
P08-1117,P05-1022,0,0.0890764,"Missing"
P08-1117,P04-1054,0,0.0535599,"n-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems"
P08-1117,P08-1079,1,0.824531,"to identify specialized phrase types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations be"
P08-1117,P07-1030,1,0.762465,"e types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities;"
P08-1117,P07-2040,0,0.0615941,"unberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to"
P08-1117,J93-2004,0,0.0322102,"represented by commas, there are two main strands of research with similar goals: 1) systems that directly analyze commas, whether labeling them with syntactic information or correcting inappropriate use in text; and 2) systems that extract relations from text, typically by trying to identify paraphrases. The significance of interpreting the role of commas in sentences has already been identified by (van Delden and Gomez, 2002; Bayraktar et al., 1998) and others. A review of the first line of research is given in (Say and Akman, 1997). In (Bayraktar et al., 1998) the WSJ PennTreebank corpus (Marcus et al., 1993) is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created. However, they do not study the extraction of entailed relations as a function of the comma’s interpretation. Furthermore, the syntactic patterns they identify are unlexicalized and would not support the level of semantic relations that we show in this paper. Finally, theirs is a manual process completely dependent on syntactic patterns. While our comma resolution system uses syntactic parse information as its main source of features, the approach we have developed focuses on the"
P08-1117,P06-1015,0,0.0143877,"o relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of senten"
P08-1117,P06-1102,0,0.0129114,"tem has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general ap"
P08-1117,W04-2401,1,0.775126,"eloping comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited ac"
P08-1117,P06-2094,0,0.0190797,", it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and li"
P08-1117,W04-3206,0,0.0131183,", so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of sentences from WSJ (not among"
P13-1089,D12-1102,1,0.511983,"gin-based Decomposed Amortized Inference Gourab Kundu∗ and Vivek Srikumar∗ and Dan Roth University of Illinois, Urbana-Champaign Urbana, IL. 61801 {kundu2, vsrikum2, danr}@illinois.edu Abstract some structures like sequences or parse trees, specialized and tractable dynamic programming algorithms have proven to be very effective. However, as the structures under consideration become increasingly complex, the computational problem of predicting structures can become very expensive, and in the worst case, intractable. In this paper, we focus on an inference technique called amortized inference (Srikumar et al., 2012), where previous solutions to inference problems are used to speed up new instances. The main observation that leads to amortized inference is that, very often, for different examples of the same size, the structures that maximize the score are identical. If we can efficiently identify that two inference problems have the same solution, then we can re-use previously computed structures for newer examples, thus giving us a speedup. This paper has two contributions. First, we describe a novel algorithm for amortized inference called margin-based amortization. This algorithm is on an examination"
P13-1089,D11-1003,0,0.153732,"ence can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances and the number of unique observed part-of-speech structures in the Gigaword corpus. Note that the"
P13-1089,J08-2002,0,0.0174154,"for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inference variables are obtained from a classifier trained on the PropBank corpus. Constraints encode structural and linguistic knowledge about the problem. For details about the formulations of the inference problem, please see (Punyakanok et al., 2008). Recall from Sectio"
P13-1089,P06-2019,0,0.0254204,"oal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inferenc"
P13-1089,J02-3001,0,0.00542721,"across these two sets of entities to obtain two independent inference problems. Tasks We report the performance of inference on two NLP tasks: semantic role labeling and the task of extracting entities and relations from text. In both cases, we used an existing formulation for structured inference and only modified the inference calls. We will briefly describe the problems and the implementation and point the reader to the literature for further details. Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sentence and annotating them with semantic roles (Gildea and Jurafsky, 2002; Palmer et al., 2010) . For example, in the sentence Mrs. Haag plays Eltiani., the verb plays takes two arguments: Mrs. Haag, the actor, labeled as A0 and Eltiani, the role, labeled as A1. It has been shown in prior work (Punyakanok et al., 2008; Toutanova et al., 2008) that making a globally coherent prediction boosts performance of SRL. In this work, we used the SRL system of (Punyakanok et al., 2008), where one inference problem is generated for each verb and each inference variables encodes the decision that a given constituent in the sentence takes a specific role. The scores for the inf"
P13-1089,W10-2924,0,0.150342,"typically occurs much more frequently than the others. Figure 1 illustrates this observation in the context of part-of-speech tagging. If we can efficiently characterize and identify inference instances that have the same solution, we can take advantage of previously performed computation without paying the high computational cost of inference. We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) provides a convenient analytical tool for representing structured prediction problems. The general setting con"
P13-1089,D10-1125,0,0.0992846,"algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of"
P13-1089,J08-2005,1,0.924523,"3 Association for Computational Linguistics data. Furthermore, among the observed structures, a small subset typically occurs much more frequently than the others. Figure 1 illustrates this observation in the context of part-of-speech tagging. If we can efficiently characterize and identify inference instances that have the same solution, we can take advantage of previously performed computation without paying the high computational cost of inference. We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling (Punyakanok et al., 2008) and the problem of recognizing entities and relations in text (Roth and Yih, 2007; Kate and Mooney, 2010). In these problems, the inference problem has been framed as an integer linear program (ILP). We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods. 2 Problem Definition and Notation Structured output prediction encompasses a wide variety of NLP problems like part-of-speech tagging, parsing and machine translation. The language of 0-1 integer linear programs (ILP) prov"
P13-1089,N12-1008,0,0.0318112,"mance metrics – the percentage decrease in the number of ILP calls, and the percentage decrease in the wall-clock inference time. These are comparable to the speedup and clock speedup defined in (Srikumar et al., 2012). For measuring time, since other aspects of prediction (like feature extraction) are the same across all settings, we only measure the time taken for inference and ignore other aspects. For both 2 Discussion Lagrangian Relaxation in the literature In the literature, in applications of the Lagrangian relaxation technique (such as (Rush and Collins, 2011; Chang and Collins, 2011; Reichart and Barzilay, 2012) and others), the relaxed problems are solved using specialized algorithms. However, in both the relaxations considered in this paper, even the relaxed problems cannot be solved without an ILP solver, and yet we can see improvements from decomposition in Table 1. To study the impact of amortization on running time, we modified our decomposition based inference algorithm to solve each sub-problem using the ILP solver instead of amortization. In these experiments, we ran Lagrangian relaxation for until convergence or at most T iterations. After T iterations, we call the ILP solver and solve the"
P13-1089,W06-1616,0,0.0276182,"nd the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observati"
P13-1089,W04-2401,1,0.726344,"with a score. The goal of inference is to find the highest scoring global assignment of the variables from a feasible set of assignments, which is defined by linear inequalities. While efficient inference algorithms exist for special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivati"
P13-1089,P11-1008,0,0.0951259,"the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances and the number of unique observed part-of-speech structures in the Gigaw"
P13-1089,D10-1001,0,0.0243174,"or special families of structures (like linear chains and trees), in the general case, inference can be computationally intractable. One approach to deal with the computational complexity of inference is to use an off-the-shelf ILP solver for solving the inference problem. This approach has seen increasing use in the NLP community over the last several years (for example, (Roth and Yih, 2004; Clarke and Lapata, 2006; Riedel and Clarke, 2006) and many others). Other approaches for solving inference include the use of cutting plane inference (Riedel, 2009), dual decomposition (Koo et al., 2010; Rush et al., 2010) and the related method of Lagrangian relaxation (Rush and Collins, 2011; Chang and Collins, 2011). (Srikumar et al., 2012) introduced the notion of an amortized inference algorithm, defined as an inference algorithm that can use previous predictions to speed up inference time, thereby giving an amortized gain in inference time over the lifetime of the program. The motivation for amortized inference comes from the observation that though the number of possible structures could be large, in practice, only a small number of these are ever seen in real Figure 1: Comparison of number of instances"
P17-1173,P14-1130,0,0.172407,"Given two feature extractors f1 , f2 ∈ F, their conjunction (denoted by &) can be interpreted as an extension of Boolean conjunction. Indicator features like bigram are predicates for certain observations. Conjoining indicator features for two predicates is equivalent to an indicator feature for the Boolean conjunction of the predicates. More generally, with feature extractors that produce real valued vectors, the conjunction will produce their tensor product. The equivalence of feature conjunctions to tensor products has been explored and exploited in recent literature for various NLP tasks (Lei et al., 2014; Srikumar and Manning, 2014; Gormley et al., 2015; Lei et al., 2015). We can further generalize this with an additional observation that is crucial for the rest of this paper. We argue that the conjunction operator produces symmetric tensor products rather than general tensor products. To see why, consider the bigram example. Though we defined the bigram feature as the conjunction of w-1 and w, their ordering is irrelevant from classification perspective – the eventual goal is to associate weights with this combination of features. This observation allows us to formally define the conjunction"
P17-1173,N15-1121,0,0.0966885,"by &) can be interpreted as an extension of Boolean conjunction. Indicator features like bigram are predicates for certain observations. Conjoining indicator features for two predicates is equivalent to an indicator feature for the Boolean conjunction of the predicates. More generally, with feature extractors that produce real valued vectors, the conjunction will produce their tensor product. The equivalence of feature conjunctions to tensor products has been explored and exploited in recent literature for various NLP tasks (Lei et al., 2014; Srikumar and Manning, 2014; Gormley et al., 2015; Lei et al., 2015). We can further generalize this with an additional observation that is crucial for the rest of this paper. We argue that the conjunction operator produces symmetric tensor products rather than general tensor products. To see why, consider the bigram example. Though we defined the bigram feature as the conjunction of w-1 and w, their ordering is irrelevant from classification perspective – the eventual goal is to associate weights with this combination of features. This observation allows us to formally define the conjunction operator as: (f1 &f2 ) (x) = vec (f1 (x) f2 (x)) (2) Here, vec (·) s"
P17-1173,D09-1005,0,0.0257152,"smallest (or most efficient) factorization. The junction tree construction determines the factorization quality. Semirings in NLP. Semirings abound in NLP, though primarily as devices to design efficient inference algorithms for various graphical models (e.g. Wainwright and Jordan, 2008; Sutton et al., 2012). Goodman (1999) synthesized various parsing algorithms in terms of semiring operations. Since then, we have seen several explorations of the interplay between weighted dynamic programs and semirings for inference in tasks such as parsing and machine translation (e. g. Eisner et al., 2005; Li and Eisner, 2009; Lopez, 2009; Gimpel and Smith, 2009). Allauzen et al. (2003) developed efficient algorithms for constructing statistical language models by exploiting the algebraic structure of the probability semiring. Feature Extraction and Modeling Languages. Much work around features in NLP is aimed at improving classifier accuracy. There is some work on developing languages to better construct feature spaces (Cumby and Roth, 2002; Broda et al., 2013; Sammons et al., 2016), but they do not formalize feature extraction from an algebraic perspective. We expect that the algorithm proposed in this paper can"
P17-1173,E09-1061,0,0.0267647,"icient) factorization. The junction tree construction determines the factorization quality. Semirings in NLP. Semirings abound in NLP, though primarily as devices to design efficient inference algorithms for various graphical models (e.g. Wainwright and Jordan, 2008; Sutton et al., 2012). Goodman (1999) synthesized various parsing algorithms in terms of semiring operations. Since then, we have seen several explorations of the interplay between weighted dynamic programs and semirings for inference in tasks such as parsing and machine translation (e. g. Eisner et al., 2005; Li and Eisner, 2009; Lopez, 2009; Gimpel and Smith, 2009). Allauzen et al. (2003) developed efficient algorithms for constructing statistical language models by exploiting the algebraic structure of the probability semiring. Feature Extraction and Modeling Languages. Much work around features in NLP is aimed at improving classifier accuracy. There is some work on developing languages to better construct feature spaces (Cumby and Roth, 2002; Broda et al., 2013; Sammons et al., 2016), but they do not formalize feature extraction from an algebraic perspective. We expect that the algorithm proposed in this paper can be integrate"
P17-1173,D11-1139,0,0.144231,"the original work that developed the feature representations for further details. For both the original and the factorized feature extractors, we report (a) the number of additions and conjunctions at the template level, and, (b) the time for feature extraction on the entire dataset. For the time measurements, we report average times for the original and factorized feature extractors over five paired runs to average out variations in system load.1 6.1 Text Chunking We use data from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000) of text chunking and the feature set described by Martins et al. (2011), consisting of the following templates extracted at each word: (1) Up to 3-grams of POS tags within a window of size ten centered at the word, (2) up to 3-grams of words, within a window of size six centered at the word, and (3) up to 2-grams of word shapes, within a window of size 1 We performed all our experiments on a server with 128GB RAM and 24 CPU cores, each clocking at 2600 MHz. Setting Original Factorized Size + & 47 75 47 54 Average feature extraction time (ms) 17776.6 4294.2 Table 1: Comparison of the original and factorized feature extractors for the text chunking task. The time i"
P17-1173,P15-1013,0,0.0176725,"libraries geared towards designing feature rich models (e.g. McCallum et al., 2009; Chang et al., 2015). Representation vs. Speed. As the recent successes (Goodfellow et al., 2016) of distributed representations show, the representational capacity of a feature space is of primary importance. Indeed, several recent lines of work that use distributed representations have independently identified the connection between conjunctions (of features or factors in a factor graph) and tensor products (Lei et al., 2014; Srikumar and Manning, 2014; Gormley et al., 2015; Yu et al., 2015; Lei et al., 2015; Primadhanty et al., 2015). They typically impose sparsity or low-rank requirements to induce better representations for learning. In this paper, we use the connection between tensor products and conjunctions to prove algebraic properties of feature extractors, leading to speed improvements via factorization. In this context, we note that in both our experiments, the number of conjunctions are reduced by factorization. We argue that this is an important saving because conjunctions can be a more expensive operation. This is especially true when dealing with dense feature representations, as is increasingly common with w"
P17-1173,H05-1036,0,0.0283503,"hat we will find the smallest (or most efficient) factorization. The junction tree construction determines the factorization quality. Semirings in NLP. Semirings abound in NLP, though primarily as devices to design efficient inference algorithms for various graphical models (e.g. Wainwright and Jordan, 2008; Sutton et al., 2012). Goodman (1999) synthesized various parsing algorithms in terms of semiring operations. Since then, we have seen several explorations of the interplay between weighted dynamic programs and semirings for inference in tasks such as parsing and machine translation (e. g. Eisner et al., 2005; Li and Eisner, 2009; Lopez, 2009; Gimpel and Smith, 2009). Allauzen et al. (2003) developed efficient algorithms for constructing statistical language models by exploiting the algebraic structure of the probability semiring. Feature Extraction and Modeling Languages. Much work around features in NLP is aimed at improving classifier accuracy. There is some work on developing languages to better construct feature spaces (Cumby and Roth, 2002; Broda et al., 2013; Sammons et al., 2016), but they do not formalize feature extraction from an algebraic perspective. We expect that the algorithm propo"
P17-1173,E09-1037,0,0.0253502,"rization. The junction tree construction determines the factorization quality. Semirings in NLP. Semirings abound in NLP, though primarily as devices to design efficient inference algorithms for various graphical models (e.g. Wainwright and Jordan, 2008; Sutton et al., 2012). Goodman (1999) synthesized various parsing algorithms in terms of semiring operations. Since then, we have seen several explorations of the interplay between weighted dynamic programs and semirings for inference in tasks such as parsing and machine translation (e. g. Eisner et al., 2005; Li and Eisner, 2009; Lopez, 2009; Gimpel and Smith, 2009). Allauzen et al. (2003) developed efficient algorithms for constructing statistical language models by exploiting the algebraic structure of the probability semiring. Feature Extraction and Modeling Languages. Much work around features in NLP is aimed at improving classifier accuracy. There is some work on developing languages to better construct feature spaces (Cumby and Roth, 2002; Broda et al., 2013; Sammons et al., 2016), but they do not formalize feature extraction from an algebraic perspective. We expect that the algorithm proposed in this paper can be integrated into such feature const"
P17-1173,J99-4004,0,0.247086,"entences, etc) to a vector space and show that this set forms a commutative semiring with respect to feature addition and feature conjunction. An immediate consequence of the semiring characterization is a computational one. Every semiring admits the Generalized Distributive Law (GDL) Algorithm (Aji and McEliece, 2000) that exploits the distributive property to provide computational speedups. Perhaps the most common manifestation of this algorithm in NLP is in the form of inference algorithms for factor graphs and Bayesian networks like the max-product, maxsum and sum-product algorithms (e.g. Goodman, 1999; Kschischang et al., 2001). When applied to feature extractors, the GDL algorithm can refactor a feature extractor into a faster one by reducing redundant computation. In this paper, we propose a junction tree construction to allow such refactoring. Since the refactoring is done at the feature template level, the actual computational savings grow as classifiers encounter more examples. We demonstrate the practical utility of our approach by factorizing existing feature sets for text chunking and relation extraction. We show that, by reducing the number of operations performed, we can obtain s"
P17-1173,L16-1645,1,0.783278,"een weighted dynamic programs and semirings for inference in tasks such as parsing and machine translation (e. g. Eisner et al., 2005; Li and Eisner, 2009; Lopez, 2009; Gimpel and Smith, 2009). Allauzen et al. (2003) developed efficient algorithms for constructing statistical language models by exploiting the algebraic structure of the probability semiring. Feature Extraction and Modeling Languages. Much work around features in NLP is aimed at improving classifier accuracy. There is some work on developing languages to better construct feature spaces (Cumby and Roth, 2002; Broda et al., 2013; Sammons et al., 2016), but they do not formalize feature extraction from an algebraic perspective. We expect that the algorithm proposed in this paper can be integrated into such feature construction languages, and also into libraries geared towards designing feature rich models (e.g. McCallum et al., 2009; Chang et al., 2015). Representation vs. Speed. As the recent successes (Goodfellow et al., 2016) of distributed representations show, the representational capacity of a feature space is of primary importance. Indeed, several recent lines of work that use distributed representations have independently identified"
P17-1173,P15-1015,0,0.0246583,"ntations, as is increasingly common with word vectors and neural networks, because conjunctions of dense feature vectors are tensor products, which can be slow. Finally, while training classifiers can be time consuming, when trained classifiers are deployed, feature extraction will dominate computation time over the classifier’s lifetime. However, the prediction step includes both feature extraction and computing inner products between features and weights. Many features may be associated with zero weights because of sparsity-inducing learning (e.g. Andrew and Gao, 2007; Martins et al., 2011; Strubell et al., 2015). Since these two aspects are orthogonal to each other, the factorization algorithm presented in this paper can be used to speed up extraction of those features that have non-zero weights. 8 Conclusion In this paper, we studied the process of feature extraction using an algebraic lens. We showed that the set of feature extractors form a commutative semiring over addition and conjunction. We exploited this characterization to develop a factorization algorithm that simplifies feature extractors to be more computationally efficient. We demonstrated the practical value of the refactoring algorithm"
P17-1173,D15-1205,0,0.0675874,"r conjunction (denoted by &) can be interpreted as an extension of Boolean conjunction. Indicator features like bigram are predicates for certain observations. Conjoining indicator features for two predicates is equivalent to an indicator feature for the Boolean conjunction of the predicates. More generally, with feature extractors that produce real valued vectors, the conjunction will produce their tensor product. The equivalence of feature conjunctions to tensor products has been explored and exploited in recent literature for various NLP tasks (Lei et al., 2014; Srikumar and Manning, 2014; Gormley et al., 2015; Lei et al., 2015). We can further generalize this with an additional observation that is crucial for the rest of this paper. We argue that the conjunction operator produces symmetric tensor products rather than general tensor products. To see why, consider the bigram example. Though we defined the bigram feature as the conjunction of w-1 and w, their ordering is irrelevant from classification perspective – the eventual goal is to associate weights with this combination of features. This observation allows us to formally define the conjunction operator as: (f1 &f2 ) (x) = vec (f1 (x) f2 (x))"
P17-1173,W00-0726,0,0.245667,"g feature representations that we briefly describe. We refer the reader to the original work that developed the feature representations for further details. For both the original and the factorized feature extractors, we report (a) the number of additions and conjunctions at the template level, and, (b) the time for feature extraction on the entire dataset. For the time measurements, we report average times for the original and factorized feature extractors over five paired runs to average out variations in system load.1 6.1 Text Chunking We use data from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000) of text chunking and the feature set described by Martins et al. (2011), consisting of the following templates extracted at each word: (1) Up to 3-grams of POS tags within a window of size ten centered at the word, (2) up to 3-grams of words, within a window of size six centered at the word, and (3) up to 2-grams of word shapes, within a window of size 1 We performed all our experiments on a server with 128GB RAM and 24 CPU cores, each clocking at 2600 MHz. Setting Original Factorized Size + & 47 75 47 54 Average feature extraction time (ms) 17776.6 4294.2 Table 1: Comparison of the original"
P17-1173,N15-1155,0,0.0234365,"struction languages, and also into libraries geared towards designing feature rich models (e.g. McCallum et al., 2009; Chang et al., 2015). Representation vs. Speed. As the recent successes (Goodfellow et al., 2016) of distributed representations show, the representational capacity of a feature space is of primary importance. Indeed, several recent lines of work that use distributed representations have independently identified the connection between conjunctions (of features or factors in a factor graph) and tensor products (Lei et al., 2014; Srikumar and Manning, 2014; Gormley et al., 2015; Yu et al., 2015; Lei et al., 2015; Primadhanty et al., 2015). They typically impose sparsity or low-rank requirements to induce better representations for learning. In this paper, we use the connection between tensor products and conjunctions to prove algebraic properties of feature extractors, leading to speed improvements via factorization. In this context, we note that in both our experiments, the number of conjunctions are reduced by factorization. We argue that this is an important saving because conjunctions can be a more expensive operation. This is especially true when dealing with dense feature repr"
P17-1173,P05-1053,0,0.0654889,"irst, we see that the factorization reduces the number of feature conjunction operations. Thus, to produce exactly the same feature vector, the factorized feature extractor does less work. The time results show that this computational gain is not merely a theoretical one; it also manifests itself practically. 6.2 Relation Extraction Our second experiment is based on the task of relation extraction using the English section of the ACE 2005 corpus (Walker et al., 2006). The goal is to identify semantic relations between two entity mentions in text. We use the feature representation developed by Zhou et al. (2005) as part of an investigation of how various lexical, syntactic and semantic sources of information affect the relation extraction task. To this end, the feature set consists of word level information about mentions, their entity types, their relationships with chunks, path features from parse trees, and semantic features based on WordNet and various word lists. Given the complexity of the features, we do not describe them here and refer the reader to the original work for details. Note that compared to the chunking features, these features are more diverse in their computational costs. We repo"
P17-1173,P03-1006,0,\N,Missing
P18-1018,P17-4019,1,0.730567,"ittle Prince, both to assess whether the scheme was applicable without major guidelines changes and to prepare the annotators for this genre. For the final annotation study, we chose chapters 4 and 5, in which 242 markables of 52 types were identified heuristically (§6.2). The types of, to, in, as, from, and for, as well as possessives, occurred at least 10 times. Annotators had the option to mark units as false positives using special labels (see §4) in addition to expressing uncertainty about the unit. For the annotation process, we adapted the open source web-based annotation tool UCCAApp (Abend et al., 2017) to our workflow, by extending it with a type-sensitive ranking module for the list of categories presented to the annotators. Annotators. Five annotators (A, B, C, D, E), all authors of this paper, took part in this study. All are computational linguistics researchers with advanced training in linguistics. Their involvement in the development of the scheme falls on a spectrum, with annotator A being the most active figure in guidelines development, and annotator E not being Labels involved in developing the guidelines and learning the scheme solely from reading the manual. Annotators A, B, an"
P18-1018,W17-6901,0,0.0631207,"OPIC ) flows from O RIG INATOR to R ECIPIENT , perhaps via an I NSTRU MENT . For AGENT , C O -AGENT , E XPERIENCER , O RIGINATOR, R ECIPIENT, B ENEFICIARY, P OS SESSOR, and S OCIAL R EL, the object of the preposition is prototypically animate. Because prepositions and possessives cover a vast swath of semantic space, limiting ourselves to 50 categories means we need to address a great many nonprototypical, borderline, and special cases. We have done so in a 75-page annotation manual with over 400 example sentences (Schneider et al., 2018). Finally, we note that the Universal Semantic Tagset (Abzianidze and Bos, 2017) defines a crosslinguistic inventory of semantic classes for content and function words. SNACS takes a similar approach to prepositions and possessives, which in Abzianidze and Bos’s (2017) specification are simply tagged REL, which does not disambiguate the nature of the relational meaning. Our categories can thus be understood as refinements to REL. 3.3 Adopting the Construal Analysis Hwang et al. (2017) have pointed out the perils of teasing apart and generalizing preposition semantics so that each use has a clear supersense label. One key challenge they identified is that the preposition i"
P18-1018,W13-2322,1,0.846996,"l., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitati"
P18-1018,C16-1256,0,0.420421,"t to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotation study or release a corpus to establish its feasibility empirically. We address that gap here. Second, 75 categories is an unwieldy number for both annotators and disambiguation systems. Some are quite specialized and extremely rare in STREUSLE 3.0, which causes data sparseness issues for supervised learning. In fact, the only published disambiguation system for preposition supersenses collapsed the distinctions to just 12 labels (Gonen and Goldberg, 2016). Hwang et al. (2017) remarked that solving the aforementioned problem could remove the need for many of the specialized categories and make the taxonomy more tractable for annotators and systems. We substantiate this here, defining a new hierarchy with just 50 categories (SNACS, §3) and providing disambiguation results for the full set of distinctions. Finally, given the semantic overlap of possessive case and the preposition of, we saw an opportunity to broaden the application of the scheme to include possessives. Our reannotated corpus, STREUSLE 4.0, thus has supersense annotations for over"
P18-1018,C10-2052,0,0.0584241,"Missing"
P18-1018,L18-1242,1,0.843466,". Three labels never appear in the annotated corpus: T EMPORAL from the C IRCUMSTANCE hierarchy, and PARTI CIPANT and C ONFIGURATION which are both the highest supersense in their respective hierarchies. While all remaining supersenses are attested as scene roles, there are some that never occur as functions, such as O RIGINATOR, which is most often realized as P OSSESSOR or S OURCE, and E XPERI ENCER . It is interesting to note that every subtype of C IRCUMSTANCE (except T EMPORAL) appears as both scene role and function, whereas many of the subtypes of the other two hierarchies are lim189 8 Blodgett and Schneider (2018) detail the extension of the scheme to possessives. 9 In the corpus, lexical expression tokens appear alongside a lexical category indicating which inventory of supersenses, if any, applies. SNACS-annotated units are those with ADP (adposition), PP, PRON . POSS (possessive pronoun), etc., whereas DISC (discourse) and CCONJ expressions do not receive any supersense. Refer to the STREUSLE README for details. ited to either role or function. This reflects our view that prepositions primarily capture circumstantial notions such as space and time, but have been extended to cover other semantic rela"
P18-1018,P11-2056,0,0.0371708,"Missing"
P18-1018,D09-1047,0,0.0768296,"a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to"
P18-1018,S17-1022,1,0.841894,"Missing"
P18-1018,P14-1120,0,0.460243,"omberg, 2010). Possessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on c"
P18-1018,L16-1630,0,0.0318812,"2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016)."
P18-1018,S07-1005,0,0.14038,"Missing"
P18-1018,J09-2002,0,0.169265,"Missing"
P18-1018,P14-5010,0,0.00262509,"tems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters. Gold tokenization was used throughout. Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation—i.e., tokens with special labels (see §4) were excluded. To test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0 (Manning et al., 2014).13 Named entity tags from the default 12-class CoreNLP model were used in all conditions. 6.2 Target Identification §3.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as 13 The CoreNLP parser was trained on all 5 genres of the English Web Treebank—i.e., a superset of our training set. Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested"
P18-1018,W04-2609,0,0.110573,"ll be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbas"
P18-1018,J05-1004,0,0.433013,"Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotate"
P18-1018,saint-dizier-2006-prepnet,0,0.045623,"can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple le"
P18-1018,W16-1712,1,0.912313,"Missing"
P18-1018,W15-1612,1,0.845402,"oridou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banaresc"
P18-1018,D11-1012,1,0.862736,"ternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009"
P18-1018,Q13-1019,1,0.934474,"ings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitations of our approach became clear to us over time. First, as pointed out by Hwang et al. (2017), the one-label-per-token assumption in STREUSLE is flawed because it in some cases puts into conflict the semantic role of the PP with respect to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotati"
P18-1018,N09-3017,0,0.0322753,"lations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, m"
P18-1018,P13-1037,0,0.0377858,"s—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are t"
P18-1018,S07-1051,0,0.0481426,"ssessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meanin"
P18-1018,L16-1262,0,\N,Missing
P19-1028,D15-1075,0,0.0428723,"ithout any extra trainable parameters needed. The improvement is particularly strong with small training sets. With more data, neural models are less reliant on external information. As a result, the improvement with larger datasets is smaller. How does it compare to pretrained encoders? Pretrained encoders (e.g. ELMo and BERT (Devlin et al., 2018)) improve neural models with improved representations, while our framework augNatural Language Inference Unlike in the machine comprehension task, here we explore logic rules that bridge attention neurons and output neurons. We use the SNLI dataset (Bowman et al., 2015), and base our framework on a variant of the decomposable attention (DAtt, Parikh et al., 2016) model where we replace its projection encoder with bidirectional LSTM (namely L-DAtt). Model Again, we abstract the pipeline of LDAtt model, only focusing on layers which our framework works on. Given a premise p and a hypothesis h, we summarize the model as: p, h = encoder(p), encoder(h) ← − − a ,→ a = σ(layers(p, h)) −, → − y = σ(layers(p, h, ← a a )) (6) (7) (8) − and → − Here, σ is the softmax activation, ← a a are bidirectional attentions, y are probabilities for labels Entailment, Contradictio"
P19-1028,P18-1224,0,0.021896,"with constraint penalties is reminiscent of the Constrained Conditional Model of Chang et al. (2012). Recently, we have seen some work that allows backpropagating through structures (e.g. Huang et al., 2015; Kim et al., 2017; Yogatama et al., 2017; Niculae et al., 2018; Peng et al., 2018, and the references within). Our framework differs from them in that structured inference is not mandantory here. We believe that there is room to study the interplay of these two approaches. Also related to our attention augmentation is using word relatedness as extra input feature to attention neurons (e.g. Chen et al., 2018). 6 Conclusions In this paper, we presented a framework for introducing constraints in the form of logical statements to neural networks. We demonstrated the process of converting first-order logic into differentiable components of networks without extra learnable parameters and extensive redesign. Our experiments were designed to explore the flexibility of our framework with different constraints in diverse tasks. As our experiments showed, our framework allows neural models to benefit from external knowledge during learning and prediction, especially when training data is limited. 7 Acknowle"
P19-1028,N19-1244,0,0.0466412,"ized teacher-student network is used to distill rules into network parameters. This work could be seen as an instance of knowledge distillation (Hinton et al., 2015). Instead of such extensive changes to the learning procedure, our framework retains the original network design and augments existing interpretable layers. Regularization with Logic Several recent lines of research seek to guide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning with Structures Traditional structured prediction models (e.g. Smith, 2011) natur"
P19-1028,P18-1013,0,0.0395426,"o network parameters. This work could be seen as an instance of knowledge distillation (Hinton et al., 2015). Instead of such extensive changes to the learning procedure, our framework retains the original network design and augments existing interpretable layers. Regularization with Logic Several recent lines of research seek to guide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning with Structures Traditional structured prediction models (e.g. Smith, 2011) naturally admit constraints of the kind described in this paper."
P19-1028,P16-1228,0,0.158037,"e their data dependence if they were guided by easily stated rules such as: Prefer aligning phrases that are marked as similar according to an external resource, e.g., ConceptNet (Liu and Singh, 2004). If such declaratively stated rules can be incorporated into training neural networks, then they can provide the inductive bias that can reduce data dependence for training. That general neural networks can represent such Boolean functions is known and has been studied both from the theoretical and empirical perspectives (e.g. Maass et al., 1994; Anthony, 2003; Pan and Srikumar, 2016). Recently, Hu et al. (2016) exploit this property to train a neural network to mimic a teacher network that uses structured rules. In this paper, we seek to directly incorporate such structured knowledge into a neural network architecture without substantial changes to the training methods. We focus on three questions: 1. Can we integrate declarative rules with endto-end neural network training? 2. Can such rules help ease the need for data? 3. How does incorporating domain expertise compare against large training resources powered by pre-trained representations? The first question poses the key technical challenge we a"
P19-1028,D15-1166,0,0.0233997,"y in low-data regimes. 1 Introduction Neural models demonstrate remarkable predictive performance across a broad spectrum of NLP tasks: e.g., natural language inference (Parikh et al., 2016), machine comprehension (Seo et al., 2017), machine translation (Bahdanau et al., 2015), and summarization (Rush et al., 2015). These successes can be attributed to their ability to learn robust representations from data. However, such end-to-end training demands a large number of training examples; for example, training a typical network for machine translation may require millions of sentence pairs (e.g. Luong et al., 2015). The difficulties and expense of curating large amounts of annotated data are well understood and, consequently, massive datasets may not be available for new tasks, domains or languages. In this paper, we argue that we can combat the data hungriness of neural networks by taking advantage of domain knowledge expressed as Vivek Srikumar University of Utah svivek@cs.utah.edu Paragraph: Gaius Julius Caesar (July 100 BC – 15 March 44 BC), Roman general, statesman, Consul and notable author of Latin prose, played a critical role in the events that led to the demise of the Roman Republic and the ri"
P19-1028,D18-1538,0,0.136921,"ent network is used to distill rules into network parameters. This work could be seen as an instance of knowledge distillation (Hinton et al., 2015). Instead of such extensive changes to the learning procedure, our framework retains the original network design and augments existing interpretable layers. Regularization with Logic Several recent lines of research seek to guide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning with Structures Traditional structured prediction models (e.g. Smith, 2011) naturally admit constraint"
P19-1028,K18-1007,0,0.097048,"knowledge distillation (Hinton et al., 2015). Instead of such extensive changes to the learning procedure, our framework retains the original network design and augments existing interpretable layers. Regularization with Logic Several recent lines of research seek to guide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning with Structures Traditional structured prediction models (e.g. Smith, 2011) naturally admit constraints of the kind described in this paper. Indeed, our approach for using logic as a template-language is similar to"
P19-1028,D16-1244,0,0.287627,"in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes. 1 Introduction Neural models demonstrate remarkable predictive performance across a broad spectrum of NLP tasks: e.g., natural language inference (Parikh et al., 2016), machine comprehension (Seo et al., 2017), machine translation (Bahdanau et al., 2015), and summarization (Rush et al., 2015). These successes can be attributed to their ability to learn robust representations from data. However, such end-to-end training demands a large number of training examples; for example, training a typical network for machine translation may require millions of sentence pairs (e.g. Luong et al., 2015). The difficulties and expense of curating large amounts of annotated data are well understood and, consequently, massive datasets may not be available for new tasks, doma"
P19-1028,P18-1173,0,0.0248179,"ured prediction models (e.g. Smith, 2011) naturally admit constraints of the kind described in this paper. Indeed, our approach for using logic as a template-language is similar to Markov Logic Networks (Richardson and Domingos, 2006), where logical forms are compiled into Markov networks. Our formulation augments model scores with constraint penalties is reminiscent of the Constrained Conditional Model of Chang et al. (2012). Recently, we have seen some work that allows backpropagating through structures (e.g. Huang et al., 2015; Kim et al., 2017; Yogatama et al., 2017; Niculae et al., 2018; Peng et al., 2018, and the references within). Our framework differs from them in that structured inference is not mandantory here. We believe that there is room to study the interplay of these two approaches. Also related to our attention augmentation is using word relatedness as extra input feature to attention neurons (e.g. Chen et al., 2018). 6 Conclusions In this paper, we presented a framework for introducing constraints in the form of logical statements to neural networks. We demonstrated the process of converting first-order logic into differentiable components of networks without extra learnable param"
P19-1028,D14-1162,0,0.0808114,"Missing"
P19-1028,N18-1202,0,0.0257675,"ata to see how performances vary from baselines. For detailed model setup, please refer to the appendices. 4.1 Machine Comprehension Attention is a widely used intermediate state in several recent neural models. To explore the augmentation over such neurons, we focus on attention-based machine comprehension models on SQuAD (v1.1) dataset (Rajpurkar et al., 2016). We seek to use word relatedness from external resources (i.e., ConceptNet) to guide alignments, and thus to improve model performance. Model We base our framework on two models: BiDAF (Seo et al., 2017) and its ELMoaugmented variant (Peters et al., 2018). Here, we provide an abstraction of the two models which our framework will operate on: p, q = encoder(p), encoder(q) ← − − a ,→ a = σ(layers(p, q)) −, → − y, z = σ(layers(p, q, ← a a )) (3) (4) (5) where p and q are the paragraph and query re− spectively, σ refers to the softmax activation, ← a → − and a are the bidirectional attentions from q to p and vice versa, y and z are the probabilities of answer boundaries. All other aspects are abstracted as encoder and layers. Augmentation By construction of the attention neurons, we expect that related words should be aligned. In a knowledge-drive"
P19-1028,D16-1264,0,0.0647052,"ents, our goal is to study the modeling flexibility of our framework and its ability to improve performance, especially with decreasing amounts of training data. To study low data regimes, our augmented networks are trained using varying amounts of training data to see how performances vary from baselines. For detailed model setup, please refer to the appendices. 4.1 Machine Comprehension Attention is a widely used intermediate state in several recent neural models. To explore the augmentation over such neurons, we focus on attention-based machine comprehension models on SQuAD (v1.1) dataset (Rajpurkar et al., 2016). We seek to use word relatedness from external resources (i.e., ConceptNet) to guide alignments, and thus to improve model performance. Model We base our framework on two models: BiDAF (Seo et al., 2017) and its ELMoaugmented variant (Peters et al., 2018). Here, we provide an abstraction of the two models which our framework will operate on: p, q = encoder(p), encoder(q) ← − − a ,→ a = σ(layers(p, q)) −, → − y, z = σ(layers(p, q, ← a a )) (3) (4) (5) where p and q are the paragraph and query re− spectively, σ refers to the softmax activation, ← a → − and a are the bidirectional attentions fro"
P19-1028,N15-1118,0,0.196906,"Boolean logic. Hu et al. (2016) introduced an imitation learning framework where a specialized teacher-student network is used to distill rules into network parameters. This work could be seen as an instance of knowledge distillation (Hinton et al., 2015). Instead of such extensive changes to the learning procedure, our framework retains the original network design and augments existing interpretable layers. Regularization with Logic Several recent lines of research seek to guide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning w"
P19-1028,D15-1044,0,0.0453538,"augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes. 1 Introduction Neural models demonstrate remarkable predictive performance across a broad spectrum of NLP tasks: e.g., natural language inference (Parikh et al., 2016), machine comprehension (Seo et al., 2017), machine translation (Bahdanau et al., 2015), and summarization (Rush et al., 2015). These successes can be attributed to their ability to learn robust representations from data. However, such end-to-end training demands a large number of training examples; for example, training a typical network for machine translation may require millions of sentence pairs (e.g. Luong et al., 2015). The difficulties and expense of curating large amounts of annotated data are well understood and, consequently, massive datasets may not be available for new tasks, domains or languages. In this paper, we argue that we can combat the data hungriness of neural networks by taking advantage of dom"
P19-1028,W00-0726,0,0.337108,"Missing"
P19-1028,D18-1215,0,0.0912835,"uide training neural networks by integrating logical rules in the form of additional terms in the loss functions (e.g., Rocktäschel et al., 2015) that essentially promote constraints among output labels (e.g., Du et al., 2019; Mehta et al., 2018), promote agreement (Hsu et al., 2018) or reduce inconsistencies across predictions (Minervini and Riedel, 2018). Furthermore, Xu et al. (2018) proposed a general design of loss functions using symbolic knowledge about the outputs. Fischer et al. (2019) described a method for for deriving losses that are friendly to gradient-based learning algorithms. Wang and Poon (2018) proposed a framework for integrating indirect supervision expressed via probabilistic logic into neural networks. Learning with Structures Traditional structured prediction models (e.g. Smith, 2011) naturally admit constraints of the kind described in this paper. Indeed, our approach for using logic as a template-language is similar to Markov Logic Networks (Richardson and Domingos, 2006), where logical forms are compiled into Markov networks. Our formulation augments model scores with constraint penalties is reminiscent of the Constrained Conditional Model of Chang et al. (2012). Recently, w"
P19-1563,D18-1074,0,0.226305,"prevalent online or text-based counseling services.1 We ground our study in a style of therapy called Motivational Interviewing (MI, Miller and Rollnick, 2003, 2012), which is widely used for treating addiction-related problems. To help train therapists, and also to monitor therapy quality, utterances in sessions are annotated using a set of behavioral codes called Motivational Interviewing Skill Codes (MISC, Miller et al., 2003). Table 1 shows standard therapist and patient (i.e., client) codes with examples. Recent NLP work (Tanana et al., 2016; Xiao et al., 2016; P´erez-Rosas et al., 2017; Huang et al., 2018, inter alia) has studied the problem of using MISC to assess completed sessions. Despite its usefulness, automated post hoc MISC labeling does not address the desiderata for ongoing sessions identified above; such models use information from utterances yet to be said. To provide real-time feedback to therapists, we define two complementary dialogue observers: 1. Categorization: Monitoring an ongoing session by predicting MISC labels for therapist and client utterances as they are made. 2. Forecasting: Given a dialogue history, forecasting the MISC label for the next utterance, thereby both al"
P19-1563,D14-1162,0,0.0834908,"ntioned in §2, all our experiments are based on the MISC codes grouped by Xiao et al. (2016). 5.1 Preprocessing and Model Setup An MI session contains about 500 utterances on average. We use a sliding window of size N = 8 utterances with padding for the initial ones. We assume that we always know the identity of the speaker for all utterances. Based on this, we split the sliding windows into a client and therapist windows to train separate models. We tokenized and lower-cased utterances using spaCy (Honnibal and Montani, 2017). To embed words, we concatenated 300-dimensional Glove embeddings (Pennington et al., 2014) with ELMo vectors (Peters et al., 2018). The appendix details the model setup and hyperparameter choices. 5603 5.2 Results Best Models. Our goal is to discover the best client and therapist models for the two tasks. We identified the following best configurations using F1 score on the development set: 1. Categorization: For client, the best model does not need any word or utterance attention. For the therapist, it uses GMGRUH for word attention and ANCHOR42 for utterance attention. We refer to these models as CC and CT respectively 2. Forecasting: For both client and therapist, the best model"
P19-1563,E17-1106,0,0.088709,"Missing"
P19-1563,N18-1202,0,0.0294902,"on the MISC codes grouped by Xiao et al. (2016). 5.1 Preprocessing and Model Setup An MI session contains about 500 utterances on average. We use a sliding window of size N = 8 utterances with padding for the initial ones. We assume that we always know the identity of the speaker for all utterances. Based on this, we split the sliding windows into a client and therapist windows to train separate models. We tokenized and lower-cased utterances using spaCy (Honnibal and Montani, 2017). To embed words, we concatenated 300-dimensional Glove embeddings (Pennington et al., 2014) with ELMo vectors (Peters et al., 2018). The appendix details the model setup and hyperparameter choices. 5603 5.2 Results Best Models. Our goal is to discover the best client and therapist models for the two tasks. We identified the following best configurations using F1 score on the development set: 1. Categorization: For client, the best model does not need any word or utterance attention. For the therapist, it uses GMGRUH for word attention and ANCHOR42 for utterance attention. We refer to these models as CC and CT respectively 2. Forecasting: For both client and therapist, the best model uses no word attention, and uses SELF42"
P19-1563,2005.sigdial-1.6,0,0.0218505,"(R EC) such as “Sounds like you really wanted to give up and you’re unhappy about the relapse.” Such an expert may also anticipate important cues from the client. The forecasting task seeks to mimic the intent of such a seasoned therapist: Given a dialogue history Hn and the next speaker’s identity sn+1 , predict the MISC code ln+1 of the yet unknown next utterance un+1 . The MISC forecasting task is a previously unstudied problem. We argue that forecasting the type of the next utterance, rather than selecting or generating its text as has been the focus of several recent lines of work (e.g., Schatzmann et al., 2005; Lowe et al., 2015; Yoshino et al., 2018), allows the human in the loop (the therapist) the freedom to creatively participate in the conversation within the parameters defined by the seasoned observer, and perhaps even rejecting suggestions. Such an observer could be especially helpful for training therapists (Imel et al., 2017). The forecasting task is also related to recent work on detecting antisocial comments in online conversations (Zhang et al., 2018) whose goal is to provide an early warning for such events. 4 Models for MISC Prediction Modeling the two tasks defined in §3 requires add"
P19-1563,J00-3003,0,0.686751,"Missing"
P19-1563,P17-1018,0,0.0155235,"le, identified by the choice of the functions fm and fc , converts word encodings in each utterance v ij into attended word encodings z ij . To use them in the HGRU skeleton, we will encode them a second time using a BiGRU to produce attention-enhanced utterance vectors. For brevity, we will refer to these vectors as v i for the utterance ui . If word attention is used, these attended vectors will be treated as word encodings. To complete this discussion, we need to instantiate the two functions. We use two commonly used attention mechanisms: BiDAF (Seo et al., 5602 2016) and gated matchLSTM (Wang et al., 2017). For simplicity, we replace the sequence encoder in the latter with a BiGRU and refer to it as GMGRU. Table 3 shows the corresponding definitions of fc and fm . We refer the reader to the original papers for further details. In subsequent sections, we will refer to the two attended versions of the HGRU as B I DAFH and GMGRUH . 4.3 Utterance-level Attention While we assume that the history of utterances is available for both our tasks, not every utterance is relevant to decide a MISC label. For categorization, the relevance of an utterance to the anchor may be important. For example, a complex"
P19-1563,P18-1125,0,0.0272564,"type of the next utterance, rather than selecting or generating its text as has been the focus of several recent lines of work (e.g., Schatzmann et al., 2005; Lowe et al., 2015; Yoshino et al., 2018), allows the human in the loop (the therapist) the freedom to creatively participate in the conversation within the parameters defined by the seasoned observer, and perhaps even rejecting suggestions. Such an observer could be especially helpful for training therapists (Imel et al., 2017). The forecasting task is also related to recent work on detecting antisocial comments in online conversations (Zhang et al., 2018) whose goal is to provide an early warning for such events. 4 Models for MISC Prediction Modeling the two tasks defined in §3 requires addressing four questions: (1) How do we encode a dialogue and its utterances? (2) Can we discover discriminative words in each utterance? (3) Can we discover which of the previous utterances are relevant? (4) How do we handle label imbalance in our data? Many recent advances in neural networks can be seen as plug-and-play components. To facilitate the comparative study of models, we will describe components that address the above 5601 questions. In the rest of"
P19-1563,W15-4640,0,\N,Missing
Q13-1019,J09-2001,0,0.0591225,"Missing"
Q13-1019,N10-1066,1,0.849328,"if we had a weight vector w, we could predict the full structure using inference as follows: y = arg max wT Φ(x, y) (4) y0 We propose an iterative learning algorithm to learn this weight vector. In the following discussion, for a labeled example (x, y∗ ), we refer to the missing part of its structure as h(y∗ ). That is, h(y∗ ) is the assignment to the arguments of the relation and their types. We use the notation r(y) to denote the relation label specified by a structure y. Our learning algorithm is closely related to recently developed latent variable based frameworks (Yu and Joachims, 2009; Chang et al., 2010a; Chang et al., 2010b), where the supervision provides only partial annotation. We begin by defining two additional inference procedures: 1. Latent Inference: Given a weight vector w and a partially labeled example (x, y∗ ), we can ‘complete’ the rest of the structure by inferring the highest scoring assignment to the missing parts. In the algorithm, we call this procedure LatentInf (w, x, y∗ ), which solves the following maximization problem: ˆ = arg maxy wT Φ(x, y), y s.t. (5) ∗ r(y) = r(y ). 2. Loss augmented inference: This is a variant of the the standard loss augmented inference for str"
Q13-1019,clarke-etal-2012-nlp,1,0.845595,"Missing"
Q13-1019,D09-1047,0,0.285663,"ition Project (Litkowski and Hargraves, 2005) and the related SemEval 2007 shared task of word sense disambiguation of prepositions (Litkowski and Hargraves, 2007). The Preposition Project identifies preposition senses based on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while"
Q13-1019,N10-1138,0,0.0391634,"Missing"
Q13-1019,J02-3001,0,0.0916152,"roduce a new inventory of preposition relations that covers the 34 prepositions that formed the basis of the SemEval 2007 task of preposition sense disambiguation. 2. We model preposition relations, arguments and their types jointly and propose a learning algorithm that learns to predict all three using training data that annotates only relation labels. 3. We show that jointly predicting relations with 232 word sense not only improves the relation predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary wo"
Q13-1019,N10-1115,0,0.0161165,"n Monday books on Shakespeare Table 1: List of preposition relations ning example, the relation label is Cause. We represent the predicted relation label by r. Arguments The relation label crucially depends on correctly identifying the arguments of the preposition, which are death and pneumonia in our running example. While a parser can identify the arguments of a preposition, simply relying on the parser may impose an upper limit on the accuracy of relation prediction. We build an oracle experiment to highlight this limitation. Table 2 shows the recall of the easy-first dependency parser of (Goldberg and Elhadad, 2010) on Section 23 of the Penn Treebank for identifying the governor and object of prepositions. We define heuristics that generate a candidate governors and objects for a preposition. For the governor, this set includes the previous verb or noun and for the object, it includes only the next noun. The row labeled Best(Parser, Heuristics) shows the performance of an oracle predictor which selects the true governor/object if present among the parser’s prediction and the heuristics. We see that, even for the in-domain case, if we are able to re-rank the candidates, we could achieve a big improvement"
Q13-1019,C10-2052,0,0.793611,"on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while disambiguating the sense of a preposition does indeed reveal nuances of its meaning, it leads to a proliferation of labels to be predicted. Most importantly, sense labels do not transfer to other prepositions that expre"
Q13-1019,P98-2127,0,0.0631926,"35 Figure 1 shows the hypernym hierarchy for the word pneumonia. In this case, synsets in the hypernym hierarchy, like pathological state or physical condition, would also include ailments like flu. pneumonia =&gt; respiratory disease =&gt; disease =&gt; illness =&gt; ill health =&gt; pathological state =&gt; physical condition =&gt; condition =&gt; state =&gt; attribute =&gt; abstraction =&gt; entity Figure 1: Hypernym hierarchy for the word pneumonia We define a semantic type to be a cluster of words. In addition to WordNet hypernyms, we also cluster verbs, nouns and adjectives using the dependencybased word similarity of (Lin, 1998) and treat cluster membership as types. These are described in detail in Section 5.1. Relation prediction involves not only identifying the arguments, but also selecting the right semantic type for them, which together, help predicting the relation label. Given an argument candidate and a collection of possible types (given by WordNet or the similarity based clusters), we need to select one of the types. For example, in the WordNet case, we need to pick one of the hypernyms in the hypernym hierarchy. Thus, for the governor and object, we have a set of type labels, comprised of one element for"
Q13-1019,W04-2705,0,0.0696821,"gnificant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focuses on semantic relations expressed by transitive prepositions1 . We can define the two prediction tasks for prepositions as follows: identifying the relation label for a preposition, and predicting the arguments of the relation. Prepositions can mark arguments (both core and adjunct) for verbal and nominal predicates. In addition, they can also trigger relations that are not part of other predicates. For example, in sentence"
Q13-1019,J09-2002,0,0.384977,"Missing"
Q13-1019,J05-1004,0,0.0849467,"ion predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focuses on semantic relations expressed by transitive prepositions1 . We can define the two prediction tasks for prepositions as follows: identifying the relation label for a preposition, and predicting the arguments of the relation. Prepositions can mark arguments (both core and adjunct) for verbal and nominal predicates. In addition, they can also trigger relations that are not part of other pre"
Q13-1019,J08-2005,1,0.224743,"that covers the 34 prepositions that formed the basis of the SemEval 2007 task of preposition sense disambiguation. 2. We model preposition relations, arguments and their types jointly and propose a learning algorithm that learns to predict all three using training data that annotates only relation labels. 3. We show that jointly predicting relations with 232 word sense not only improves the relation predictor, but also gives a significant improvement in sense prediction. 2 Prepositions & Predicate-Argument Semantics Semantic role labeling (cf. (Gildea and Jurafsky, 2002; Palmer et al., 2010; Punyakanok et al., 2008) and others) is the task of converting text into a predicate-argument representation. Given a trigger word or phrase in a sentence, this task solves two related prediction problems: (a) identifying the relation label, and (b) identifying and labeling the arguments of the relation. This problem has been studied in the context of verb and nominal triggers using the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations over the Penn Treebank, and also using the FrameNet lexicon (Fillmore et al., 2003), which allows arbitrary words to trigger semantic frames. This paper focu"
Q13-1019,W04-2401,1,0.739506,"vious iteration’s weight vector influence the learning. 4.4 Joint inference between preposition senses and relations By defining preposition relations as disjoint sets of preposition senses, we effectively have a hierarchical relationship between senses and relations. This suggests that joint inference can be employed between sense and relation predictions with a validity constraint connecting the two. The idea of employing inference to combine independently trained predictors to obtain a coherent output structure has been used for various NLP tasks in recent years, starting with the work of (Roth and Yih, 2004; Roth and Yih, 2007). We use the features defined by (Hovy et al., 2010), which we write as φs (x, s) for a given input x and sense label s, and train a separate preposition sense model on the SemEval data with features φs (x, s) using the structural SVM algorithm. Thus, we have two weight vectors – the one for predicting preposition relations described earlier, and the preposition sense weight vector. At prediction time, for a given input, we find the highest scoring joint assignment to the relation, arguments and types and the sense, subject to the constraint that the sense and the relation"
Q13-1019,D11-1012,1,0.761915,"the relations to aid natural language understanding and makes the prediction task harder than it should be: using the standard word sense classification approach, we need to train a separate classifier for each word because the labels are defined per-preposition. In other words, we cannot share features across the different prepositions. This motivates the need to combine such senses of prepositions into the same class label. In this direction, (O’Hara and Wiebe, 2009) describes an inventory of preposition relations obtained using Penn Treebank function tags and frame elements from FrameNet. (Srikumar and Roth, 2011) merged preposition senses of seven prepositions into relation labels. (Litkowski, 2012) also suggests collapsing the definitions of prepositions into a smaller set of semantic classes. To aid better generalization and to reduce the label complexity, we follow this line of work to define a set of relation labels which abstract word senses across prepositions2 . 3 Preposition-triggered Relations This section describes the inventory of preposition relations introduced in this paper, and then identifies the components of the preposition relation extraction problem. 3.1 Preposition Relation Invent"
Q13-1019,N09-3017,0,0.521283,"es preposition senses based on their definitions in the Oxford Dictionary of English. There are 332 different labels to be predicted with a wide variance in the number of senses per preposition ranging from 2 (during and as) to 25 (on). For example, according to the preposition sense inventory, the preposition from in sentence (2) above will be labeled with the sense from:12(9) to indicate a cause. (Dahlmeier et al., 2009) added sense annotation to seven prepositions in four sections of the Penn Treebank with the goal of studying their interaction with verb arguments. Using the SemEval data, (Tratz and Hovy, 2009) and (Hovy et al., 2010) showed that the arguments offer an important cue to identify the sense of the preposition and (Tratz, 2011) showed further improvements by refining the sense inventory. However, though these works used a dependency parser to identify arguments, in order to overcome parsing errors, they augment the parser’s predictions using part-of-speech based heuristics. We argue that, while disambiguating the sense of a preposition does indeed reveal nuances of its meaning, it leads to a proliferation of labels to be predicted. Most importantly, sense labels do not transfer to other"
Q13-1019,S07-1051,0,0.703348,"Missing"
Q13-1019,S07-1005,0,\N,Missing
Q13-1019,J13-3006,0,\N,Missing
Q13-1019,P08-1037,0,\N,Missing
Q13-1019,C98-2122,0,\N,Missing
S17-1022,N09-1057,0,0.0472258,"We also acknowledge that there is a level of construal contributed by the verb. For example, Alex in Alex sent the package to Pam can be AGENT or S OURCE depending whether the interpretation is focused on the agency of the argument or the spatial relation it has in reference to the action described by the verb. These verb-triggered construals have been previously explored, most notably by Jackendoff (1990). Perspective can also be evident in the choice of syntactic constructions, e.g., active vs. passive voice (I made a mistake versus Mistakes were made), which can be connected to sentiment (Greene and Resnik, 2009). We specifically focus on the construal that arises from the adposition in a given sentence. The preposition by gives the impression that the stimulus is responsible for triggering an instinctive fear reflex (i.e., C AUSER), while about portrays the thing feared as the content or T OPIC of thought.6 In some languages, the experiencer can be conceptualized as a recipient of the emotion or feeling, thus licensing dative marking.7 In the Hebrew example (8a), the experiencer of bodily perception is marked with the dative preposition l(e)- (Berman, 1982). Similarly, in Hindi, the dative postpostio"
S17-1022,W06-1670,0,0.0556475,"mar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inventory of categories through extensive deliberation involving the use of dictionaries, corpora, and pilot annotation experiments. They call the categories supersenses to emphasize their similarity to coarse-grained classifications of nouns and verbs that go by that name (Ciaramita and Altun, 2006; Schneider et al., 2012). The at examples in (1) are accompanied by the appropriate supersenses from the supersense scheme. Most supersenses resemble thematic roles (cf. Fillmore (1968)); a few others are needed to describe preposition-marked relations between entities. There are multiple English prepositions per supersense; e.g., “in the city” and “on the table” would join “at 123 Main St.” in being labeled as L OCATIONs. We understand the supersenses as prototype-based categories, and in some cases use heuristics like paraphrasability (“in order to” for P URPOSE) and WH-question words (“Why"
S17-1022,D09-1047,0,0.409241,"cs, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inve"
S17-1022,J05-1004,0,0.346456,"facilitate inter-annotator consistency: our experience thus far is that annotators benefit greatly from examples illustrating the possible supersenses that can be assigned to a preposition. If initial pilots are successful, we would then need to decide whether to annotate the role and function together or in separate stages. Because the function reflects one of the adposition’s prototypical senses, it may often be deterministic given the adposition and scene role, in which case we could focus annotators’ efforts on the scene roles. Existing annotations for lexical resources such as PropBank (Palmer et al., 2005), VerbNet (Palmer et al., 2017; Kipper et al., 2008), and FrameNet (Fillmore and Baker, 2009) might go a long way toward disambiguating the scene role, limiting the effort required from annotators. 6.5 Linguistic Utility of Annotated Data Assuming the above theoretical and practical concerns are surmountable, annotated corpora would facilitate empirical studies of the nature and limits of adposition/case construal within and across languages. For example: Is it the case that some of the supersense labels can only serve as scene roles, or only as functions? (A hypothesis is that PARTICI PANT su"
S17-1022,picca-etal-2008-supersense,0,0.466974,"icial to us; at the very least, it splits hairs in a way that would be difficult to explain to annotators. Below, we instead argue that the idea of construal/conceptualization offers a more principled answer; in our new analysis, the T OPIC suggested by about and the S TIMULUS suggested by cared can coexist. http://tiny.cc/prepwiki 180 3.2 Applying the Supersenses to Other Languages One of the premises of using unlexicalized supersenses was that the scheme would port well to other 4 http://tiny.cc/prepwiki/index.php/Category: SST-Topic languages (as the WordNet noun and verb supersenses have: Picca et al., 2008; Schneider et al., 2012, inter alia). To test this, we have begun applying the existing supersenses to three new languages, namely, Hebrew, Hindi, and Korean. Pilot annotation in these languages has echoed the fundamental problem discussed in the previous section. Consider the Hindi examples below. In (4a), the experiencer of an emotion is marked with a postposition kaa, the genitive case marker in Hindi. (4) a. [Hindi]: E XPERIENCER vs. P OSSESSOR bipaashaa kaa gussaa Bipasha GEN anger “Bipasha’s anger” 4.1 b. [Hindi]: E XPERIENCER bipaashaa bahut gussaa hui Bipasha very angry became “Bipash"
S17-1022,W16-1712,1,0.893848,"Missing"
S17-1022,P12-2050,1,0.931703,"ider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inventory of categories through extensive deliberation involving the use of dictionaries, corpora, and pilot annotation experiments. They call the categories supersenses to emphasize their similarity to coarse-grained classifications of nouns and verbs that go by that name (Ciaramita and Altun, 2006; Schneider et al., 2012). The at examples in (1) are accompanied by the appropriate supersenses from the supersense scheme. Most supersenses resemble thematic roles (cf. Fillmore (1968)); a few others are needed to describe preposition-marked relations between entities. There are multiple English prepositions per supersense; e.g., “in the city” and “on the table” would join “at 123 Main St.” in being labeled as L OCATIONs. We understand the supersenses as prototype-based categories, and in some cases use heuristics like paraphrasability (“in order to” for P URPOSE) and WH-question words (“Why?” for P URPOSE and E XPL"
S17-1022,W15-1612,1,0.944534,"language—and discuss how this representation would allow for a simpler inventory of labels. 1 Introduction Prepositions and postpositions (collectively adpositions) are widespread in the world’s languages as grammatical markers expressing spatial, temporal, thematic,1 and other kinds of semantic relations. Unfortunately for semantic processing, a handful of high-frequency types carry an immense payload by way of extreme polysemy. Thus, disambiguation of adpositional meaning is crucial to piecing together the interpretation of a sentence (§2). A line of previous work (Srikumar and Roth, 2013a; Schneider et al., 2015, 2016, see §2) has developed a scheme for broad-coverage annotation 1 Nathan Schneider Georgetown University In the sense of thematic roles (agent, patient, etc.). of adpositions with an eye toward building automatic disambiguation systems. Their most recent proposal consists of an inventory of 75 categorical labels known as supersenses that characterize the polysemy of English prepositions in a lexicallyneutral and coarse-grained fashion. They envision disambiguation as assigning a single one of these supersenses to each preposition token. While formalizing disambiguation via singlelabel cla"
S17-1022,Q13-1019,1,0.952209,"ocessing of domaingeneral language—and discuss how this representation would allow for a simpler inventory of labels. 1 Introduction Prepositions and postpositions (collectively adpositions) are widespread in the world’s languages as grammatical markers expressing spatial, temporal, thematic,1 and other kinds of semantic relations. Unfortunately for semantic processing, a handful of high-frequency types carry an immense payload by way of extreme polysemy. Thus, disambiguation of adpositional meaning is crucial to piecing together the interpretation of a sentence (§2). A line of previous work (Srikumar and Roth, 2013a; Schneider et al., 2015, 2016, see §2) has developed a scheme for broad-coverage annotation 1 Nathan Schneider Georgetown University In the sense of thematic roles (agent, patient, etc.). of adpositions with an eye toward building automatic disambiguation systems. Their most recent proposal consists of an inventory of 75 categorical labels known as supersenses that characterize the polysemy of English prepositions in a lexicallyneutral and coarse-grained fashion. They envision disambiguation as assigning a single one of these supersenses to each preposition token. While formalizing disambigu"
S17-1022,N09-3017,0,0.0768667,"In cognitive linguistics, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al."
S17-1022,S07-1051,0,0.0372896,"on semantics broadly. In cognitive linguistics, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized sen"
S19-1003,C18-1139,0,0.059255,"Missing"
S19-1003,K18-1028,0,0.0160742,"uce high quality embeddings even with smaller document collections (e.g., all documents related to a specific project in a company). Second, because a feature can be shared by different words, each training update for a feature will update all the word embeddings containing this feature. This can lead to better generalization. Third, it presents an elegant solution for the outof-vacabulary (OOV) problem. Once we define the feature template, we can extract features of any word, then we can compute the embedding for it. Some recent work (Pinter et al., 2017; Kim et al., 2018; Zhao et al., 2018; Artetxe et al., 2018) address the OOV problem using pre-trained embeddings and mimicking them by training a second model using substrings of a given word. Instead, here we can use arbitrary features and do not need pre-trained embeddings. Then, the global log loss can be rewritten as: `gl (x, Y) = − X cx (y)S(x, y)+ y∈Y X   log exp cx (y 0 )S(x, y 0 ) + 1 y 0 ∈L (12) Note that both local and global models are dominated by the O(|L|) summation, which suggests they have same computational cost. 4.4 Featurizing Words and Contexts In the scoring function of a pair (x, y), i.e., Eq. 3, we use two feature functions φ"
S19-1003,Q17-1010,0,0.232536,"prediction as an XML problem allows us to define a unifying framework for word embeddings. Consequently, we can systematically analyze the problem of training word embeddings using lessons from the XML literature. In particular, we can featurize both inputs and outputs — in our case, contexts and words. Apart from featurization, loss functions and normalization of probability are also design choices available. We show that our approach subsumes several standard word embedding learning methods: specific design choices give us familiar models such as CBOW (Mikolov et al., 2013a) and FASTT EXT (Bojanowski et al., 2017)1 . Our experiments study the interplay between the amount of data needed to train embeddings, and the features for words and contexts. We show that, when trained on the same amount of data, using word and context features outperforms the original CBOW and FAST T EXT on both the standard analogy evaluation and a variant where words have introduce typographical errors. Featurizing words and contexts reduces data dependency for training and can achieve similar results as CBOW and FAST T EXT trained on a 10x larger corpus. Finally, we also show that the trained embeddings offer better representat"
S19-1003,D17-1010,0,0.139778,"ur understanding of their parts. In our example, the word is composed of google and -ize, both of which have their own meanings and the composition (google + -ize) gives cues as to what googlize may mean. A reader may use their understanding of the word google and the fact that -ize is a common suffix to create verbs to hypothesize the meaning of the word. The above example illustrates the following principle: A word is not the smallest meaning unit, but the most common one. We argue that we should utilize the internal information of words when we train word embeddings. Some recent work (e.g. Pinter et al., 2017; Kim et al., 2018; Bojanowski et al., 2017; Schick and Sch¨utze, 2018) applies our assumptions implicitly by using character-level information to embed words. While character-based features help capture the internal structure of the word, several other aspects may be helpful, e.g. linguistically motivated prefixes and suffixes, the shape of the word and other possible features. §4.4 describes the various choices we explore. words that could occur in the context form the label set Y for that input. The label set is a subset of all labels L, i.e., the entire vocabulary. Following this intuition"
S19-1003,L16-1645,1,0.882324,"Missing"
S19-1003,C18-1216,0,0.0691123,"heir parts. In our example, the word is composed of google and -ize, both of which have their own meanings and the composition (google + -ize) gives cues as to what googlize may mean. A reader may use their understanding of the word google and the fact that -ize is a common suffix to create verbs to hypothesize the meaning of the word. The above example illustrates the following principle: A word is not the smallest meaning unit, but the most common one. We argue that we should utilize the internal information of words when we train word embeddings. Some recent work (e.g. Pinter et al., 2017; Kim et al., 2018; Bojanowski et al., 2017; Schick and Sch¨utze, 2018) applies our assumptions implicitly by using character-level information to embed words. While character-based features help capture the internal structure of the word, several other aspects may be helpful, e.g. linguistically motivated prefixes and suffixes, the shape of the word and other possible features. §4.4 describes the various choices we explore. words that could occur in the context form the label set Y for that input. The label set is a subset of all labels L, i.e., the entire vocabulary. Following this intuition, in the rest of t"
S19-1003,N15-1010,0,0.0484031,"Missing"
S19-1003,P14-2050,0,0.0396858,"tion, which suggests they have same computational cost. 4.4 Featurizing Words and Contexts In the scoring function of a pair (x, y), i.e., Eq. 3, we use two feature functions φ and ψ to extract features from the label and context respectively. This design choice dictates the information we wish to provide to the model about words and contexts. CBOW and word2vecf uses indicators for the target words, while FAST T EXT uses both the words and their constituent character ngrams. For context, CBOW and FAST T EXT aggregate the same features as the target word, but over the context words. Word2vecf (Levy and Goldberg, 2014) uses dependency information to featurize the context. We generalize these by allowing user or domain dependent features. The output of feature functions φ and ψ is a sparse vector and each dimension is a binary value, which indicates the existence of corresponding feature. Though the φ and ψ functions, we can easily incorporate extra information into word embeddings from other resources. For example, we can use hand-crafted gazetteers to indicate whether two words can belong to the same type. If both Beijing and Paris are in a list of locations, we can identify similarity between the words wi"
S19-1003,W16-2503,0,0.0596027,"Missing"
S19-1003,D18-1059,0,0.0187205,"tures can help produce high quality embeddings even with smaller document collections (e.g., all documents related to a specific project in a company). Second, because a feature can be shared by different words, each training update for a feature will update all the word embeddings containing this feature. This can lead to better generalization. Third, it presents an elegant solution for the outof-vacabulary (OOV) problem. Once we define the feature template, we can extract features of any word, then we can compute the embedding for it. Some recent work (Pinter et al., 2017; Kim et al., 2018; Zhao et al., 2018; Artetxe et al., 2018) address the OOV problem using pre-trained embeddings and mimicking them by training a second model using substrings of a given word. Instead, here we can use arbitrary features and do not need pre-trained embeddings. Then, the global log loss can be rewritten as: `gl (x, Y) = − X cx (y)S(x, y)+ y∈Y X   log exp cx (y 0 )S(x, y 0 ) + 1 y 0 ∈L (12) Note that both local and global models are dominated by the O(|L|) summation, which suggests they have same computational cost. 4.4 Featurizing Words and Contexts In the scoring function of a pair (x, y), i.e., Eq. 3, we use t"
S19-1003,D14-1162,0,0.0936213,"a side effect. One instantiation of the word prediction task, namely CBOW (Mikolov et al., 2013a), frames it Introduction The distributional hypothesis (Firth, 1935; Harris, 1954) has been a cornerstone in NLP. For example, Firth (1935) writes: . . . the complete meaning of a word is always contextual, and no study of meaning apart from a complete context can be taken seriously. Operationally, in modern NLP, word embeddings capture this idea and are typically trained using neural language models or word collocations (e.g. Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018). Is word meaning exclusively defined by its context? In this paper, we argue that while the word 22 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 22–32 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics as the multi-class classification problem of predicting a word given a context. We argue that the task is more appropriately framed as multi-label classification — multiple words can fit in the same context. Moreover, since the label set (all words) is massive, word predictio"
S19-1003,N18-1202,0,0.0455126,"tiation of the word prediction task, namely CBOW (Mikolov et al., 2013a), frames it Introduction The distributional hypothesis (Firth, 1935; Harris, 1954) has been a cornerstone in NLP. For example, Firth (1935) writes: . . . the complete meaning of a word is always contextual, and no study of meaning apart from a complete context can be taken seriously. Operationally, in modern NLP, word embeddings capture this idea and are typically trained using neural language models or word collocations (e.g. Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018). Is word meaning exclusively defined by its context? In this paper, we argue that while the word 22 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 22–32 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics as the multi-class classification problem of predicting a word given a context. We argue that the task is more appropriately framed as multi-label classification — multiple words can fit in the same context. Moreover, since the label set (all words) is massive, word prediction is an instance of e"
W15-0702,N09-1035,0,0.0298078,"terns in sounds and characters. tools, or using some combination of the two. In RhymeDesign we use the hybrid approach. For external knowledge we rely on the Carnegie Mellon University (CMU) pronunciation dictionary (CMU, 1998). The CMU dictionary provides the phoneme transcriptions of over 125K words in the North American English lexicon. Words are mapped to anywhere from one to three transcriptions, taking into account differing pronunciations as well as instances of homographs. Syllable boundaries are not provided in the original CMU transcriptions; however, the syllabified CMU dictionary (Bartlett et al., 2009) addresses this problem by training a classifier to identify syllable boundaries. When relying on any dictionary there is a high likelihood, particularly in the domain of poetry, that a given text will have one or multiple out-ofdictionary words. To address this, we’ve integrated existing letter-to-sound (LTS) rules (Black et al., 1998) and syllable segmentation algorithms (Bartlett et al., 2009) to predict the phoneme transcriptions of 17 out-of-dictionary words. Adapted from CMU’s Festivox voice building tools (Black, n.d.), the LTS system was trained on the CMU dictionary in order to genera"
W15-0702,P85-1034,0,0.717271,"umanities to computational linguistics. Our research is grounded in two sources of inquiry: sonic analysis specific to poetry and literature, and formalisms for describing sound. The latter problem of recognizing phonetic units of words is a well studied one; we refer the reader to (Jurafsky and Martin, 2008) for an 13 overview. A significant body of research, stemming from multiple fields, has been devoted to analyzing poetry. A number of tools and algorithms have been designed for teaching (Tucker, n.d.), analyzing (Plamondon, 2006; Kao and Jurafsky, 2012; Meneses et al., 2013) translating (Byrd and Chodorow, 1985; Genzel et al., 2010; Greene et al., 2010; Reddy and Knight, 2011), and generating (Manurung et al., 2000; Jiang and Zhou, 2010; Greene et al., 2010) poetry, all of which attend, to some degree, to sound and rhyme. While this work inspires our current research, it considers a much more limited, traditional definition of rhyme. As a result, these tools and algorithms disregard many of the sound-related patterns that we seek to reveal. The growing body of research analyzing rhyme in hip hop and rap lyrics (Kawahara, 2007; Hirjee and Brown, 2009; Hirjee and Brown, 2010; Buda, 2004; Addanki and W"
W15-0702,D10-1016,0,0.0408749,"Missing"
W15-0702,D10-1051,0,0.0142546,") translating (Byrd and Chodorow, 1985; Genzel et al., 2010; Greene et al., 2010; Reddy and Knight, 2011), and generating (Manurung et al., 2000; Jiang and Zhou, 2010; Greene et al., 2010) poetry, all of which attend, to some degree, to sound and rhyme. While this work inspires our current research, it considers a much more limited, traditional definition of rhyme. As a result, these tools and algorithms disregard many of the sound-related patterns that we seek to reveal. The growing body of research analyzing rhyme in hip hop and rap lyrics (Kawahara, 2007; Hirjee and Brown, 2009; Hirjee and Brown, 2010; Buda, 2004; Addanki and Wu, 2013; Wu et al., 2013b) considers a broader and more flexible definition of rhyme. Because these lyrics are meant primarily to be heard, the emphasis is placed on rhymes that occur in close proximity, as opposed to rhymes in poetry that can occur anywhere across a poem. Furthermore, rhyme analysis in hip hop and rap is purely sonic, and thus does not include visual rhyme. Several visualization tools that support the close reading of poetry allow users to interactively explore individual sounds and sonic patterns within text, and consider a broader range of sonic d"
W15-0702,W12-2502,0,0.132328,"ng sound in text stems from multiple fields, from digital humanities to computational linguistics. Our research is grounded in two sources of inquiry: sonic analysis specific to poetry and literature, and formalisms for describing sound. The latter problem of recognizing phonetic units of words is a well studied one; we refer the reader to (Jurafsky and Martin, 2008) for an 13 overview. A significant body of research, stemming from multiple fields, has been devoted to analyzing poetry. A number of tools and algorithms have been designed for teaching (Tucker, n.d.), analyzing (Plamondon, 2006; Kao and Jurafsky, 2012; Meneses et al., 2013) translating (Byrd and Chodorow, 1985; Genzel et al., 2010; Greene et al., 2010; Reddy and Knight, 2011), and generating (Manurung et al., 2000; Jiang and Zhou, 2010; Greene et al., 2010) poetry, all of which attend, to some degree, to sound and rhyme. While this work inspires our current research, it considers a much more limited, traditional definition of rhyme. As a result, these tools and algorithms disregard many of the sound-related patterns that we seek to reveal. The growing body of research analyzing rhyme in hip hop and rap lyrics (Kawahara, 2007; Hirjee and Br"
W15-0702,2013.mtsummit-papers.14,0,0.0234429,"Missing"
W15-0702,C08-1048,0,\N,Missing
W15-0702,P11-2014,0,\N,Missing
W15-1209,P03-1054,0,0.0110194,"tworks (RNNs) for predicting MISC codes. It is possible that a model capturing semantic and syntactic similarity in text can perform better than n-gram models in identifying reflections in MI sessions. The present study aimed to test (1) whether recursive neural networks (RNNs) (Socher, 2014) can be used to predict utterance-level patient MISC codes and (2) whether RNNs can improve the prediction accuracy of these codes over ngram models. Following the basic procedure described in (Socher, 2014), we developed a Recursive Neural Network model to achieve these aims. We used the Stanford parser (Klein and Manning, 2003) to create parse trees that modeled the language structure of patient and therapist utterances. These sentencelevel models were then used as input into a Maximum Entropy Markov Model (MEMM), a type of sequence model that uses the sentence and surrounding context to predict MISC codes. The recursive neural networks were designed using the ’standard’ model (Socher et al., 2011) with a single weight matrix to combine each node in the tree. We tested both a standard RNN model and an RNN that utilized a dependency parsing of the sentence. Once a final model was tuned, the performance of each model"
W15-1209,D14-1162,0,0.0755016,"speaker role and did not have to distinguish the roles of the speakers as patient or therapist. We trained two different models – one that uses indicators for only unigrams in the utterance and the second that uses indicators for unigrams, bigrams and trigrams in the utterance. 3.3 Recursive Neural Network Our second feature set uses recursive neural network (RNN) models, which are variants of the ideas presented in (Socher, 2014). The models were initialized with word vectors (i.e., numeric representations of word tokens) that were pre-trained using word vectors generated by the Glove model (Pennington et al., 2014). The RNNs in this paper relied mostly on the standard model for combining nodes of a recursive tree. For example, for combining word vector 1 a1 (e.g., numeric representation of ”hate”) and word vector 2 a2 (e.g., numeric representation of “hangovers”), the two vectors are multiplied through a weight matrix Wm that is shared across the tree in order to combine the individual words (e.g., “hate” and “hangovers”) into a new vector that combines the meaning of both inputs, a1,2 (e.g., “hate hangovers”). This is performed through the function:     a1 +b p1,2 = tanh Wm a2 where a1 ,a2 and p1,2"
W15-1209,D11-1014,0,0.0202886,"ve the prediction accuracy of these codes over ngram models. Following the basic procedure described in (Socher, 2014), we developed a Recursive Neural Network model to achieve these aims. We used the Stanford parser (Klein and Manning, 2003) to create parse trees that modeled the language structure of patient and therapist utterances. These sentencelevel models were then used as input into a Maximum Entropy Markov Model (MEMM), a type of sequence model that uses the sentence and surrounding context to predict MISC codes. The recursive neural networks were designed using the ’standard’ model (Socher et al., 2011) with a single weight matrix to combine each node in the tree. We tested both a standard RNN model and an RNN that utilized a dependency parsing of the sentence. Once a final model was tuned, the performance of each model predicting change talk and sustain talk codes was examined by comparing RNNs with an n-gram based model using cross-validation. The main goals of this paper are to 1. Define the challenging and interesting problem of identifying client change and sustain talk in psychotherapy transcripts. 2. Explore and evaluate methods of using continuous word representations to identify the"
W15-1209,P13-1045,0,0.0144607,"ably identify important linguistic features in MI. This study represents an initial attempt at predicting the more difficult-to-identify patient behaviors, which are central to much of the research on MI. More work is needed to improve these models, and it is likely that performance could be improved by going beyond word counting models, for example, by using the syntactic structure of sentences as well as the context of surrounding utterances. NLP applications have been successful in areas in which human annotators can clearly label the construct of interest (e.g., sentiment in movie reviews(Socher et al., 2013b), classifying news articles(Rubin et al., 2012)). Psychotherapy generally and ‘change talk’ within MI specifically are often concerned with latent psychological states of human experience. Verbalizations of reducing drug use are hypothesized to be observed indicators of a patient’s inclination to change their behavior and is mutually dependent on both their own previous linguistic behavior as well as the therapist’s. This is a challenging, new arena for NLP application and development, and one that will only be successful through the tight collaboration of NLP researchers and domain experts."
W15-1209,D13-1170,0,0.00746368,"ably identify important linguistic features in MI. This study represents an initial attempt at predicting the more difficult-to-identify patient behaviors, which are central to much of the research on MI. More work is needed to improve these models, and it is likely that performance could be improved by going beyond word counting models, for example, by using the syntactic structure of sentences as well as the context of surrounding utterances. NLP applications have been successful in areas in which human annotators can clearly label the construct of interest (e.g., sentiment in movie reviews(Socher et al., 2013b), classifying news articles(Rubin et al., 2012)). Psychotherapy generally and ‘change talk’ within MI specifically are often concerned with latent psychological states of human experience. Verbalizations of reducing drug use are hypothesized to be observed indicators of a patient’s inclination to change their behavior and is mutually dependent on both their own previous linguistic behavior as well as the therapist’s. This is a challenging, new arena for NLP application and development, and one that will only be successful through the tight collaboration of NLP researchers and domain experts."
W15-1209,Q14-1017,0,\N,Missing
W15-1612,J09-2001,0,0.0616612,"s were clustered automatically, then the clusters were manually refined and given names). Detailed in Srikumar and Roth (2013a), those categories cut across preposition types to combine related TPP senses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’H"
W15-1612,W13-2322,1,0.777114,"Missing"
W15-1612,bhatia-etal-2014-unified,0,0.0287958,"cial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. Th"
W15-1612,W07-1604,0,0.0343143,"unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comp"
W15-1612,W06-1670,0,0.146858,"and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numer"
W15-1612,W06-1207,0,0.0343708,"nses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Nei"
W15-1612,D09-1047,0,0.617638,"matic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of"
W15-1612,J14-1002,1,0.642481,"more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the goal of describing and documenting the uses of individual prepositions in a lexical resource rather than labeling a corpus with free-text preposition annotations. We hope that the latter, token-driven approach will be taken for annotating text with preposition supersenses so that those annotations will be suitable for training statistical NLP systems. 2 Our Approach With the end of free-text semantic annotation in mind, we develop and document a preposition"
W15-1612,C04-1198,0,0.0252432,"es. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framewor"
W15-1612,C10-2052,0,0.261708,"cs in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the go"
W15-1612,P11-2056,0,0.150935,"Missing"
W15-1612,hwang-etal-2014-criteria,1,0.867098,"Missing"
W15-1612,W07-1601,0,0.0452238,"Missing"
W15-1612,P14-1120,0,0.245532,"Missing"
W15-1612,S07-1005,0,0.348619,"Missing"
W15-1612,H94-1020,0,0.121481,"Missing"
W15-1612,W10-1827,0,0.02899,"The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Saint-Dizier, 2006b) aimed to describe the semantics of prepositions across several languages; however, it seems not to have progressed beyond the preliminary stages. Thus far, our approach has focused on English, but aims to define supersense categories semantically rather than by language-specific criteria (e.g., syntactic tests) so as to encourage its adaptation to other languages in the future. 1.1 1.2 1 Background Linguistic Approaches Most studies of preposition semantics are limited to so"
W15-1612,W03-0411,0,0.252975,"Missing"
W15-1612,J09-2002,0,0.154496,"Missing"
W15-1612,D10-1032,0,0.0281672,"ns are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairl"
W15-1612,saint-dizier-2006-prepnet,0,0.462283,"3, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Sain"
W15-1612,P12-2050,1,0.404449,"; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki i"
W15-1612,N15-1177,1,0.172806,"wang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and"
W15-1612,Q13-1019,1,0.876864,"e descriptive annotation scheme for prepositions must deal with these messy facts. Following a brief discussion of existing approaches to preposition semantics (§1), this paper offers a new approach to characterizing their functions at a coarsegrained level. Our scheme is intended to apply to almost all preposition tokens, though some are excluded on the grounds that they belong to a larger multiword expression or are purely syntactic (§2). The rest of the paper is devoted to our coarse semantic categories, supersenses (§3).2 Many of these categories are based on previous proposals—primarily, Srikumar and Roth (2013a) (so-called preposition relations) and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also"
W15-1612,N09-3017,0,0.194973,"of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspecti"
W15-1612,tsvetkov-etal-2014-augmenting-english,1,0.604089,"m into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples"
W15-1612,S07-1051,0,0.17518,"apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotat"
W15-1612,P98-1013,0,\N,Missing
W15-1612,C98-1013,0,\N,Missing
W16-0304,baccianella-etal-2010-sentiwordnet,0,0.0140204,"uanced way. 2 Sentiment Analysis There is a long tradition in the field of Natural Language Processing (NLP) for trying to correctly identify the sentiment of passages of text and as a result there are a large number of techniques that have been tested (for a review on the subject, see Pang and Lee (2008)). Some common methods involve using n-grams combined with classifier models (SVM, CRF, Naive Bayes) to identify the sentiment of sentences or passages (Pak and Paroubek, 2015). Another method involves using pre-compiled dictionaries of common terms with their polarity (positive or negative) (Baccianella et al., 2010). As with many NLP methods, researchers have attempted to go beyond the mere presentation of words and use sentence structure and contextual information to improve accuracy. Along these lines, more recently researchers have used deep learning techniques to improve accuracy on sentiment datasets, with some success (Maas et al., 2011; Socher et al., 2013). 2.1 381 times in our collection of therapy transcripts. Moreover, psychotherapy text typically comes from transcribed dialogue - not written communication. Modeling strategies that work well on written text may perform poorly on spoken languag"
W16-0304,P11-1015,0,0.0609777,"g n-grams combined with classifier models (SVM, CRF, Naive Bayes) to identify the sentiment of sentences or passages (Pak and Paroubek, 2015). Another method involves using pre-compiled dictionaries of common terms with their polarity (positive or negative) (Baccianella et al., 2010). As with many NLP methods, researchers have attempted to go beyond the mere presentation of words and use sentence structure and contextual information to improve accuracy. Along these lines, more recently researchers have used deep learning techniques to improve accuracy on sentiment datasets, with some success (Maas et al., 2011; Socher et al., 2013). 2.1 381 times in our collection of therapy transcripts. Moreover, psychotherapy text typically comes from transcribed dialogue - not written communication. Modeling strategies that work well on written text may perform poorly on spoken language. For example, methods that require parse trees (recursive neural nets) may have difficulty on the disfluencies, fillers and fragments that come from dialogue. Databases used for sentiment analysis have come from a variety of written prose ranging from classic literature (Yussupova et al., 2012; Qiu et al., 2011; Liu and Zhang, 20"
W16-0304,J11-1002,0,0.0404551,"ith some success (Maas et al., 2011; Socher et al., 2013). 2.1 381 times in our collection of therapy transcripts. Moreover, psychotherapy text typically comes from transcribed dialogue - not written communication. Modeling strategies that work well on written text may perform poorly on spoken language. For example, methods that require parse trees (recursive neural nets) may have difficulty on the disfluencies, fillers and fragments that come from dialogue. Databases used for sentiment analysis have come from a variety of written prose ranging from classic literature (Yussupova et al., 2012; Qiu et al., 2011; Liu and Zhang, 2012), news articles (see Pang and Lee (2008) for a list of databases), to social media text (for examples see Bohlouli et al. (2015), Gokulakrishnan et al. (2012) and Pak and Paroubek (2015)). Databases have been created from archived text via the Internet. Additionally, researchers have used a variety of techniques to harvest a live feed of tweets and posts from social media outlets as Twitter and Facebook, respectively, so as to access fresh data (Bohlouli et al., 2015). Virtually all of the databases for sentiment analysis are written and none (that we are aware of) come f"
W16-0304,D13-1170,0,0.334285,"with classifier models (SVM, CRF, Naive Bayes) to identify the sentiment of sentences or passages (Pak and Paroubek, 2015). Another method involves using pre-compiled dictionaries of common terms with their polarity (positive or negative) (Baccianella et al., 2010). As with many NLP methods, researchers have attempted to go beyond the mere presentation of words and use sentence structure and contextual information to improve accuracy. Along these lines, more recently researchers have used deep learning techniques to improve accuracy on sentiment datasets, with some success (Maas et al., 2011; Socher et al., 2013). 2.1 381 times in our collection of therapy transcripts. Moreover, psychotherapy text typically comes from transcribed dialogue - not written communication. Modeling strategies that work well on written text may perform poorly on spoken language. For example, methods that require parse trees (recursive neural nets) may have difficulty on the disfluencies, fillers and fragments that come from dialogue. Databases used for sentiment analysis have come from a variety of written prose ranging from classic literature (Yussupova et al., 2012; Qiu et al., 2011; Liu and Zhang, 2012), news articles (se"
W16-0304,pak-paroubek-2010-twitter,0,\N,Missing
W16-1712,P08-1037,0,0.0242269,"of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clu"
W16-1712,bonial-etal-2014-propbank,1,0.91389,"Missing"
W16-1712,W07-1604,0,0.0246655,"his paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and ev"
W16-1712,W06-1670,0,0.0856608,"asses that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘ind"
W16-1712,D09-1047,0,0.77986,"ities, prepositions serve as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and"
W16-1712,hashemi-hwa-2014-comparison,0,0.105801,"tated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotate"
W16-1712,L16-1629,1,0.821862,"opBank expertise) checked the gold PropBank annotations, agreeing that 5 of the tokens were clearly incorrect. This analysis tells us that obvious errors with both types of annotation are indeed present in the corpus (11 tokens in the sample), adding some noise to the supersense–function tag correspondences. However, the outright errors are probably dwarfed by difficult/borderline cases for which the annotations are not entirely consistent throughout the corpus. For example, on time (i.e., ‘not late’) is variously annotated as S TATE, M ANNER, and T IME. Inconsistency detection methods (e.g., Hollenstein et al., 2016) may help identify these— though it remains to be seen whether methods developed for nouns and verbs would succeed on function words so polysemous as prepositions. Summary. The (mostly) clean correspondences of the supersenses to the independently annotated PropBank modifier labels speak to the linguistic validity of our supersense hierarchy. On the other hand, the confusion evident for the supersense labels corresponding to PropBank’s numbered arguments suggests further analysis and refinement is necessary for both annotation schemes. Some of these issues—especially correspondences between la"
W16-1712,C10-2052,0,0.267964,"arners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previ"
W16-1712,S14-1001,0,0.0160165,"al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English preposition"
W16-1712,P14-1120,0,0.236706,"s-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition token per sentence. By contrast, we will fully annotate documents for all preposition tokens. No interannotator agreement figures have been reported for the PDEP data to indicate its quality, or the overall difficulty of token annotation with TPP senses across a broad range of prepositions. 2.2 Supersenses From the literature on other kinds of supersenses, there is reason to believe that t"
W16-1712,S07-1005,0,0.110407,"Missing"
W16-1712,W03-0411,0,0.0816644,"Missing"
W16-1712,J05-1004,1,0.526768,"Missing"
W16-1712,picca-etal-2008-supersense,0,0.131684,"rdNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they gro"
W16-1712,N13-1076,1,0.852834,"for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotati"
W16-1712,P12-2050,1,0.867655,"v et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen"
W16-1712,schneider-etal-2014-comprehensive,1,0.808201,"with sparse training data. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infi"
W16-1712,N15-1177,1,0.85174,"ta. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infinitival to and infinitival-s"
W16-1712,W15-1612,1,0.871481,"own University Jena D. Hwang IHMC Vivek Srikumar University of Utah jhwang@ihmc.us svivek@cs.utah.edu nschneid@inf.ed.ac.uk Meredith Green Abhijit Suresh Kathryn Conger Tim O’Gorman University of Colorado at Boulder Martha Palmer {laura.green,abhijit.suresh,kathryn.conger,timothy.ogorman,martha.palmer}@colorado.edu Abstract (1) I have been going to/D ESTINATION the Wildwood_,_NJ for/D URATION over 30 years for/P URPOSE summer~vacations We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). The preposition supersenses are organized hierarchically and designed to facilitate comprehensive manual annotation. Our dataset is publicly released on the web.1 1 (2) It is close to/L OCATION bus_lines for/D ESTINATION Opera_Plaza (3) I was looking~to/`i bring a customer to/D ESTINATION their lot to/P URPOSE buy a car Figure 1: Preposition supersenses illustrating the polysemy of to and for. Both can mark a D ESTINATION or P URPOSE, while there are other functions that do not overlap. The syntactic complement use of infinitival to is tagged as `i. The over token in (1) receives the label A"
W16-1712,W97-0811,0,0.211554,"itions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic function"
W16-1712,W12-0514,0,0.0797358,"ew corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requir"
W16-1712,D11-1012,1,0.950338,"e as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation system"
W16-1712,Q13-1019,1,0.913045,"translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English p"
W16-1712,N09-3017,0,0.440547,"ly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition t"
W16-1712,D11-1116,0,0.0234141,"et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotating tokens, and restrict each sense 5 php 100 http://www.clres.com/db/classes/ClassAnalysis. Superset Possessor Co-Agent Creator Whole Elements Instance Agent Species Causer Quantity Configuration Patient Accompanier Co-Patient Undergoer Reciprocation Purpose Theme Participant Experiencer Stimulus Via Place Value Path Manner Time Frequency Duration Temporal Circumstance Extent Location Beneficiary Ins"
W16-1712,D15-1243,0,0.0468006,"Missing"
W16-1712,W13-0906,0,0.0262783,"nemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hov"
W16-1712,tsvetkov-etal-2014-augmenting-english,1,0.844664,", a disambiguation system trained on this dataset will therefore be biased and perform poorly on an ecologically valid sample of tokens. preposition supersenses (Schneider et al., 2015) will be more scalable and useful than senses. The term supersense has been applied to lexical semantic classes that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al"
W16-1712,S07-1051,0,0.3404,"ing second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual prepositio"
W16-1712,W15-1806,0,\N,Missing
W16-1712,S16-1084,1,\N,Missing
W19-3316,P13-1023,1,0.855673,"e subject cannot be ranked lower than the direct object (e.g., a subject construed as a T HEME cannot have a direct object construed as an AGENT). Indirect objects in the English double object construction4 are treated as R ECIPIENT construals. (18) I sent [John]R ECIPIENT↝R ECIPIENT a cake. (19) I sent a cake [to John]R ECIPIENT↝G OAL . (20) I baked [John]R ECIPIENT↝R ECIPIENT a cake. (21) I paid [John]R ECIPIENT↝R ECIPIENT [$10]C OST↝C OST . Interannotator Agreement Study Data. We piloted our guidelines using a sample of 100 scenes from the English UCCA-annotated Wiki corpus5 as detailed by Abend and Rappoport (2013). UCCA is a scheme for annotating coarsegrained predicate-argument structure such that syntactically varied paraphrases and translations should receive similar analyses. It captures both static and dynamic scenes and their participants, but does not mark semantic roles. Annotators. Four annotators (A, B, C, D), all authors of this paper, took part in this study. All are computational linguistics researchers. Datasets. Prior to development of guidelines for subjects and objects, one of the annotators (Annotator A) sampled 106 Wiki documents (44k tokens) and tagged all 10k instances of UCCA Part"
W19-3316,S17-1022,1,0.738698,"Missing"
W19-3316,J05-1004,0,0.198024,"positions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabulary predicates are encountered. But is a lexicon really necessary for role annotation? A general-purpose set of role labels with detailed criteria for each can potentially bypass coverage limitations of lexicon-based approaches, while still supporting some degree of generalization across grammatical paraphrases. Second, the nonreliance on a lexicon potentially simplifies the annotation process in some respects. For example, no explicit predicate disamb"
W19-3316,Q15-1034,0,0.0554848,"Missing"
W19-3316,P18-1018,1,0.601633,"orical semantic roles (Fillmore, 1968, 1982; Levin, 1993) or bundles of proto-properties (Dowty, 1991; Reisinger et al., 2015) that generalize across verbs. A parallel line of work (§2) has looked at the meanings coded by grammatical phrase-markers such as prepositions and possessives and how to disambiguate them. These inquiries necessarily overlap because many prepositions mark verb arguments or modifiers. Consequently, insights from the study of prepositions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabular"
