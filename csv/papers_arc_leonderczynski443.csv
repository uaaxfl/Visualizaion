2021.vardial-1.8,Discriminating Between Similar Nordic Languages,2021,-1,-1,2,0,641,rene haas,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\aa}l), Faroese and Icelandic."
2021.sustainlp-1.12,Hyperparameter Power Impact in Transformer Language Model Training,2021,-1,-1,4,0,901,lucas chavannes,Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing,0,"Training large language models can consume a large amount of energy. We hypothesize that the language model{'}s configuration impacts its energy consumption, and that there is room for power consumption optimisation in modern large language models. To investigate these claims, we introduce a power consumption factor to the objective function, and explore the range of models and hyperparameter configurations that affect power. We identify multiple configuration factors that can reduce power consumption during language model training while retaining model quality."
2021.nodalida-main.47,{D}an{FEVER}: claim verification dataset for {D}anish,2021,-1,-1,2,0,2728,jeppe norregaard,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"We present a dataset, DanFEVER, intended for multilingual misinformation research. The dataset is in Danish and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language."
2021.hcinlp-1.16,An {IDR} Framework of Opportunities and Barriers between {HCI} and {NLP},2021,-1,-1,2,0,6060,nanna inie,Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,0,"This paper presents a framework of opportunities and barriers/risks between the two research fields Natural Language Processing (NLP) and Human-Computer Interaction (HCI). The framework is constructed by following an interdisciplinary research-model (IDR), combining field-specific knowledge with existing work in the two fields. The resulting framework is intended as a departure point for discussion and inspiration for research collaborations."
2021.bsnlp-1.3,Abusive Language Recognition in {R}ussian,2021,-1,-1,2,0,12037,kamil saitov,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,0,"Abusive phenomena are commonplace in language on the web. The scope of recognizing abusive language is broad, covering many behaviors and forms of expression. This work addresses automatic detection of abusive language in Russian. The lexical, grammatical and morphological diversity of Russian language present potential difficulties for this task, which is addressed using a variety of machine learning approaches. Finally, competitive performance is reached over multiple domains for this investigation into automatic detection of abusive language in Russian."
2021.acl-long.247,Annotating Online Misogyny,2021,-1,-1,3,0,13069,philine zeinert,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts."
2020.semeval-1.188,{S}em{E}val-2020 Task 12: Multilingual Offensive Language Identification in Social Media ({O}ffens{E}val 2020),2020,-1,-1,7,0,622,marcos zampieri,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers."
2020.lrec-1.303,Accelerated High-Quality Mutual-Information Based Word Clustering,2020,-1,-1,3,1,2714,manuel ciosici,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Word clustering groups words that exhibit similar properties. One popular method for this is Brown clustering, which uses short-range distributional information to construct clusters. Specifically, this is a hard hierarchical clustering with a fixed-width beam that employs bi-grams and greedily minimizes global mutual information loss. The result is word clusters that tend to outperform or complement other word representations, especially when constrained by small datasets. However, Brown clustering has high computational complexity and does not lend itself to parallel computation. This, together with the lack of efficient implementations, limits their applicability in NLP. We present efficient implementations of Brown clustering and the alternative Exchange clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods."
2020.lrec-1.430,Offensive Language and Hate Speech Detection for {D}anish,2020,-1,-1,2,0,17576,gudbjartur sigurbergsson,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual. We construct a Danish dataset DKhate containing user-generated comments from various social media platforms, and to our knowledge, the first of its kind, annotated for various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of 0.74, and the best performing system for Danish achieves a macro averaged F1-score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1-score of 0.62, while the best performing system for Danish achieves a macro averaged F1-score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1-score of 0.56, and the best performing system for Danish achieves a macro averaged F1-score of 0.63. Our work for both the English and the Danish language captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying."
2020.fever-1.6,Maintaining Quality in {FEVER} Annotation,2020,-1,-1,1,1,642,leon derczynski,Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER),0,"We propose two measures for measuring the quality of constructed claims in the FEVER task. Annotating data for this task involves the creation of supporting and refuting claims over a set of evidence. Automatic annotation processes often leave superficial patterns in data, which learning systems can detect instead of performing the underlying task. Humans also can leave these superficial patterns, either voluntarily or involuntarily (due to e.g. fatigue). The two measures introduced attempt to detect the impact of these superficial patterns. One is a new information-theoretic and distributionality based measure, \textit{DCI}; and the other an extension of neural probing work over the ARCT task, \textit{utility}. We demonstrate these measures over a recent major dataset, that from the English FEVER task in 2019."
2020.coling-tutorials.4,Detection and Resolution of Rumors and Misinformation with {NLP},2020,-1,-1,1,1,642,leon derczynski,Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts,0,Detecting and grounding false and misleading claims on the web has grown to form a substantial sub-field of NLP. The sub-field addresses problems at multiple different levels of misinformation detection: identifying check-worthy claims; tracking claims and rumors; rumor collection and annotation; grounding claims against knowledge bases; using stance to verify claims; and applying style analysis to detect deception. This half-day tutorial presents the theory behind each of these steps as well as the state-of-the-art solutions.
W19-6121,Political Stance in {D}anish,2019,-1,-1,2,0,23697,rasmus lehmann,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"The task of stance detection consists of classifying the opinion within a text towards some target. This paper seeks to generate a dataset of quotes from Danish politicians, label this dataset to allow the task of stance detection to be performed, and present annotation guidelines to allow further expansion of the generated dataset. Furthermore, three models based on an LSTM architecture are designed, implemented and optimized to perform the task of stance detection for the generated dataset. Experiments are performed using conditionality and bi-directionality for these models, and using either singular word embeddings or averaged word embeddings for an entire quote, to determine the optimal model design. The simplest model design, applying neither conditionality or bi-directionality, and averaged word embeddings across quotes, yields the strongest results. Furthermore, it was found that inclusion of the quotes politician, and the party affiliation of the quoted politician, greatly improved performance of the strongest model."
W19-6122,Joint Rumour Stance and Veracity Prediction,2019,-1,-1,3,0,23698,anders lillie,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"The net is rife with rumours that spread through microblogs and social media. Not all the claims in these can be verified. However, recent work has shown that the stances alone that commenters take toward claims can be sufficiently good indicators of claim veracity, using e.g. an HMM that takes conversational stance sequences as the only input. Existing results are monolingual (English) and mono-platform (Twitter). This paper introduces a stance-annotated Reddit dataset for the Danish language, and describes various implementations of stance classification models. Of these, a Linear SVM provides predicts stance best, with 0.76 accuracy / 0.42 macro F1. Stance labels are then used to predict veracity across platforms and also across languages, training on conversations held in one language and using the model on conversations held in another. In our experiments, monolinugal scores reach stance-based veracity accuracy of 0.83 (F1 0.68); applying the model across languages predicts veracity of claims with an accuracy of 0.82 (F1 0.67). This demonstrates the surprising and powerful viability of transferring stance-based veracity prediction across languages."
W19-6138,Bornholmsk Natural Language Processing: Resources and Tools,2019,0,1,1,1,642,leon derczynski,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"This paper introduces language processing resources and tools for Bornholmsk, a language spoken on the island of Bornholm, with roots in Danish and closely related to Scanian. This presents an overview of the language and available data, and the first NLP models for this living, minority Nordic language. Sammenfattnijng p{\aa} borrijnholmst: D{\ae}jnna artikkelijn introduserer naturspr{\aa}gsresurser {\aa} varktoi for borrijnholmst, ed spr{\aa}g a d{\ae}r snakkes p{\aa} {\""o}n Borrijnholm me r{\o}dder i danst {\aa} i n{\ae}r familia me sk{\aa}nst. Artikkelijn gjer ed {\^a}uersyn {\^a}uer spr{\aa}ged {\aa} di datan som fijnnes, {\aa} di fosste NLP mod{\ae}llarna for d{\ae}tta l{\ae}wenes nordiska minnret{\^a}lsspr{\aa}ged."
W19-6141,The Lacunae of {D}anish Natural Language Processing,2019,0,4,3,0,2720,andreas kirkedal,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"Danish is a North Germanic language spoken principally in Denmark, a country with a long tradition of technological and scientific innovation. However, the language has received relatively little attention from a technological perspective. In this paper, we review Natural Language Processing (NLP) research, digital resources and tools which have been developed for Danish. We find that availability of models and tools is limited, which calls for work that lifts Danish NLP a step closer to the privileged languages. Dansk abstrakt: Dansk er et nordgermansk sprog, talt prim{\ae}rt i kongeriget Danmark, et land med st{\ae}rk tradition for teknologisk og videnskabelig innovation. Det danske sprog har imidlertid v{\ae}ret genstand for relativt begr{\ae}nset opm{\ae}rksomhed, teknologisk set. I denne artikel gennemg{\aa}r vi sprogteknologi-forskning, -ressourcer og -v{\ae}rkt{\o}jer udviklet for dansk. Vi konkluderer at der eksisterer et f{\aa}tal af modeller og v{\ae}rkt{\o}jer, hvilket indbyder til forskning som l{\o}fter dansk sprogteknologi i niveau med mere priviligerede sprog."
S19-2147,"{S}em{E}val-2019 Task 7: {R}umour{E}val, Determining Rumour Veracity and Support for Rumours",2019,0,13,7,0.862467,25115,genevieve gorrell,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of {``}fake news{''} has become a mainstream concern. However automated support for rumour verification remains in its infancy. It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase. Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour{'}s veracity. As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity. The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts. There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants. We received 22 system submissions (a 70{\%} increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved."
N19-1157,Quantifying the morphosyntactic content of Brown Clusters,2019,0,0,2,1,2714,manuel ciosici,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Brown and Exchange word clusters have long been successfully used as word representations in Natural Language Processing (NLP) systems. Their success has been attributed to their seeming ability to represent both semantic and syntactic information. Using corpora representing several language families, we test the hypothesis that Brown and Exchange word clusters are highly effective at encoding morphosyntactic information. Our experiments show that word clusters are highly capable at distinguishing Parts of Speech. We show that increases in Average Mutual Information, the clustering algorithms{'} optimization goal, are highly correlated with improvements in encoding of morphosyntactic information. Our results provide empirical evidence that downstream NLP systems addressing tasks dependent on morphosyntactic information can benefit from word cluster features."
S18-1179,{IUCM} at {S}em{E}val-2018 Task 11: Similar-Topic Texts as a Comprehension Knowledge Source,2018,0,0,2,0,28918,sofia reznikova,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the IUCM entry at SemEval-2018 Task 11, on machine comprehension using commonsense knowledge. First, clustering and topic modeling are used to divide given texts into topics. Then, during the answering phase, other texts of the same topic are retrieved and used as commonsense knowledge. Finally, the answer is selected. While clustering itself shows good results, finding an answer proves to be more challenging. This paper reports the results of system evaluation and suggests potential improvements."
W17-4418,Results of the {WNUT}2017 Shared Task on Novel and Emerging Entity Recognition,2017,29,28,1,1,642,leon derczynski,Proceedings of the 3rd Workshop on Noisy User-generated Text,0,"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet {``}so.. kktny in 30 mins?!{''} {--} even human experts find the entity {`}kktny{'} hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging named entities in noisy text."
S17-2006,{S}em{E}val-2017 Task 8: {R}umour{E}val: Determining rumour veracity and support for rumours,2017,0,45,1,1,642,leon derczynski,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Media is full of false claims. Even Oxford Dictionaries named {``}post-truth{''} as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics {--} each having their own families of claims and replies {--} and use these to pose two concrete challenges as well as the results achieved by participants on these challenges."
aker-etal-2017-simple,Simple Open Stance Classification for Rumour Analysis,2017,22,5,2,0,25116,ahmet aker,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction."
W16-3928,{T}witter Geolocation Prediction Shared Task of the 2016 Workshop on Noisy User-generated Text,2016,6,20,3,0,33670,bo han,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"This paper presents the shared task for English Twitter geolocation prediction in WNUT 2016. We discuss details of task settings, data preparations and participant systems. The derived dataset and performance figures from each system provide baselines for future research in this realm."
S16-1165,{S}em{E}val-2016 Task 12: Clinical {T}emp{E}val,2016,15,60,4,0.129688,224,steven bethard,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Clinical TempEval 2016 evaluated temporal information extraction systems on the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015."
L16-1040,"Complementarity, {F}-score, and {NLP} Evaluation",2016,15,7,1,1,642,leon derczynski,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper addresses the problem of quantifying the differences between entity extraction systems, where in general only a small proportion a document should be selected. Comparing overall accuracy is not very useful in these cases, as small differences in accuracy may correspond to huge differences in selections over the target minority class. Conventionally, one may use per-token complementarity to describe these differences, but it is not very useful when the set is heavily skewed. In such situations, which are common in information retrieval and entity recognition, metrics like precision and recall are typically used to describe performance. However, precision and recall fail to describe the differences between sets of objects selected by different decision strategies, instead just describing the proportional amount of correct and incorrect objects selected. This paper presents a method for measuring complementarity for precision, recall and F-score, quantifying the difference between entity extraction approaches."
L16-1587,{GATE}-Time: Extraction of Temporal Expressions and Events,2016,17,4,1,1,642,leon derczynski,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"GATE is a widely used open-source solution for text processing with a large user community. It contains components for several natural language processing tasks. However, temporal information extraction functionality within GATE has been rather limited so far, despite being a prerequisite for many application scenarios in the areas of natural language processing and information retrieval. This paper presents an integrated approach to temporal information processing. We take state-of-the-art tools in temporal expression and event recognition and bring them together to form an openly-available resource within the GATE infrastructure. GATE-Time provides annotation in the form of TimeML events and temporal expressions complying with this mature ISO standard for temporal semantic annotation of documents. Major advantages of GATE-Time are (i) that it relies on HeidelTime for temporal tagging, so that temporal expressions can be extracted and normalized in multiple languages and across different domains, (ii) it includes a modern, fast event recognition and classification tool, and (iii) that it can be combined with different linguistic pre-processing annotations, and is thus not bound to license restricted preprocessing components."
C16-1111,Broad {T}witter Corpus: A Diverse Named Entity Recognition Resource,2016,33,13,1,1,642,leon derczynski,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the CoNLL{'}2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens {--} a mere 15{\%} the size of CoNLL{'}2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations."
C16-1182,Representation and Learning of Temporal Relations,2016,31,1,1,1,642,leon derczynski,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Determining the relative order of events and times described in text is an important problem in natural language processing. It is also a difficult one: general state-of-the-art performance has been stuck at a relatively low ceiling for years. We investigate the representation of temporal relations, and empirically evaluate the effect that various temporal relation representations have on machine learning performance. While machine learning performance decreases with increased representational expressiveness, not all representation simplifications have equal impact."
W15-5402,Handling and Mining Linguistic Variation in {UGC},2015,0,0,1,1,642,leon derczynski,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,None
W15-4306,{USFD}: {T}witter {NER} with Drift Compensation and Linked Data,2015,22,2,1,1,642,leon derczynski,Proceedings of the Workshop on Noisy User-generated Text,0,"This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from Linked Data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift - a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task."
W15-0211,Analysis of Temporal Expressions Annotated in Clinical Notes,2015,18,5,3,0,1864,hegler tissot,Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11),0,"Annotating the semantics of time in language is important. THYME (Styler et al., 2014) is a recent temporal annotation standard for clinical texts. This paper examines temporal expressions in the first major corpus released under this standard. It investigates where the standard has proven difficult to apply, and gives a series of recommendations regarding temporal annotation in this important domain."
S15-2101,{S}wiss-Chocolate: Combining Flipout Regularization and Random Forests with Artificially Built Subsystems to Boost Text-Classification for Sentiment,2015,9,0,5,0,32094,fatih uzdilli,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We describe a classifier for predicting message-level sentiment of English microblog messages from Twitter. This paper describes our submission to the SemEval2015 competition (Task 10). Our approach is to combine several variants of our previous yearxe2x80x99s SVM system into one meta-classifier, which was then trained using a random forest. The main idea is that the meta-classifier allows the combination of the strengths and overcome some of the weaknesses of the artificially-built individual classifiers, and adds additional non-linearity. We were also able to improve the linear classifiers by using a new regularization technique we call flipout."
S15-2136,{S}em{E}val-2015 Task 6: Clinical {T}emp{E}val,2015,7,73,2,0.129688,224,steven bethard,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Clinical TempEval 2015 brought the temporal information extraction tasks of past TempEval campaigns to the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations."
S15-2141,{UFPRS}heffield: Contrasting Rule-based and Support Vector Machine Approaches to Time Expression Identification in Clinical {T}emp{E}val,2015,11,1,4,0,1864,hegler tissot,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We present two approaches to time expression identification, as entered in to SemEval2015 Task 6, Clinical TempEval. The first is a comprehensive rule-based approach that favoured recall, and which achieved the best recall for time expression identification in Clinical TempEval. The second is an SVM-based system built using readily available components, which was able to achieve a competitive F1 in a short development time. We discuss how the two approaches perform relative to each other, and how characteristics of the corpus affect the suitability of different approaches and their outcomes."
R15-1016,"Tune Your Brown Clustering, Please",2015,26,10,1,1,642,leon derczynski,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Brown clustering, an unsupervised hierarchical clustering technique based on ngram mutual information, has proven useful in many NLP applications. However, most uses of Brown clustering employ the same default configuration; the appropriateness of this configuration has gone predominantly unexplored. Accordingly, we present information for practitioners on the behaviour of Brown clustering in order to assist hyper-parametre tuning, in the form of a theoretical model of Brown clustering utility. This model is then evaluated empirically in two sequence labelling tasks over two text types. We explore the dynamic between the input corpus size, chosen number of classes, and quality of the resulting clusters, which has an impact for any approach using Brown clustering. In every scenario that we examine, our results reveal that the values most commonly used for the clustering are sub-optimal."
R15-1017,Temporal Relation Classification using a Model of Tense and Aspect,2015,12,2,1,1,642,leon derczynski,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Determining the temporal order of events in a text is difficult. However, it is crucial to the extraction of narratives, plans, and context. We suggest that a simple, established framework of tense and aspect provides a viable model for ordering a subset of events and times in a given text. Using this framework, we investigate extracting features that represent temporal information and integrate these in a machine learning approach. These features improve event-event ordering."
R15-1018,Efficient Named Entity Annotation through Pre-empting,2015,24,0,1,1,642,leon derczynski,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Linguistic annotation is time-consuming and expensive. One common annotation task is to mark entities - such as names of people, places and organisations - in text. In a document, many segments of text often contain no entities at all. We show that these segments are worth skipping, and demonstrate a technique for reducing the amount of entity-less text examined by annotators, which we call preempting. This technique is evaluated in a crowdsourcing scenario, where it provides downstream performance improvements for the same size corpus."
sabou-etal-2014-corpus,Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines,2014,52,49,3,0,19778,marta sabou,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Crowdsourcing is an emerging collaborative approach that can be used for the acquisition of annotated corpora and a wide range of other linguistic resources. Although the use of this approach is intensifying in all its key genres (paid-for crowdsourcing, games with a purpose, volunteering-based approaches), the community still lacks a set of best-practice guidelines similar to the annotation best practices for traditional, expert-based corpus acquisition. In this paper we focus on the use of crowdsourcing methods for corpus acquisition and propose a set of best practice guidelines based in our own experiences in this area and an overview of related literature. We also introduce GATE Crowd, a plugin of the GATE platform that relies on these guidelines and offers tool support for using crowdsourcing in a more principled and efficient manner."
E14-4014,Passive-Aggressive Sequence Labeling with Discriminative Post-Editing for Recognising Person Entities in Tweets,2014,19,6,1,1,642,leon derczynski,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Recognising entities in social media text is difficult. NER on newswire text is conventionally cast as a sequence labeling problem. This makes implicit assumptions regarding its textual structure. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%."
E14-2016,{DKIE}: Open Source Information Extraction for {D}anish,2014,12,6,1,1,642,leon derczynski,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Danish is a major Scandinavian language spoken daily by around six million people. However, it lacks a unified, open set of NLP tools. This demonstration will introduce DKIE, an extensible open-source toolkit for processing Danish text. We implement an information extraction architecture for Danish within GATE, including integrated third-party tools. This implementation includes the creation of a substantial set of corpus annotations for dataintensive named entity recognition. The final application and dataset is made are openly available, and the part-of-speech tagger and NER model also operate independently or with the Stanford NLP toolkit."
E14-2025,The {GATE} Crowdsourcing Plugin: Crowdsourcing Annotated Corpora Made Easy,2014,8,22,3,0,11076,kalina bontcheva,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Crowdsourcing is an increasingly popular,n collaborative approach for acquiringn annotated corpora. Despite this, reusen of corpus conversion tools and user interfacesn between projects is still problematic,n since these are not generally maden available. This demonstration will introducen the new, open-source GATE Crowdsourcingn plugin, which offers infrastructuraln support for mapping documents ton crowdsourcing units and back, as well asn automatically generating reusable crowdsourcingn interfaces for NLP classificationn and selection tasks. The entire workflown will be demonstrated on: annotatingn named entities; disambiguating words andn named entities with respect to DBpedian URIs; annotation of opinion holders andn targets; and sentiment."
W13-0107,Empirical Validation of Reichenbach{'}s Tense Framework,2013,0,1,1,1,642,leon derczynski,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
S13-2001,"{S}em{E}val-2013 Task 1: {T}emp{E}val-3: Evaluating Time Expressions, Events, and Temporal Relations",2013,21,183,3,0,37291,naushad uzzaman,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems xe2x80x90 in each task and in general. In this paper, we describe the participantsxe2x80x99 approaches, results, and the observations from the results, which may guide future research in this area."
R13-1011,{T}wit{IE}: An Open-Source Information Extraction Pipeline for Microblog Text,2013,28,134,2,0,11076,kalina bontcheva,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Twitter is the largest source of microblog text, responsible for gigabytes of human discourse every day. Processing microblog text is difficult: the genre is noisy, documents have little context, and utterances are very short. As such, conventional NLP tools fail when faced with tweets and other microblog text. We present TwitIE, an open-source NLP pipeline customised to microblog text at every stage. Additionally, it includes Twitter-specific data import and metadata handling. This paper introduces each stage of the TwitIE pipeline, which is a modification of the GATE ANNIE open-source pipeline for news text. An evaluation against some state-of-the-art systems is also presented."
R13-1015,Recognising and Interpreting Named Temporal Expressions,2013,27,6,2,0,3140,matteo brucato,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This paper introduces a new class of temporal expression xe2x80x90 named temporal expressions xe2x80x90 and methods for recognising and interpreting its members. The commonest temporal expressions typically contain date and time words, like April or hours. Research into recognising and interpreting these typical expressions is mature in many languages. However, there is a class of expressions that are less typical, very varied, and difficult to automatically interpret. These indicate dates and times, but are harder to detect because they often do not contain time words and are not used frequently enough to appear in conventional temporally-annotated corpora xe2x80x90 for example Michaelmas or Vasant Panchami. Using Wikipedia and linked data, we automatically construct a resource of English named temporal expressions, and use it to extract training examples from a large corpus. These examples are then used to train and evaluate a named temporal expression recogniser. We also introduce and evaluate rules for automatically interpreting these expressions, and we observe that use of the rules improves temporal annotation performance over existing corpora."
R13-1026,{T}witter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data,2013,29,147,1,1,642,leon derczynski,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difficult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7% tagging accuracy (90.5% on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8% and sentence error by 12.2%. The model, training data and tools are made available."
P13-2114,Temporal Signals Help Label Temporal Relations,2013,31,11,1,1,642,leon derczynski,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatically determining the temporal order of events and times in a text is difficult, though humans can readily perform this task. Sometimes events and times are related through use of an explicit co-ordination which gives information about the temporal relation: expressions like xe2x80x9cbeforexe2x80x9d and xe2x80x9cas soon asxe2x80x9d. We investigate the rxcbx86 ole that these co-ordinating temporal signals have in determining the type of temporal relations in discourse. Using machine learning, we improve upon prior approaches to the problem, achieving over 80% accuracy at labelling the types of temporal relation between events and times that are related by temporal signals."
llorens-etal-2012-timen,{TIMEN}: An Open Temporal Expression Normalisation Resource,2012,19,46,2,1,37290,hector llorens,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Temporal expressions are words or phrases that describe a point, duration or recurrence in time. Automatically annotating these expressions is a research goal of increasing interest. Recognising them can be achieved with minimally supervised machine learning, but interpreting them accurately (normalisation) is a complex task requiring human knowledge. In this paper, we present TIMEN, a community-driven tool for temporal expression normalisation. TIMEN is derived from current best approaches and is an independent tool, enabling easy integration in existing systems. We argue that temporal expression normalisation can only be effectively performed with a large knowledge base and set of rules. Our solution is a framework and system with which to capture this knowledge for different languages. Using both existing and newly-annotated data, we present results showing competitive performance and invite the IE community to contribute to a knowledge base in order to solve the temporal expression normalisation problem."
derczynski-etal-2012-massively,Massively Increasing {TIMEX}3 Resources: A Transduction Approach,2012,18,4,1,1,642,leon derczynski,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. Gold standard temporally-annotated resources are limited in size, which makes research using them difficult. Standards have also evolved over the past decade, so not all temporally annotated data is in the same format. We vastly increase available human-annotated temporal expression resources by converting older format resources to TimeML/TIMEX3. This task is difficult due to differing annotation methods. We present a robust conversion tool and a new, large temporal expression resource. Using this, we evaluate our conversion process by using it as training data for an existing TimeML annotation tool, achieving a 0.87 F1 measure - better than any system in the TempEval-2 timex recognition exercise."
S10-1075,{USFD}2: Annotating Temporal Expresions and {TLINK}s for {T}emp{E}val-2,2010,12,22,1,1,642,leon derczynski,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe the University of Sheffield system used in the TempEval-2 challenge, USFD2. The challenge requires the automatic identification of temporal entities and relations in text.n n USFD2 identifies and anchors temporal expressions, and also attempts two of the four temporal relation assignment tasks. A rule-based system picks out and anchors temporal expressions, and a maximum entropy classifier assigns temporal link labels, based on features that include descriptions of associated temporal signal words. USFD2 identified temporal expressions successfully, and correctly classified their type in 90% of cases. Determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge."
derczynski-gaizauskas-2010-analysing,Analysing Temporally Annotated Corpora with {CAV}a{T},2010,9,16,1,1,642,leon derczynski,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT."
W08-1805,A Data Driven Approach to Query Expansion in Question Answering,2008,15,13,1,1,642,leon derczynski,Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,0,"Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions.n n In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method.n n Data driven extension words were found to help in over 70% of difficult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA."
