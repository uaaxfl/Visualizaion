1999.mtsummit-1.81,W98-0508,0,0.105028,"Missing"
1999.mtsummit-1.81,W98-1511,0,0.0226836,"Missing"
1999.mtsummit-1.81,P98-1083,0,0.0472335,"Missing"
1999.mtsummit-1.81,P97-1062,0,0.0216655,"Missing"
1999.mtsummit-1.81,W98-0501,0,0.038142,"Missing"
1999.mtsummit-1.81,P98-2132,0,0.0380987,"Missing"
1999.mtsummit-1.81,P98-2144,0,0.0312878,"Missing"
1999.mtsummit-1.81,P99-1033,0,0.0336544,"Missing"
1999.mtsummit-1.81,W98-1510,0,0.046465,"Missing"
1999.mtsummit-1.81,A97-1011,0,0.101046,"Missing"
1999.mtsummit-1.81,E99-1026,0,0.0164838,"ency grammars have recently received increased attention and interest from computational linguists. While there has always been active research in dependency grammars and related theories since the publication of Tesni`ere’s ´ ements de syntaxe structurale (1959), dependency-based El´ approaches now also seem to become more popular for practical projects and applications such as annotating corpora (Kurohashi and Nagao, 1997; Hajic, 1998) and building parsers (e.g., Kurohashi and Nagao, 1994; Tapanainen and J¨arvinen, 1997; Arnola, 1998; Haruno et al., 1998; Oflazer, 1999; Sekine et al., 1999; Uchimoto et al., 1999). This interest has also manifested itself in a recent workshop on the processing of dependency-based grammars (Kahane and Polgu`ere, 1998). The objective of this paper is to present the results of a rule-based implementation of a deterministic parsing algorithm for Japanese. Although our parser does not perform quite as accurately as recent statistical parsers or Kurohashi and Nagao’s knowledge-based parser (see section 7), we believe that our experiment provides some interesting insights. On the one hand, our results offer support for some observations made by Sekine et al. (1999) regarding"
1999.mtsummit-1.81,C98-1080,0,\N,Missing
1999.mtsummit-1.81,C98-2139,0,\N,Missing
1999.mtsummit-1.81,C98-2128,0,\N,Missing
2009.mtsummit-plenaries.15,W09-1505,1,0.837952,"ex to child nodes of ‘b’ in bytes root node value size of index to child nodes of root in bytes index key for ‘a’ coming from root relative offset of node ‘a’ (13 − 8 = 5) index key for ‘b’ coming from root relative offset of node ‘b’ (13 − 2 = 11) (c) Trie representation in a contiguous byte array. In practice, each field may vary in length. Figure 3: A count table (a) stored in a trie structure (b) and the trie’s sequential representation in a file (c). As the size of the count table increases, the trie-based storage becomes more efficient, provided that the keys have common prefixes ( from Germann et al. (2009)). on desktop computers: first, to provide a sandbox, a safe testbed for software development that allows a machine to crash without crashing the actual computer or interfering with the outside world. And second, to run two operating systems concurrently on a single computer. Since the physical host computer and the guest running as a virtual machine can communicate via networking protocols, it is possible, for example, to emulate a Linux server on a PC running Windows. Virtualization technology thus allows deployment of software developed on one OS on another without the need to port software"
2009.mtsummit-plenaries.15,W05-0822,1,0.737099,", we first briefly describe the Portage machine translation system developed at the National Research Council Canada. Section 3 introduces the concept of virtualization. In Section 4 we discuss the design and use of PortageLive, an instantiation of Portage as a virtual machine. We tested PortageLive in a number of scenarios, from small, compact machines that could run on a laptop to parallelizations on computer networks composed of workstations that are typical of office environments. The results of these experiments are reported in Section 5. Section 6 concludes the paper. 2 Portage Portage (Sadat et al., 2005) is a state-of-the-art system for phrase-based statistical machine translation (Koehn et al., 2003). The advantage of statistical approaches to machine translation is that they require much less human labor than language-specific systems carefully designed and constructed by human experts in particular languages or language pairs. Instead of human expertise, the system relies on existing collections of translations to learn how to translate. Under active development since 2004, Portage has been employed to translate between a wide variety of language pairs, involving such languages as Arabic,"
2009.mtsummit-plenaries.15,N03-1017,0,\N,Missing
2014.amta-workshop.3,W14-0313,0,0.0820453,"of the small size of the test set, this is hardly surprising. In general, we should expect the benefit of adding post-edited data immediately to the knowledge base of the SMT system to vary widely depending on the repetitiveness of the source text, and on how well the translation domain is already covered by the background corpus. 4 Related Work User-adaptive MT has received considerable research interest in recent years. Due to space limitations, we can only briefly mention a few closely related efforts here. A survey of recent work can be found, for example, in the recent journal article by Bertoldi et al. (2014b). Ortiz-Mart´ınez et al. (2010), Bertoldi et al. (2014b), and Denkowski et al. (2014) all present systems that can be updated incrementally. Ortiz-Mart´ınez et al. (2010) present a system that can trained be incrementally from scratch with translations that are produced in an interactive computer-aided translation scenario. The work by Bertoldi et al. (2014b) relies on cache-based models that keep track of how recently phrase pairs in the translation model and n-grams in the language models have been used in the translation pipeline and give higher scores to recently used items. They also au"
2014.amta-workshop.3,2012.iwslt-papers.12,0,0.0142571,"ey also augment the phrase table with entries extracted from post-edited translations. The work by Denkowski et al. (2014) is the closest to the work presented in this paper.6 Working with the cdec decoder (Dyer et al., 2010), they also use suffix arrays to construct phrase table entries on demand. In addition, they provide mechanisms to update the language model and re-tune the system parameters. Focusing on dynamic adjustment of system parameters (feature function values and combination weights), Mart´ınez-G´omez et al. (2012) investigate various online learning algorithms for this purpose. Blain et al. (2012) and Bertoldi et al. (2014a) describe 6 Incidentally, Denkowski (personal communication) is using the implementation presented here to port the work of Denkowski et al. (2014) to the Moses framework. 28 online word alignment algorithms that can produce the word alignments necessary for phrase extraction. 5 Conclusions I have presented a new phrase table for the Moses system that computes phrase table entries on the fly. It outperforms existing phrase table implementations in Moses in terms of speed, without sacrificing translation quality. This is accomplished by a new way of computing phrase-"
2014.amta-workshop.3,P05-1032,0,0.166059,"22, 2014 This paper presents dynamic phrase tables as an alternative, implemented within the open-source statistical machine translation (SMT) system Moses (Koehn et al., 2007).1 Rather than simply looking up pre-computed entries from a database, they construct their entries on the fly by sampling word-aligned parallel data. The underlying corpus can be amended dynamically with low latency, for example by feeding post-edited output back to the translation server. New additions to the corpus can be exploited for future translations immediately. While the underlying mechanisms are not new (cf. Callison-Burch et al., 2005; Lopez, 2007), the work reported here eliminates two major concerns about the use of bitext sampling for phrase table entry construction on demand: translation speed and translation quality. The experimental evaluation shows that in terms of speed, the sampling phrase table clearly outperforms current implementations of the work by Zens and Ney (2007). It comes close to the translation speed achievable with the hashbased compact phrase table implementation of Junczys-Dowmunt (2012). It should be noted that if translation speed is a serious concern, it is easy to pre-compute and store or cache"
2014.amta-workshop.3,P05-1033,0,0.109495,"urce phrase occurrences and their corresponding translations from a pre-indexed bitext. For indexing, they use suffix arrays (Manber and Myers, 1990). A suffix array is an array of all token positions in a given linear sequence of tokens (e.g., a text or a DNA sequence), sorted in lexicographic order of the sub-sequence of tokens starting at the respective position. The use of suffix-array-based bitext sampling in the context of MT has been explored at length by Lopez (2007) as well as Schwartz and Callison-Burch (2010), especially with respect to Hierarchical Phrasebased Translation (HPBSMT; Chiang, 2005, 2007). 22 A great advantage of the suffix-array-based approach is that it is relatively cheap and easy to augment the underlying corpus. To add a pair of sentences to the parallel corpus, all we need to do is to construct a suffix array for the added material (O(n log n), where n is the number of tokens in the added material), and then merge-sort the original suffix array (of length m) with the new suffix array (O(n + m)). While corpus sampling is common practice in other branches of MT research (especially HPBSMT, due to the prohibitive size of pre-computed, general-purpose, widecoverage ru"
2014.amta-workshop.3,P11-2031,0,0.049839,"Missing"
2014.amta-workshop.3,E14-1042,0,0.30117,"xpect the benefit of adding post-edited data immediately to the knowledge base of the SMT system to vary widely depending on the repetitiveness of the source text, and on how well the translation domain is already covered by the background corpus. 4 Related Work User-adaptive MT has received considerable research interest in recent years. Due to space limitations, we can only briefly mention a few closely related efforts here. A survey of recent work can be found, for example, in the recent journal article by Bertoldi et al. (2014b). Ortiz-Mart´ınez et al. (2010), Bertoldi et al. (2014b), and Denkowski et al. (2014) all present systems that can be updated incrementally. Ortiz-Mart´ınez et al. (2010) present a system that can trained be incrementally from scratch with translations that are produced in an interactive computer-aided translation scenario. The work by Bertoldi et al. (2014b) relies on cache-based models that keep track of how recently phrase pairs in the translation model and n-grams in the language models have been used in the translation pipeline and give higher scores to recently used items. They also augment the phrase table with entries extracted from post-edited translations. The work b"
2014.amta-workshop.3,P10-4002,0,0.0135553,"n trained be incrementally from scratch with translations that are produced in an interactive computer-aided translation scenario. The work by Bertoldi et al. (2014b) relies on cache-based models that keep track of how recently phrase pairs in the translation model and n-grams in the language models have been used in the translation pipeline and give higher scores to recently used items. They also augment the phrase table with entries extracted from post-edited translations. The work by Denkowski et al. (2014) is the closest to the work presented in this paper.6 Working with the cdec decoder (Dyer et al., 2010), they also use suffix arrays to construct phrase table entries on demand. In addition, they provide mechanisms to update the language model and re-tune the system parameters. Focusing on dynamic adjustment of system parameters (feature function values and combination weights), Mart´ınez-G´omez et al. (2012) investigate various online learning algorithms for this purpose. Blain et al. (2012) and Bertoldi et al. (2014a) describe 6 Incidentally, Denkowski (personal communication) is using the implementation presented here to port the work of Denkowski et al. (2014) to the Moses framework. 28 onl"
2014.amta-workshop.3,W06-1607,0,0.189545,"ment between s and t. The four feature functions are as follows. • the conditional phrase-level ‘forward‘ translation probability p (t |s) • the conditional phrase-level ‘backward‘ translation probability p (s |t) • the joint ‘lexical forward‘ probability of all target words, given the source phrase Q|t| (and possibly a word alignment between the two phrases): k=0 p (tk |s, As,t ). • the corresponding joint ‘lexical backward‘ probability Q|s| k=0 p (sk |t, As,t ). In order to achieve better translations, phrase-level probabilities are typically smoothed by Good-Turing or Kneser-Ney smoothing (Foster et al., 2006). The underlying counts and smoothing parameters are computed based on a complete list of phrase pairs extracted from the word-aligned parallel training corpus. 2.2 Bitext sampling Except for toy examples, pre-computed phrase tables are typically very large, with the exact size of course depending on the maximum phrase length chosen and the size of the underlying corpus. The phrase table used for the timing experiments reported in Section 3.2, for example, consists of over 90 million distinct pairs of phrases of up to 7 words extracted from a moderately sized parallel corpus of fewer than 2 mi"
2014.amta-workshop.3,W09-1505,1,0.845832,"f up to 7 words extracted from a moderately sized parallel corpus of fewer than 2 million parallel sentences of German-English text. The large sizes of phrase tables make it impractical to fully load them into memory at translation time. Fully loaded into memory in the Moses decoder, the phrase table of the aforementioned system requires well over 100 GB of RAM and takes far beyond an hour to load. Therefore, phrase tables are usually converted to a disk-based representation, with phrase table entries retrieved from disk when needed. There are several such representations (Zens and Ney, 2007; Germann et al., 2009; Junczys-Dowmunt, 2012), two of which (Zens and Ney, 2007; Junczys-Dowmunt, 2012) have been integrated into the Moses system. As an alternative to pre-computed phrase tables, Callison-Burch et al. (2005) suggested to compute phrase table entries on the fly at runtime by extracting and scoring a sample of source phrase occurrences and their corresponding translations from a pre-indexed bitext. For indexing, they use suffix arrays (Manber and Myers, 1990). A suffix array is an array of all token positions in a given linear sequence of tokens (e.g., a text or a DNA sequence), sorted in lexicogra"
2014.amta-workshop.3,D07-1104,0,0.109435,"dynamic phrase tables as an alternative, implemented within the open-source statistical machine translation (SMT) system Moses (Koehn et al., 2007).1 Rather than simply looking up pre-computed entries from a database, they construct their entries on the fly by sampling word-aligned parallel data. The underlying corpus can be amended dynamically with low latency, for example by feeding post-edited output back to the translation server. New additions to the corpus can be exploited for future translations immediately. While the underlying mechanisms are not new (cf. Callison-Burch et al., 2005; Lopez, 2007), the work reported here eliminates two major concerns about the use of bitext sampling for phrase table entry construction on demand: translation speed and translation quality. The experimental evaluation shows that in terms of speed, the sampling phrase table clearly outperforms current implementations of the work by Zens and Ney (2007). It comes close to the translation speed achievable with the hashbased compact phrase table implementation of Junczys-Dowmunt (2012). It should be noted that if translation speed is a serious concern, it is easy to pre-compute and store or cache phrase table"
2014.amta-workshop.3,P02-1040,0,0.0886107,"Missing"
2014.amta-workshop.3,N07-1062,0,0.169972,"by sampling an indexed bitext. While this approach has been used for years in hierarchical phrase-based translation, the PBSMT community has been slow to adopt this paradigm, due to concerns that this would be slow and lead to lower translation quality. The experiments conducted in the course of this work provide evidence to the contrary: without loss in translation quality, the sampling phrase table ranks second out of four in terms of speed, being slightly slower than hash table look-up (Junczys-Dowmunt, 2012) and considerably faster than current implementations of the approach suggested by Zens and Ney (2007). In addition, the underlying parallel corpus can be updated in real time, so that professionally produced translations can be used to improve the quality of the machine translation engine immediately. 1 Introduction In recent years, there has been an increasing interest in integrating machine translation (MT) into the professional translator’s work flow. With translation memories (TM) firmly established as a productivity tool in the translation industry, it is a conceptually obvious extension of this paradigm to include machine translation engines as virtual TMs in the set-up. One major obsta"
2014.amta-workshop.3,N03-1017,0,\N,Missing
2014.amta-workshop.3,J07-2003,0,\N,Missing
2020.iwslt-1.14,W19-5301,0,0.0264101,"Missing"
2020.iwslt-1.14,W19-5304,1,0.833337,"during decoding time to translate a multi-domain test set, experimented at the sentence, cluster (subdocument) and document levels. 1 We mistakenly used an outdated dataset which is larger but noisier. The dataset was extracted from crawled texts with encoding issues and inconsistent handling of Japanese characters “プ” and “で”. Baseline with Rule-Based Cleaning Preprocessing We first tokenise our data at word-level, which is commonly done for the Japanese and Chinese (Barrault et al., 2019; Nakazawa et al., 2019). While it is unclear whether word-level or character-level models are superior (Bawden et al., 2019), wordlevel segmentation could resolve ambiguity and dramatically reduce sequence length. The tools we use are KyTea (Neubig et al., 2011) for Japanese and Jieba fast2 for Chinese. 2.2 Rule-based cleaning We then apply a series of rule-based cleaning operations on both existing and crawled data to create baseline models. These steps are mostly inspired by submissions to the corpus filtering task at WMT 2018 (Koehn et al., 2018). The task shows that effective corpus filtering brings substantial gain in translation performance. Language identification: One way of parallel corpus filtering is to"
2020.iwslt-1.14,P05-1032,0,0.0385481,"In contrast, Poncelas et al. (2018) synthesise data similar to the whole test set. They leverage a feature decay algorithm to select monolingual data in the target language that are similar to test sentences translated by a generic source-to-target model. Then, the selected sentences are back-translated to source language (Sennrich et al., 2016), forming synthetic parallel sentences for fine-tuning. In our work, we adopt a pure phrase-coverage approach, which is compatible for both sentence and document level retrieval. As originally suggested for phrase-pair extraction in phrase-based SMT by Callison-Burch et al. (2005) and Zhang and Vogel (2005), we index the source side of the training data via a suffix array (Manber and Myers, 1990) for very fast identification of sentence pairs that contain a given phrase. Then we simply use the test data as a query to retrieval sentences based on n-gram overlapping. Figure 1 shows how efficiently our sentence retrieval method scales up. ·10−2 4 3 2 1 0 0 1,000 2,000 3,000 4,000 5,000 Number of sentences as query Figure 1: Average time to query one sentence against number of sentences in the query. We set a threshold T , such that n-grams which occur more than T times in"
2020.iwslt-1.14,chu-etal-2012-chinese,0,0.0291258,"rovement in BLEU after applying rule-based 3 https://github.com/berniey/hanziconv https://github.com/didi/iwslt2020 open domain translation/tree/master/eval 4 123 3 Chinese and Japanese Mapping In ancient times, Japanese borrowed (at that time, traditional) Chinese characters (Hanzi) to use as a written form (Kanji). After a long time of coand separate evolution (e.g. Chinese simplification), the relationship between Hanzi and Kanji is complicated. Some Hanzi and Kanji stay unchanged, some develop different meanings, and some develop different written forms. A detailed description is given by Chu et al. (2012). More importantly, they released a Kanji to traditional and simplified Hanzi mapping table. With each Kanji being a key, there can be zero, one or many corresponding traditional and simplified Hanzi. In total, there are mapping entries for around 5700 Kanji to simplified Hanzi. Chu et al. (2013) use this character mapping to enhance word segmentation in statistical machine translation (SMT). Recently, Song et al. (2020) map characters in a Chinese corpus to Japanese, making it a pseudo-Japanese corpus for the purpose of pre-training Japanese↔English NMT. In our work, we take a step forward to"
2020.iwslt-1.14,C18-1111,0,0.0160776,"opy filtering” in Table 2, where we observe that translation performance improves as the size of training data drops. Thus we further experiment with 20, 10 and 5 million data on Transformer-Big. Results are displayed in the same table under category “(5) deeper models”. In addition, we run ensemble decoding, combining the models trained on 10 million and 5 million sentences, and report results in the same table in category “(6) ensembles”. 5 Ad-hoc Domain Adaptation NMT is sensitive to domain mismatch (Koehn and Knowles, 2017), and there are numerous techniques for domain adaptation for NMT (Chu and Wang, 2018). Some model and training techniques require prior knowledge of the domain and cannot be easily applied. Nonetheless, one method that can be adopted during test sentence translation is retrieving samples that are similar to the input from the available training data, and fine-tuning a trained generic model on these samples. Such ad-hoc domain adaptation can be done at sentence level (Farajian et al., 2017; Li et al., 2018) or document level (Poncelas et al., 2018). 5.1 Similar sentence retrieval A crucial factor for domain adaptation to work is to accurately retrieval representative sentences"
2020.iwslt-1.14,W17-4715,0,0.0244452,"se both share a set of common characters. Hence, we decide to relax this rule by keeping all sentences (pairs) which are identified as either Chinese or Japanese using langid.py (Lui and Baldwin, 2012). This inevitably leaves 2 https://github.com/deepcs233/jieba fast, a faster implementation of Jieba 122 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 122–129 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 some Chinese on the Japanese side and vice versa. It might have a beneficial copying effect (Currey et al., 2017), especially given the vocabulary overlap between the two languages. Length ratio: We use the provided high-quality existing data to estimate the average Japanese and Chinese sentence lengths at character-level. We find the length ratio of Japanese to Chinese is about 1.4 to 1. We remove sentence pairs which have a length ratio outside the 3 standard deviations from this mean. This a lenient choice in order to keep short translations. This is applied to both existing and crawled data. Sentence length: We remove sentence pairs with more than 70 tokens on the Chinese side or more than 100 tokens"
2020.iwslt-1.14,W17-4713,0,0.0205739,"ame table in category “(6) ensembles”. 5 Ad-hoc Domain Adaptation NMT is sensitive to domain mismatch (Koehn and Knowles, 2017), and there are numerous techniques for domain adaptation for NMT (Chu and Wang, 2018). Some model and training techniques require prior knowledge of the domain and cannot be easily applied. Nonetheless, one method that can be adopted during test sentence translation is retrieving samples that are similar to the input from the available training data, and fine-tuning a trained generic model on these samples. Such ad-hoc domain adaptation can be done at sentence level (Farajian et al., 2017; Li et al., 2018) or document level (Poncelas et al., 2018). 5.1 Similar sentence retrieval A crucial factor for domain adaptation to work is to accurately retrieval representative sentences of 125 Average time per sentence (sec) test sentences. Farajian et al. (2017) store training data in the Lucene search engine and take the top-scoring outcomes ranked by sentence-level BLEU. Li et al. (2018) use word-based reverse indexing and explore three similarity measures: Levenshtein Distance, cosine similarity between average word embeddings, and cosine similarity between sentence embeddings from N"
2020.iwslt-1.14,P13-2121,0,0.0259047,"Missing"
2020.iwslt-1.14,W18-6478,0,0.0138734,"data. We only map characters on the respective source side and leave the target side of the training data as it is. We then train models on the mapped data for both directions, with results displayed in Table 2 as “(3) mapping”. We observe that aggressive mapping is marginally better than conservative on Ja→Zh and much better on Zh→Ja. Thus, we pick aggressive mapping for our following experiments. 4 Filtering Based on Cross-Entropy Our initial rule-based cleaning shows its effectiveness through improvement in BLEU scores. We further adopt two filtering steps based on crossentropy proposed by Junczys-Dowmunt (2018): 4.1 Dual conditional cross-entropy Dual conditional cross-entropy score is obtained from the absolute difference between crossentropies of two translation models in inverse directions, weighted by the sum of cross-entropies (1) 4.2 Language model cross-entropy difference The previous step ensures the adequacy of sentence pairs, but it does not pick out unnatural sentences. For example, a concatenation of texts from a website’s navigation bar, together with its translation, get a good score by fulfilling adequacy. To alleviate this issue, we apply cross-entropy difference scoring. The score f"
2020.iwslt-1.14,P18-4020,1,0.892097,"Missing"
2020.iwslt-1.14,W17-3204,0,0.0187459,"itecture for both translation directions. We report BLEU scores in category “(4) cross-entropy filtering” in Table 2, where we observe that translation performance improves as the size of training data drops. Thus we further experiment with 20, 10 and 5 million data on Transformer-Big. Results are displayed in the same table under category “(5) deeper models”. In addition, we run ensemble decoding, combining the models trained on 10 million and 5 million sentences, and report results in the same table in category “(6) ensembles”. 5 Ad-hoc Domain Adaptation NMT is sensitive to domain mismatch (Koehn and Knowles, 2017), and there are numerous techniques for domain adaptation for NMT (Chu and Wang, 2018). Some model and training techniques require prior knowledge of the domain and cannot be easily applied. Nonetheless, one method that can be adopted during test sentence translation is retrieving samples that are similar to the input from the available training data, and fine-tuning a trained generic model on these samples. Such ad-hoc domain adaptation can be done at sentence level (Farajian et al., 2017; Li et al., 2018) or document level (Poncelas et al., 2018). 5.1 Similar sentence retrieval A crucial fac"
2020.iwslt-1.14,D18-2012,0,0.0286958,"but we perform de-tokenisation and normalisation of fullwidth numbers and punctuation symbols for our final submission to make the texts natural Chinese or Japanese. Model training For the baseline model, we try out three combinations of data, namely existing only, crawled only and both. For Ja→Zh and Zh→Ja, this results in six models. As a comparison, we also train vanilla models without previously described cleaning steps. All models are Transformer-Base with default configurations (Vaswani et al., 2017). We use Marian (Junczys-Dowmunt et al., 2018) to train our systems, with SentencePiece (Kudo and Richardson, 2018) applied on tokenised data. As stated previously, Chinese and Japanese share some characters, so it is intuitive to use a shared vocabulary between source and target, and to enable threeway weight-tying between source, target and output embeddings (Press and Wolf, 2017). We report character-level BLEU on development set, using the evaluation script provided.4 The baseline results are shown in Table 2 as “(1) vanilla” and “(2) rule-based cleaning”. We see a significant improvement in BLEU after applying rule-based 3 https://github.com/berniey/hanziconv https://github.com/didi/iwslt2020 open dom"
2020.iwslt-1.14,L18-1146,0,0.067743,"Missing"
2020.iwslt-1.14,P12-3005,0,0.0198329,"iltering task at WMT 2018 (Koehn et al., 2018). The task shows that effective corpus filtering brings substantial gain in translation performance. Language identification: One way of parallel corpus filtering is to restrict source sentences to be in the source language, and target sentences to be in the target language. However, distinguishing between Japanese and Chinese, particularly short sentences, is tricky because both share a set of common characters. Hence, we decide to relax this rule by keeping all sentences (pairs) which are identified as either Chinese or Japanese using langid.py (Lui and Baldwin, 2012). This inevitably leaves 2 https://github.com/deepcs233/jieba fast, a faster implementation of Jieba 122 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 122–129 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 some Chinese on the Japanese side and vice versa. It might have a beneficial copying effect (Currey et al., 2017), especially given the vocabulary overlap between the two languages. Length ratio: We use the provided high-quality existing data to estimate the average Japanese and Chinese senten"
2020.iwslt-1.14,P11-2093,0,0.0113134,"mistakenly used an outdated dataset which is larger but noisier. The dataset was extracted from crawled texts with encoding issues and inconsistent handling of Japanese characters “プ” and “で”. Baseline with Rule-Based Cleaning Preprocessing We first tokenise our data at word-level, which is commonly done for the Japanese and Chinese (Barrault et al., 2019; Nakazawa et al., 2019). While it is unclear whether word-level or character-level models are superior (Bawden et al., 2019), wordlevel segmentation could resolve ambiguity and dramatically reduce sequence length. The tools we use are KyTea (Neubig et al., 2011) for Japanese and Jieba fast2 for Chinese. 2.2 Rule-based cleaning We then apply a series of rule-based cleaning operations on both existing and crawled data to create baseline models. These steps are mostly inspired by submissions to the corpus filtering task at WMT 2018 (Koehn et al., 2018). The task shows that effective corpus filtering brings substantial gain in translation performance. Language identification: One way of parallel corpus filtering is to restrict source sentences to be in the source language, and target sentences to be in the target language. However, distinguishing between"
2020.iwslt-1.14,E17-2025,0,0.0221593,"nly and both. For Ja→Zh and Zh→Ja, this results in six models. As a comparison, we also train vanilla models without previously described cleaning steps. All models are Transformer-Base with default configurations (Vaswani et al., 2017). We use Marian (Junczys-Dowmunt et al., 2018) to train our systems, with SentencePiece (Kudo and Richardson, 2018) applied on tokenised data. As stated previously, Chinese and Japanese share some characters, so it is intuitive to use a shared vocabulary between source and target, and to enable threeway weight-tying between source, target and output embeddings (Press and Wolf, 2017). We report character-level BLEU on development set, using the evaluation script provided.4 The baseline results are shown in Table 2 as “(1) vanilla” and “(2) rule-based cleaning”. We see a significant improvement in BLEU after applying rule-based 3 https://github.com/berniey/hanziconv https://github.com/didi/iwslt2020 open domain translation/tree/master/eval 4 123 3 Chinese and Japanese Mapping In ancient times, Japanese borrowed (at that time, traditional) Chinese characters (Hanzi) to use as a written form (Kanji). After a long time of coand separate evolution (e.g. Chinese simplification)"
2020.iwslt-1.14,P16-1009,0,0.0324886,"ings from NMT. Additionally, they suggest an alternative approach, phrase coverage, inspired by phrase-based SMT, when no high-scoring match is found. Sentence-level adaptation is computationally expensive because, for each sentence, a separate model needs to be fine-tuned. In contrast, Poncelas et al. (2018) synthesise data similar to the whole test set. They leverage a feature decay algorithm to select monolingual data in the target language that are similar to test sentences translated by a generic source-to-target model. Then, the selected sentences are back-translated to source language (Sennrich et al., 2016), forming synthetic parallel sentences for fine-tuning. In our work, we adopt a pure phrase-coverage approach, which is compatible for both sentence and document level retrieval. As originally suggested for phrase-pair extraction in phrase-based SMT by Callison-Burch et al. (2005) and Zhang and Vogel (2005), we index the source side of the training data via a suffix array (Manber and Myers, 1990) for very fast identification of sentence pairs that contain a given phrase. Then we simply use the test data as a query to retrieval sentences based on n-gram overlapping. Figure 1 shows how efficient"
2020.iwslt-1.14,2020.acl-srw.37,0,0.013623,"nzi and Kanji is complicated. Some Hanzi and Kanji stay unchanged, some develop different meanings, and some develop different written forms. A detailed description is given by Chu et al. (2012). More importantly, they released a Kanji to traditional and simplified Hanzi mapping table. With each Kanji being a key, there can be zero, one or many corresponding traditional and simplified Hanzi. In total, there are mapping entries for around 5700 Kanji to simplified Hanzi. Chu et al. (2013) use this character mapping to enhance word segmentation in statistical machine translation (SMT). Recently, Song et al. (2020) map characters in a Chinese corpus to Japanese, making it a pseudo-Japanese corpus for the purpose of pre-training Japanese↔English NMT. In our work, we take a step forward to map Chinese and Japanese to each other for Chinese↔Japanese NMT directly. Without mapping as a data processing step, an NMT system needs to learn the mapping between Kanji and Hanzi implicitly. Therefore we hypothesise that mapping them before training a model will: 1. maximise character overlap percentage, reduce vocabulary size and make embeddingtying more effective, and 2. reduce the computation needed to learn to mo"
2020.iwslt-1.14,2005.eamt-1.39,0,0.02213,"8) synthesise data similar to the whole test set. They leverage a feature decay algorithm to select monolingual data in the target language that are similar to test sentences translated by a generic source-to-target model. Then, the selected sentences are back-translated to source language (Sennrich et al., 2016), forming synthetic parallel sentences for fine-tuning. In our work, we adopt a pure phrase-coverage approach, which is compatible for both sentence and document level retrieval. As originally suggested for phrase-pair extraction in phrase-based SMT by Callison-Burch et al. (2005) and Zhang and Vogel (2005), we index the source side of the training data via a suffix array (Manber and Myers, 1990) for very fast identification of sentence pairs that contain a given phrase. Then we simply use the test data as a query to retrieval sentences based on n-gram overlapping. Figure 1 shows how efficiently our sentence retrieval method scales up. ·10−2 4 3 2 1 0 0 1,000 2,000 3,000 4,000 5,000 Number of sentences as query Figure 1: Average time to query one sentence against number of sentences in the query. We set a threshold T , such that n-grams which occur more than T times in training data are disregar"
2020.lrec-1.413,cassidy-etal-2014-alveo,0,0.0760054,"Missing"
2020.lrec-1.413,W03-0810,0,0.137563,"Missing"
2020.lrec-1.413,gavrilidou-etal-2012-meta,1,0.915791,"Missing"
2020.lrec-1.413,hinrichs-krauwer-2014-clarin,0,0.182909,"Missing"
2020.lrec-1.413,P10-4005,0,0.0528445,"Missing"
2020.lrec-1.413,2020.lrec-1.420,1,0.821941,"Missing"
2020.lrec-1.413,L18-1213,1,0.812955,"0b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 addition"
2020.lrec-1.413,2020.iwltp-1.12,1,0.789706,"ercial services, written in disparate programming languages (Java/Spring, .NET, Python) with just a few days work in the first iteration, falling to a few hours once developers became more familiar with the infrastructure and required formats. 14 These requests are received and handled by the LT Service Execution Server (Section 3.1). 3370 The composition of individual services offered by ELG directly or other cloud platforms is not addressed by ELG itself. However, we experiment with workflow composition and platform interoperability in other contexts (Rehm et al., 2020a; Rehm et al., 2020b; Moreno-Schneider et al., 2020a; Moreno-Schneider et al., 2020b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent"
2020.lrec-1.413,2020.lrec-1.284,1,0.363908,"Missing"
2020.lrec-1.413,piperidis-etal-2014-meta,1,0.89681,"metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will increase the numbers in Table"
2020.lrec-1.413,L18-1205,1,0.945027,"tities of interest to users (Section 3.6), appropriately indexed and described so that they can easily search, find and select the resources that meet their requirements and deploy them, as well as visualise the LT domain activities, stakeholders and resources with specific criteria (e. g., service type, language, etc.). All entities are described in compliance with the ELG-SHARE metadata schema (Labropoulou et al., 2019; Labropoulou et al., 2020).7 The schema builds upon, consolidates and updates previous activities, especially the META-SHARE schema and its profiles (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account the ELG user requirements (Melnika et al., 2019a), recent developments in the (meta)data domain (e. g., FAIR8 , data and software citation recommendations9 , Open Science movement, etc.), and the need for establishing a common pool of resources through exchange mechanisms with collaborating projects and initiatives (Rehm et al., 2020c), cf. Section 3.6. The schema caters for the description of the ELG core entities (Figure 2), i. e., Language Technologies (tools/services), including functional services and nonfunctional ones (e. g., downloadable"
2020.lrec-1.413,piperidis-2012-meta,1,0.90149,"y into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will incr"
2020.lrec-1.413,L18-1519,1,0.634981,"any are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, domains and sectors (Vasiljevs et al., 2019) –, significantly holding back its impact. In addition, many European languages are severely under-resourced and, thus, in danger of digital language exinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014; Rehm et al., 2016a), which is why there is an enormous need for a European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). The project European Language Grid (ELG; 2019-2021) addresses this fragmentation by establishing the ELG as the primary platform and marketplace for the European LT community, both industry and research.1 The ELG is developed to be a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and 1 https://www.european-language-grid.eu resources. Once fully operational, it will enable the commercial and non-commercial E"
2020.lrec-1.413,L16-1251,1,0.856739,"Missing"
2020.lrec-1.413,P14-5010,0,\N,Missing
2020.wmt-1.17,2020.ngt-1.4,1,0.775989,"our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized back to float32. Despite the extra quantization and de-quantization involved, the increased speed at which 8-bit integer multiplication is performed more than compensates for it. Bogoychev et al. (2020) observe that smaller student presets lose BLEU when quantized. In order to counteract that, we perform model fine tuning following the work of Aji and Heafield (2020): We replace the GEMM routine implementation with a custom one that is damaged, according to the quan7 For example, for handling HTML tags in translated texts. 8 https://github.com/kpu/intgemm 194 4 Results In Table 4, we show the performance of the three models in terms of BLEU scores for the WMT 2020 cs↔en test sets and translation speed. Teacher models ran on an Nvidia GeForce GTX 1080 with a batch size of 16. Student models were run on a single CPU core on an Intel Intel(R) Xeon(R) CPU E5-2680 0 @ 2.70GHz with a batch size of 64. It should be noted that we made no effort to optimize the te"
2020.wmt-1.17,2016.amta-researchers.10,0,0.0683014,"Missing"
2020.wmt-1.17,D17-1300,0,0.0148403,"nd perform several thousand minibatch updates of the model. The damaged GEMM implementation can only produce 255 unique float values (corresponding to the 8-bit integer dequantization range) and the model quickly learns to work with those values and recovers some of the BLEU lost compared to untuned quantized model. Quantized Models Floating point operations are computationally more expensive than integer operations. However, as Han et al. (2016) have shown, neural network inference does not require the high precision of representation and computation that 32-bit floating point numbers offer. Devlin (2017) suggests a simple quantization mechanism for quantizing parameters to 16-bit integer precision and notes that support for off-the-shelf 8-bit integer matrix multiplication is lacking. Bogoychev et al. (2020) fill that gap and provide an 8-bit quantization and fine tuning scheme for Marian based on the intgemm library;8 we used that scheme for our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized b"
2020.wmt-1.17,N13-1073,0,0.196044,"Missing"
2020.wmt-1.17,2020.ngt-1.1,1,0.698048,"th the teacher model to generate the training set D0 . 3. Train a small student model on D0 . Introduction The conventional set-up of the WMT Shared Tasks on News Translation emphasizes translation quality (however measured) above all else. Constraints on the data that may be used for training in the ‘constrained’ track establish a level playing field in terms of the information available to the translation model and its training process, but there are no constraints on the computational power and effort spent to achieve the results. In contrast, the WNGT Shared Task on Efficient Translation (Heafield et al., 2020) encourages participants to submit systems that are both accurate and efficient during inference (i.e., translation). So far, there has been little interaction between the two tasks. With our joint submission between the University of Edinburgh (UEDIN) and Charles University, Prague (CUNI), we strive to bridge this gap. We submitted small, efficient systems that distilled knowledge from a more powerful teacher model via sequence-level knowledge distillation (Kim and 1 Bogoychev et al. (2020) report translation speeds of up to 3135 source words per second on a single CPU thread; the actual thro"
2020.wmt-1.17,E17-2068,0,0.0666085,"Missing"
2020.wmt-1.17,P18-4020,1,0.873993,"Missing"
2020.wmt-1.17,W18-2716,1,0.833283,"sion to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student mode"
2020.wmt-1.17,D16-1139,0,0.0795642,"beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student models were trained by UEDIN with the Marian NMT toolkit 3 3.1 Student Model Architectures The student models use the"
2020.wmt-1.17,N03-1017,0,0.0630476,"Missing"
2020.wmt-1.17,D18-2012,0,0.0346137,"hypotheses selected over the respective beam ranks. For the monolingual data, for which we obviously have no human reference translations, we simply chose the highest-scoring translation. Sentence pairs where the translation contained the same whitespace-separated sequence of words three or more times in a row, or the same sequence of one or more characters in five or more subsequent repetitions (which can happen when the recursive decoder goes into a loop) were discarded. We subsequently tokenized the synthetic teaching data (source and translations by the teacher model) with SentencePiece (Kudo and Richardson, 2018), 193 using a joint vocabulary for both languages with a size of 32,000 tokens. This vocabulary is also used by the final systems. The tokenized training data was word-aligned in both translation directions with FastAlign (Dyer et al., 2013). Directional word alignments were then symmetrized with the growdiag-final-and symmetrization algorithm (Koehn et al., 2003). These word alignments serve mainly three purposes: (a) to guide the attention mechanism during training of the student models (Liu et al., 2016) with guided alignment (Chen et al., 2016); (b) to produce shortlists of translation can"
2020.wmt-1.17,C16-1291,0,0.0504752,"Missing"
2020.wmt-1.17,W18-6424,1,0.836123,"t. value size checkpoints avg. back-translation beam search alpha max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary subm"
2020.wmt-1.17,2020.wmt-1.28,1,0.73264,". Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data"
2020.wmt-1.17,W18-6319,0,0.0332871,"Missing"
2020.wmt-1.17,P16-1009,0,0.0423625,"max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”;"
2020.wmt-1.17,W18-1819,0,0.0673101,"Missing"
2020.wmt-1.18,W18-6415,0,0.0305779,"Missing"
2020.wmt-1.18,W18-2716,0,0.0313098,"Missing"
2020.wmt-1.18,W18-6319,0,0.0217554,"Missing"
2020.wmt-1.18,W19-5304,1,0.828132,"the raw translation output by 1.3 BLEU points with a simple post-processing step that simply adjusts quotations marks to German spelling conventions. For English-to-German, we submitted an ensemble of the 8 round-2 models with the highest BLEU score with respect to the validation set (WMT19). Here, too, training on back-translated news data did not lead to an improvement over the best Round-2 model, so that we did not use any round3 model for the final submission. We were able to boost performance by increasing the batch size during training, which is in line with our findings from last year (Bawden et al., 2019), but the effect was much smaller this year. This may be due to the fact that the initial model (#10, English→German) was already trained with a fairly large batch size. Round 2: Big transformers for back-translation We then trained big transformer models for backtranslation of monolingual news data, using the “top” (see above for our blunder on German-toEnglish data selection) 25M candidates. 2.4 Back-translation of news data We used single models to translate all of the news data for German and English, adding a bit of noise in the translation by adding Gumbel noise to the output layer, thus"
2020.wmt-1.18,W18-6478,0,0.0484086,"Missing"
2021.eacl-demos.26,hinrichs-krauwer-2014-clarin,0,0.0190659,"arts in February 2021 with a duration of 912 months. In the first call, 110 proposals were accepted for evaluation with applicants from 29 countries. We received more proposals from SMEs (62) than re search organisations (48). While 79 proposals fo 2.10 Legal Entity 3 Related Work ELG builds upon previous work of the ELG con sortium and the wider European LT community, es pecially METANET/META and ELRC. In addition, we have collected more than 30 plat forms, projects or initiatives that can be considered relevant for ELG including, among others, UIMA (Ferrucci and Lally, 2003), CLARIN (Hinrichs and Krauwer, 2014), DKPro (Gurevych et al., 2007); Rehm et al. (2020a) provide an exhaustive com parison. They share at least one of the following goals with ELG, i. e., they provide: 1) a collec tion of LT/NLP tools or data sets; 2) a platform, which harvests metadata records from distributed sources, 3) a platform for the sharing of tools or data sets. While related projects do exist, the ap proach of ELG is unique. The platform that most closely resembles ELG is the National Platform for LT, operated by the Ministry of Electronics and In formation Technology in India.16 Several global technology enterpri"
2021.eacl-demos.26,2020.iwltp-1.12,1,0.888601,"Missing"
2021.eacl-demos.26,piperidis-2012-meta,1,0.736597,"four TTS and two text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) Gen"
2021.eacl-demos.26,L18-1205,1,0.930475,"ps://www.postgresql.org 8 https://www.keycloak.org 9 https://prometheus.io 10 https://helm.sh 5 2.3 Catalogue The metadata records stored in the catalogue en able access to services and data resources. They are described using the ELG metadata schema (Labropoulou et al., 2020) and can be browsed and explored. The catalogue also includes a registry of stakeholders who develop LT services or products, and relevant projects, thus providing an overview of the whole European LT landscape. The ELG metadata schema builds upon, consolidates and updates the METASHARE schema (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account ELG’s require ments, recent developments in the metadata do main (e. g., FAIR11 ), and the need for creating a common pool of resources through exchange mechanisms with collaborating initiatives. The metadata schema caters for the descrip tion of the ELG core entities, i. e., Language Technologies (tools/services), including functional services and nonfunctional ones, and Data Lan guage Resources, comprising data sets (corpora), language descriptions (i. e., models) and lexical/ conceptual resources (e. g., gazetteers, ontologies, etc.). I"
2021.eacl-demos.26,piperidis-etal-2014-meta,1,0.869104,"text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) General LT information seeke"
2021.eacl-demos.26,L16-1251,1,0.869271,"Missing"
2021.eacl-demos.26,L18-1519,1,0.849444,"et al., 2019; Rehm et al., 2020d). We describe Release 2 of the European Lan guage Grid (ELG) cloud platform.1 This scal able system is targeted to evolve into the primary 1 https://www.europeanlanguagegrid.eu. We provide a screencast demo video at https://youtu.be/LD6QadkkZiM. platform for LT in Europe. It will provide one umbrella platform for all LTs developed by the European LT landscape, including research and industry, addressing a gap that has been repeat edly raised by the European LT community for many years (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). ELG is meant to be a virtual home and marketplace for all products, services and organisations active in the LT space in Europe (Rehm et al., 2020a). The platform can be used by all stakeholders to show case, share and distribute their products, services, tools and resources. At the end of the EU project ELG (20192022), which will establish a legal en tity in early 2022, the platform will provide access to approx. 1300 commercial and noncommercial tools and services for all European languages, as well as thousands of language resources (LRs). ELG will enable t"
2021.eacl-demos.26,2020.lrec-1.422,0,0.0498058,"Missing"
2021.iwslt-1.4,2020.iwslt-1.27,0,0.0900899,"ova, 2016; Ma et al., 2019) where the system appends the output to a growing hypothesis as new inputs are available, and re-translation (Niehues et al., 2016, 2018; Arivazhagan et al., 2020a,b), where, as the name suggests, the system re-translates the whole prefix on every update to a completely new output. Retranslation approach has the advantage that we can use an unmodified, general purpose, optimised MT engine with beam-search, but we have to address the problem of flicker. That is to say, the translation of a prefix may be changed by the translation of an extended prefix. Recent work by Arivazhagan et al. (2020a) has shown that, if measures are taken to mitigate flicker, then re-translation produces results comparable to streaming approach. Since the shared task does not permit any revision of a committed hypothesis (i.e. flicker is not allowed) we focus on adapting the re-translation approach for our submission without introducing any flicker into a growing hypothesis. We describe our submission to the IWSLT 2021 shared task1 on simultaneous text-to-text English-German translation. Our system is based on the re-translation approach where the agent re-translates the whole source prefix each time it"
2021.iwslt-1.4,W18-6319,0,0.0221494,"l, so we did not pursue it further. The validation data is also pre-processed similarly to the training set. Note that this preprocessed validation set is used at training for early stopping and not for reporting the validation scores in the Table 2. Corpus Europarl Rapid News Commentary OpenSubtitle TED corpus MuST-C.v2 Sentence pairs 1.79 M 1.45 M 0.35 M 22.51 M 206 K 248 K Table 1: Corpora used in training the systems 4 Result and Analysis We evaluate the model’s performance on the full sentence translation before doing actual simultaneous translation. For this evaluation we use SacreBLEU (Post, 2018) on the MuST-C.v2 and TED 2018 test sets. The results on full sentence is shown in the Table 2. We see there is a significant improve48 34 32 30 30 28 28 26 26 BLEU BLEU 32 24 24 22 22 lm+mask mask lm baseline 20 18 2 4 6 8 AL 10 12 14 lm+mask mask lm baseline 20 18 2 16 4 6 8 AL 10 12 14 16 (b) Beam size = 12, Normalization = 0.6 (a) Beam size = 12, Normalization = 1.0 Figure 3: BLEU vs AL plots for English-German with different beam sizes and length normalization. 34 lm+mask mask lm baseline 32 30 28 28 26 26 BLEU BLEU 30 lm+mask mask lm baseline 32 24 24 22 22 20 20 18 18 2 4 6 8 DAL 10 12"
2021.iwslt-1.4,N19-1202,0,0.0923389,"Missing"
2021.iwslt-1.4,W11-2123,0,0.0556675,"ion is more stable. COMMON from MuST-C.v2. As the there is a significant overlap between MuST-C.v2 and tst20{14,15,18}, we remove the overlaps from the MuST-C.v2 training data before training. For training, we use the Marian toolkit (JunczysDowmunt et al., 2018) with the ‘base’ transformer architecture (Vaswani et al., 2017). First, we train a model using the aforementioned pre-processed training data and then fine-tune the model using MuST-C.v2 training data which is more of a domain specific data for simultaneous translation task. To train the language model for stabilisation, we use KenLM (Heafield, 2011) to train a 6-gram language model on the source-side training data. We have shown the number of sentences in each corpus in Table 1. For preprocessing we rely only on SentencePiece tokenization (Kudo and Richardson, 2018); no other preprocessing tools are applied. We use a shared vocabulary size of 32k. Standard NMT models perform well when translation is done on a full sentence but as our approach is based on retranslation, we use training data that is a 1:1 mix of full sentences and prefix pairs (Niehues et al., 2018; Arivazhagan et al., 2020a). This ensures that our model can translate both"
2021.iwslt-1.4,2020.amta-research.12,1,0.752346,"ck of the language model (LM) score of the previous token and compares it with the score of the current token. If the LM score is higher than the previous token, it keeps reading more tokens and does a re-translation only when this condition is not met. Here the LM score is the log probability of the current token given the context. Though LM score doesn’t guarantee to find meaningful unit every time but this simple approach shows it is better than the baseline approach in terms of BLEU score. Our second method of stabilising the retranslation approach is based on the idea of dynamic masking (Yao and Haddow, 2020). The dynamic mask approach finds the stable part of the target prefix by comparing the translation of the current prefix, with the translation of an extension of the current prefix. The longest common prefix (LCP) of the two translations is taken as the stable part. Figure 1 shows how dynamic masking works in general. Yao and Haddow (2020) showed that using dynamic mask could give a better flickerlatency trade-off than using a fixed mask, without affecting the translation quality of full sentences. For our IWSLT submission, we generate the extended prefixes for dynamic mask simply by appendin"
2021.iwslt-1.4,P18-4020,1,0.869837,"Missing"
2021.iwslt-1.4,2020.emnlp-main.178,0,0.0386263,"input is consumed, the agent keeps performing WRITE operations until it reaches the end of the translated sentence. The WRITE operation involves re-translating the prefix S and finding the next output word w from output prefix T . If the output prefix T has a length longer than the committed hypothesis H, it picks the (i + 1)th word of T , else sends READ signal to the agent, i being the length of the current hypothesis. ing extra READs when inside an MU. An MU is a chunk of words that has a definite translation and can be translated independently without having to wait for more input words (Zhang et al., 2020). Our first method of detecting MUs relies on the language model (LM) score. The agent keeps track of the language model (LM) score of the previous token and compares it with the score of the current token. If the LM score is higher than the previous token, it keeps reading more tokens and does a re-translation only when this condition is not met. Here the LM score is the log probability of the current token given the context. Though LM score doesn’t guarantee to find meaningful unit every time but this simple approach shows it is better than the baseline approach in terms of BLEU score. Our s"
2021.iwslt-1.4,D18-2012,0,0.0224564,"use the Marian toolkit (JunczysDowmunt et al., 2018) with the ‘base’ transformer architecture (Vaswani et al., 2017). First, we train a model using the aforementioned pre-processed training data and then fine-tune the model using MuST-C.v2 training data which is more of a domain specific data for simultaneous translation task. To train the language model for stabilisation, we use KenLM (Heafield, 2011) to train a 6-gram language model on the source-side training data. We have shown the number of sentences in each corpus in Table 1. For preprocessing we rely only on SentencePiece tokenization (Kudo and Richardson, 2018); no other preprocessing tools are applied. We use a shared vocabulary size of 32k. Standard NMT models perform well when translation is done on a full sentence but as our approach is based on retranslation, we use training data that is a 1:1 mix of full sentences and prefix pairs (Niehues et al., 2018; Arivazhagan et al., 2020a). This ensures that our model can translate both full sentences and prefixes. To create prefix pairs, we first randomly choose a position in the source sentence and then take the proportionate length of the target sentence. Along with that we also add modified prefix p"
2021.iwslt-1.4,L16-1147,0,0.0229467,"ss the flicker caused by re-translation, but could end up gluing together incompatible fragments of the hypothesis. This problem can be worse when the output prefix T flickers too much. To improve translation quality, we employ two approaches which aim at detecting meaningful units (MU) and allow3 Experimental Details We use only the officially allowed IWSLT 2021 data sets. The training data include high quality English-German parallel data from WMT 2020 (Barrault et al., 2020), English-German data from MuST-C.v2 (Di Gangi et al., 2019), the TED corpus (Cettolo et al., 2012) and OpenSubtitle (Lison and Tiedemann, 2016). For development, we use the concatenation of IWSLT test sets from 2014 and 2015. We test on IWSLT 2018 test set and tst47 ASR S =ab translate T =pqr LCP extend S0 = a b c translate T∗ = p q T0 = p q s t Figure 1: Dynamic Masking. The string a b is provided as input to the agent (in a full SLT system it would come from ASR). The MT system then produces translations of the string and its extension, compares them, and outputs the longest common prefix (LCP) prefix extension prefix extension Source Back in New York, Back in New York, UNK Back in New York, I Back in New York, I UNK Translation Zu"
2021.iwslt-1.4,2020.emnlp-demos.19,0,0.19269,"t possible trade-off between these two measures. In the IWSLT 2021 shared task on simultaneous translation, the aim was to build and evaluate simultaneous SLT systems at three different latency regimes (low, medium and high), as measured using the Average Lagging (AL; Ma et al. (2019)). 1 2 Overview of Our Submission We participated in the English→German text-totext simultaneous task. Since we re-translate the incomplete input (know as a prefix) each time it is updated, our system will try to modify the translations produced from earlier prefixes. But as the task is evaluated using SimulEval (Ma et al., 2020) which does not permit the modification of committed output (also known as flickering), we use a simple approach to generate incremental output at each re-translation step. Concretely, we apply a method inspired by the wait-k streaming approach (Ma et al., 2019) in our re-translation system in the following manner. In the task, a simultaneous SLT system is implemented as an agent which must choose between https://iwslt.org/2021/ 46 Proceedings of the 18th International Conference on Spoken Language Translation, pages 46–51 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Com"
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
D17-1156,2012.eamt-1.60,0,0.0287279,"he corresponding columns of the out-of-domain parameter matrices. This can be alternatively seen as learning matrices of parameter differences between in-domain and out-of-domain models with standard dropout, starting from a zero initialization at the beginning of fine-tuning. Therefore, equation 2 becomes ˆ + ∆W · M∆W,i,j ) · hi,j vi,j = (W (3) ˆ is the fixed out-of-domain parameter where W matrix and ∆W is the parameter difference matrix to be learned and M∆W,i,j is a Bayesian dropout mask. 1490 Evaluation 31 We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012). 3.1 Data and Methods Test sets consist of transcripts of TED talks and their translations; small amounts of in-domain training data are also provided. For English-toGerman we use IWSLT 2015 training data, while for English-to-Russian we use IWSLT 2014 training data. For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.e., automatic translations of data available only in target language “back” into the source language.3 . W"
D17-1156,P07-1033,0,0.223933,"Missing"
D17-1156,W04-3250,0,0.204806,"Missing"
D17-1156,2015.iwslt-evaluation.11,0,0.0702048,"rvised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data. This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b). In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English→German and English→Russian. We also investigate the amounts of in-domain training data needed for d"
D17-1156,D15-1123,0,0.0184422,"xt is available for the specific application domain, and it is We investigate techniques where domain adaptation starts from a pre-trained out-domain model, and only needs to process the in-domain corpus. Since we do not need to process the large out-domain corpus during adaptation, this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available. Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015). These techniques are largely orthogonal to 1489 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1489–1494 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ours1 and can be used in combination. In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training. inal out-of-domain model was also trained with dropout. 2 L2-norm regularization is widely used for machine learning and statistical models. For linear models, it corresponds to imposing a diagonal Gaussian prior with zero"
D17-1156,W16-2323,1,\N,Missing
D17-1156,E17-3017,1,\N,Missing
D17-1156,P16-1162,1,\N,Missing
D17-1156,P16-1009,1,\N,Missing
D17-1156,N09-1068,0,\N,Missing
E14-2007,J93-2003,0,0.0320766,"Missing"
E14-2007,2010.eamt-1.18,1,0.868591,"Missing"
E14-2007,2005.mtsummit-papers.19,1,0.793548,"Missing"
E14-2007,D08-1051,1,0.931323,"Missing"
E14-2007,J09-1002,1,\N,Missing
E14-2007,P07-2045,1,\N,Missing
E14-2007,2012.eamt-1.5,1,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
germann-1998-making,W98-1426,0,\N,Missing
germann-1998-making,J93-2004,0,\N,Missing
germann-1998-making,P97-1003,0,\N,Missing
germann-1998-making,H94-1121,0,\N,Missing
germann-1998-making,P95-1034,0,\N,Missing
germann-1998-making,P98-1116,0,\N,Missing
germann-1998-making,C98-1112,0,\N,Missing
germann-1998-making,P97-1062,0,\N,Missing
germann-1998-making,P89-1005,0,\N,Missing
germann-1998-making,P98-2144,0,\N,Missing
germann-1998-making,C98-2139,0,\N,Missing
germann-1998-making,W98-0212,1,\N,Missing
germann-1998-making,C98-2128,0,\N,Missing
germann-1998-making,P98-2132,0,\N,Missing
N03-1010,H94-1028,0,0.188762,"Missing"
N03-1010,1993.eamt-1.1,0,0.157536,"Missing"
N03-1010,P01-1030,1,0.424124,"mulas for the calculation of alignment probabilities according to the various models can be found in Brown et al. (1993). It should be noted here that the calculation EDofFHG IKJthe F alignment probability of an entire alignment ( ) has linear complexity. Well will show below that by re-evaluating only fractions of EFHG LMJF an alignment ( ), we can reduce the evaluation cost to a constant time factor.     3 Decoding 3.1 Decoding Algorithm The task of the decoder is to revert the process just described. In this subsection we recapitulate the greedy hillclimbing algorithm presented in Germann et al. (2001). In contrast to all other decoders mentioned in Sec. 1, this algorithm does not process the input one word at a time to incrementally build up a full translation hypothesis. Instead, it starts out with a complete gloss of the input sentence, aligning each input word with the word that maximizes the inverse (with respect to the noisy channel approach) translation probability N . (Note that for the calculation of the alignment probability, N is used.) The decoder then systematically tries out various types of changes to the alignment: changing the translation of a word, inserting extra words, r"
N03-1010,J99-4005,0,0.132497,"Missing"
N03-1010,W01-1408,0,0.118146,"Missing"
N03-1010,P02-1040,0,0.118911,"rictions on decoder performance, we translated the Chinese test corpus 101 times with restrictions on the maximum swap 6 100 short news texts; 878 text segments; ca. 25K tokens/words. BLEU score 0.145 0.144 0.143 0.142 0.141 0.140 0.139 0.138 10 9 8 7 6 maximum swap distance 5 4 3 2 1 0 0 1 2 3 4 5 6 7 8 9 10 maximum swap segment size  deFigure 5: BLEUscores for the Chinese test set ( coding) in dependence of maximum swap distance and maximum swap segment size. distance (MSD) and the maximum swap segment size (MSSS) ranging from 0 to 10 and evaluated the translations with the BLEU7 metric (Papineni et al., 2002). The results are plotted in Fig. 5. On the one hand, the plot seems to paint a pretty clear picture on the low end: score improvements are comparatively large initially but level off quickly. Furthermore, the slight slope suggests slow but continuous improvements as swap restrictions are eased. For the Arabic test data from the same evaluation, we obtained a similar shape (although with a roughly level plateau). On the other hand, the ‘bumpiness’ of the surface raises the question as to which of these differences are statistically 7 In a nutshell, the BLEU score measures the n-gram overlap be"
N03-1010,2001.mtsummit-papers.68,0,0.0794265,"Missing"
N03-1010,C00-2123,0,0.0984636,"Missing"
N03-1010,P97-1047,0,0.172958,"Missing"
N03-1010,J93-2003,0,\N,Missing
N03-1010,J90-2002,0,\N,Missing
P01-1030,J99-4005,1,\N,Missing
P01-1030,J93-2003,0,\N,Missing
P01-1030,P97-1047,0,\N,Missing
P01-1030,P96-1021,0,\N,Missing
P01-1030,P97-1037,0,\N,Missing
P08-4006,ahrenberg-etal-2002-system,0,0.083665,"uently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw lines between associated words, as shown in Fig. 1. The second is to use an alignment matrix (Fig. 2), where the rows of the matrix correspond to the words of the sentence in one language and the columns to the words of that sentence’s translation into the other language. Marks in the matrix’s cells indicate whether the words represented by the row and column of the cell are linked or not. A third technique, emplo"
P08-4006,P06-1097,0,0.0218482,"t and designed to run as a web application. 1 2 Introduction Sub-sentential alignments of parallel text play an important role in statistical machine translation (SMT). Aligning parallel data on the word- or phrase-level is typically one of the first steps in building SMT systems, as those alignments constitute the basis for the construction of probabilistic translation dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw lines between associated"
P08-4006,W07-1520,1,0.696179,"word- or phrase-level is typically one of the first steps in building SMT systems, as those alignments constitute the basis for the construction of probabilistic translation dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw lines between associated words, as shown in Fig. 1. The second is to use an alignment matrix (Fig. 2), where the rows of the matrix correspond to the words of the sentence in one language and the columns to the word"
P08-4006,P06-1065,0,0.016004,"emented in JavaScript and designed to run as a web application. 1 2 Introduction Sub-sentential alignments of parallel text play an important role in statistical machine translation (SMT). Aligning parallel data on the word- or phrase-level is typically one of the first steps in building SMT systems, as those alignments constitute the basis for the construction of probabilistic translation dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw"
P08-4006,J03-1002,0,0.00163846,"gh configuration files. The tool is implemented in JavaScript and designed to run as a web application. 1 2 Introduction Sub-sentential alignments of parallel text play an important role in statistical machine translation (SMT). Aligning parallel data on the word- or phrase-level is typically one of the first steps in building SMT systems, as those alignments constitute the basis for the construction of probabilistic translation dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visual"
P08-4006,smith-jahr-2000-cairo,0,0.409606,"n dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw lines between associated words, as shown in Fig. 1. The second is to use an alignment matrix (Fig. 2), where the rows of the matrix correspond to the words of the sentence in one language and the columns to the words of that sentence’s translation into the other language. Marks in the matrix’s cells indicate whether the words represented by the row and column of the cell are linked or not."
P08-4006,H05-1010,0,0.0114097,"les. The tool is implemented in JavaScript and designed to run as a web application. 1 2 Introduction Sub-sentential alignments of parallel text play an important role in statistical machine translation (SMT). Aligning parallel data on the word- or phrase-level is typically one of the first steps in building SMT systems, as those alignments constitute the basis for the construction of probabilistic translation dictionaries. Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. T"
P08-4006,tiedemann-2006-isa,0,0.0786375,"o evaluating their performance (e.g., Och and Ney, 2003; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2006, among many others). For the sake of simplicity, we will in the following use the term “word alignment” 1 Yawat was first presented at the 2007 Linguistic Annotation Workshop (Germann, 2007). Word alignment visualization Over the years, numerous tools for the visualization and creation of word alignments have been developed (e.g., Melamed, 1998; Smith and Jahr, 2000; Ahrenberg et al., 2002; Rassier and Pedersen, 2003; Daum´e; Tiedemann; Hwa and Madnani, 2004; Lambert, 2004; Tiedemann, 2006). Most of them employ one of two visualization techniques. The first is to draw lines between associated words, as shown in Fig. 1. The second is to use an alignment matrix (Fig. 2), where the rows of the matrix correspond to the words of the sentence in one language and the columns to the words of that sentence’s translation into the other language. Marks in the matrix’s cells indicate whether the words represented by the row and column of the cell are linked or not. A third technique, employed in addition to drawing lines by Melamed (1998) and as the sole mechanism by Tiedemann (2006), is to"
P18-4017,P13-1020,0,0.0377116,"Missing"
P18-4017,E17-3017,0,0.0141188,"new arrivals for downstream processing. In addition to a generic RSS feed monitor, we use custom data monitors that are tailored to specific data sources, e.g. the specific APIs that broadcaster-specific news apps use for updates. The main task of these specialized modules is to map between data fields of the source API’s specific (json) response and the data fields used within the Platform. 3.3 Machine Translation The machine translation engines for language translation use the Marian decoder (JunczysDowmunt et al., 2016) for translation, with neural models trained with the Nematus toolkit (Sennrich et al., 2017). In the near future, we plan to switch to Marian entirely for training and translation. Currently supported translation directions are from German, Arabic, Russian, Spanish, and Latvian into English. Systems for translation from Farsi, Portuguese and Ukranian into English are planned. 10 The Platform currently is designed to handle 9 languages: English, Arabic, Farsi, German, Latvian, Portuguese, Russian, Spanish, and Ukrainian. 11 https://github.com/andre-martins/TurboParser www.elastic.co aurelia.io 101 occupied by each cluster corresponding to the number of items within the cluster. A clic"
P18-4017,N16-1174,0,0.0128526,"their performance Space contraints prevent us from discussing the NLP components in more detail here. Detailed information about the various components can be found in the project deliverables 3.1, 4.1, and 5.1, Automatic Speech Recognition The ASR modules within the Platform are built on top of CloudASR (Klejch et al., 2015); the underlying speech recognition models are trained with the Kaldi toolkit (Povey et al., 2011). Punctuation is added using a neural MT engine that was trained 8 Topic Classification The topic classifier uses a hierarchical attention model for document classification (Yang et al., 2016) trained on nearly 600K manually annotated documents in 8 languages.10 Text-based data is retrieved by data feed modules that poll the providing source at regular intervals for new data. The data is downloaded and entered into the database, which then schedules the new arrivals for downstream processing. In addition to a generic RSS feed monitor, we use custom data monitors that are tailored to specific data sources, e.g. the specific APIs that broadcaster-specific news apps use for updates. The main task of these specialized modules is to map between data fields of the source API’s specific ("
P18-4020,P07-2045,1,0.0306875,"Missing"
P18-4020,E17-2025,0,0.0357645,"83.9 73.0 67.9 54.8 46.6 35.3 40 23.5 20 12.4 60 0 100 Deep RNN 80 60 40 20 7.8 42.5 33.437.1 28.2 22.8 17.2 13.1 0 100 Transformer 80 54.9 49.0 42.9 37.6 30.4 23.4 16.4 60 40 20 0 9.1 1 2 3 4 5 6 7 8 Number of GPUs Figure 1: Training speed in thousands of source tokens per second for shallow RNN, deep RNN and Transformer model. Dashed line projects linear scale-up based on single-GPU performance. ory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017a) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 and decayed as the inverse square root of the number of updates after 16,000 updates. When training the transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. 118 W a¨ Si hl e e n ei n Ta en s be tatu - rfe h ss la im tz M e fe nu¨ s . tleg e E"
P18-4020,W17-4774,1,0.824066,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,E17-3017,1,0.852816,"Missing"
P18-4020,I17-1013,1,0.851844,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,P16-1162,1,0.624087,"work, we implemented many efficient meta- ple scripts at https://github.com/marian-nmt/ algorithms. These include multi-device (GPU or marian-examples. 117 test2017 UEdin WMT17 (single) +Ensemble of 4 +R2L Reranking 33.9 35.1 36.2 27.5 28.3 28.3 Deep RNN (single) +Ensemble of 4 +R2L Reranking 34.3 35.3 35.9 27.7 28.2 28.7 Transformer (single) +Ensemble of 4 +R2L Reranking 35.6 36.4 36.8 28.8 29.4 29.5 Source tokens per second ×103 test2016 Source tokens per second ×103 System • preprocessing of training data, tokenization, true-casing4 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.5 • training of a shallow model for backtranslation on parallel WMT17 data; • translation of 10M German monolingual news sentences to English; concatenation of artificial training corpus with original data (times two) to produce new training data; • training of four left-to-right (L2R) deep models (either RNN-based or Transformer-based); • training of four additional deep models with right-to-left (R2L) orientation; 6 • ensemble-decoding with four L2R models resulting in an n-best list of 12 hypotheses per input sentence; • rescoring of n-best list with four R2L models, a"
P18-4020,N18-1055,1,0.879647,"Missing"
P18-4020,P17-4012,0,0.0756084,"lconquers-patent-translation-in-majorwipo-roll-out/ Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels. Intel has contributed and is optimizing a CPU backend. Marian grew out of a C++ re-implementation of Nematus (Sennrich et al., 2017b), and still maintains binary-compatibility for common models. Hence, we will compare speed mostly against Nematus. OpenNMT (Klein et al., 2017), perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus. Marian is distributed under the MIT license and available from https://marian-nmt. github.io or the GitHub repository https: //github.com/marian-nmt/marian. 2 Design Outline We will very briefly discuss the design of Marian. Technical details of the implementations will be provided in later work. 2.1 Custom Auto-Differentiation Engine The deep-learning back-end included in Marian is based on reverse-mode auto-differentiation with dynamic computation graphs and among the established mach"
W01-1409,J93-2003,0,0.00657549,"Missing"
W01-1409,P01-1030,1,0.18292,"Missing"
W07-1520,ahrenberg-etal-2002-system,0,0.267368,"Missing"
W07-1520,P06-1065,0,0.073296,"Missing"
W07-1520,tiedemann-2006-isa,0,\N,Missing
W07-1520,P06-1097,0,\N,Missing
W09-1505,D07-1090,0,0.101614,"d), Church et al. (2007) present a compressed tri36 gram model that combines Stolcke (1998) pruning with Golomb (1966) coding of inter-arrival times in the (sparse) range of hash values computed by the hash function. One major drawback of their method of storage is that search is linear in the total number of keys in the worst case (usually mediated by auxiliary data structures that cache information). Since hash-based implementations of token sequence-based NLP databases usually don’t store the search keys, it is not possible to iterate through such databases. 4.6 Distributed implementations Brants et al. (2007) present an LM implementation that distributes very large language models over a network of language model servers. The delay due to network latency makes it inefficient to issue individual lookup requests to distributed language models. As Brants et al. point out: “Onboard memory is around 10,000 times faster” than access via the network. Instead, requests are batched and sent to the server in chunks of 1,000 or 10,000 requests. 5 Experiments We present here the results of empirical evaluations of the effectiveness of TPTs for encoding ngram language models and phrase tables for SMT. We have"
W09-1505,D07-1021,0,0.0789508,"storage. This risk can be controlled by the design of the hash function. Talbot and Brants (2008) show that Bloomier filters (Chazelle et al., 2004) can be used to create perfect hash functions for language models. This guarantees that there are no collisions between existing entries in the database but does not eliminate the risk of false positives for items that are not in the database. For situations where space is at a premium and speed negotiable (e.g., in interactive context-based spelling correction, where the number of lookups is not in the range of thousands or millions per second), Church et al. (2007) present a compressed tri36 gram model that combines Stolcke (1998) pruning with Golomb (1966) coding of inter-arrival times in the (sparse) range of hash values computed by the hash function. One major drawback of their method of storage is that search is linear in the total number of keys in the worst case (usually mediated by auxiliary data structures that cache information). Since hash-based implementations of token sequence-based NLP databases usually don’t store the search keys, it is not possible to iterate through such databases. 4.6 Distributed implementations Brants et al. (2007) pre"
W09-1505,W06-3113,0,0.21183,"Missing"
W09-1505,W07-0712,0,0.0899977,"tions is on-demand loading. In the context of SMT, Zens and Ney (2007) store the phrase table on disk, represented as a trie with relative offsets, so that sections of the trie can be loaded into memory without rebuilding them. During translation, only those sections of the trie that actually match the input are loaded into memory. They report that their approach is “not slower than the traditional approach”, which has a significant load time overhead. They do not provide a comparison of pure processing speed ignoring the initial table load time overhead of the “traditional approach”. IRSTLM (Federico and Cettolo, 2007) offers the option to use a custom page manager that relegates part of the structure to disk via memory-mapped files. The difference with our use of memory mapping is that IRSTLM still builds the structure in memory and then swaps part of it out to disk. 4.4 Lossy compression and pruning Large models can also be reduced in size by lossy compression. Both SRILM and IRSTLM offer tools for language model pruning (Stolcke, 1998): if probability values for long contexts can be approximated well by the back-off computation, the respective entries are dropped. Another form of lossy compression is the"
W09-1505,D07-1103,0,0.0590986,"Missing"
W09-1505,P07-2045,0,0.00347231,"lity values and backoff weights is used to reduce the amount of memory needed to store probability values and back-off weights (see Section 4.4 below). 4.2 Model filtering Many research systems offer the option to filter the models at load time or offline, so that only information pertaining to tokens that occur in a given input is kept in memory; all other database entries are skipped. Language model implementations that offer model filtering at load time include the SRILM toolkit (Stolcke, 2002) and the Portage LM implementation (Badr et al., 2007). For translation tables, the Moses system (Koehn et al., 2007) as well as Portage offer model filtering (Moses: offline; Portage: offline and/or at load time). Model filtering requires that the input is known when the respective program is started and therefore is not feasible for server implementations. 4.3 On-demand loading A variant of model filtering that is also viable for server implementations is on-demand loading. In the context of SMT, Zens and Ney (2007) store the phrase table on disk, represented as a trie with relative offsets, so that sections of the trie can be loaded into memory without rebuilding them. During translation, only those secti"
W09-1505,W05-0822,0,0.0494321,"PT load 1 w/s2 time w/s 178s 5.9 2.67 170s 5.6 0.91 154s 5.5 0.12 TPPT + TPLM load w/s1 w/s2 time3 &lt; 1s 5.5 5.5 &lt; 1s 5.6 5.6 &lt; 1s 5.3 5.2 Baseline: Portage’s implementation as pointer structure with load-time filtering. TP: Tightly packed; PT: phrase table; LM: language model 1 words per second, excluding load time (pure translation time after model loading) 2 words per second, including load time (bottom line translation speed) 5.2 6 Conclusions TPTs in statistical machine translation To test the usefulness of TPTs in a more realistic setting, we integrated them into the Portage SMT system (Sadat et al., 2005) and ran large-scale translations in parallel batch processes on a cluster. Both language models and translation tables were encoded as TPTs and compared against the native Portage implementation. The system was trained on ca. 5.2 million parallel sentences from the Canadian Hansard (English: 101 million tokens; French: 113 million tokens). The language model statistics are given in Table 2; the phrase table contained about 60.6 million pairs of phrases up to length 8. The test corpus of 1134 sentences was translated from English into French in batches of 1, 10, and 47 or 48 sentences.6 Transl"
W09-1505,P08-1058,0,0.0389885,"ation quality. Pruning and lossy compression are orthogonal to the approach taken in TPTs. The two approaches can be combined to achieve even more compact language models and phrase tables. 4.5 Hash functions An obvious alternative to the use of trie structures is the use of hash functions that map from n-grams to slots containing associated information. With hash-based implementations, the keys are usually not stored at all in the database; hash collisions and therefore lookup errors are the price to be paid for compact storage. This risk can be controlled by the design of the hash function. Talbot and Brants (2008) show that Bloomier filters (Chazelle et al., 2004) can be used to create perfect hash functions for language models. This guarantees that there are no collisions between existing entries in the database but does not eliminate the risk of false positives for items that are not in the database. For situations where space is at a premium and speed negotiable (e.g., in interactive context-based spelling correction, where the number of lookups is not in the range of thousands or millions per second), Church et al. (2007) present a compressed tri36 gram model that combines Stolcke (1998) pruning wi"
W09-1505,N07-1062,0,0.04844,"mplementations that offer model filtering at load time include the SRILM toolkit (Stolcke, 2002) and the Portage LM implementation (Badr et al., 2007). For translation tables, the Moses system (Koehn et al., 2007) as well as Portage offer model filtering (Moses: offline; Portage: offline and/or at load time). Model filtering requires that the input is known when the respective program is started and therefore is not feasible for server implementations. 4.3 On-demand loading A variant of model filtering that is also viable for server implementations is on-demand loading. In the context of SMT, Zens and Ney (2007) store the phrase table on disk, represented as a trie with relative offsets, so that sections of the trie can be loaded into memory without rebuilding them. During translation, only those sections of the trie that actually match the input are loaded into memory. They report that their approach is “not slower than the traditional approach”, which has a significant load time overhead. They do not provide a comparison of pure processing speed ignoring the initial table load time overhead of the “traditional approach”. IRSTLM (Federico and Cettolo, 2007) offers the option to use a custom page man"
W10-1717,W08-0304,0,0.0612136,"Missing"
W10-1717,W09-0439,1,0.800854,"alf of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output by our decoder were large and needed to be shrunk dramatically for the algorithm to function wel"
W10-1717,W07-0717,1,0.856161,"kward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, cont"
W10-1717,P07-1019,0,0.047578,"MM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the ta"
W10-1717,2005.iwslt-1.8,0,\N,Missing
W10-1717,N04-1033,0,\N,Missing
W10-1717,D08-1076,0,\N,Missing
W10-1717,P03-1021,0,\N,Missing
W12-3135,P05-1032,0,0.0257708,"n this year’s shared task. Due to space limitations, many details will have to be skipped. 2 System Description 2.1 Grammatical framework The central idea underlying this work is that grammar constrains word reordering: we are allowed to permute siblings in a CFG tree, or the governor and its dependents in a dependency structure, but we are not allowed to break phrase coherence by moving 1 “Phrase” being any contiguous sequence of words in this context, as in PBSMT. 2 Except that we do not pre-compute phrase tables but construct them dynamically on the fly using suffix arrays, as suggested by Callison-Burch et al. (2005). 293 words out of their respective sub-tree. Obviously, we need to be careful in the precise formulation of our grammar, so as not to over-constrain word order options. For example, the German parse tree for the phrase ein1 [zu hoher]2 Preis3 in Fig. 2 below rules out the proper word order of its English translation [too high]2 a1 price3 . NP Det ein1 N′ AP N [zu hoher]2 Preis3 Figure 2: X-bar syntax can be too restrictive. This tree does not allow the word order of the English translation [too high]2 a1 price3 . In her analysis of phrasal cohesion in translation, Fox (2002) pointed out that"
W12-3135,N12-1047,0,0.0122922,"n may lie in the fact that we used only the Europarl data for training.3 However, our system also lags far behind a baseline Moses system trained on the same subset of data used for our system, which achieves a BLEU score of .184. Since our feature functions are very similar to those used in MOSES, we suspect that better tuning of the feature weights might close the gap. We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al., 2008) and batch MIRA (Cherry and Foster, 2012). 5 Conclusion We have presented a variant of PBSMT that uses syntactic information from source-side parses in order to account better for word-order differences in German-to-English machine translation, while preserving the advantages of PBSMT. Several components were developed from scratch, such as a dependency parser for German and a reordering model for parse constituents, as well as several novel variants 3 Participation in the shared task was a short term decision, and we did not have the time to re-train our system. 296 of n-gram based fluency measures. While our results for this year’s"
W12-3135,J07-2003,0,0.0536419,"s (1997) Inversion Transaction Grammar (ITG), assumes that word order differences can be accounted for by hierarchical inversion of adjacent blocks of text. Yamada and Knight (2001) present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation. Collins et al. (2005) perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure. Building on Chiang (2007), several systems under active development (e.g., Weese et al., 2011; Dyer et al., 2010) rely on synchronous context-free grammars to deal with word order differences. In essence, these systems parse the input while synchronously building a parse tree in the translation target language, using probabilities of the source and target trees as well as correspondence probabilities to evaluate translation hypotheses. 292 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 292–297, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics “Dieser1 Vorsc"
W12-3135,P05-1066,0,0.0450227,"generally acknowledged, word order differences are not entirely arbitrary. By and large they follow syntactic structure. An analysis of word-aligned French-English data by Fox (2002) showed that word alignment links rarely cross syntactic boundaries. Wu’s (1997) Inversion Transaction Grammar (ITG), assumes that word order differences can be accounted for by hierarchical inversion of adjacent blocks of text. Yamada and Knight (2001) present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation. Collins et al. (2005) perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure. Building on Chiang (2007), several systems under active development (e.g., Weese et al., 2011; Dyer et al., 2010) rely on synchronous context-free grammars to deal with word order differences. In essence, these systems parse the input while synchronously building a parse tree in the translation target language, using probabilities of the source and target trees as well as cor"
W12-3135,P96-1025,0,0.0794941,"h part of Europarl corpus (v.5). The language model for English was trained on all monolingual data available for WMT-2010. We true-cased, but did not lower-case the data. Word alignment was performed with multi-threaded Giza++ (Gao and Vogel, 2008). In order to bootstrap training data for our parser, we parsed the German side of the Europarl corpus with the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) and converted the CFG structures to dependency structures using simple hand-written heuristics to identify the head in each phrase, similar to those used by Magerman (1995) and Collins (1996). This head was then selected as the governor of the respective phrase. Part-of-speech tagging and lemmatization on the English side as well as the German development and test data was performed with the tool TreeTagger (Schmid, 1995). For tuning the model parameters, we tried to apply pairwise rank optimization (PRO) (Hopkins and May, 2011), but we were not able to achieve results that beat our hand-tuned parameter settings. 4 Evaluation Unfortunately, with a BLEU score of .121, (.150 after several bug fixes in the program code), our system performed extremely poorly in the shared task. We ha"
W12-3135,P10-4002,0,0.0251502,"an be accounted for by hierarchical inversion of adjacent blocks of text. Yamada and Knight (2001) present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation. Collins et al. (2005) perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure. Building on Chiang (2007), several systems under active development (e.g., Weese et al., 2011; Dyer et al., 2010) rely on synchronous context-free grammars to deal with word order differences. In essence, these systems parse the input while synchronously building a parse tree in the translation target language, using probabilities of the source and target trees as well as correspondence probabilities to evaluate translation hypotheses. 292 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 292–297, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics “Dieser1 Vorschlag2 wird3 sicherlich4 im5 Ausschuß6 gründlich7 diskutiert8 werden9 müssen10 .” “This1"
W12-3135,C96-1058,0,0.0288687,"u hoher]2 Preis3 Figure 2: X-bar syntax can be too restrictive. This tree does not allow the word order of the English translation [too high]2 a1 price3 . In her analysis of phrasal cohesion in translation, Fox (2002) pointed out that phrasal cohesion is greater with respect to dependency structure than with respect to constituent structure. We therefore decided to rely on the segmentation granularity inherent in dependency parses. 2.2 Parsing For parsing, we developed our own hybrid leftcorner dependency parser for German. In many respects, it is inspired by the work on dependency parsing by Eisner (1996) (edge factorization) and McDonald et al. (2005) (choice of features for edge scores). From the generative point of view, we can imagine the following generative process: We start with the root word of the sentence. A Markov process then generates this word’s immediate dependents from left to right, at some point placing the head word itself. The dependents (but not the head word) are then expanded recursively in the same fashion. At parse time we process the input left to right, deciding for each word what its governors are, or whether it governs some items to its left or right. Since each wo"
W12-3135,W08-0509,0,0.0191407,"y to p (are |can you). The n-gram cdf feature models the event as a Bernoulli experiment. Suppose, for example, that p (are |you) = .01, and we have observed can you 1000 times, but have never seen can you are. Then the expected count of observations is 10 and cdf (0 |1000; .01) = (1 − .01)1000 ≈ .000043 3 Training and tuning The system was trained on the German-English part of Europarl corpus (v.5). The language model for English was trained on all monolingual data available for WMT-2010. We true-cased, but did not lower-case the data. Word alignment was performed with multi-threaded Giza++ (Gao and Vogel, 2008). In order to bootstrap training data for our parser, we parsed the German side of the Europarl corpus with the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) and converted the CFG structures to dependency structures using simple hand-written heuristics to identify the head in each phrase, similar to those used by Magerman (1995) and Collins (1996). This head was then selected as the governor of the respective phrase. Part-of-speech tagging and lemmatization on the English side as well as the German development and test data was performed with the tool TreeTagger (Schmid, 1995)."
W12-3135,D08-1076,0,0.0419925,"xplanation for it. A partial explanation may lie in the fact that we used only the Europarl data for training.3 However, our system also lags far behind a baseline Moses system trained on the same subset of data used for our system, which achieves a BLEU score of .184. Since our feature functions are very similar to those used in MOSES, we suspect that better tuning of the feature weights might close the gap. We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al., 2008) and batch MIRA (Cherry and Foster, 2012). 5 Conclusion We have presented a variant of PBSMT that uses syntactic information from source-side parses in order to account better for word-order differences in German-to-English machine translation, while preserving the advantages of PBSMT. Several components were developed from scratch, such as a dependency parser for German and a reordering model for parse constituents, as well as several novel variants 3 Participation in the shared task was a short term decision, and we did not have the time to re-train our system. 296 of n-gram based fluency me"
W12-3135,P95-1037,0,0.0240381,"on the German-English part of Europarl corpus (v.5). The language model for English was trained on all monolingual data available for WMT-2010. We true-cased, but did not lower-case the data. Word alignment was performed with multi-threaded Giza++ (Gao and Vogel, 2008). In order to bootstrap training data for our parser, we parsed the German side of the Europarl corpus with the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) and converted the CFG structures to dependency structures using simple hand-written heuristics to identify the head in each phrase, similar to those used by Magerman (1995) and Collins (1996). This head was then selected as the governor of the respective phrase. Part-of-speech tagging and lemmatization on the English side as well as the German development and test data was performed with the tool TreeTagger (Schmid, 1995). For tuning the model parameters, we tried to apply pairwise rank optimization (PRO) (Hopkins and May, 2011), but we were not able to achieve results that beat our hand-tuned parameter settings. 4 Evaluation Unfortunately, with a BLEU score of .121, (.150 after several bug fixes in the program code), our system performed extremely poorly in the"
W12-3135,P05-1012,0,0.0312526,"can be too restrictive. This tree does not allow the word order of the English translation [too high]2 a1 price3 . In her analysis of phrasal cohesion in translation, Fox (2002) pointed out that phrasal cohesion is greater with respect to dependency structure than with respect to constituent structure. We therefore decided to rely on the segmentation granularity inherent in dependency parses. 2.2 Parsing For parsing, we developed our own hybrid leftcorner dependency parser for German. In many respects, it is inspired by the work on dependency parsing by Eisner (1996) (edge factorization) and McDonald et al. (2005) (choice of features for edge scores). From the generative point of view, we can imagine the following generative process: We start with the root word of the sentence. A Markov process then generates this word’s immediate dependents from left to right, at some point placing the head word itself. The dependents (but not the head word) are then expanded recursively in the same fashion. At parse time we process the input left to right, deciding for each word what its governors are, or whether it governs some items to its left or right. Since each word has exactly one governor (bar the root word),"
W12-3135,P06-1055,0,0.0490957,"observed can you 1000 times, but have never seen can you are. Then the expected count of observations is 10 and cdf (0 |1000; .01) = (1 − .01)1000 ≈ .000043 3 Training and tuning The system was trained on the German-English part of Europarl corpus (v.5). The language model for English was trained on all monolingual data available for WMT-2010. We true-cased, but did not lower-case the data. Word alignment was performed with multi-threaded Giza++ (Gao and Vogel, 2008). In order to bootstrap training data for our parser, we parsed the German side of the Europarl corpus with the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) and converted the CFG structures to dependency structures using simple hand-written heuristics to identify the head in each phrase, similar to those used by Magerman (1995) and Collins (1996). This head was then selected as the governor of the respective phrase. Part-of-speech tagging and lemmatization on the English side as well as the German development and test data was performed with the tool TreeTagger (Schmid, 1995). For tuning the model parameters, we tried to apply pairwise rank optimization (PRO) (Hopkins and May, 2011), but we were not able to achieve result"
W12-3135,W11-2160,0,0.0733256,"order differences can be accounted for by hierarchical inversion of adjacent blocks of text. Yamada and Knight (2001) present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation. Collins et al. (2005) perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure. Building on Chiang (2007), several systems under active development (e.g., Weese et al., 2011; Dyer et al., 2010) rely on synchronous context-free grammars to deal with word order differences. In essence, these systems parse the input while synchronously building a parse tree in the translation target language, using probabilities of the source and target trees as well as correspondence probabilities to evaluate translation hypotheses. 292 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 292–297, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics “Dieser1 Vorschlag2 wird3 sicherlich4 im5 Ausschuß6 gründlich7 diskutiert8 werden9"
W12-3135,J97-3002,0,0.448829,"Missing"
W12-3135,P01-1067,0,0.0989395,"he finite part of the verb complex and additional elements (separable prefixes, participles, infinitives, etc.) form a bracket that encloses most of the arguments and other adverbial As is generally acknowledged, word order differences are not entirely arbitrary. By and large they follow syntactic structure. An analysis of word-aligned French-English data by Fox (2002) showed that word alignment links rarely cross syntactic boundaries. Wu’s (1997) Inversion Transaction Grammar (ITG), assumes that word order differences can be accounted for by hierarchical inversion of adjacent blocks of text. Yamada and Knight (2001) present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation. Collins et al. (2005) perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure. Building on Chiang (2007), several systems under active development (e.g., Weese et al., 2011; Dyer et al., 2010) rely on synchronous context-free grammars to deal with word order differences."
W12-3135,N07-1051,0,\N,Missing
W12-3135,P07-2045,0,\N,Missing
W12-3135,N03-1017,0,\N,Missing
W12-3135,W02-1039,0,\N,Missing
W12-3135,D11-1125,0,\N,Missing
W13-2203,W12-4204,0,0.54368,"valuate machine translation is not new. Gim´enez and M`arquez (2007) proposed using automatically assigned semantic role labels as a feature in a combined MT metric. The main difference between this application of semantic roles and MEANT is that arguments for specific verbs are taken into account, instead of just applying the subset agent, patient and benefactor. This idea would probably help human annotators to handle sentences with passives, copulas and other constructions which do not easily match the most basic arguments. On the other hand, verb specific arguments are language dependent. Bojar and Wu (2012), applying HMEANT to English-to-Czech MT output, identified a number of problems with HMEANT, and suggested a variety of improvements. In some respects, this work is very similar, except that our goal is to evaluate HMEANT along a range of intrinsic properties, to determine how useful the metric really is to evaluation campaigns such as the workshop on machine translation. 3 Evaluation with HMEANT 3.1 Annotation Procedure The goal of the HMEANT metric is to capture essential semantic content, but still be simple and fast. There are two stages to the annotation, the first of which is semantic r"
W13-2203,W07-0718,1,0.869138,"ion, there is still no consensus on how to evaluate machine translation based on human judgements. (Hutchins and Somers, 1992; Przybocki et al., 2009). One obvious approach is to ask annotators to rate translation candidates on a numerical scale. Under the DARPA TIDES program, the Linguistic Data Consortium (2002) developed an evaluation scheme that relies on two five-point scales representing fluency and adequacy. This was also the human evaluation scheme used in the annual MT competitions sponsored by NIST (2005). In an analysis of human evaluation results for the WMT ’07 workshop, however, Callison-Burch et al. (2007) found high correlation between fluency and adequacy scores assigned by individual annotators, suggesting that human annotators are not able to separate these two evaluation dimensions easily. Furthermore these absolute scores show low inter-annotator agreement. Instead of giving absolute quality assessments, annotators appeared to be using their ratings to rank translation candidates according to their overall preference for one over the other. In line with these findings, Callison-Burch et al. (2007) proposed to let annotators rank translation candidates directly, without asking them to assi"
W13-2203,P11-1023,0,0.125845,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,lo-wu-2010-evaluating,0,0.0158008,"ing does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks of those evaluation measures, which we discuss in Sec. 2, they could just as well have been evaluating the human adequacy and contrastive judgements using HMEANT. Human evaluation metrics need to be judged on other intrinsic qualities, which we describe below. The aim of this paper is to evaluate the effectiveness of HMEANT, with the goal of using it to judge the relative merits of different MT systems,"
W13-2203,W11-1002,0,0.330392,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,N12-1017,0,0.0331991,"eded by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply because they have access to sets of reference translations which are simply too small. However, the fact is that even if one does have access to large numbers of translations, it is very difficult to determine whether the reference correctly captures the essential semantic content of the references. The idea of using semantic role"
W13-2203,W12-4206,0,0.0163075,"evaluation, as it is essential for building better automatic metrics, and therefore a more fundamental problem. The overall HMEANT score for MT evaluation is computed as the f-score from the counts of matches of frames and their role fillers between the reference and the MT output. Unmatched frames are excluded from the calculation together with all their corresponding roles. In recognition that preservation of some types of semantic relations may be more important than others for a human to understand a sentence, one may want to weight them differently in the computation of the HMEANT score. Lo and Wu (2012) train weights for each role filler type to optimise correlation with human adequacy judgements. As an unsupervised alternative, they suggest weighting roles according to their frequency as approximation to their importance. Since the main focus of the current paper is the annotation of the actions, roles and alignments that HMEANT depends on, we do not explore such different weight-setting schemes, but set the weights uniformly, with the exception of a partial alignment, which is given a weight of 0.5. HMEANT is thus defined as follows: 4 4.1 Systems and Data Sets We performed HMEANT evaluati"
W13-2203,P08-4006,1,0.829538,"espective sentence. They were then presented with the output of several machine translation systems for the same source sentence, one system at a time, with the reference translation and its annotations visible in the left half of the screen (cf. Fig. 1). For each system, the annotators were asked to annotate semantic frames and slot fillers in the translation first, and then align them with frame heads and slot fillers in the human reference translation. Annotations and alignment were performed with Edi-HMEANT2 , a web-based annotation tool for HMEANT that we developed on the basis of Yawat (Germann, 2008). The tool allows the alignment of slots from different semantic frames, and the alignment of slots of different types; however, such alignments are not considered in the computation of the final HMEANT score. The annotation guidelines were essentially those used in Bojar and Wu (2012), with some additional English examples, and a complete set of German examples. For ease of comparison with prior work, we used the same set of semantic role labels as Bojar and Wu (2012), shown in Table 1. Given the restriction that the head of a frame can consist of only one word, a convention was made that all"
W13-2203,W12-3101,0,0.0132441,"02), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The lo"
W13-2203,J05-1004,0,0.0443523,"s used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotator"
W13-2203,C12-1083,0,0.0311324,"uage. Efficiency Whilst HMEANT evaluation will never be as fast as, for example, the contrastive judgements used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has mea"
W13-2203,P02-1040,0,0.101424,"a that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lop"
W13-2203,W12-3123,0,0.0290208,"ts to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply bec"
W13-2203,W09-0441,0,0.049194,"translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems."
W13-2203,Y12-1062,0,0.0303318,"Missing"
W13-2203,W07-0738,0,\N,Missing
W13-2203,W12-3129,0,\N,Missing
W13-2203,P13-1023,0,\N,Missing
W13-2203,W11-2101,0,\N,Missing
W13-2816,P11-2031,0,0.01261,"in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in local maxima. The app"
W13-2816,C12-2032,0,0.0222513,"translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method shifts the balance to the target side; the choice between potential homophone alternatives is made primarily by the target language model, though the source language weights and the translation model are a"
W13-2816,W09-0439,0,0.0299907,"ose that each word w ˆi in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in"
W13-2816,2005.mtsummit-papers.11,0,0.00505547,"s trained from a small bicorpus of domain language. With automatic evaluation, the weighted graph method yields an improvement of about +0.63 BLEU points, while the rulebased method scores about the same as the baseline. On contrastive manual evaluation, both methods give highly significant improvements (p < 0.0001) and score about equally when compared against each other. 1 Introduction and motivation The data used to train Statistical Machine Translation (SMT) systems is most often taken from the proceedings of large multilingual organisations, the generic example being the Europarl corpus (Koehn, 2005); for academic evaluation exercises, the test data may well also be taken from the same source. Texts of this kind are carefully cleaned-up formal language. However, real MT systems often need to handle text from very different genres, which as usual causes problems. This paper addresses a problem common in domains containing informally written text: spelling errors based on homophone confusions. Concretely, the work reported was carried out in the context of the ACCEPT project, which deals with the increasingly important topic of translating online forum posts; the experiments we describe wer"
W13-2816,N10-1064,0,0.0214978,"ins on ne rec¸oit pas l’alerte). ... (at least we do not recoit alert). .. (at least it does not receive the alert). Figure 1: Examples of homophone errors in French forum data, contrasting English translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method s"
W13-2816,2012.amta-papers.25,1,0.781932,", achieves an average BLEU score of 42.47 on this set. 3.1 The rule-based approach Under the ACCEPT project, a set of lightweight pre-editing rules have been developed specifically for the Symantec Forum translation task. Some of the rules are automatic (direct reformulations); others present the user with a set of suggestions. The evaluations described in Gerlach et al. (2013) demonstrate that pre-editing with the rules has a significant positive effect on the quality of SMTbased translation. The implemented rules address four main phenomena: differences between informal and formal language (Rayner et al., 2012), differences between local French and English word-order, elThe set of Acrolinx pre-editing rules potentially relevant to resolution of homophone errors was applied to the devtest b set test corpus (Section 2.1). In order to be able to make a fair comparison with the weighted-graph method, we only used rules with a unique suggestion, which could be run automatically. Applying these rules produced 430 changed words in the test corpus, but did not change the average BLEU score significantly (42.38). Corrections made with a human in the loop, used as “oracle” input for the SMT system, by the 111"
W13-2816,bredenkamp-etal-2000-looking,0,\N,Missing
W13-2816,P07-2045,0,\N,Missing
W13-2816,P00-1056,0,\N,Missing
W14-0307,W13-2212,1,0.891936,"Missing"
W14-0307,2012.amta-papers.22,0,0.164962,"hey also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATE CAT 3 and CASMACAT 4 , which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of different machine translation systems on post-editor productivity and behaviour. 3 3.2 A total of 15 different machine translation systems participated in the evaluation campaign. We selected four different systems that differ in their architecture and use of training data: Experimental Design • an anonymized popular online translation system built by a large Internet company (ONLINE - B) We thus carried out an experiment on an English– German news translation task, using output from four"
W14-0307,W12-3123,0,0.0422099,"ineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-editing than others. These relationships may very well also depend on the lan1 Ulrich Germann? ugermann@inf.ed.ac.uk ? School of Informatics University of Edinburgh 2 Related Work Koponen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et"
W14-0307,2013.mtsummit-wptp.1,0,0.390584,"o convenience (the authors of this study are fluent in both languages), but also because this language pair poses special challenges to current machine translation technology, due to the syntactic divergence of the two languages. We selected data from the most recent evaluation campaign. The subset chosen for our postediting task comprises 9 different news stories, originally written in English, with a total of 500 sentences. Details are shown in Table 1. and takes longer. They also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATE CAT 3 and CASMACAT 4 , which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of differen"
W14-0307,2012.amta-wptp.2,0,0.0913981,"onen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et al. (2012) conclude that the observed differences in edit times can be explained at least in part also by the types of necessary edits and the associated cognitive effort. Deleting superfluous function words, for example, appears to be cognitively simple and takes little time, whereas inserting translations for untranslated words requires more cognitive effort 2 http://www.nist.gov/itl/iad/mig/openmt.cfm http://www.casmacat.eu/index.php?n=Main.Downloads 38 Workshop on Humans and Computer-assisted Translation, pages 38–46, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistic"
W14-0307,2013.mtsummit-wptp.10,0,0.29281,"Missing"
W14-0307,W13-2221,1,0.886948,"Missing"
W14-0307,2001.mtsummit-papers.68,0,0.0634897,"009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸sˇ et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-editing than others. These relationships may very well also depend on the lan1 Ulrich Germann? ugermann@inf.ed.ac.uk ? School of Informatics University of Edinburgh 2 Related Work Koponen (2012) exam"
W14-0307,2011.eamt-1.2,0,0.194111,"Missing"
W14-0307,2006.amta-papers.25,0,0.145761,"anslation output. This is what we will do in this section. In Section 6, we will consider which parts of the final translation were actually changed by the post-editor and discuss the difference. 5.1 sub 18.9 20.0 19.8 21.9 HTER as Quality Measure The edit distance between machine translation output and human reference translation can be measured in the number of insertions, deletions, substitutions and (phrasal) moves. A metric that simply counts the minimal number of such edit operations and divides it by the length of the human reference translation is the translation edit rate, short TER (Snover et al., 2006). If the human reference translation is created from the machine translation output to minimise the number of edit operations needed for an acceptable translation, this variant is called humanmediated TER, or HTER. Note that in our experiment the post-editors are not strictly trying to minimise the number of edit operations — they may be inclined to make additional changes due to arbitrary considerations of style or perform edits that are faster rather than minimise the number of operations (e.g., deleting whole passages and rewriting them). 5.3 Edits by Post-Editor Table 5 displays the edit r"
W14-0307,W13-2229,0,0.0275354,"Missing"
W14-0307,2013.mtsummit-posters.7,0,0.087959,"Missing"
W14-0307,W12-3102,1,\N,Missing
W14-0307,P02-1040,0,\N,Missing
W14-0307,N10-1078,1,\N,Missing
W14-0307,federico-etal-2012-iwslt,0,\N,Missing
W14-0307,W13-2201,1,\N,Missing
W14-0307,2009.mtsummit-btm.7,0,\N,Missing
W14-0307,2011.eamt-1.7,0,\N,Missing
W16-2368,P99-1068,0,0.868159,"Missing"
W16-2368,J03-3002,0,0.409544,"Missing"
W16-2368,P97-1063,0,0.0153339,"l Linguistics respective first r  k columns, we obtain a lowrank representation that approximates the original term-document Matrix: T0m×r S0r×r (D0n×r )T ≈ M. (Note, by the way, that D0 S0 D0T is the cosine similarity matrix in the new low-dimensional vector space.) 2.2 the terms of the two languages into a common semantic space with 1,000 dimensions.2 Via fold-in, all monolingual documents from the collection that have been labelled by the language recogniser as being in one or the other of the language in question are also be mapped into this common space. We then use Competitive Linking (Melamed, 1997) to obtain a bipartite alignment of documents: first, we rank all possible bipartite alignment hypotheses by score. Processing the list of hypotheses in descending order, we keep all hypotheses that do not overlap or conflict with higher-ranking hypotheses and discard the others. (In fact, competitive linking is what the official evaluation procedure for this shared task does; for the purpose of participation in the Shared Task, it is sufficient to produce a ranked list). Document fold-in To map a new document into this vector space, we compute the corresponding new row to be added 0 0−1 to D0"
W17-4739,P10-2041,0,0.0320685,"Missing"
W17-4739,buck-etal-2014-n,1,0.80914,"Missing"
W17-4739,W17-4705,0,0.0149621,"Missing"
W17-4739,D14-1179,0,0.00518473,"Missing"
W17-4739,P16-1009,1,0.524474,"ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 1 Introduction We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.1 We submitted neural machine translation systems trained with Nematus (Sennrich et al., 2017). Our setup is based on techniques described in last year’s system description (Sennrich et al., 2016a), including the use of subword models (Sennrich et al., 1 2 Novelties Here we describe the main differences to last year’s systems. We provide trained models and training commands at http://data.statmt.org/wmt17_systems/ 389 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 389–399 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 Subword Segmentation stacked architecture. Implementations of both of these architectures are available in Nematus. For completeness, we here reproduce the description of the"
W17-4739,P16-1162,1,\N,Missing
W17-4739,E17-2025,0,\N,Missing
W18-2508,E17-3017,0,0.0174835,"we use custom data monitors that are tailored to specific data sources, e.g. the specific APIs that broadcaster-specific news apps use for updates. The main task of these specialized modules is to map between data fields of the source API’s specific response (typically in JSON6 format), and the data fields used within the Platform. 3.4 3.9 Automatic Speech Recognition (D3.1) Machine Translation (D3.1) The machine translation engines for language translation currently use the Marian7 decoder (Junczys-Dowmunt et al., 2016) for translation with neural MT models trained with the Nematus toolkit (Sennrich et al., 2017). We have recently switched to the Marian toolkit for training. 3.6 Topic Classification (D3.1) The topic classifier uses a hierarchical attention model for document classification (Yang et al., 5 6 7 Named Entity Recognition and Linking (D4.1) Databases The Platform currently relies on two databases. The central database in the NLP processing pipeline is an instance of RethinkDB9 , a documentoriented database that allows clients to subscribe to a continuous stream of notifications about changes in the database. This allows clients (e.g. the task scheduler) to be notified about the lastest inc"
W18-2508,P13-1020,0,0.0215058,"original source when needed. 2016) trained on nearly 600K manually annotated documents in 8 languages. 3.3 For Named Entity Recognition, we use TurboEntityRecognizer, a component within TurboParser8 (Martins et al., 2009). Recognized entities and relations between them (or propositions about them) about them are linked to a knowledge base of facts using techniques developed by Paikens et al. (2016). 3.7 Incoming stories are clustered into storylines with Aggarwal and Yu’s (2006) online clustering algorithm. The resulting storylines are summarized with the extractive summarization algorithm by Almeida and Martins (2013). 3.8 Other Data Feeds Text-based data is retrieved by data feed modules that poll the providing source at regular intervals for new data. The data is downloaded and entered into the database, which then again notifies the task scheduler, which in turn schedules the new arrivals for downstream processing. In addition to a generic RSS feed monitor, we use custom data monitors that are tailored to specific data sources, e.g. the specific APIs that broadcaster-specific news apps use for updates. The main task of these specialized modules is to map between data fields of the source API’s specific"
W18-2508,N16-1174,0,0.0598306,"Missing"
W18-6412,W18-6401,1,0.759353,"airs (both directions). For these experiments, we use the same training sets and data preparation as in our system submissions, but train the deep RNNs with a working memory of 10GB, validating every 1,000 steps, and testing for convergence with a patience of 10. We use exponential smoothing and show the results on a single smoothed model. From the results in Table 7 we see that the multihead/hop extension has a small positive effect on B LEU in most language pairs. 27.6 28.2 27.7 28.1 26.9 Table 5: Results for EN↔TR systems on official WMT test sets. human evaluation from the findings paper (Bojar et al., 2018). In terms of the clustering provided by the organisers, we were in the top constrained cluster (i.e. no significant difference was observed between ours and the best constrained system) for EN→CS, DE→EN, ET→EN, FI→EN, TR→EN and EN→TR, i.e. 6/14 language pairs. Nevertheless, Table 6 still shows that our systems generally lag behind the best submitted systems. This is contrast to the 2017 shared task, where we achieved the highest scores in most of the language pairs where we submitted systems. We hypothesise that other groups have taken fuller advantage of the transformer architecture, and als"
W18-6412,P18-1008,0,0.0363778,"with the raw data. 2.3 Model Architecture For this submission we considered two types of sequence-to-sequence architectures: a transformer (Vaswani et al., 2017) and a deep RNN, specifically the BiDeep GRU encoder-decoder (Miceli Bar400 one et al., 2017). Both architectures4 are implemented in the Marian open source neural machine translation framework (Junczys-Dowmunt et al., 2018). For the transformer architecture we used the “wmt2017-transformer” setup from the Marian example collection5 . We extended the RNN with with multi-head and multi-hop attention. Multi-head attention is similar to Chen et al. (2018), with an MLP attention mechanism using a single tanh hidden layer followed by one soft-max layer for each attention heads. We further include an optional projection layer on the attended context with layer normalisation in order to avoid increasing the total size of the attended context. Let C ∈ RNs ×de be the input sentence representation produced by the encoder, where Ns is the source sentence length and de is the top-level bidirectional encoder state dimension. Let s ∈ Rdd be an internal decoder state at some step. Then for source sentence position i we compute a vector of M attention weig"
W18-6412,W17-4715,1,0.879995,"experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also applied weighting of different corpora to most language pairs, particularly DE↔EN (Section 3.5). Extensions to Back-translation For TR↔EN (Section 3.7) we used copied monolingual data (Currey et al., 2017a) and iterative back-translation. System Details 2.1 Data and Selection All our systems were constrained in the sense that they only used the supplied parallel data (including ParaCrawl) for training the systems. We also used the monolingual news crawls to create extra synthetic parallel data by back-translation, for all language pairs, and by copying monolingual data for TR↔EN. During training we generally used newsdev2016 or newstest2016 for validation, and newstest2017 for development testing (i.e. model selection), except for ZH↔EN, and ET↔EN, where we used the recent newsdev sets instead"
W18-6412,N13-1073,0,0.0722204,"Missing"
W18-6412,W16-2323,1,0.938501,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6412,P07-2045,0,0.00708596,"e of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a validation set (newstest2016) and observed the point where translation quality started to deteriorate. We used the translation plausibility score at this point as the threshold for selecting data for training the final systems. 2.2 Preprocessing For most language pairs, our preprocessing setup consisted of the Moses pipeline (Koehn et al., 2007) of normalisation, tokenisation and truecasing, followed by byte-pair encoding (BPE) (Sennrich et al., 2016c). We generally applied joint BPE, with the number of merge operations set on a per-pair basis, detailed in Section 3. Different pipelines were used for processing the two languages written in non-Latin scripts (i.e. Chinese and Russian), also explained in Section 3. For some language pairs (those including Czech, Estonian, Finnish and German) we used the preprocessed data provided by the organisers (which is preprocessed up to truecasing), whilst for the others we started with the raw d"
W18-6412,P12-3005,0,0.0282872,"id identified a significant proportion of the data as these other two Slavic languages, but on inspecting a sample, they were found nearly always to be Czech. The issue with langid is that we just give it the text, without providing any prior knowledge, when in actual fact there is a strong prior that CzEng sentences are really Czech and English, by construction 25 fi-en et-en 20 15 Bleu Language Identifier Filtering This was applied to the CS↔EN and DE↔EN corpora, based on observations that CzEng, and ParaCrawl both contain sentence pairs in the “wrong” language. For CS↔EN we applied langid (Lui and Baldwin, 2012) to both sids of the data, removing any sentences whose English side is not labelled as English, or whose Czech is not labelled as Czech, Slovak or Slovenian3 . For DE↔EN, we just applied langid to ParaCrawl and retained only those pairs where each side was identified as the ‘correct’ language by langid. This reduced the size of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a va"
W18-6412,W17-4710,1,0.858373,"ll as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also"
W18-6412,P10-2041,0,0.0719435,"Missing"
W18-6412,W17-4739,1,0.864199,"3 times the tokens on the other side, following Hassan et al. (2018). After preprocessing the corpus size was 23.6M sentences. We then applied BPE using 18,000 merge operations and we used the top 18,000 BPE segments as vocabulary. We augmented our data with backtranslated 4 The BiDeep GRU is obtainable using the -best-deep option. 5 https://github.com/marian-nmt/ marian-examples 401 6 The implementation of the multi-head and multi-hop attention architectures is available at: https://github. com/EdinburghNLP/marian-dev 7 https://marian-nmt.github.io 8 https://github.com/fxsjy/jieba ZH↔EN from Sennrich et al. (2017), which consists of 8.6M sentences for EN→ZH and 19.7M for ZH→EN. We trained using the BiDeep architecture with multi-head attention with 1 hop and 3 heads. We decoded using an ensemble of 5 L2R systems and a beam of 12 for EN→ZH and 6 L2R systems and a beam of 12 for ZH→EN. Due to time constraints, we were not able to train any of the systems to convergence. 3.2 Czech ↔ English After preprocessing, language filtering (see Sections 2.1 and 2.2), and removing any parallel sentences where neither side contains an ASCII letter, we were left with around 50M sentence pairs. We then learned a joint"
W18-6412,P16-1162,1,0.881886,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W19-5304,D18-1332,1,0.852639,"ameters that work effectively for the ZH-EN direction. Transfomer-base with larger feed-forward network We test Wang et al.’s (2018) recommendation to use the base transformer architecture and increase the feed-forward network (FFNN) size to 4096 instead of using a transformer-big model. Ultra-large mini-batches We follow Smith et al.’s (2018) recommendation to dramatically increase the mini-batch size towards the end of training in order to improve convergence.12 Once our model stopped improving on the development set, we increased the mini-batch size 50-fold by delaying the gradient update (Bogoychev et al., 2018) to avoid running into memory issues. This increases the average mini-batch size to 13,500 words. 3.4 German → English Following the success of Edunov et al. (2018) in WMT18, we decided to focus on the use of large amounts of monolingual data in the target language. 12 BLEU Word-level segmentation for ZH Transformer-base + Larger FFNN + Ultra-large mini-batches + Ultra-large mini-batches Transformer-big 24.1 23.7 24.4 24.2 11.3 Character-level segmentation for ZH Transformer-base 20.4 Table 7: ZH→EN results on the development set. In addition, we performed fine tuning on data selected specific"
W19-5304,P18-4020,1,0.871629,"Missing"
W19-5304,N19-1423,0,0.010992,"671 10,650 9,979 7,807 5,083 7,993 1.4M 7.0 21.1 2.1 17.0 1.5 26.4 19.1 19.1 13.4 HI News Common crawl Emille Wiki-dump News Bombay IIT News 200M 3.7M 0.9M 0.4M 0.2M 45.1M 23.6M Semi-supervised MT with cross-lingual language model pre-training We followed the unsupervised training approach in (Lample and Conneau, 2019) to train two MT systems, one for EN↔GU and a second for HI→GU.6 This involves training unsupervised NMT models with an additional supervised MT training step. Initialisation of the models is done by pre-training parameters using a masked language modelling objective as in Bert (Devlin et al., 2019), individually for each language (MLM, which stands for masked language modelling) and/or cross-lingually (TLM, which stands for translation language modelling). The TLM objective is the MLM objective Monolingual data EN GU Creation of synthetic parallel data 23.6 21.9 16.6 17.7 15.4 18.7 17.0 4 Table 2: EN-GU Parallel training data used. Average length is calculated in number of tokens per sentence. For the parallel corpora, this is calculated for the first language indicated (i.e. EN, GU, then EN) We pre-processed all data using standard scripts 104 anoopkunchukuttan.github.io/indic_ nlp_lib"
W19-5304,D18-1045,0,0.193835,"lish, and English→Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English↔Gujarati, we also explored semisupervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German→English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English→Czech, we compared different pre-processing and tokenisation regimes. 1 Introduction The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English-Gujarati (EN↔GU), English-Chinese (EN↔ZH), GermanEnglish (DE→EN) and English-Czech (EN→CS). All our systems are neural machine translation (NMT) systems trained in constrained data conditions with the Marian1 toolkit (Junczys-Dowmunt et al., 2018). The different language pairs pose very different challenges, due to the characteristics of the languages involved and arguably mor"
W19-5304,I05-3017,0,0.0875027,"times the number of tokens on the other side, following Haddow et al. (2018). After pre-processing, the corpus size was 23.6M sentences. We applied BPE with 32,000 merge operations to the English side of the corpora and then removed any tokens appearing fewer than 10 times (which were mostly noise), 11 https://github.com/fxsjy/jieba Character-level Chinese A Chinese characterlevel model is not the same as an English character level model, as it is relatively common for Chinese characters to represent whole words by themselves (in the PKU corpus used for the 2005 Chinese segmentation bakeoff (Emerson, 2005), a Chinese word contains on average 1.6 characters). As such, a Chinese character-level model is much more similar to using a BPE model with very few merge operations on English. We hypothesised that using raw Chinese characters in tokenised text makes sense as they form natural subword units. We segmented all Chinese sentences into characters, but kept non-Chinese characters unsegmented in order to allow for English words and numbers to be kept together as individual units. We then applied BPE with 1,000 merges, which splits the English words in the corpora into mostly trigrams and numbers a"
W19-5304,W17-4713,0,0.0339139,"e to 13,500 words. 3.4 German → English Following the success of Edunov et al. (2018) in WMT18, we decided to focus on the use of large amounts of monolingual data in the target language. 12 BLEU Word-level segmentation for ZH Transformer-base + Larger FFNN + Ultra-large mini-batches + Ultra-large mini-batches Transformer-big 24.1 23.7 24.4 24.2 11.3 Character-level segmentation for ZH Transformer-base 20.4 Table 7: ZH→EN results on the development set. In addition, we performed fine tuning on data selected specifically for the test set prior to translation, similar to the method suggested by Farajian et al. (2017), but with data selection for the entire test set instead of individual sentences. 4.1 Approach Our approach this year is summarised as follows. 1. Back-translate all available mono-lingual English NewsCrawl data (after filtering out very long sentences). As can be seen in Table 8, the amount of monolingual data vastly outweighs the amount of parallel data available. Results We identified the best single system for each language direction (Tables 6 and 7) and ensembled four models trained separately using different random seeds. We also trained right-to-left models, but they got lower scores o"
W19-5304,W18-6412,1,0.838955,"he original parallel data is inconsistently segmented across different corpora so in order to get a consistent segmentation, we desegmented all the Chinese data and resegmented it using the Jieba tokeniser with the default dictionary.11 We then removed any sentences that did not contain Chinese characters on the Chinese side or contained only Chinese characters on the English side. We also cleaned up all sentences containing links, sentences longer than 50 words, as well as sentences in which the number of tokens on either side was > 1.3 times the number of tokens on the other side, following Haddow et al. (2018). After pre-processing, the corpus size was 23.6M sentences. We applied BPE with 32,000 merge operations to the English side of the corpora and then removed any tokens appearing fewer than 10 times (which were mostly noise), 11 https://github.com/fxsjy/jieba Character-level Chinese A Chinese characterlevel model is not the same as an English character level model, as it is relatively common for Chinese characters to represent whole words by themselves (in the PKU corpus used for the 2005 Chinese segmentation bakeoff (Emerson, 2005), a Chinese word contains on average 1.6 characters). As such,"
W19-5304,W18-2703,0,0.0200237,"the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yielded a very low BLEU score of 7.8 on the GU→EN development set using a Nematus-trained shallow RNN with heavy regularisation,5 and similar scores were found for a Moses phrase-based translation system. Our solution was to train models for the creation of synthetic data that exploit both monolingual and parallel data during training. 2.2.1 Parallel data EN-GU GU-HI EN-HI Software data Wikipedia Wiki titles v1 Govin Bilingual dictionary Bible Emille Emille Bombay IIT 107,637 18,033 11"
W19-5304,P18-1007,0,0.0213961,"air encoding (BPE) (Sennrich et al., 2016b) can be simplified with no loss to performance. We replace BPE with the segmentation algorithm based on a Unigram Language Model (ULM) from SentencePiece, which is built into Marian. In both cases we learn 32k subword units jointly on 10M sampled English and Czech sentences. We gradually remove the elements of the pipeline and find no significant difference between the two segmentation algorithms (Table 10). We do observe a performance drop when subword resampling is used, but this has been shown to be more effective particularly for Asian languages (Kudo, 2018). For the following English-Czech experiments, we use ULM segmentation on raw text. 5.2 Experiment settings We use the transformer-base and transformer-big architectures described in Section 3.3. Models are regularised with dropout between transformer layers of 0.2 and in attention of 0.1 and feed-forward layers of 0.1, label smoothing and exponential smoothing: 0.1 and 0.0001 respectively. We optimise with Adam with a learning rate of 0.0003 and linear warm-up for first 16k updates, followed by inverted squared decay. For Transformer Big models we decrease the learning rate to 0.0002. We use"
W19-5304,D18-2012,0,0.108128,"Missing"
W19-5304,J82-2005,0,0.72863,"Missing"
W19-5304,W17-4710,1,0.899961,"Missing"
W19-5304,W18-6424,0,0.035354,". We use mini-batches dynamically fitted into 48GB of GPU memory on 4 GPUs and delay gradient updates to every second iteration, which results in mini-batches of 1-1.2k sentences. We use early stopping with a patience of 5 based on the wordlevel cross-entropy on the newsdev2016 data set. Each model is validated every 5k updates, and we use the best model checkpoint according to uncased BLEU score. Decoding is performed with beam search with a beam size of 6 with length normalisation. Additionally, we reconstruct Czech quotation marks using regular expressions as the only post-processing step (Popel, 2018). 112 5.3 Experiments and Results Results of our models are shown in Table 11. Lang. System Dev 2017 2018 EN-CS Transformer-base + Data filtering 26.7 27.1 22.9 23.4 22.9 22.6 CS-EN Transformer-base + Back-translation 32.6 37.3 28.8 31.9 30.3 32.4 EN-CS Base + Back-transl. → Transformer-big + Ensemble x2 28.4 29.6 29.6 25.1 26.3 26.5 25.1 26.2 26.3 of varying the ratio between between genuine and synthetic parallel training data. For EN→ZH, we showed that character-based decoding into Chinese produces better results than the standard subword segmentation approach. In EN→CS, we also studied the"
W19-5304,W18-6319,0,0.0360199,"↔ZH, we investigate character-level pre-processing for Chinese compared with subword segmentation. For EN→CS, we show that it is possible in high resource settings to simplify pre-processing by removing steps. 1 https://marian-nmt.github.io NMT Training settings In all experiments, we test state-of-the-art training techniques, including using ultra-large mini-batches for DE→EN and EN↔ZH, implemented as optimiser delay. Results summary Automatic evaluation results for all final systems on the WMT19 test set are summarised in Table 1. Throughout the paper, BLEU is calculated using S ACRE BLEU2 (Post, 2018) unless otherwise indicated. A selection of our final models are available to download.3 Gujarati ↔ English 2 One of the main challenges for translation between English↔Gujarati is that it is a low-resource language pair; there is little openly available parallel data and much of this data is domain-specific and/or noisy (cf. Section 2.1). Our aim was therefore to experiment how additional available data 2 https://github.com/mjpost/sacreBLEU See data.statmt.org/wmt19_systems/ for our released models and running scripts. 3 103 Proceedings of the Fourth Conference on Machine Translation (WMT), V"
W19-5304,P16-1009,1,0.927459,"ote that we did not have access to the corpora provided by the Technology Development for Indian Languages Programme, as they were only available to Indian citizens. Lang(s) Corpus #sents Ave. len. from the Moses toolkit (Koehn et al., 2007): normalisation, tokenisation, cleaning (of training data only, with a maximum sentence length of 80 tokens) and true-casing for English data, using a model trained on all available news data. The Gujarati data was additionally pre-tokenised using the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yiel"
W19-5304,P16-1162,1,0.705506,"ote that we did not have access to the corpora provided by the Technology Development for Indian Languages Programme, as they were only available to Indian citizens. Lang(s) Corpus #sents Ave. len. from the Moses toolkit (Koehn et al., 2007): normalisation, tokenisation, cleaning (of training data only, with a maximum sentence length of 80 tokens) and true-casing for English data, using a model trained on all available news data. The Gujarati data was additionally pre-tokenised using the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yiel"
W19-5304,W18-6430,0,0.0890608,"Missing"
W19-5304,P07-2045,1,\N,Missing
W19-5304,W18-6401,0,\N,Missing
W19-5304,W18-6425,0,\N,Missing
W19-5304,W17-4739,1,\N,Missing
