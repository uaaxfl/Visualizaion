2020.louhi-1.1,D19-6215,1,0.568907,"fication on eponyms are analysed on a new corpus specifically made for this purpose. 3.1.1 The different methods hide information to varying degrees. Pseudo replaces some information, but, for example, may retain information about time between different events by shifting dates consistently. For Class, there is still information about the type of PHI found, which is missing for Mask. Remove removes not only the PHI itself but also the context surrounding it, i.e. all sentences where a PHI is found are removed. The pseudonymisation algorithm was similar to the one described in (Dalianis, 2019; Berg et al., 2019). The changes are based on the error analysis in (Berg et al., 2019). The changes are: De-identification Process The de-identification process consists of two main steps: (i) identification of PHI and (ii) concealment of PHI. 3.1.2 Optimising the F-score Optimisation for different F-scores was carried out to obtain models with higher recall at the expense of precision, with the aim of investigating the effect this has on downstream clinical NER. Deleger et al. (2013) introduced a recall bias by changing the predicted label of non-PHI tokens with systemgenerated probability less or equal to a t"
2020.lrec-1.547,W19-6502,1,0.727794,"s vocabulary together with its label. The system predicts labels based on which label a token has been assigned to the most often. If the word has not been sen before it is tagged as not being personally identifiable information. The concept is the same as NLTK’s Unigram Tagger5 . Although this is implemented in Scikit-learn. The model is used as a baseline to indicate how a simple method would work and to enable comparison to other data sets. The tagger is meant to somewhat indicate the difficulty of the task. 3.3.1. Feature Set The CRF is based on experiments with feature sets described in (Berg and Dalianis, 2019) except for a few altered regular expressions and no lemma or part of speech information used. Each token is, first of all, itself a feature. Further lexical features used are parts of words. It also uses orthographic information to identify capitalisation, punctuation, pure numbers or a mix of numbers and letters. It also uses regular expressions to identify dates and phone numbers as well as binary dictionary features of whether a word exists in lists for first names, last names, hospitals and locations. Lexical information is also available for neighbouring words. 3.4. Bootstrapping Cycle 3"
2020.lrec-1.547,N04-4028,0,0.147474,"l. (2001). It predicts sequences of labels based on sequences in the input. A set of features is typically defined to extract features for each word in a sentence. The CRF tries to determine weights that will maximise the likelihood of leading to the labels in the training data. The marginal probability specifies the model’s confidence in each label of an input sequence, without the regard of the outcome other variables and can be used to measure a models’ confidence in its predicted labelling (Sutton and McCallum, 2012). This can be computed through Constrained Forward-Backward, described in Culotta and McCallum (2004). The linear-chain Conditional Random Fields model is implemented with sklearn-CRFSuite4 . 3.3. Unigram Tagger The unigram tagger is used as a baseline and is a vocabulary based tagger. During training, the tagger adds every token to its vocabulary together with its label. The system predicts labels based on which label a token has been assigned to the most often. If the word has not been sen before it is tagged as not being personally identifiable information. The concept is the same as NLTK’s Unigram Tagger5 . Although this is implemented in Scikit-learn. The model is used as a baseline to i"
2020.lrec-1.547,P08-2017,0,0.0417815,"personal names, addresses, phone numbers, dates and locations. Standard methods for de-identification is Named Entity Recognition by identifying personally identifiable information and then removing them (Meystre, 2015). The American HIPAA Privacy Rule states that a health record is de-identified if eighteen types of identifiers are removed from the record (HIPAA, 2003). De-identification systems are designed to identify and remove this personally identifiable information (Velupillai et al., 2009; Stubbs et al., 2015). The annotation process for supervised machine learning methods is costly (Haertel et al., 2008). The annotation costs for clinical text is even more costly as the process must be carried out in a protected environment with a limited number of annotators who must have signed confidentiality agreements. Dernoncourt et al. (2017) estimated, from numbers presented in (Neamatullah et al., 2008)1 , that manual de-identification of the clinic MIMIC dataset, would require at least two annotators and a total 1 Neamatullah et al. (2008) implemented a rule-based Deidsystem in the programming language Perl. The system obtained a recall of 0.943. of 5,000 hours to annotate the whole 100-million-word"
2020.lrec-1.547,W10-1111,0,0.0450469,"Missing"
2020.lrec-1.547,N06-1020,0,0.0637347,"ta. In this paper, the research question addressed is: Is it possible to use semi-supervised learning to obtain more highquality training data? 2. Previous research Self-training is likely the easiest method of using unlabelled data and one of the first attempts of semi-supervised learning. Self-training essentially starts with building a single classifier with labelled data, and then iteratively label unlabelled data (Nigam and Ghani, 2000). The newly labelled predictions are combined with the actual training data, treating the predictions as the truth and used to label more unlabelled data (McClosky et al., 2006). Self-training is normally not very effective. Since the classifier each time uses its predictions to teach itself, there is a considerable 4444 3. risk that mistakes are reinforced throughout and amplified. The Yarowsky Algorithm is an example of self-training, which was initially used for word sense disambiguation (Yarowsky, 1995). It makes the assumption that it is unlikely for multiple occurrences of the same word to have different meanings in the same discourse. This assumption is used to select words with high confidence and then adding words in the same discourse with the same sense to"
2020.lrec-1.547,W09-3003,0,0.0853365,"Missing"
2020.lrec-1.547,D09-1158,0,0.049832,"s an example of self-training, which was initially used for word sense disambiguation (Yarowsky, 1995). It makes the assumption that it is unlikely for multiple occurrences of the same word to have different meanings in the same discourse. This assumption is used to select words with high confidence and then adding words in the same discourse with the same sense to the training data despite lower confidence. Self-training has also been shown to improve results when used for adapting data from one domain to another since it does not rely as much on existing annotations (McClosky et al., 2006). Wu et al. (2009) introduced a domain adaptive bootstrapping method with a selection criterion relying on finding domain-specific and domain-independent nongeneral instances which may work as a bridge between the domains. There are cases where self-training may lead to a substantial deterioration of the accuracy in a system (Zhou et al., 2012). If the unlabelled data favours one particular class of data, the risk of over-fitting increases. Self-training also likely to introduce noise, and with too much noise the classifier’s accuracy will deteriorate. There is also a risk that the selected unlabelled data will"
2020.lrec-1.547,P95-1026,0,0.648217,"ingle classifier with labelled data, and then iteratively label unlabelled data (Nigam and Ghani, 2000). The newly labelled predictions are combined with the actual training data, treating the predictions as the truth and used to label more unlabelled data (McClosky et al., 2006). Self-training is normally not very effective. Since the classifier each time uses its predictions to teach itself, there is a considerable 4444 3. risk that mistakes are reinforced throughout and amplified. The Yarowsky Algorithm is an example of self-training, which was initially used for word sense disambiguation (Yarowsky, 1995). It makes the assumption that it is unlikely for multiple occurrences of the same word to have different meanings in the same discourse. This assumption is used to select words with high confidence and then adding words in the same discourse with the same sense to the training data despite lower confidence. Self-training has also been shown to improve results when used for adapting data from one domain to another since it does not rely as much on existing annotations (McClosky et al., 2006). Wu et al. (2009) introduced a domain adaptive bootstrapping method with a selection criterion relying"
2020.multilingualbio-1.1,W01-1703,1,0.56336,"erman. Since the datasets that are handled in this work are very large, to help with the consistency of the expected output, all the words were lower-cased. Also, noise and Swedish stop word removal was carried out to help reduce the number of features before classification and produce consistent results. Stop-words do not convey any significant semantics in the output result, consequently they were discarded. Finally, lemmatisation was performed to transform the Swedish words into their dictionary form, a procedure highly important in our study as the Swedish language is highly inflectional (Carlberger et al., 2001). The library used for stop word removal and lemmatisation was NLTK8 . Methods The following section provides a description of the methods used in this work. In Figure 1 the process of the method that was used is depicted. 4.1. Text Preprocessing Following the class labeling, text assigned to each patient is then pre-processed, so as to bring it in a format that is analyzable and predictable. Swedish is different from English thus the techniques used are different, for example 7 NPU, http://www.ifcc.org/ ifcc-scientific-division/sd-committees/ c-npu/ 8 3 Natural Language Toolkit, https://www.n"
2020.multilingualbio-1.1,W15-2620,0,0.0130634,"inical notes found in EHRs and using medical terminologies, transformed them to a de-identified matrix. Eriksson et al. (2013) identified a wide range of drugs by creating an ADE dictionary from a Danish EHR. Furthermore, Henriksson et al. (2015) focused on Swedish EHR data and reported improvement in ADE detection by exploiting multiple semantic spaces built on different sizes, as opposed to a single semantic space. Finally, the combination of local and global representation of words and entities has proven to yield better accuracy than using them in isolation for ADE prediction according to Henriksson (2015). To the best of our knowledge existing data mining approaches for ADE prediction in Swedish EHRs, have been mainly focusing on utilizing specific structured data types. Moreover, many of the studies do not take into account the importance of considering variable window lengths depending on the ADE studied. Exploiting a very large patient history window length can add noise to the data and a very small window size can eliminate useful and informative predictors. Contributions. This paper, follows the work of Bamba and Papapetrou (2019) utilizing variable window lengths, but instead incorporati"
2021.nodalida-main.22,W19-6503,1,0.737839,"2009). The researchers manually annotated and de-identified 100 electronic patient records (EPRs) deriving from five different clinics (Neurology, Orthopaedia, Infection, Dental Surgery and Nutrition) at Karolinska University Hospital. The gold standard consists of unstructured text (around 174,000 tokens in total) and is known as the Stockholm EPR PHI corpus. It has 4,700 annotated instances distributed over 8 PHIclasses. It has been further developed to Stockholm EPR PHI Pseudo corpus, which contains only surrogate names, addresses, phone numbers, etc., and is partly available for research (Dalianis, 2019). 3 3.1 Data NorSynthClinical A corpus of Norwegian synthetic clinical text, the NorSynthClinical corpus3 , formed the basis of the created gold standard. NorSynthClinical is considered the first publicly available resource of Norwegian clinical text (Rama et al., 2018). It is written by one clinician with large experience with clinical work and genetic cardiology. The corpus describes patients’ family history relating to cases of cardiac disease, and according to Rama et al. (2018), it consists of 477 sentences and 6030 tokens. Only a few of these tokens can be characterised as PHI. 4 Method"
2021.nodalida-main.22,W18-5613,0,0.0997272,"ing on the Spanish language. 63 systems were evaluated and 61 received an F1 -measure score above 0.70, and the highest score was 0.97. As the gold standard seems to have served its purpose, Marimon et al. (2019) provides a good example of how to solve data sparsity problems. The lack of publicly available clinical text in Norwegian places limitations on the development of gold standards and tools for de-identification of Norwegian clinical text. Recently, there have been developments of open datasets for Named Entity Recognition (NER) of the Norwegian language, most notably NorSynthClinical (Rama et al., 2018) and NorNE (Jørgensen et al., 2020). NorSynthClinical is a small dataset of synthetic clinical text, focusing on family history information (further described in Section 3) (Rama et al., 2018). While the development of NorNE resulted in a sizeable dataset with approximately 300,000 tokens for each written variant of Norwegian and a rich entity set, most PHI entity types are missing (Jørgensen et al., 2020). Only a few attempts aiming at developing deidentification tools focusing on the Norwegian language have previously been made. One of these was conducted by Bjurstrøm and Singh (2013). They"
2021.nodalida-main.23,D19-6215,1,0.915919,"increase the recall to 0.9209 using the same model by under-sampling negative tokens, thus tokens not belonging to a PHI. However, this came at the cost of significant 3 BioBERT, https://github.com/dmis-lab/ biobert 4 BlueBERT, https://github.com/ncbi-nlp/ bluebert 5 clinicalBERT, https://github.com/ EmilyAlsentzer/clinicalBERT PHI Class First Name Last Name Phone Number Age Full Date Date Part Health Care Unit Location Organisation Total decrease in precision to 0.8819. Regarding the application of models trained on pseudonymised clinical data for NERC on authentic data, there is a study by Berg et al. (2019) where the authors achieved at highest recall of 0.5510 using a LSTM network. The experiment was repeated with a classic CRF and the recall decreased to 0.4983. 3 Data and Methods This section describes the data, tools and methods used in this study. First, the EPR data set is described in Section 3.1. Then, Section 3.2 describes the BERT-models used and how they were fine-tuned. Lastly, Section 3.3 describes how the models were evaluated in a number of experiments. 3.1 Data The data used in this study is Stockholm EPR PHI Corpus6 Stockholm EPR PHI Corpus is part of the research infrastructure"
2021.nodalida-main.23,2020.lrec-1.547,1,0.801885,"Missing"
2021.nodalida-main.23,W19-6503,1,0.831667,"Missing"
2021.nodalida-main.23,2020.clinicalnlp-1.17,0,0.243745,"al., 2019). In the last two years, however, transformer-based language models such as BERT have achieved stateof-the-art results in several NLP task on commonly used data sets (Devlin et al., 2019). BERT is a general-purpose language model developed by Devlin et al. (2019). In essence, BERT is a neural network based on transformers. Transformers are a type of deep learning model designed to handle sequential data, such as natural language text. Since their introduction in 2017 (Vaswani et al., 2017), transformers have been widely used across a variety of NLP tasks, not least on clinical text (Lewis et al., 2020). The benefit of transformerbased models over previous architectures is that they do not require the sequential data to be processed in order, allowing for parallelization of the training process. This has made it possible to develop large pre-trained models such as BERT, which have been fitted on larger amounts of data than was previously feasible. Since the first BERT-model was released in 2018, several models with modified architecture and different data used in pre-training have been released, including the multilingual M-BERT1 . M-BERT is pre-trained on texts in 104 languages, including S"
2021.nodalida-main.54,W19-6502,1,0.841926,"more fluent to read and when also removing the annotation tags it inconceives the identification of potentially remaining sensitive data in plain sight and protects the identification of PHIs. This method is called Hiding in Plain Sight (HIPS), (Carrell et al., 2012). A substantial amount of studies have been published on the de-identification of text (Meystre et al., 2010; Stubbs et al., 2015). While most of these studies have focused on English, research have been carried out for French (Grouin and N´ev´eol, 2014), Spanish (Marimon et al., 2019), Danish (Pantazos et al., 2016) and Swedish (Berg and Dalianis, 2019) as well as Japanese (Kajiyama et al., 2020). Generally, high recall is preferred over high precision in de-identification research as the privacy of the individuals describe is of paramount importance. It is therefore important to not miss any sensitive information. With regards to pseudonymisation, there are fewer studies. One of the first was a study by Sweeney (1996), which described a system for HideText, http://www.hidetext.se is the platform where HB Deid is commercialised. 1 PII, Personally identifying information, is a more general term which includes other domains. This paper describ"
2021.nodalida-main.54,W19-6503,1,0.924017,"ent to read and when also removing the annotation tags it inconceives the identification of potentially remaining sensitive data in plain sight and protects the identification of PHIs. This method is called Hiding in Plain Sight (HIPS), (Carrell et al., 2012). A substantial amount of studies have been published on the de-identification of text (Meystre et al., 2010; Stubbs et al., 2015). While most of these studies have focused on English, research have been carried out for French (Grouin and N´ev´eol, 2014), Spanish (Marimon et al., 2019), Danish (Pantazos et al., 2016) and Swedish (Berg and Dalianis, 2019) as well as Japanese (Kajiyama et al., 2020). Generally, high recall is preferred over high precision in de-identification research as the privacy of the individuals describe is of paramount importance. It is therefore important to not miss any sensitive information. With regards to pseudonymisation, there are fewer studies. One of the first was a study by Sweeney (1996), which described a system for HideText, http://www.hidetext.se is the platform where HB Deid is commercialised. 1 PII, Personally identifying information, is a more general term which includes other domains. This paper describ"
2021.nodalida-main.54,2020.lrec-1.870,0,0.087305,"Missing"
carlsson-dalianis-2010-influence,E99-1001,0,\N,Missing
D19-6215,W19-6502,1,0.71603,"the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 118–125 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 ing, non-sensitive, pseudonymised Swedish clinical text can be useful in a new and different context; considering the normal variations in the distribution and nature of PHI information, and potential effects of scrubbing (Berman, 2003), that is, removing and modifying PHIs that was carried out to patient records during the de-identification process. 2 Murry et al. (2013), by Berg and Dalianis (2019), showed few benefits of combining non-medical public text and sensitive clinical notes to build a de-identification system for medical records. More recently, deep learning approaches using recurrent neural networks seem to yield significant improvements over traditional rules-based methods or statistical machine learning (Dernoncourt et al., 2017). Still, recent studies indicate that combining several approaches will yield the best results. For instance, the best system in a recent de-identification shared task was a combination of bidirectional LSTM, CRF and a rule-based subsystem (Liu et a"
D19-6215,N16-1030,0,0.0130654,"inska&quot; and &quot;R54, Karolinska, Solna&quot; respectively. The proportion of unique instances, ’Unique’, is shown as a percentage. The two models were evaluated on both the real data set that is annotated for PHI, but not pseudonymised, ’Pseudo-Real’, as well as on the pseudonymised data set, ’Pseudo-Pseudo’. For additional comparison basis models trained on the real data set were evaluated on test sets from the same data set, ’Real-Real’. 3.2.1 CRF In this study, the CRF algorithm implemented in CRFSuite (Okazaki, 2007) is used with the sklearn-crfsuite wrapper2 and the LSTM architecture described by Lample et al. (2016), based on an open-source implementation with Tensorflow3 is used. The linear-chain Conditional Random Fields model, implemented with sklearn-CRFSuite4 , Methods Using the de-identified and pseudonymised data set, two models were trained based on two machine learning algorithms; CRF and the deep learning algorithm LSTM. The two algorithms were chosen since both have been shown to produce state of the art performance, and applying the two on Swedish clinical data sets makes for an informative comparison. 2 sklearn-crfsuite, https://sklearn-crfsuite. readthedocs.io 3 Sequence tagging, https://gi"
dalianis-etal-2010-creating,W08-1403,1,\N,Missing
dalianis-etal-2010-creating,W05-0809,0,\N,Missing
dalianis-etal-2010-creating,P03-1035,0,\N,Missing
dalianis-etal-2010-creating,O03-5002,0,\N,Missing
dalianis-etal-2010-creating,W07-2430,0,\N,Missing
dalianis-etal-2010-creating,W07-2420,0,\N,Missing
dalianis-jongejan-2006-hand,braasch-olsen-2004-sto,0,\N,Missing
dalianis-jongejan-2006-hand,W01-1703,1,\N,Missing
dalianis-velupillai-2010-certain,W08-0606,0,\N,Missing
dalianis-velupillai-2010-certain,W09-4611,0,\N,Missing
dalianis-velupillai-2010-certain,W08-0607,0,\N,Missing
dalianis-velupillai-2010-certain,W09-1304,0,\N,Missing
dalianis-velupillai-2010-certain,W04-3103,0,\N,Missing
karlgren-etal-2008-experiments,dalianis-jongejan-2006-hand,1,\N,Missing
karlgren-etal-2008-experiments,A97-1011,0,\N,Missing
P09-1017,W01-1703,1,0.749581,"our algorithm also has been tested with. Stempel (Białecki 2004) is a stemmer for Polish that is trained on Polish full form – lemma pairs. When tested with inflected out-ofvocabulary (OOV) words Stempel produces 95.4 percent correct stems, of which about 81 percent also happen to be correct lemmas. Hedlund (2001) used two different approaches to automatically find stemming rules from a corpus, for both Swedish and English. Unfortunately neither of these approaches did beat the hand crafted rules in the Porter stemmer for English (Porter 1980) or the Euroling SiteSeeker stemmer for Swedish, (Carlberger et al. 2001). Jongejan & Haltrup (2005) constructed a trainable lemmatizer for the lexicographical task of finding lemmas outside the existing dictionary, bootstrapping from a training set of full form – lemma pairs extracted from the existing dictionary. This lemmatizer looks only at the suffix part of the word. Its performance was compared with a stemmer using hand crafted stemming rules, the Euroling SiteSeeker stemmer for Swedish, Danish and Norwegian, and also with a stemmer for Greek, (Dalianis & Jongejan 2006). The results showed that lemmatizer was as good as the stemmer for Swedish, slightly bett"
P09-1017,dalianis-jongejan-2006-hand,1,0.947201,"ic process to create lemmatization rules is described in the following sections. By reserving a small part of the available training data for testing it is possible to quite accurately estimate the probability that the lemmatizer would produce the right lemma given any unknown word belonging to the language, even without requiring that the user masters the language (Kohavi 1995). On the downside, letting a program construct lemmatization rules requires an extended list of full form – lemma pairs that the program can exercise on – at least tens of thousands and possibly over a million entries (Dalianis and Jongejan 2006). 3.4 Criteria for success The main challenge for the training algorithm is that it must produce rules that accurately lemmatize OOV words. This requirement translates to two opposing tendencies during training. On the one hand we must trust rules with a wide basis of training examples more than rules with a small basis, which favours rules with patterns that fit many words. On the other hand we have the incompatible preference for cautious rules with rather specific patterns, because these must be better at avoiding erroneous rule applications than rules with generous patterns. The envisaged"
P09-1017,A97-1016,0,\N,Missing
R09-1026,gellerstam-etal-2000-bank,0,0.0381545,"of 50 percent in the case of one true positive and one true negative, and 10 percent in the case of one true positive and nine true negatives. In order to rule out any random fluke in the choice of true negative(s) for each true positive both experiments were carried out 10 times, making new random pairings each time. An average was then taken, calculated over these ten runs. As in [11] we have extracted a-priori probabilities of prefix classes from reference corpora. Since we are dealing with the language pair Swedish-English we have used a Swedish reference corpus, the Swedish Parole corpus [4], and an English ditto, the British National Corpus [1]. The Swedish reference corpus is comprised of roughly 20 million words. In order to have a comparable English reference corpus we have only used the first 20 million words of BNC. These two corpora can be seen as the expected distribution of the prefix classes for each language, while each text’s feature vector then is the deviation to the expected distribution. We would like to find if a deviation from the expected frequency distribution pattern in one language in the pair could possibly reflect a similar deviation in the other. In this"
R09-1026,1999.mtsummit-1.79,0,0.0334612,"age Information Retrieval, Identification of Parallel Text, Prefix Frequency Distribution, A-priori Probability. 1. Introduction Dictionaries are an important part of natural language processing tasks and linguistic work. Domain-specific dictionaries can for example be used in cross-language web and intranet search engines. Word alignment tools are often used for the creation of bilingual word lists. These tools need parallel corpora to work properly. One source is Internet and the multilingual web sites there. Unfortunately these web sites are often only parallel with regard to web pages. In [6] and in [2] are described different heuristics to download and identify parallel text. However, these methods are not enough since the downloaded parallel text still can be very noisy. For example [13] found only 45 percent parallel text pairs on the multilingual parallel web site Hallå Norden (Hello Scandinavia) that was intended to be completely parallel and the parallel pages contained 5 percent nonparallel elements. Therefore, we found a need to develop and evaluate a new method for identifying parallel and non-parallel texts in corpora covering different language pairs. 2. Related Work Th"
R09-1026,J05-4003,0,0.0206535,"d the parallel pages contained 5 percent nonparallel elements. Therefore, we found a need to develop and evaluate a new method for identifying parallel and non-parallel texts in corpora covering different language pairs. 2. Related Work The distinction between a parallel and a comparable corpus is very important and has been discussed in for example [10] and also in [3]. Freely available multilingual resources are often noisy and non-parallel sections need to be removed. Many methods for identifying such sections automatically have been proposed. Maximum entropy (ME) classification is used in [7] in order to improve machine translation performance. From large Chinese, Arabic and English nonparallel newspaper corpora, parallel data was extracted. For this method, a bilingual dictionary and a small amount of parallel data for the ME classifier is needed. By selecting pairs of similar documents from two monolingual corpora, all possible sentence pairs are passed through a wordoverlap based filter and then sent to the ME classifier. The authors reported significant improvements over the baseline for Arabic-English and for Chinese-English In [3] a method for extracting parallel sentences t"
R09-1026,steinberger-etal-2006-jrc,0,0.0371003,"term frequency classes towards prefix frequency classes, i.e. the leading characters of each token. This way a document’s fingerprint effectively is represented by a feature vector containing the frequency of each prefix of a set length n occurring in the corpus. Fingerprinting using prefix frequencies has for example been used in information retrieval for filtering of similar documents written in the same language [11]. We here attempt to utilize this notion in cross-language text alignment. 136 4. Data sets and experimental setup In this set of experiments we have used the JRC-Acquis corpus [12]. This corpus consists of European Union law texts, which are domain specific and also very specific in their structure. Many texts are listings of regulations with numerical references to other law texts 1 and named entities (such as countries). We have investigated the language pair Swedish-English, i.e. we used Swedish as a source language attempting to find the corresponding parallel text in English. We have also used only those documents that have a counterpart in both languages, resulting in a total of 20.145 document pairs. In order to delimit the search space for the practicality of th"
R09-1026,W04-3208,0,0.0250423,"the downloaded parallel text still can be very noisy. For example [13] found only 45 percent parallel text pairs on the multilingual parallel web site Hallå Norden (Hello Scandinavia) that was intended to be completely parallel and the parallel pages contained 5 percent nonparallel elements. Therefore, we found a need to develop and evaluate a new method for identifying parallel and non-parallel texts in corpora covering different language pairs. 2. Related Work The distinction between a parallel and a comparable corpus is very important and has been discussed in for example [10] and also in [3]. Freely available multilingual resources are often noisy and non-parallel sections need to be removed. Many methods for identifying such sections automatically have been proposed. Maximum entropy (ME) classification is used in [7] in order to improve machine translation performance. From large Chinese, Arabic and English nonparallel newspaper corpora, parallel data was extracted. For this method, a bilingual dictionary and a small amount of parallel data for the ME classifier is needed. By selecting pairs of similar documents from two monolingual corpora, all possible sentence pairs are passe"
R09-1026,W08-1403,1,0.756414,"and linguistic work. Domain-specific dictionaries can for example be used in cross-language web and intranet search engines. Word alignment tools are often used for the creation of bilingual word lists. These tools need parallel corpora to work properly. One source is Internet and the multilingual web sites there. Unfortunately these web sites are often only parallel with regard to web pages. In [6] and in [2] are described different heuristics to download and identify parallel text. However, these methods are not enough since the downloaded parallel text still can be very noisy. For example [13] found only 45 percent parallel text pairs on the multilingual parallel web site Hallå Norden (Hello Scandinavia) that was intended to be completely parallel and the parallel pages contained 5 percent nonparallel elements. Therefore, we found a need to develop and evaluate a new method for identifying parallel and non-parallel texts in corpora covering different language pairs. 2. Related Work The distinction between a parallel and a comparable corpus is very important and has been discussed in for example [10] and also in [3]. Freely available multilingual resources are often noisy and non-pa"
skeppstedt-etal-2012-rule,sjobergh-kann-2004-finding,0,\N,Missing
skeppstedt-etal-2012-rule,schuler-etal-2008-system,0,\N,Missing
skeppstedt-etal-2012-rule,N06-4006,0,\N,Missing
skeppstedt-etal-2012-rule,P09-3003,0,\N,Missing
skeppstedt-etal-2012-rule,ogren-etal-2008-constructing,0,\N,Missing
skeppstedt-etal-2012-rule,W09-4507,0,\N,Missing
W01-1703,W01-1706,1,0.809038,"ocuments, around 1-3 percent improvement from no stemming, except on very small document collections. Popovic & Wilett (1992) showed that stemming on a small collection of 400 abstracts in Slovene and queries of average length of 7 words increased precision in information retrieval with 40 percent. In the above experiments the relation between the number of documents (500 to 180 000 documents) in the document collection and the number of unique questions range between 0.1 percent and 10 percent of the document collection. 3. THE KTH NEWS CORPUS From the KTH News Corpus, described in detail in Hassel (2001), we selected 54 487 news articles from the period May 25, 2000 to November 4, 2000. From this sub-corpus we randomly selected 100 texts and manually tagged a question and answer pair central to each text; see Figure 4, for an example. Question &lt;top&gt; &lt;num&gt; Number: 35 &lt;desc&gt; Description: (Natural Language question) 1 Precision = number of found relevant documents / total number of found documents Recall = number of found relevant documents / total number of relevant documents -3- Online Proceedings of NODALIDA 2001 Vem är koncernchef på Telenor? (Who is CEO at Telenor?) &lt;/top&gt; Answer &lt;top&gt; &lt;num"
W05-1706,W01-1703,1,0.603809,"Missing"
W05-1706,W05-1710,0,0.0390453,"Missing"
W05-1706,sjobergh-kann-2004-finding,0,0.0385089,"ounds. 39 Proceedings of the 15th NODALIDA conference, Joensuu 2005 We saw also that the two insurance company websites had a larger amount of compound queries in form of studentförsäkring, skolförsäkring, garageförsäkring, villalarm, huslarm, hemlarm, bergvärme, luftvärmepump Proper nouns Östrasjukhu set Gothiacup Gröntkort Idrottenshu s Välacentru m Ideal split (not carried out) Östra sjukhuset Gothia cup Grönt kort Idrottens hus Ling@JoY 1, 2006 (compounds with -insurance, -alarm, -heatpump) that does not give any hits without decomposition. We connected the compound splitter described in (Sjöbergh & Kann 2004) to the search engine. Nouns Ideal split (not carried out) fossila bränslen fenomeno grafi läs- och skriv svårigheter fossilabränslen fenomenografi läs-och skriv svårigheter Väla centrum Table 2 and 3. The table shows five proper nouns and three nouns where the compound splitter failed. Compound Mullvad Helsingborgsdagblad bilbarnstol uppsatsdatabas missbruksbehandling missbruksvård arbetskraftinvandring arbetskraftsinvandrare ordningsvaktsutbildning gruppliv Nattliv Visakort luftvärmepump Oversplitting mull vad Helsingborgs dag blad bil barn stol Uppsats data bas miss bruks behandling miss br"
W08-1403,W05-0809,0,0.053263,"he corpora were very sparse the word alignment results for the combinations of Swedish, Danish, Norwegian and Icelandic were surprisingly good compared to other experiments with larger corpora. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. 10 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 10–16 Manchester, August 2008 A shared task on languages with sparse resources is described in Martin et al (2005). The language pairs processed were English-Inuktitut, Romanian-English and English-Hindi, where the English-Inuktitut parallel corpus contained around 4 million words for English and 2 millions words for Inuktitut. English-Hindi had less words, 60 000 words and 70 000 words respectively. The languages with the largest corpora obtained best word alignment results, for English-Inuktitut over 90 percent precision and recall and for English-Hindi 77 percent precision and 68 percent recall. One conclusion from the shared task was that it is worth using additional resources for languages with very"
W08-1403,W07-2420,0,0.273381,"ay: Related Work Word alignment systems have been used in previous research projects for automatically creating dictionaries. In Charitakis (2007) Uplug was used for aligning words in a Greek-English parallel corpus. The corpus was relatively sparse, containing around 200 000 words for each language, downloaded from two different bilingual web sites. A sample of 498 word pairs from Uplug were evaluated by expert evaluators and the result was 51 percent correctly translated words (frequency > 3). When studying high frequent word pairs (>11), there were 67 percent correctly translated words. In Megyesi & Dahlqvist (2007) an experiment is described where they had 150 000 words in Swedish and 126 000 words in Turkish that gave 69 percent correct translations (Uplug being one of the main tools used). In this work the need for parallel corpora in different language combinations is also discussed. The ITools’ suite for word alignment that was used in Nyström et al (2006) on a medical parallel corpus, containing 174 000 Swedish words and 153 000 English words, created 31 000 word pairs with 76 percent precision and 77 percent recall. In this work the word alignment was produced interactively. • • 11 create basic cl"
W08-1403,P06-1011,0,0.0584325,"Missing"
W08-1403,J03-1002,0,0.00251543,"rder to find out where more work would need to be done and where performance is actually acceptable. We have limited the work by only testing one system (Uplug) with basic settings. Our experiments and results are described in further detail in the following sections. Conclusions and future work are discussed in the final section. 2 2.1 Word Alignment: Uplug We have chosen to use the Uplug word alignment system since it is a non-commercial system which does not need a pre-trained model and is easy to use. It is also updated continuously and incorporates other alignment models, such as GIZA++ (Och & Ney 2003). We did not want to evaluate the performance of different systems in the work presented here, but rather evaluate the performance of only one system applied on different language combinations and on sparse corpora. Evaluating the performance of different systems is an important and interesting research problem, but is left for future work. An evaluation of two word alignment systems Plug (Uplug) and Arcade is described in Ahrenberg et al (2000). The Uplug system implements a word alignment process that combines different statistical measures for finding word alignment candidates and is fully"
W08-1403,ahrenberg-etal-2000-evaluation,0,0.153499,"l system which does not need a pre-trained model and is easy to use. It is also updated continuously and incorporates other alignment models, such as GIZA++ (Och & Ney 2003). We did not want to evaluate the performance of different systems in the work presented here, but rather evaluate the performance of only one system applied on different language combinations and on sparse corpora. Evaluating the performance of different systems is an important and interesting research problem, but is left for future work. An evaluation of two word alignment systems Plug (Uplug) and Arcade is described in Ahrenberg et al (2000). The Uplug system implements a word alignment process that combines different statistical measures for finding word alignment candidates and is fully automatic. It is also possible to combine statistical measures with linguistic information, such as part-of-speech tags. In the preprocessing steps the corpora are converted to an xmlformat and they are also sentence aligned. We have chosen to use basic settings for all corpora in the different language pairs, in order to evaluate the effect of this. The default word alignment settings in Uplug works in the following way: Related Work Word align"
W08-1403,E03-1026,0,0.0922311,"ld cover a larger part of the specific vocabulary, in order to help the people in the Nordic countries to find and learn the concepts in their neighboring countries. The entire set of texts on the web site was treated as one multilingual parallel corpus. From this we extracted parallel corpora for each language pair. We discovered, as expected, that the corpora were very sparse, containing on average less than 80 000 words per language pair. We needed to construct 10 different dictionaries and therefore we processed 10 pairs of parallel text sets. We have used the Uplug word alignment system (Tiedemann 2003a), for the creation of the dictionaries. The system and motivation for the choice of system is further discussed in Section 2.1. Hallå Norden is a web site with information regarding mobility between the Nordic countries in five different languages; Swedish, Danish, Norwegian, Icelandic and Finnish. We wanted to create a Nordic cross-language dictionary for the use in a cross-language search engine for Hallå Norden. The entire set of texts on the web site was treated as one multilingual parallel corpus. From this we extracted parallel corpora for each language pair. The corpora were very spar"
W08-1403,W07-2430,0,0.270136,"r finding word alignment candidates and is fully automatic. It is also possible to combine statistical measures with linguistic information, such as part-of-speech tags. In the preprocessing steps the corpora are converted to an xmlformat and they are also sentence aligned. We have chosen to use basic settings for all corpora in the different language pairs, in order to evaluate the effect of this. The default word alignment settings in Uplug works in the following way: Related Work Word alignment systems have been used in previous research projects for automatically creating dictionaries. In Charitakis (2007) Uplug was used for aligning words in a Greek-English parallel corpus. The corpus was relatively sparse, containing around 200 000 words for each language, downloaded from two different bilingual web sites. A sample of 498 word pairs from Uplug were evaluated by expert evaluators and the result was 51 percent correctly translated words (frequency > 3). When studying high frequent word pairs (>11), there were 67 percent correctly translated words. In Megyesi & Dahlqvist (2007) an experiment is described where they had 150 000 words in Swedish and 126 000 words in Turkish that gave 69 percent co"
W08-1403,dalianis-jongejan-2006-hand,1,0.842,"174 181 219 7.2 9.4 34.6 16.2 4.3 33.2 10.2 30.0 14.4 16.1 Table 3: Produced dictionary words and error rate was aligned with the Finnish word asukasluku (number of inhabitants). Another error which was produced for all combinations with Finnish was lisätieto (more information) which was aligned with ytterligere (additional, more) in Norwegian (and equivalent words in Swedish and Danish), an example of an error where the head word is missing. Many texts had sentences pointing to further information, which might explain this type of error. The lemmatizers produced some erroneous word forms. In Dalianis & Jongejan (2006) the CST lemmatizer was evaluated and reported an average error rate of nine percent. Moreover, since the lemmatization process is performed on the resulting word lists, and not within the original context in which the words occur, the automatic lemmatization is more difficult for the two lemmatizers used in this project. These errors have not been included in our evaluation since they are not produced by the Uplug alignment procedure. We can also see in Table 3 that deleting nonparallel texts using our simple algorithm did not improve the overall results significantly. Perhaps our simple algo"
W08-1403,tiedemann-2008-synchronizing,0,0.0413157,"Missing"
W09-4606,ahrenberg-etal-2000-evaluation,0,0.0342781,"y needed to support query expansion. Thus, the gold standards included only Swedish nouns and adjectives with different spelling than their English equivalents. The words with identical spelling as their translations (most of the proper names and abbreviations) were omitted because they did not require query expansion, and hence, were not important for evaluation. The sample terms with missing or indirect translations were also left out, i.e. only ‘regular’ links were allowed in the gold standards. Evaluation To evaluate the Uplug output, we used a prior evaluation method with gold standards (Ahrenberg et al 2000). This evaluation requires additional tailor-made software. However, one can re-use the gold standards for different types of parallel corpora (e.g. with and without POStags). In addition, prior evaluation allows for more accurate measurement of the system output because it is based on the corpora used by the system. The gold standards were built by manually annotating links in the sentence-aligned Swedish-English parallel corpora, in accordance to the manual annotation guidelines (Merkel 1999). We omitted, however, the definite articles in the gold standards in order to make them more consist"
W09-4606,C00-1015,0,0.0345718,"he Uplug output was presented both in XML format (with word link certainty and other clues) and in text format, as a frequency table with word frequency, source and target terms (table 2). 40 40 40 40 sustainable responsibility proposal increase significant improvement in the Uplug output. We attributed this fact to insufficient accuracy in the lemmatization rules, and thus continued to use corpora with inflected forms remaining. The English-Swedish, English-Danish, and EnglishNorwegian frequency tables were used to create a Swedish-Danish-Norwegian dictionary using English as pivot language (Borin 2000, Sjöbergh 2005). The Swedish, Danish, and Norwegian tokens which were linked to the identical English tokens were considered to be equivalents. For example, Swedish hållbar, Danish bæredygtig, and Norwegian bærekraftig were linked in the Uplug output to the English word sustainable (table 3); therefore the three Scandinavian words could be aligned to each other. This method is rather approximate and may align words which do not have the same meaning. Nevertheless, we found it useful in creating multi-lingual dictionaries for expanding search queries. To achieve better precision, we extracted"
W09-4606,A00-1031,0,0.134954,"Missing"
W09-4606,W01-1703,1,0.771502,"Missing"
W09-4606,P91-1023,0,0.233828,"since many researchers recommended it and Uplug has been used with successful results for other languages, e.g. Swedish and Turkish (Megyesi & Dahlqvist 2007). 328 Using Uplug and SiteSeeker to construct a cross language search engine Uplug is a word alignment tool for parallel corpora and was developed at Uppsala University by Jörg Tiedemann (Tiedemann 2003, Uplug 2008). Uplug works excellent (we have used version 0.1.9d) even though it can be memory consuming, mostly when doing sentence alignment in large corpora. The memory problem, however, can be easily solved with ‘hard delimiter’ tags (Gale and Church 1991). We executed Uplug on the parallel texts written in English and Swedish, English and Danish, and English and Norwegian. The news articles were extracted from the RSS file, language classified with LingPipe (2006), and merged into one corpus file per language. To allow sentence alignment only within article boundaries, we added hard delimiters. The corpus files were tokenized with built-in Uplug scripts and aligned with a sentence aligner based on the statistical model of sentence length (Gale and Church 1991). The output was then word aligned with Uplug, which uses a combination of statistica"
W09-4606,1999.mtsummit-1.45,0,0.0782743,"Missing"
W09-4606,W07-2420,0,0.0183155,"59 364 229 215 Eng-Dan 1 638 299 992 272 516 Eng-Nor 1 666 305 866 278 626 Total 4 873 865 222 780 357 Table 1. Number of news texts and words in different corpora Apart from the news texts, the Nordic Council website contains other parallel or semi-parallel texts, for example organization, regulations, procedures, fact sheets etc. However, these documents are very few compared to the news texts. 5 Word alignment As a word alignment tool we decided to use Uplug, since many researchers recommended it and Uplug has been used with successful results for other languages, e.g. Swedish and Turkish (Megyesi & Dahlqvist 2007). 328 Using Uplug and SiteSeeker to construct a cross language search engine Uplug is a word alignment tool for parallel corpora and was developed at Uppsala University by Jörg Tiedemann (Tiedemann 2003, Uplug 2008). Uplug works excellent (we have used version 0.1.9d) even though it can be memory consuming, mostly when doing sentence alignment in large corpora. The memory problem, however, can be easily solved with ‘hard delimiter’ tags (Gale and Church 1991). We executed Uplug on the parallel texts written in English and Swedish, English and Danish, and English and Norwegian. The news article"
W09-4606,W02-1012,0,0.0604428,"ion and 77 percent recall. It is well known that stemming in information retrieval increases precision and recall (e.g. Carlberger et al 2001), therefore one could assume that stemming eventually would improve word alignment. However, Strömbäck (2005) has experimented to use lemmatization before executing Uplug on an English-Swedish corpus, and his results do not give any clear indication whether stemming is useful in word alignment. Schrader (2004) shows that lemmatization and tagging of English and German parallel text decrease precision but improve recall in word alignment. Toutanova et al (2002) showed up to 16 percent error reduction in word alignment for English and French (Hansard parallel corpora) using POS tagging. Compound splitting, which can be done automatically with high accuracy (Sjöbergh and Kann 2006), is another approach that could give good results before performing word alignment, see Popoviç et al (2006), though they do not write how large the improvement is. Thus, the previous research raised a number of important research questions and problems: Does POS-tagging improve word alignment quality? What is the optimal size of the parallel corpus to obtain good quality b"
W09-4606,W08-1400,0,0.0617235,"Missing"
W09-4606,J93-1004,0,\N,Missing
W09-4606,W08-1403,1,\N,Missing
W09-4606,W07-2430,0,\N,Missing
W09-4606,W01-1715,0,\N,Missing
W10-1108,gellerstam-etal-2000-bank,0,0.0957353,"Missing"
W10-1108,C04-1140,0,0.0241693,"et al., 2009.) 2.4 Related studies Since most of the available clinical documents are in free-text form, a number of stylistically oriented efforts to characterize the data from various angles have taken place. This may include various topics, from viewing detailed information about specific items (e.g. readability, Kim et al., 2007) to identifying patterns and structures in order to provide better technology to automatically process the sublanguage (Pakhomov et al., 2006). The majority of such efforts investigate different aspects of linguistic features at a monolingual level, for instance, Hahn & Wermter (2004); Tomanek et al., (2007); Chung (2009); Harkema et al., (2009); while for a thorough review of various related issues see Meystre et al., (2008). In the Nordic context, Josefsson (1999) discusses Swedish clinical language and shows examples on how verb constructions in a clinical setting differ from a non clinical setting. One claim is that the physician unmarks the verb forms for agentivity when writing about the patient and what actions she takes, for example, Patienten hallucinerar [The patient hallucinates] instead of the normal form Patienten får hallucinationer [The patient experiences h"
W10-3012,D09-1145,0,0.204506,"Missing"
W10-3012,W10-3001,0,0.0863299,"Missing"
W10-3012,P09-2044,0,0.0668221,"58.2; Task 2, scopes: 39.6 and cues: 78.5. 1 Introduction This paper reports experiments to detect uncertainty in text. The experiments are part of the two shared tasks given by CoNLL-2010 (Farkas et al., 2010). The first task is to identify uncertain sentences; the second task is to detect the cue phrase which makes the sentence uncertain and to mark its scope or span in the sentence. 84 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84–91, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics ful feature. Ganter and Strube (2009) consider weasel tags in Wikipedia articles as hedge cues, and achieve results of 0.70 BEP using word- and distance based features on a test set automatically derived from Wikipedia, and 0.69 BEP on a manually annotated test set using syntactic patterns as features. These results suggest that syntactic features are useful for identifying weasels that ought to be tagged. However, evaluation is performed on balanced test sets, which gives a higher baseline. Weasels are employed when speakers attempt to convince the listener of something they most likely are certain of themselves, by anchoring th"
W10-3012,W08-0607,0,0.293441,"on (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1 -score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and that preceded by epistemic verbs and nouns. On the fruit-fly dataset (Medlock and Briscoe, 2007) they achieve 0.85 BEP, and on the BMC dataset (Szarvas, 2008) they achieve 0.82 BEP. Light et al. (2004) also found that most of the uncertain sentences appeared towards the end of the abstract, indicating that the position of an uncertain sentence might be a use85 features, we allow combined features by the Cartesian product of the feature set extensions of two or more feature templates. the Wikipedia domain. 2.2 Feature Definitions The asymmetric"
W10-3012,P08-1033,0,0.340304,"d for the special case of logistic regression, Ng (2004) proved that the sample complexity grows only logarithmically in the number of irrelevant features, instead of linearly as when regularising with respect to the L2 norm. Our preliminary experiments indicated that L1 -regularisation is superior to L2 -regularisation in the biological domain, while slightly inferior in Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1 -score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and that preceded by e"
W10-3012,A97-1011,0,0.0667407,"Missing"
W10-3012,W04-3103,0,0.271624,"Missing"
W10-3012,P07-1125,0,0.0716195,"is known to give preference to sparse models and for the special case of logistic regression, Ng (2004) proved that the sample complexity grows only logarithmically in the number of irrelevant features, instead of linearly as when regularising with respect to the L2 norm. Our preliminary experiments indicated that L1 -regularisation is superior to L2 -regularisation in the biological domain, while slightly inferior in Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1 -score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” ve"
W10-3012,W08-0606,0,0.0812106,"Missing"
W10-3012,W09-1304,0,0.262715,"d margin violation, we add the L2 -regularisation term kwk2 . By making use of the loss augmented decoding function Table 1: Top feature templates for sentence level hedge and weasel detection. rent lemma as features) and 82.82 F-score (using a Support Vector Machine classifier and a complex feature set including keyword and dependency relation information), respectively. On the task of automatic scope resolution, best results are reported as 59.66 (F-score) and 61.13 (accuracy), ¨ ur and respectively, on the full paper subset. Ozg¨ Radev (2009) use a rule-based method for this subtask, while Morante and Daelemans (2009) use three different classifiers as input to a CRF-based meta-learner, with a complex set of features, including hedge cue information, current and surrounding token information, distance information and location information. 3.1 Learning and Optimisation Framework In recent years, a wide range of different approaches to general structured prediction problems, of which sequence labelling is a special case, have been suggested. Among others, Conditional Random Fields (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003), and Structured Support Vector Machines (Tsochantaridis"
W10-3102,J08-4004,0,0.0247002,"ach is perhaps linguistically less interesting, not giving any information on uncertainties in general, if the aim is to search for diseases and symptoms in patients, it should be sufficient. In light of the discussion above, the question to what extent the annotations in the constructed consensus capture a general perception of certainty or uncertainty must be posed. Since it is constructed using a majority method with three annotators, who had a relatively low pairwise agreement, the corpus could probably not be said to be a precise capture of what is a certainty or uncertainty. However, as Artstein and Poesio (2008) point out, it cannot be said that there is a fixed level of agreement that is valid for all purposes of a corpus, but the agreement must be high enough for a certain purpose. Therefore, if the information on whether 10 there was a unanimous annotation of a sentence or not is retained, serving as an indicator of how typical an expression of certainty or uncertainty is, the constructed corpus can be a useful resource. Both for studying how uncertainty in clinical text is constructed and perceived, and as one of the resources that is used for learning to automatically detect certainty and uncert"
W10-3102,dalianis-velupillai-2010-certain,1,0.819367,"sed both on English and German news text for training and evaluation. Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al., 2003). We have used the Stanford NER CRF for training and evaluation of our consensus. 2.4 The annotated Swedish clinical corpus for negation and speculation A process to create an annotated clinical corpus for negation and speculation is described in Dalianis and Velupillai (2010). A total of 6 740 randomly extracted sentences from a very large clinical corpus in Swedish were annotated by three non-clinical annotators. The sentences were extracted from the text field Assessment (Bed¨omning in Swedish). Each sentence and its context from the text field Assessment were presented to the annotators who could use five different annotation classes to annotate the corpora. The annotators had discussions every two days on the previous days’ work led by the experiment leader. As described in Velupillai (2010), the annotation guidelines were inspired by the BioScope Corpus guide"
W10-3102,P05-1045,0,0.0273272,"was annotated by two students and their work was led by a chief annotator. The students were not allowed to discuss their annotations with each other, except at regular meetings, but they were allowed to discuss with the chief annotator. In the cases where the two student annotators agreed on the annotation, that annotation was chosen for the final corpus. In the cases where they did not agree, an annotation made by the chief annotator was chosen. 2.3 The Stanford NER based on CRF The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text. For example in the CoNLL-2003, where the topic was language-independent named entity recognition, Stanford NER CRF was used both on English and German news text for training and evaluation. Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al., 2003). We have used the Stanford NER CRF for training an"
W10-3102,W03-0428,0,0.0208678,"ine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text. For example in the CoNLL-2003, where the topic was language-independent named entity recognition, Stanford NER CRF was used both on English and German news text for training and evaluation. Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al., 2003). We have used the Stanford NER CRF for training and evaluation of our consensus. 2.4 The annotated Swedish clinical corpus for negation and speculation A process to create an annotated clinical corpus for negation and speculation is described in Dalianis and Velupillai (2010). A total of 6 740 randomly extracted sentences from a very large clinical corpus in Swedish were annotated by three non-clinical annotators. The sentences were extracted from the text field Assessment (Bed¨omning in Swedish). Each sentence and its context from the text field Assessment were presented to the annotators wh"
W10-3102,W09-1105,0,0.259687,"es a trial to automatically identify speculative sentences in radiology reports, using Maximum Entropy Models. Advanced feature selection mechanisms were used to automatically extract cue words for speculation from an initial seed set of cues. This, combined with manual selection of the best extracted candidates for cue words, as well as with outer dictionaries of cue words, yielded an F-score of 82.1 percent for detecting speculations in radiology reports. An evaluation was also made on scientific texts, and it could be concluded that cue words for detecting speculation were domain-specific. Morante and Daelemans (2009) describe a machine learning system detecting the scope of nega6 tions, which is based on meta-learning and is trained and tested on the annotated BioScope Corpus. In the clinical part of the corpus, the authors obtained a precision of 100 percent, a recall of 97.5 percent and finally an F-score of 98.8 percent on detection of cue words for negation. The authors used TiMBL (Tilburg Memory Based Learner), which based its decision on features such as the words annotated as negation cues and the two words surrounding them, as well as the part of speech and word forms of these words. For detection"
W10-3102,W10-1103,1,0.827802,"dentical. This will be the result of differences in how the text is interpreted, but also of differences in how the instructions for annotation are 5 Previous studies on detection of negation and speculation in clinical text Clinical text often contains reasoning, and thereby many uncertain or negated expressions. When, for example, searching for patients with a specific symptom in a clinical text, it is thus important to be able to detect if a statement about this symptom is negated, certain or uncertain. The first approach to identifying negations in Swedish clinical text was carried out by Skeppstedt (2010), by whom the well-known NegEx algorithm (Chapman et al., 2001), created for English clinical text, was adapted to Swedish clinical text. Skeppstedt obtained a precision of 70 percent and a recall of 81 percent in identifying negated diseases and symptoms in Swedish clinical text. The NegEx algorithm is purely rule-based, using lists of cue words indicating that a preceding or following disease or symptom is negated. The English version of NegEx (Chapman et al., 2001) obtained a precision of 84.5 percent and a recall of 82.0 percent. 1 This research has been carried out after approval from the"
W10-3102,P08-1033,0,0.313805,"stances annotated for negation. The authors tried several machine learning algorithms for detecting negated findings and diseases, including hidden markov models, conditional random fields and decision trees. The best results were obtained with cascaded decision trees, with nodes consisting of regular expressions for negation patterns. The regular expressions were automatically learnt, using the LCS (longest common subsequence) algorithm on the training data. The cascaded decision trees, built with LCS, gave a precision of 94.4 percent, a recall of 97.4 percent and an F-score of 95.9 percent. Szarvas (2008) describes a trial to automatically identify speculative sentences in radiology reports, using Maximum Entropy Models. Advanced feature selection mechanisms were used to automatically extract cue words for speculation from an initial seed set of cues. This, combined with manual selection of the best extracted candidates for cue words, as well as with outer dictionaries of cue words, yielded an F-score of 82.1 percent for detecting speculations in radiology reports. An evaluation was also made on scientific texts, and it could be concluded that cue words for detecting speculation were domain-sp"
W10-3102,W10-3103,0,0.173127,"nglish and German news text for training and evaluation. Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al., 2003). We have used the Stanford NER CRF for training and evaluation of our consensus. 2.4 The annotated Swedish clinical corpus for negation and speculation A process to create an annotated clinical corpus for negation and speculation is described in Dalianis and Velupillai (2010). A total of 6 740 randomly extracted sentences from a very large clinical corpus in Swedish were annotated by three non-clinical annotators. The sentences were extracted from the text field Assessment (Bed¨omning in Swedish). Each sentence and its context from the text field Assessment were presented to the annotators who could use five different annotation classes to annotate the corpora. The annotators had discussions every two days on the previous days’ work led by the experiment leader. As described in Velupillai (2010), the annotation guidelines were inspired by the BioScope Corpus guide"
W10-3102,W08-0606,0,0.207006,"Missing"
W13-5635,W11-4641,1,0.891833,"Missing"
W13-5635,S12-1035,0,0.0475005,"Missing"
W13-5635,W09-1304,0,0.0546057,"Missing"
W13-5635,J12-2005,0,0.0512417,"Missing"
W13-5635,D10-1070,0,0.0579091,"Missing"
W15-2617,W14-1113,0,0.090256,"Missing"
W16-2926,W01-1703,1,0.568276,"he Regional Ethical Review Board in Stockholm (Etikpr¨ovningsn¨amnden i Stockholm), permission number 2012/1838-31/3. 192 stop words were removed using a stop word list from Python’s natural language toolkit (NLTK)2 . Lowercase conversion and stop word removal are standard preprocessing techniques that simplify the corpus and are usually assumed to not affect the classification (Uysal and Gunal, 2014). Stemming has been shown to work well for many tasks. Specifically for Swedish, that has a rich morphology, it has been shown that stemming improves the results for information retrieval tasks, (Carlberger et al., 2001). Therefore the text in the patient records was stemmed using the Python Snowball stemmer3 for Swedish. encoders. An auto encoder is a type of neural network that first encodes and then decodes input data. Auto encoders are trained to reproduce the data sent through them (Bengio, 2009). The cross entropy cost function was used for the auto encoders in this study. When the sparse auto encoders were stacked, each encoder was trained to reproduce the encoded data of the previous encoder. This unsupervised training constituted the pretraining phase of the network. After the pretraining was complet"
W16-2926,W09-4506,0,0.0423257,"contain a nice overview of over 44 different ap2 Previous research A number of different methods have been used to detect healthcare associated infections using electronic patient records (EPRs). These range from manual methods, methods that use the structured fields of the EPRs, the clinical free text, as well as methods using both. Automatic methods have 191 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 191–195, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics used rules as well as machine learning methods. Proux et al. (Proux et al., 2009; Proux et al., 2011) describe a rule based system for French patient records. Tanushi et al. (2015) describe another rule based system for Swedish patient records. Tanushi et al. detected urinary tract infections on 1,867 care episodes and they obtained a precision of 0.98, a specificity of 0.99 and negative predictive value 0.99, but a recall (sensitivity) of 0.60. One approach used machine learning based systems as SVM and Random Forest on the Stockholm EPR Detect-HAI Corpus in (Ehrentraut et al., 2014), where the authors obtained the best results using Random Forest with precision 0.83, re"
W16-2926,W11-4207,0,0.0659826,"Missing"
W19-6502,N16-1030,0,0.178349,"e corpus is annotated with part-ofspeech tags, morphological analysis, lemma as well as ten named-entity classes. The used classes are person, place, institution, animal, myth2 , product, work, measurements (with 2 EPR 928 923 135 56 500 710 1,021 148 4,421 ates and places and the animal annotation consists of names of animals. The myth annotation consists of names of mythical cre10 but used validation data for early stopping. The LSTM has only been evaluated on the three first folds due to time constraints. 3.2 the development set. The model was then evaluated on the test set. The CRF layer (Lample et al., 2016) was not used as it did not show any benefits for the validation set compared to using only LSTM. Method This study compares the predictive powers for three models based on the data described above. The first model is only trained on data from the Stockholm EPR PHI Corpus, the second model is trained on data from the Stockholm EPR PHI Corpus and the Stockholm EPR Domain Corpus. The last model is trained on the Stockholm EPR PHI Corpus and SUC. All models are evaluated on data from the Stockholm EPR PHI Corpus using ten cross fold-validation. The result is evaluated with micro averaged entity-b"
W19-6503,W15-2604,0,0.02545,"PHI. Personal names are replaced from a list of real names from US Census Bureau having a frequency above 144, meaning 0.004 percent of the data. Gender of personal names are replaced consistently. Combinations of street and street numbers are not reoccurring as in the original corpus. Email addresses are replaced with a set of a random set of characters as the length in the original email address. Meystre et al. (2014) carried out a study where 86 patient records in English were deidentified and where none of the five treating physicians could recognise their patients after deidentification. Grouin et al. (2015) carried out an experiment where they de-identified a group of patient records in French and they asked physicians to identify Related research One of the first attempt in creating surrogates after the DEID process was presented by Sweeney (1996). Dates were replaced with a similar date nearby. Personal names were replaced with a fictitious unique name that sounded reasonable. The article does not mention how the system processed locations and phone numbers and other PHI. In (Douglass et al., 2004), a similar approach is described where dates were shifted by the same random number of weeks or"
W96-0508,P84-1002,0,0.109148,"Missing"
