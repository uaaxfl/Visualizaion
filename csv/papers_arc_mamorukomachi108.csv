2021.wat-1.5,Machine Translation with Pre-specified Target-side Words Using a Semi-autoregressive Model,2021,-1,-1,5,0,306,seiichiro kondo,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"We introduce our TMU Japanese-to-English system, which employs a semi-autoregressive model, to tackle the WAT 2021 restricted translation task. In this task, we translate an input sentence with the constraint that some words, called restricted target vocabularies (RTVs), must be contained in the output sentence. To satisfy this constraint, we use a semi-autoregressive model, namely, RecoverSAT, due to its ability (known as {``}forced translation{''}) to insert specified words into the output sentence. When using {``}forced translation,{''} the order of inserting RTVs is a critical problem. In this work, we aligned the source sentence and the corresponding RTVs using GIZA++. In our system, we obtain word alignment between a source sentence and the corresponding RTVs and then sort the RTVs in the order of their corresponding words or phrases in the source sentence. Using the model with sorted order RTVs, we succeeded in inserting all the RTVs into output sentences in more than 96{\%} of the test sentences. Moreover, we confirmed that sorting RTVs improved the BLEU score compared with random order RTVs."
2021.wat-1.13,{TMU} {NMT} System with {J}apanese {BART} for the Patent task of {WAT} 2021,2021,-1,-1,2,1,343,hwichan kim,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations."
2021.wat-1.20,{TMEKU} System for the {WAT}2021 Multimodal Translation Task,2021,-1,-1,2,1,366,yuting zhao,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use."
2021.naacl-srw.16,Comparison of Grammatical Error Correction Using Back-Translation Models,2021,-1,-1,4,1,307,aomi koyama,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using BT have employed the same architecture for both the GEC and BT models. However, GEC models have different correction tendencies depending on the architecture of their models. Thus, in this study, we compare the correction tendencies of GEC models trained on pseudo data generated by three BT models with different architectures, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. In addition, we investigate the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the performance of each error type compared with using a single BT model with different seeds."
2021.naacl-srw.18,Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation,2021,-1,-1,5,0,306,seiichiro kondo,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Recently, neural machine translation is widely used for its high translation accuracy, but it is also known to show poor performance at long sentence translation. Besides, this tendency appears prominently for low resource languages. We assume that these problems are caused by long sentences being few in the train data. Therefore, we propose a data augmentation method for handling long sentences. Our method is simple; we only use given parallel corpora as train data and generate long sentences by concatenating two sentences. Based on our experiments, we confirm improvements in long sentence translation by proposed data augmentation despite the simplicity. Moreover, the proposed method improves translation quality more when combined with back-translation."
2021.naacl-main.197,From Masked Language Modeling to Translation: Non-{E}nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding,2021,-1,-1,8,0,3851,rob goot,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification."
2021.findings-acl.194,Neural Combinatory Constituency Parsing,2021,-1,-1,4,0,7980,zhousi chen,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.acl-srw.15,Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings,2021,-1,-1,3,0,12448,seiichi inoue,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model."
2020.wmt-1.70,Towards Multimodal Simultaneous Neural Machine Translation,2020,21,0,4,1,3853,aizhan imankulova,Proceedings of the Fifth Conference on Machine Translation,0,"Simultaneous translation involves translating a sentence before the speaker{'}s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages."
2020.wmt-1.120,{TMUOU} Submission for {WMT}20 Quality Estimation Shared Task,2020,-1,-1,4,0,13965,akifumi nakamachi,Proceedings of the Fifth Conference on Machine Translation,0,We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.
2020.wat-1.4,Translation of New Named Entities from {E}nglish to {C}hinese,2020,-1,-1,5,0,14090,zizheng zhang,Proceedings of the 7th Workshop on Asian Translation,0,"New things are being created and new words are constantly being added to languages worldwide. However, it is not practical to translate them all manually into a new foreign language. When translating from an alphabetic language such as English to Chinese, appropriate Chinese characters must be assigned, which is particularly costly compared to other language pairs. Therefore, we propose a task of generating and evaluating new translations from English to Chinese focusing on named entities. We defined three criteria for human evaluation{---}fluency, adequacy of pronunciation, and adequacy of meaning{---}and constructed evaluation data based on these definitions. In addition, we built a baseline system and analyzed the output of the system."
2020.wat-1.7,{TMU} {J}apanese-{E}nglish Multimodal Machine Translation System for {WAT} 2020,2020,-1,-1,4,0,14094,hiroto tamura,Proceedings of the 7th Workshop on Asian Translation,0,"We introduce our TMU system submitted to the Japanese{\textless}-{\textgreater}English Multimodal Task (constrained) for WAT 2020 (Nakazawa et al., 2020). This task aims to improve translation performance with the help of another modality (images) associated with the input sentences. In a multimodal translation task, the dataset is, by its nature, a low-resource one. Our method used herein augments the data by generating noisy translations and adding noise to existing training images. Subsequently, we pretrain a translation model on the augmented noisy data, and then fine-tune it on the clean data. We also examine the probabilistic dropping of either the textual or visual context vector in the decoder. This aims to regularize the network to make use of both features while training. The experimental results indicate that translation performance can be improved using our method of textual data augmentation with noising on the target side and probabilistic dropping of either context vector."
2020.wat-1.15,{K}orean-to-{J}apanese Neural Machine Translation System using Hanja Information,2020,-1,-1,3,1,343,hwichan kim,Proceedings of the 7th Workshop on Asian Translation,0,"In this paper, we describe our TMU neural machine translation (NMT) system submitted for the Patent task (KoreanâJapanese) of the 7th Workshop on Asian Translation (WAT 2020, Nakazawa et al., 2020). We propose a novel method to train a Korean-to-Japanese translation model. Specifically, we focus on the vocabulary overlap of Korean Hanja words and Japanese Kanji words, and propose strategies to leverage Hanja information. Our experiment shows that Hanja information is effective within a specific domain, leading to an improvement in the BLEU scores by +1.09 points compared to the baseline."
2020.paclic-1.61,Neural Machine Translation from Historical {J}apanese to Contemporary {J}apanese Using Diachronically Domain-Adapted Word Embeddings,2020,-1,-1,3,0,15921,masashi takaku,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.nlptea-1.1,Non-Autoregressive Grammatical Error Correction Toward a Writing Support System,2020,-1,-1,2,0,15986,hiroki homma,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,0,"There are several problems in applying grammatical error correction (GEC) to a writing support system. One of them is the handling of sentences in the middle of the input. Till date, the performance of GEC for incomplete sentences is not well-known. Hence, we analyze the performance of each model for incomplete sentences. Another problem is the correction speed. When the speed is slow, the usability of the system is limited, and the user experience is degraded. Therefore, in this study, we also focus on the non-autoregressive (NAR) model, which is a widely studied fast decoding method. We perform GEC in Japanese with traditional autoregressive and recent NAR models and analyze their accuracy and speed."
2020.nlptea-1.11,{TMU}-{NLP} System Using {BERT}-based Pre-trained Model to the {NLP}-{TEA} {CGED} Shared Task 2020,2020,-1,-1,2,0,16014,hongfei wang,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,0,"In this paper, we introduce our system for NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis (CGED). In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we treat the grammar error diagnosis (GED) task as a grammatical error correction (GEC) problem and propose a method that incorporates a pre-trained model into an encoder-decoder model to solve this problem."
2020.ngt-1.15,{E}nglish-to-{J}apanese Diverse Translation by Combining Forward and Backward Outputs,2020,-1,-1,4,1,3204,masahiro kaneko,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"We introduce our TMU system that is submitted to The 4th Workshop on Neural Generation and Translation (WNGT2020) to English-to-Japanese (EnâJa) track on Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task. In most cases machine translation systems generate a single output from the input sentence, however, in order to assist language learners in their journey with better and more diverse feedback, it is helpful to create a machine translation system that is able to produce diverse translations of each input sentence. However, creating such systems would require complex modifications in a model to ensure the diversity of outputs. In this paper, we investigated if it is possible to create such systems in a simple way and whether it can produce desired diverse outputs. In particular, we combined the outputs from forward and backward neural translation models (NMT). Our system achieved third place in EnâJa track, despite adopting only a simple approach."
2020.lrec-1.26,Construction of an Evaluation Corpus for Grammatical Error Correction for Learners of {J}apanese as a Second Language,2020,-1,-1,5,1,307,aomi koyama,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The NAIST Lang-8 Learner Corpora (Lang-8 corpus) is one of the largest second-language learner corpora. The Lang-8 corpus is suitable as a training dataset for machine translation-based grammatical error correction systems. However, it is not suitable as an evaluation dataset because the corrected sentences sometimes include inappropriate sentences. Therefore, we created and released an evaluation corpus for correcting grammatical errors made by learners of Japanese as a Second Language (JSL). As our corpus has less noise and its annotation scheme reflects the characteristics of the dataset, it is ideal as an evaluation corpus for correcting grammatical errors in sentences written by JSL learners. In addition, we applied neural machine translation (NMT) and statistical machine translation (SMT) techniques to correct the grammar of the JSL learners{'} sentences and evaluated their results using our corpus. We also compared the performance of the NMT system with that of the SMT system."
2020.lrec-1.157,Automated Essay Scoring System for Nonnative {J}apanese Learners,2020,-1,-1,5,0,16917,reo hirao,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this study, we created an automated essay scoring (AES) system for nonnative Japanese learners using an essay dataset with annotations for a holistic score and multiple trait scores, including content, organization, and language scores. In particular, we developed AES systems using two different approaches: a feature-based approach and a neural-network-based approach. In the former approach, we used Japanese-specific linguistic features, including character-type features such as {``}kanji{''} and {``}hiragana.{''} In the latter approach, we used two models: a long short-term memory (LSTM) model (Hochreiter and Schmidhuber, 1997) and a bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2019), which achieved the highest accuracy in various natural language processing tasks in 2018. Overall, the BERT model achieved the best root mean squared error and quadratic weighted kappa scores. In addition, we analyzed the robustness of the outputs of the BERT model. We have released and shared this system to facilitate further research on AES for Japanese as a second language learners."
2020.eamt-1.12,Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions,2020,-1,-1,2,1,366,yuting zhao,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the visual modality is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions."
2020.coling-main.193,Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction,2020,-1,-1,3,1,3203,kengo hotate,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this study, we propose a beam search method to obtain diverse outputs in a local sequence transduction task where most of the tokens in the source and target sentences overlap, such as in grammatical error correction (GEC). In GEC, it is advisable to rewrite only the local sequences that must be rewritten while leaving the correct sequences unchanged. However, existing methods of acquiring various outputs focus on revising all tokens of a sentence. Therefore, existing methods may either generate ungrammatical sentences because they force the entire sentence to be changed or produce non-diversified sentences by weakening the constraints to avoid generating ungrammatical sentences. Considering these issues, we propose a method that does not rewrite all the tokens in a text, but only rewrites those parts that need to be diversely corrected. Our beam search method adjusts the search token in the beam according to the probability that the prediction is copied from the source sentence. The experimental results show that our proposed method generates more diverse corrections than existing methods without losing accuracy in the GEC task."
2020.coling-main.415,Cross-lingual Transfer Learning for Grammatical Error Correction,2020,-1,-1,5,0,21505,ikumi yamashita,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this study, we explore cross-lingual transfer learning in grammatical error correction (GEC) tasks. Many languages lack the resources required to train GEC models. Cross-lingual transfer learning from high-resource languages (the source models) is effective for training models of low-resource languages (the target models) for various tasks. However, in GEC tasks, the possibility of transferring grammatical knowledge (e.g., grammatical functions) across languages is not evident. Therefore, we investigate cross-lingual transfer learning methods for GEC. Our results demonstrate that transfer learning from other languages can improve the accuracy of GEC. We also demonstrate that proximity to source languages has a significant impact on the accuracy of correcting certain types of errors."
2020.coling-main.573,{SOME}: Reference-less Sub-Metrics Optimized for Manual Evaluations of Grammatical Error Correction,2020,-1,-1,4,0,21695,ryoma yoshimura,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a reference-less metric trained on manual evaluations of system outputs for grammatical error correction (GEC). Previous studies have shown that reference-less metrics are promising; however, existing metrics are not optimized for manual evaluations of the system outputs because no dataset of the system output exists with manual evaluation. This study manually evaluates outputs of GEC systems to optimize the metrics. Experimental results show that the proposed metric improves correlation with the manual evaluation in both system- and sentence-level meta-evaluation. Our dataset and metric will be made publicly available."
2020.acl-srw.5,Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner{'}s Error Tendency,2020,-1,-1,3,0,22485,yujin takahashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Recently, several studies have focused on improving the performance of grammatical error correction (GEC) tasks using pseudo data. However, a large amount of pseudo data are required to train an accurate GEC model. To address the limitations of language and computational resources, we assume that introducing pseudo errors into sentences similar to those written by the language learners is more efficient, rather than incorporating random pseudo errors into monolingual data. In this regard, we study the effect of pseudo data on GEC task performance using two approaches. First, we extract sentences that are similar to the learners{'} sentences from monolingual data. Second, we generate realistic pseudo errors by considering error types that learners often make. Based on our comparative results, we observe that F0.5 scores for the Russian GEC task are significantly improved."
2020.acl-srw.11,Zero-shot {N}orth {K}orean to {E}nglish Neural Machine Translation by Character Tokenization and Phoneme Decomposition,2020,-1,-1,3,1,343,hwichan kim,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes.We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline."
2020.aacl-srw.10,Towards a Standardized Dataset on {I}ndonesian Named Entity Recognition,2020,-1,-1,3,0,3857,siti khairunnisa,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"In recent years, named entity recognition (NER) tasks in the Indonesian language have undergone extensive development. There are only a few corpora for Indonesian NER; hence, recent Indonesian NER studies have used diverse datasets. Although an open dataset is available, it includes only approximately 2,000 sentences and contains inconsistent annotations, thereby preventing accurate training of NER models without reliance on pre-trained models. Therefore, we re-annotated the dataset and compared the two annotations{'} performance using the Bidirectional Long Short-Term Memory and Conditional Random Field (BiLSTM-CRF) approach. Fixing the annotation yielded a more consistent result for the organization tag and improved the prediction score by a large margin. Moreover, to take full advantage of pre-trained models, we compared different feature embeddings to determine their impact on the NER task for the Indonesian language."
2020.aacl-main.20,{C}hinese Grammatical Correction Using {BERT}-based Pre-trained Model,2020,-1,-1,4,0,16014,hongfei wang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we verify the effectiveness of two methods that incorporate a pre-trained model into an encoder-decoder model on Chinese grammatical error correction tasks. We also analyze the error type and conclude that sentence-level errors are yet to be addressed."
2020.aacl-main.83,Stronger Baselines for Grammatical Error Correction Using a Pretrained Encoder-Decoder Model,2020,23,0,2,1,16918,satoru katsumata,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Studies on grammatical error correction (GEC) have reported on the effectiveness of pretraining a Seq2Seq model with a large amount of pseudodata. However, this approach requires time-consuming pretraining of GEC because of the size of the pseudodata. In this study, we explored the utility of bidirectional and auto-regressive transformers (BART) as a generic pretrained encoder-decoder model for GEC. With the use of this generic pretrained model for GEC, the time-consuming pretraining can be eliminated. We find that monolingual and multilingual BART models achieve high performance in GEC, with one of the results being comparable to the current strong results in English GEC."
W19-6604,Debiasing Word Embeddings Improves Multimodal Machine Translation,2019,25,0,2,1,309,tosho hirasawa,Proceedings of Machine Translation Summit XVII: Research Track,0,"In recent years, pretrained word embeddings have proved useful for multimodal neural machine translation (NMT) models to address the shortage of available datasets. However, the integration of pretrained word embeddings has not yet been explored extensively. Further, pretrained word embeddings in high dimensional spaces have been reported to suffer from the hubness problem. Although some debiasing techniques have been proposed to address this problem for other natural language processing tasks, they have seldom been studied for multimodal NMT models. In this study, we examine various kinds of word embeddings and introduce two debiasing techniques for three multimodal NMT models and two language pairs -- English-German translation and English-French translation. With our optimal settings, the overall performance of multimodal models was improved by up to 1.93 BLEU and 2.02 METEOR for English-German translation and 1.73 BLEU and 0.95 METEOR for English-French translation."
W19-5360,Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation,2019,0,4,5,0,21695,ryoma yoshimura,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of sentence BLEU using filtered pseudo-references. We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by paraphrasing in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references."
W19-4413,(Almost) Unsupervised Grammatical Error Correction using Synthetic Comparable Corpus,2019,0,1,2,1,16918,satoru katsumata,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at BEA2019. As a result, we achieved an F0.5 score of 28.31 points with the test data."
W19-4422,{TMU} Transformer System Using {BERT} for Re-ranking at {BEA} 2019 Grammatical Error Correction on Restricted Track,2019,0,2,4,1,3204,masahiro kaneko,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction1 (GEC). It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model. A re-ranker can evaluate the naturalness of a corrected sentence using language models trained on large corpora. On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners. Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner{'}s grammatical errors. Therefore, we propose to fine-tune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W{\&}I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance."
W19-4431,Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of {J}apanese as a Second Language,2019,0,0,3,1,16641,mio arai,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner{'}s writing."
P19-3001,{S}akura: Large-scale Incorrect Example Retrieval System for Learners of {J}apanese as a Second Language,2019,0,0,3,1,16641,mio arai,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This study develops an incorrect example retrieval system, called Sakura, using a large-scale Lang-8 dataset for Japanese language learners. Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. If a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners can revise their composition themselves. Considering the usability of retrieving incorrect examples, our proposed system uses a large-scale corpus to expand the coverage of incorrect examples and presents correct expressions along with incorrect expressions. Our intrinsic and extrinsic evaluations indicate that our system is more useful than a previous system."
P19-2020,Controlling Grammatical Error Correction Using Word Edit Rate,2019,0,0,4,1,3203,kengo hotate,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"When professional English teachers correct grammatically erroneous sentences written by English learners, they use various methods. The correction method depends on how much corrections a learner requires. In this paper, we propose a method for neural grammar error correction (GEC) that can control the degree of correction. We show that it is possible to actually control the degree of GEC by using new training data annotated with word edit rate. Thereby, diverse corrected sentences is obtained from a single erroneous sentence. Moreover, compared to a GEC model that does not use information on the degree of correction, the proposed method improves correction accuracy."
N19-3012,Multimodal Machine Translation with Embedding Prediction,2019,16,0,4,1,309,tosho hirasawa,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand visual objects and their relations with natural languages. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 F-score for rare word translation."
N19-1344,Multi-Task Learning for {J}apanese Predicate Argument Structure Analysis,2019,0,0,2,0,26257,hikaru omori,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"An event-noun is a noun that has an argument structure similar to a predicate. Recent works, including those considered state-of-the-art, ignore event-nouns or build a single model for solving both Japanese predicate argument structure analysis (PASA) and event-noun argument structure analysis (ENASA). However, because there are interactions between predicates and event-nouns, it is not sufficient to target only predicates. To address this problem, we present a multi-task learning method for PASA and ENASA. Our multi-task models improved the performance of both tasks compared to a single-task model by sharing knowledge from each task. Moreover, in PASA, our models achieved state-of-the-art results in overall F1 scores on the NAIST Text Corpus. In addition, this is the first work to employ neural networks in ENASA."
D19-5221,{J}apanese-{R}ussian {TMU} Neural Machine Translation System using Multilingual Model for {WAT} 2019,2019,0,0,3,1,3853,aizhan imankulova,Proceedings of the 6th Workshop on Asian Translation,0,"We introduce our system that is submitted to the News Commentary task (Japanese{\textless}-{\textgreater}Russian) of the 6th Workshop on Asian Translation. The goal of this shared task is to study extremely low resource situations for distant language pairs. It is known that using parallel corpora of different language pair as training data is effective for multilingual neural machine translation model in extremely low resource scenarios. Therefore, to improve the translation quality of Japanese{\textless}-{\textgreater}Russian language pair, our method leverages other in-domain Japanese-English and English-Russian parallel corpora as additional training data for our multilingual NMT model."
Y18-3008,{TMU} {J}apanese-{C}hinese Unsupervised {NMT} System for {WAT} 2018 Translation Task,2018,0,0,3,1,7981,longtu zhang,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3014,{TMU} {J}apanese-{E}nglish Neural Machine Translation System using Generative Adversarial Network for {WAT} 2018,2018,0,0,3,1,23890,yukio matsumura,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-1033,Long Short-Term Memory for {J}apanese Word Segmentation,2018,-1,-1,2,0,27505,yoshiaki kitagawa,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1035,The Rule of Three: Abstractive Text Summarization in Three Bullet Points,2018,0,0,2,1,25432,tomonori kodaira,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,"Neural network-based approaches have become widespread for abstractive text summarization. Though previously proposed models for abstractive text summarization addressed the problem of repetition of the same contents in the summary, they did not explicitly consider its information structure. One of the reasons these previous models failed to account for information structure in the generated summary is that standard datasets include summaries of variable lengths, resulting in problems in analyzing information flow, specifically, the manner in which the first sentence is related to the following sentences. Therefore, we use a dataset containing summaries with only three bullet points, and propose a neural network-based abstractive summarization model that considers the information structures of the generated summaries. Our experimental results show that the information structure of a summary can be controlled, thus improving the performance of the overall summarization."
Y18-1054,{J}apanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention,2018,-1,-1,2,0,27523,ryosuke miyazaki,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-6456,{RUSE}: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation,2018,0,14,3,1,13966,hiroki shimanaka,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that cannot be captured by local features based on character or word N-grams. Although training sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only."
W18-6303,Neural Machine Translation of Logographic Language Using Sub-character Level Information,2018,1,0,2,1,7981,longtu zhang,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units. However, important differences between languages with logographic and alphabetic writing systems have long been overlooked. This study focuses on these differences and uses a simple approach to improve the performance of NMT systems utilizing decomposed sub-character level information for logographic languages. Our results indicate that our approach not only improves the translation capabilities of NMT systems between Chinese and English, but also further improves NMT systems between Chinese and Japanese, because it utilizes the shared information brought by similar sub-character units."
W18-0521,Complex Word Identification Based on Frequency in a Learner Corpus,2018,0,4,2,1,367,tomoyuki kajiwara,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We introduce the TMU systems for the Complex Word Identification (CWI) Shared Task 2018. TMU systems use random forest classifiers and regressors whose features are the number of characters, the number of words, and the frequency of target words in various corpora. Our simple systems performed best on 5 tracks out of 12 tracks. Our ablation analysis revealed the usefulness of a learner corpus for CWI task."
W18-0544,{TMU} System for {SLAM}-2018,2018,0,1,3,1,3204,masahiro kaneko,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We introduce the TMU systems for the second language acquisition modeling shared task 2018 (Settles et al., 2018). To model learner error patterns, it is necessary to maintain a considerable amount of information regarding the type of exercises learners have been learning in the past and the manner in which they answered them. Tracking an enormous learner{'}s learning history and their correct and mistaken answers is essential to predict the learner{'}s future mistakes. Therefore, we propose a model which tracks the learner{'}s learning history efficiently. Our systems ranked fourth in the English and Spanish subtasks, and fifth in the French subtask."
P18-3016,Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models,2018,21,0,4,1,16918,satoru katsumata,"Proceedings of {ACL} 2018, Student Research Workshop",0,"Encoder-decoder models typically only employ words that are frequently used in the training corpus because of the computational costs and/or to exclude noisy words. However, this vocabulary set may still include words that interfere with learning in encoder-decoder models. This paper proposes a method for selecting more suitable words for learning encoders by utilizing not only frequency, but also co-occurrence information, which we capture using the HITS algorithm. The proposed method is applied to two tasks: machine translation and grammatical error correction. For Japanese-to-English translation, this method achieved a BLEU score that was 0.56 points more than that of a baseline. It also outperformed the baseline method for English grammatical error correction, with an F-measure that was 1.48 points higher."
N18-4014,{J}apanese Predicate Conjugation for Neural Machine Translation,2018,4,0,4,0,23227,michiki kurosawa,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Neural machine translation (NMT) has a drawback in that can generate only high-frequency words owing to the computational costs of the softmax function in the output layer. In Japanese-English NMT, Japanese predicate conjugation causes an increase in vocabulary size. For example, one verb can have as many as 19 surface varieties. In this research, we focus on predicate conjugation for compressing the vocabulary size in Japanese. The vocabulary list is filled with the various forms of verbs. We propose methods using predicate conjugation information without discarding linguistic information. The proposed methods can generate low-frequency words and deal with unknown words. Two methods were considered to introduce conjugation information: the first considers it as a token (conjugation token) and the second considers it as an embedded vector (conjugation feature). The results using these methods demonstrate that the vocabulary size can be compressed by approximately 86.1{\%} (Tanaka corpus) and the NMT models can output the words not in the training data set. Furthermore, BLEU scores improved by 0.91 points in Japanese-to-English translation, and 0.32 points in English-to-Japanese translation with ASPEC."
N18-4015,Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations,2018,18,0,3,1,13966,hiroki shimanaka,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only."
L18-1152,Construction of a {J}apanese Word Similarity Dataset,2018,-1,-1,2,0,29667,yuya sakaizawa,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5911,Suggesting Sentences for {ESL} using Kernel Embeddings,2017,0,0,2,0,31447,kent shioda,Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017),0,"Sentence retrieval is an important NLP application for English as a Second Language (ESL) learners. ESL learners are familiar with web search engines, but generic web search results may not be adequate for composing documents in a specific domain. However, if we build our own search system specialized to a domain, it may be subject to the data sparseness problem. Recently proposed word2vec partially addresses the data sparseness problem, but fails to extract sentences relevant to queries owing to the modeling of the latent intent of the query. Thus, we propose a method of retrieving example sentences using kernel embeddings and N-gram windows. This method implicitly models latent intent of query and sentences, and alleviates the problem of noisy alignment. Our results show that our method achieved higher precision in sentence retrieval for ESL in the domain of a university press release corpus, as compared to a previous unsupervised method used for a semantic textual similarity task."
W17-5703,Improving {J}apanese-to-{E}nglish Neural Machine Translation by Paraphrasing the Target Language,2017,5,4,3,0,31472,yuuki sekizawa,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"Neural machine translation (NMT) produces sentences that are more fluent than those produced by statistical machine translation (SMT). However, NMT has a very high computational cost because of the high dimensionality of the output layer. Generally, NMT restricts the size of vocabulary, which results in infrequent words being treated as out-of-vocabulary (OOV) and degrades the performance of the translation. In evaluation, we achieved a statistically significant BLEU score improvement of 0.55-0.77 over the baselines including the state-of-the-art method."
W17-5704,Improving Low-Resource Neural Machine Translation with Filtered Pseudo-Parallel Corpus,2017,14,3,3,1,3853,aizhan imankulova,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"Large-scale parallel corpora are indispensable to train highly accurate machine translators. However, manually constructed large-scale parallel corpora are not freely available in many language pairs. In previous studies, training data have been expanded using a pseudo-parallel corpus obtained using machine translation of the monolingual corpus in the target language. However, in low-resource language pairs in which only low-accuracy machine translation systems can be used, translation quality is reduces when a pseudo-parallel corpus is used naively. To improve machine translation performance with low-resource language pairs, we propose a method to expand the training data effectively via filtering the pseudo-parallel corpus using a quality estimation based on back-translation. As a result of experiments with three language pairs using small, medium, and large parallel corpora, language pairs with fewer training data filtered out more sentence pairs and improved BLEU scores more significantly."
W17-5716,{T}okyo Metropolitan University Neural Machine Translation System for {WAT} 2017,2017,0,1,2,1,23890,yukio matsumura,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT and uses long short-term memories (LSTM) as RNN. We implemented beam search and ensemble decoding in the NMT system. The system was tested on the 4th Workshop on Asian Translation (WAT 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of beam search and ensemble decoding can effectively improve the translation quality."
P17-3007,Building a Non-Trivial Paraphrase Corpus Using Multiple Machine Translation Systems,2017,18,5,3,0,32538,yui suzuki,"Proceedings of {ACL} 2017, Student Research Workshop",0,None
I17-2047,Improving {J}apanese-to-{E}nglish Neural Machine Translation by Voice Prediction,2017,6,1,4,1,23891,hayahide yamagishi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This study reports an attempt to predict the voice of reference using the information from the input sentences or previous input/output sentences. Our previous study presented a voice controlling method to generate sentences for neural machine translation, wherein it was demonstrated that the BLEU score improved when the voice of generated sentence was controlled relative to that of the reference. However, it is impractical to use the reference information because we cannot discern the voice of the correct translation in advance. Thus, this study presents a voice prediction method for generated sentences for neural machine translation. While evaluating on Japanese-to-English translation, we obtain a 0.70-improvement in the BLEU using the predicted voice."
I17-1005,Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings,2017,0,3,3,1,3204,masahiro kaneko,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this study, we improve grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Most existing algorithms for learning word embeddings usually model only the syntactic context of words so that classifiers treat erroneous and correct words as similar inputs. We address the problem of contextual information by considering learner errors. Specifically, we propose two models: one model that employs grammatical error patterns and another model that considers grammaticality of the target word. We determine grammaticality of n-gram sequence from the annotated error tags and extract grammatical error patterns for word embeddings from large-scale learner corpora. Experimental results show that a bidirectional long-short term memory model initialized by our word embeddings achieved the state-of-the-art accuracy by a large margin in an English grammatical error detection task on the First Certificate in English dataset."
I17-1009,{MIPA}: Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting,2017,19,1,2,1,367,tomoyuki kajiwara,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"We present a pointwise mutual information (PMI)-based approach to formalize paraphrasability and propose a variant of PMI, called MIPA, for the paraphrase acquisition. Our paraphrase acquisition method first acquires lexical paraphrase pairs by bilingual pivoting and then reranks them by PMI and distributional similarity. The complementary nature of information from bilingual corpora and from monolingual corpora makes the proposed method robust. Experimental results show that the proposed method substantially outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as MRR, MAP, coverage, and Spearman{'}s correlation."
W16-4603,{J}apanese-{E}nglish Machine Translation of Recipe Texts,2016,18,1,3,1,31473,takayuki sato,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"Concomitant with the globalization of food culture, demand for the recipes of specialty dishes has been increasing. The recent growth in recipe sharing websites and food blogs has resulted in numerous recipe texts being available for diverse foods in various languages. However, little work has been done on machine translation of recipe texts. In this paper, we address the task of translating recipes and investigate the advantages and disadvantages of traditional phrase-based statistical machine translation and more recent neural machine translation. Specifically, we translate Japanese recipes into English, analyze errors in the translated recipes, and discuss available room for improvements."
W16-4607,Neural Reordering Model Considering Phrase Translation and Word Alignment for Phrase-based Translation,2016,13,2,3,1,23269,shin kanouchi,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper presents an improved lexicalized reordering model for phrase-based statistical machine translation using a deep neural network. Lexicalized reordering suffers from reordering ambiguity, data sparseness and noises in a phrase table. Previous neural reordering model is successful to solve the first and second problems but fails to address the third one. Therefore, we propose new features using phrase translation and word alignment to construct phrase vectors to handle inherently noisy phrase translation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments."
W16-4620,Controlling the Voice of a Sentence in {J}apanese-to-{E}nglish Neural Machine Translation,2016,8,11,4,1,23891,hayahide yamagishi,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"In machine translation, we must consider the difference in expression between languages. For example, the active/passive voice may change in Japanese-English translation. The same verb in Japanese may be translated into different voices at each translation because the voice of a generated sentence cannot be determined using only the information of the Japanese sentence. Machine translation systems should consider the information structure to improve the coherence of the output by using several topicalization techniques such as passivization. Therefore, this paper reports on our attempt to control the voice of the sentence generated by an encoder-decoder model. To control the voice of the generated sentence, we added the voice information of the target sentence to the source sentence during the training. We then generated sentences with a specified voice by appending the voice information to the source sentence. We observed experimentally whether the voice could be controlled. The results showed that, we could control the voice of the generated sentence with 85.0{\%} accuracy on average. In the evaluation of Japanese-English translation, we obtained a 0.73-point improvement in BLEU score by using gold voice labels."
W16-3906,Disaster Analysis using User-Generated Weather Report,2016,6,1,3,0,33651,yasunobu asakura,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Information extraction from user-generated text has gained much attention with the growth of the Web.Disaster analysis using information from social media provides valuable, real-time, geolocation information for helping people caught up these in disasters. However, it is not convenient to analyze texts posted on social media because disaster keywords match any texts that contain words. For collecting posts about a disaster from social media, we need to develop a classifier to filter posts irrelevant to disasters. Moreover, because of the nature of social media, we can take advantage of posts that come with GPS information. However, a post does not always refer to an event occurring at the place where it has been posted. Therefore, we propose a new task of classifying whether a flood disaster occurred, in addition to predicting the geolocation of events from user-generated text. We report the annotation of the flood disaster corpus and develop a classifier to demonstrate the use of this corpus for disaster analysis."
P16-3001,Controlled and Balanced Dataset for {J}apanese Lexical Simplification,2016,8,5,3,1,25432,tomonori kodaira,Proceedings of the {ACL} 2016 Student Research Workshop,0,"We propose a new dataset for evaluating a Japanese lexical simplification method. Previous datasets have several deficiencies. All of them substitute only a single target word, and some of them extract sentences only from newswire corpus. In addition, most of these datasets do not allow ties and integrate simplification ranking from all the annotators without considering the quality. In contrast, our dataset has the following advantages: (1) it is the first controlled and balanced dataset for Japanese lexical simplification with high correlation with human judgment and (2) the consistency of the simplification ranking is improved by allowing candidates to have ties and by considering the reliability of annotators."
L16-1060,Analysis of {E}nglish Spelling Errors in a Word-Typing Game,2016,6,1,2,0,33854,ryuichi tachibana,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The emergence of the web has necessitated the need to detect and correct noisy consumer-generated texts. Most of the previous studies on English spelling-error extraction collected English spelling errors from web services such as Twitter by using the edit distance or from input logs utilizing crowdsourcing. However, in the former approach, it is not clear which word corresponds to the spelling error, and the latter approach requires an annotation cost for the crowdsourcing. One notable exception is Rodrigues and Rytting (2012), who proposed to extract English spelling errors by using a word-typing game. Their approach saves the cost of crowdsourcing, and guarantees an exact alignment between the word and the spelling error. However, they did not assert whether the extracted spelling error corpora reflect the usual writing process such as writing a document. Therefore, we propose a new correctable word-typing game that is more similar to the actual writing process. Experimental results showed that we can regard typing-game logs as a source of spelling errors."
C16-1109,Building a Monolingual Parallel Corpus for Text Simplification Using Sentence Similarity Based on Alignment between Word Embeddings,2016,10,7,2,1,367,tomoyuki kajiwara,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Methods for text simplification using the framework of statistical machine translation have been extensively studied in recent years. However, building the monolingual parallel corpus necessary for training the model requires costly human annotation. Monolingual parallel corpora for text simplification have therefore been built only for a limited number of languages, such as English and Portuguese. To obviate the need for human annotation, we propose an unsupervised method that automatically builds the monolingual parallel corpus for text simplification using sentence similarity based on word embeddings. For any sentence pair comprising a complex sentence and its simple counterpart, we employ a many-to-one method of aligning each word in the complex sentence with the most similar word in the simple sentence and compute sentence similarity by averaging these word similarities. The experimental results demonstrate the excellent performance of the proposed method in a monolingual parallel corpus construction task for English text simplification. The results also demonstrated the superior accuracy in text simplification that use the framework of statistical machine translation trained using the corpus built by the proposed method to that using the existing corpora."
Y15-1018,{J}apanese Sentiment Classification with Stacked Denoising Auto-Encoder using Distributed Word Representation,2015,28,2,2,0,4799,peinan zhang,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Traditional sentiment classification methods often require polarity dictionaries or crafted features to utilize machine learning. However, those approaches incur high costs in the making of dictionaries and/or features, which hinder generalization of tasks. Examples of these approaches include an approach that uses a polarity dictionary that cannot handle unknown or newly invented words and another approach that uses a complex model with 13 types of feature templates. We propose a novel high performance sentiment classification method with stacked denoising auto-encoders that uses distributed word representation instead of building dictionaries or utilizing engineering features. The results of experiments conducted indicate that our model achieves state-of-the-art performance in Japanese sentiment classification tasks."
W15-5013,Source Phrase Segmentation and Translation for {J}apanese-{E}nglish Translation Using Dependency Structure,2015,6,1,3,0,36545,junki matsuo,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-4417,Improving {C}hinese Grammatical Error Correction with Corpus Augmentation and Hierarchical Phrase-based Statistical Machine Translation,2015,14,3,2,0,36688,yinchen zhao,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"In this study, we describe our system submitted to the 2nd Workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA-2) shared task on Chinese grammatical error diagnosis (CGED). We use a statistical machine translation method already applied to several similar tasks (Brockett et al., 2006; Chiu et al., 2013; Zhao et al., 2014). In this research, we examine corpus-augmentation and explore alternative translation models including syntaxbased and hierarchical phrase-based models. Finally, we show variations using different combinations of these factors."
P15-3005,Disease Event Detection based on Deep Modality Analysis,2015,19,1,2,0,27505,yoshiaki kitagawa,Proceedings of the {ACL}-{IJCNLP} 2015 Student Research Workshop,0,"Social media has attracted attention because of its potential for extraction of information of various types. For example, information collected from Twitter enables us to build useful applications such as predicting an epidemic of influenza. However, using text information from social media poses challenges for event detection because of the unreliable nature of user-generated texts, which often include counter-factual statements. Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or xe2x80x9ctweetsxe2x80x9d. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points."
P15-1160,Who caught a cold ? - Identifying the subject of a symptom,2015,39,7,2,1,23269,shin kanouchi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The development and proliferation of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recognizing diseases and symptoms from text messages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a subject (carrier) of a disease or symptommentioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing corpus for public surveillance. In addition, we present a supervised approach for predicting the subject of a disease/symptom. The results of our experiments demonstrate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom."
W14-7006,Predicate-Argument Structure-based Preordering for {J}apanese-{E}nglish Statistical Machine Translation of Scientific Papers,2014,0,0,3,0,36546,kenichi ohwada,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
Y13-1014,Towards Automatic Error Type Classification of {J}apanese Language Learners{'} Writings,2013,13,2,2,0,40460,hiromi oyama,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Learner corpora are receiving special attention as an invaluable source of educational feedback and are expected to improve teaching materials and methodology. However, they include various types of incorrect sentences. Error type classification is an important task in learner corpora which enables clarifying for learners why a certain sentence is classified as incorrect in order to help learners not to repeat errors. To address this issue, we defined a set of error type criteria and conducted automatic classification of errors into error types in the sentences from the NAIST Goyo Corpus and achieved an accuracy of 77.6%. We also tried inter-corpus evaluation of our system on the Lang-8 corpus of learner Japanese and achieved an accuracy of 42.3%. To know the accuracy, we also investigated the classification method by human judgement and compared the difference in classification between the machine and the human."
W13-3604,{NAIST} at 2013 {C}o{NLL} Grammatical Error Correction Shared Task,2013,15,19,7,0,40836,ippei yoshimoto,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the CoNLL 2013 Shared Task. We constructed three systems: a system based on the Treelet Language Model for verb form and subjectverb agreement errors; a classifier trained on both learner and native corpora for noun number errors; a statistical machine translation (SMT)-based model for preposition and determiner errors. As for subject-verb agreement errors, we show that the Treelet Language Model-based approach can correct errors in which the target verb is distant from its subject. Our system ranked fourth on the official run."
W13-1717,{NAIST} at the {NLI} 2013 Shared Task,2013,3,6,4,1,22506,tomoya mizumoto,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper describes the Nara Institute of Science and Technology (NAIST) native language identification (NLI) system in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2."
P13-2043,Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners,2013,9,22,3,1,6885,keisuke sakaguchi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learnersxe2x80x99 corpus. Unlike previous studies, the proposed methods aim at satisfying both reliability and validity of generated distractors; distractors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learnersxe2x80x99 proficiency. Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods."
P13-2124,A Learner Corpus-based Approach to Verb Suggestion for {ESL},2013,12,4,2,0,10404,yu sawai,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion."
W12-2033,{NAIST} at the {HOO} 2012 Shared Task,2012,17,7,6,1,6885,keisuke sakaguchi,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the Helping Our Own (HOO) 2012 Shared Task. Our system targets preposition and determiner errors with spelling correction as a pre-processing step. The result shows that spelling correction improves the Detection, Correction, and Recognition F-scores for preposition errors. With regard to preposition error correction, F-scores were not improved when using the training set with correction of all but preposition errors. As for determiner error correction, there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed. Our system ranked third in preposition and fourth in determiner error corrections."
P12-2039,Tense and Aspect Error Correction for {ESL} Learners Using Global Context,2012,10,48,2,0,42662,toshikazu tajiri,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"As the number of learners of English is constantly growing, automatic error correction of ESL learners' writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction."
ogiso-etal-2012-unidic,{U}ni{D}ic for Early Middle {J}apanese: a Dictionary for Morphological Analysis of Classical {J}apanese,2012,4,6,2,0,18343,toshinobu ogiso,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In order to construct an annotated diachronic corpus of Japanese, we propose to create a new dictionary for morphological analysis of Early Middle Japanese (Classical Japanese) based on UniDic, a dictionary for Contemporary Japanese. Differences between the Early Middle Japanese and Contemporary Japanese, which prevent a na{\""\i}ve adaptation of UniDic to Early Middle Japanese, are found at the levels of lexicon, morphology, grammar, orthography and pronunciation. In order to overcome these problems, we extended dictionary entries and created a training corpus of Early Middle Japanese to adapt UniDic for Contemporary Japanese to Early Middle Japanese. Experimental results show that the proposed UniDic-EMJ, a new dictionary for Early Middle Japanese, achieves as high accuracy (97{\%}) as needed for the linguistic research on lexicon and grammar in Japanese classical text analysis."
C12-2084,The Effect of Learner Corpus Size in Grammatical Error Correction of {ESL} Writings,2012,21,27,3,1,22506,tomoya mizumoto,Proceedings of {COLING} 2012: Posters,0,"English as a Second Language (ESL) learnersxe2x80x99 writings contain various grammatical errors. Previous research on automatic error correction for ESL learnersxe2x80x99 grammatical errors deals with restricted types of learnersxe2x80x99 errors. Some types of errors can be corrected by rules using heuristics, while others are difficult to correct without statistical models using native corpora and/or learner corpora. Since adding error annotation to learnersxe2x80x99 text is time-consuming, it was not until recently that large scale learner corpora became publicly available. However, little is known about the effect of learner corpus size in ESL grammatical error correction. Thus, in this paper, we investigate the effect of learner corpus size on various types of grammatical errors, using an error correction system based on phrase-based statistical machine translation (SMT) trained on a large scale errortagged learner corpus. We show that the phrase-based SMT approach is effective in correcting frequent errors that can be identified by local context, and that it is difficult for phrase-based SMT to correct errors that need long range contextual information."
C12-1144,Joint {E}nglish Spelling Error Correction and {POS} Tagging for Language Learners Writing,2012,31,2,3,1,6885,keisuke sakaguchi,Proceedings of {COLING} 2012,0,"We propose an approach to correcting spelling errors and assigning part-of-speech (POS) tags simultaneously for sentences written by learners of English as a second language (ESL). In ESL writing, there are several types of errors such as preposition, determiner, verb, noun, and spelling errors. Spelling errors often interfere with POS tagging and syntactic parsing, which makes other error detection and correction tasks very difficult. In studies of grammatical error detection and correction in ESL writing, spelling correction has been regarded as a preprocessing step in a pipeline. However, several types of spelling errors in ESL are difficult to correct in the preprocessing, for example, homophones (e.g. *hear/here), confusion (*quiet/quite), split (*now a day/nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased) and derivation (*badly/bad), where the incorrect word is actually in the vocabulary and grammatical information is needed to disambiguate. In order to correct these spelling errors, and also typical typographical errors (*begginning/beginning), we propose a joint analysis of POS tagging and spelling error correction with a CRF (Conditional Random Field)-based model. We present an approach that achieves significantly better accuracies for both POS tagging and spelling correction, compared to existing approaches using either individual or pipeline analysis. We also show that the joint model can deal with novel types of misspelling in ESL writing."
W11-3506,Error Correcting Romaji-kana Conversion for {J}apanese Language Education,2011,8,3,2,0,44093,seiji kasahara,Proceedings of the Workshop on Advances in Text Input Methods ({WTIM} 2011),0,We present an approach to help editors of Japanese on a language learning SNS correct learnersxe2x80x99 sentences written in Roman characters by converting them into kana. Our system detects foreign words and converts only Japanese words even if they contain spelling errors. Experimental results show that our system achieves about 10 points higher conversion accuracy than traditional input method (IM). Error analysis reveals some tendencies of the errors specific to language learners.
W11-1913,Narrative Schema as World Knowledge for Coreference Resolution,2011,14,9,2,0,36951,joseph irwin,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system's performance."
W11-0318,Using the Mutual k-Nearest Neighbor Graphs for Semi-supervised Classification on Natural Language Data,2011,28,49,3,0,44419,kohei ozaki,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"The first step in graph-based semi-supervised classification is to construct a graph from input data. While the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data. To compare the performance of these two graph construction methods, we run semi-supervised classification methods on both graphs in word sense disambiguation and document classification tasks. The experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the k-nearest neighbor graphs. We attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices. The mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity."
P11-2006,{HITS}-based Seed Selection and Stop List Construction for Bootstrapping,2011,28,6,3,0,43833,tetsuo kiso,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graph-based approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti's Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg's HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method."
I11-1017,Mining Revision Log of Language Learning {SNS} for Automated {J}apanese Error Correction of Second Language Learners,2011,12,78,2,1,22506,tomoya mizumoto,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present an attempt to extract a largescale Japanese learnersxe2x80x99 corpus from the revision log of a language learning SNS. This corpus is easy to obtain in largescale, covers a wide variety of topics and styles, and can be a great source of knowledge for both language learners and instructors. We also demonstrate that the extracted learnersxe2x80x99 corpus of Japanese as a second language can be used as training data for learnersxe2x80x99 error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model."
I11-1023,{J}apanese Predicate Argument Structure Analysis Exploiting Argument Position and Type,2011,15,14,2,1,13659,yuta hayashibe,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We propose an approach to Japanese predicate argument structure analysis exploiting argument position and type. In particular, we propose the following two methods. First, in order to use information in the sentences in preceding context of the predicate more effectively, we propose an improved similarity measure between argument positions which is more robust than a previous co-reference-based measure. Second, we propose a flexible selection-and-classification approach which accounts for the minor types of arguments. Experimental results show that our proposed method achieves state-ofthe-art accuracy for Japanese predicate argument structure analysis."
I11-1033,Automatic Labeling of Voiced Consonants for Morphological Analysis of {M}odern {J}apanese Literature,2011,7,0,2,0,18339,teruaki oka,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Since the present-day Japanese use of voiced consonant mark had established in the Meiji Era, modern Japanese literary text written in the Meiji Era often lacks compulsory voiced consonant marks. This deteriorates the performance of morphological analyzers using ordinary dictionary. In this paper, we propose an approach for automatic labeling of voiced consonant marks for modern literary Japanese. We formulate the task into a binary classification problem. Our pointwise prediction method uses as its feature set only surface information about the surrounding character strings. As a consequence, training corpus is easy to obtain and maintain because we can exploit a partially annotated corpus for learning. We compared our proposed method as a preprocessing step for morphological analysis with a dictionary-based approach, and confirmed that pointwise prediction outperforms dictionary-based approach by a large margin."
I11-1046,{J}apanese Abbreviation Expansion with Query and Clickthrough Logs,2011,28,0,2,0,7543,kei uchiumi,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"A novel reranking method has been developed to refine web search queries. A label propagation algorithm was applied on a clickthrough graph, and the candidates were reranked using a query language model. Our method first enumerates query candidates with common landing pages with regard to the given query to create a clickthrough graph. Second, it calculates the likelihood of the candidates, using a language model generated from web search query logs. Finally, the candidates are sorted by the score calculated from the likelihood and label propagation. As a result, high precision and coverage were achieved in the task of Japanese abbreviation expansion, without using handcrafted training data."
P09-2048,Learning Semantic Categories from Clickthrough Logs,2009,9,5,1,1,310,mamoru komachi,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"As the web grows larger, knowledge acquisition from the web has gained increasing attention. In this paper, we propose using web search clickthrough logs to learn semantic categories. Experimental results show that the proposed method greatly outperforms previous work using only web search query logs."
I08-1047,Minimally Supervised Learning of Semantic Knowledge from Query Logs,2008,15,25,1,1,310,mamoru komachi,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We propose a method for learning semantic categories of words with minimal supervision from web search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, but makes important modifications to handle query log data for the task of acquiring semantic categories. We present experimental results comparing our method with two state-ofthe-art minimally supervised lexical knowledge extraction systems using Japanese query log data, and show that our method achieves higher precision than the previously proposed methods. We also show that the proposed method offers an additional advantage for knowledge acquisition in an Asian language for which word segmentation is an issue, as the method utilizes no prior knowledge of word segmentation, and is able to harvest new terms with correct word segmentation."
D08-1106,Graph-based Analysis of Semantic Drift in {E}spresso-like Bootstrapping Algorithms,2008,27,37,1,1,310,mamoru komachi,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Bootstrapping has a tendency, called semantic drift, to select instances unrelated to the seed instances as the iteration proceeds. We demonstrate the semantic drift of bootstrapping has the same root as the topic drift of Kleinberg's HITS, using a simplified graph-based reformulation of bootstrapping. We confirm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task. Proposed algorithms achieve superior performance to Espresso and previous graph-based WSD methods, even though the proposed algorithms have less parameters and are easy to calibrate."
W07-1522,Annotating a {J}apanese Text Corpus with Predicate-Argument and Coreference Relations,2007,13,66,2,0,12930,ryu iida,Proceedings of the Linguistic Annotation Workshop,0,"In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA-Tagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006)."
2006.iwslt-evaluation.11,Phrase reordering for statistical machine translation based on predicate-argument structure,2006,15,26,1,1,310,mamoru komachi,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe a novel phrase reordering model based on predicate-argument structure. Our phrase reordering method utilizes a general predicate-argument structure analyzer to reorder source language chunks based on predicate-argument structure. We explicitly model longdistance phrase alignments by reordering arguments and predicates. The reordering approach is applied as a preprocessing step in training phase of a phrase-based statistical MT system. We report experimental results in the evaluation campaign of IWSLT 2006."
