1999.mtsummit-1.2,1999.mtsummit-1.20,0,0.18671,", and p1 because the original English is construed as containing the verb like rather than the preposition. This sketch has, of course, been very superficial. This is partly because of the obvious constraints of time and space, but it is also because I do not wish to give the impression that the approach to machine translation I am trying to advocate turns on using one particular set of algorithms or representing grammatical structures in a particular way, or adopting a particular approach to transfer. A more particular instantiation of a least some of the ideas is described in some detail in Frank (1999) in this volume. 5 Choosing the Best Translation Our initial motivation for adopting something like contexted sets as the basic framework for a machine translation system had two components. On the one hand, we pointed out that it can introduce to a translation system the same kinds of efficiency gains, both in time and space, that charts make possible for parsing. If one wishes to pursue all paths through the search space, and if the chart constructed by the parser must be converted to disjunctive normal form before the next component can work on its output, then the claim that the parser ope"
2008.eamt-1.6,A94-1016,0,0.0304311,"ng such combinations of rule-based and statistical knowledge sources, one of the approaches being an integration of existing rule-based MT systems into a multi-engine architecture. This paper describes several incarnations of such multi-engine architectures within the project. A careful analysis of the results will guide us in the choice of further steps towards the construction of hybrid MT systems for practical applications. 2 2.1 Merging multiple MT results via a SMT decoder Architecture Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with [4]. Multi-engine systems can be roughly divided into simple architectures that try to select the best output from a number of systems but leave the individual hypotheses as is on the one hand [5–10], and more sophisticated setups on the other hand that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in [11–16]. Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not"
2008.eamt-1.6,C00-2122,0,0.0602105,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.3,0,0.0464118,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.12,0,0.0319946,"Missing"
2008.eamt-1.6,C02-1076,0,0.0555903,"Missing"
2008.eamt-1.6,P04-1063,0,0.0385466,"Missing"
2008.eamt-1.6,W05-0828,1,0.796901,"Missing"
2008.eamt-1.6,hogan-frederking-1998-evaluation,0,0.0443059,"Missing"
2008.eamt-1.6,P05-3026,0,0.029568,"Missing"
2008.eamt-1.6,E06-1005,0,0.0371106,"Missing"
2008.eamt-1.6,N07-1029,0,0.0640876,"Missing"
2008.eamt-1.6,W08-0328,1,0.816746,"combinations of highly probable partial translations. Instead of implementing a special-purpose search procedure from scratch, we transform the information contained in the MT output into a form that is suitable as input for an existing SMT decoder. This has the additional advantage that it is simple to combine resources used in standard phrase-based SMT with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. This architecture is described in more detail in [17], where also examples of results are given. It should be noted that this is certainly not the only way to combine systems. In particular, as this proposed 28 12th EAMT conference, 22-23 September 2008, Hamburg, Germany setup gives the last word to the SMT decoder, there is the risk that linguistically well-formed constructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a"
2008.eamt-1.6,P07-2045,0,0.00439401,"tructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a set of six rule-based MT engines that are partly available via web interfaces and partly installed locally. In addition to these engines, we generated phrase tables from the training data following the baseline methodology given in the description of the shared task and using the scripts included in the Moses toolkit [18]. In order to improve alignment quality, the source text and the output text of the MT systems were aligned with the help of a modified version of GIZA++ that it is able to load given models and which is embedded into a client-server setup, as described in [19]. The original Moses phrase table and separate phrase tables for each of the RBMT systems were then combined into a unified phrase table. By combining domain-specific lexical knowledge learned from the training data with more general knowledge contained in the linguistic rules, the hybrid system can both handle a wider range of syntactic"
2008.eamt-1.6,W08-0309,0,0.0966573,"sess about the particular vocabulary of the source text. 2.2 Results We submitted the results of the hybrid system as well as the results from each of the rule-based systems (suitably anonymized) to the shared task of the WMT 2008 workshop. This gives us the opportunity to compare the results with many other systems under fair conditions, both using automatic evaluation metric and comparisons involving human inspection. Detailed results of this evaluation are Fig. 1. Relative performance of system types for in-domain (EuroParl, upper row) and out-of-domain (News, lower row) data documented in [20]. By condensing several of the tables into a joint plot, it 29 12th EAMT conference, 22-23 September 2008, Hamburg, Germany becomes easier to see some of the salient patterns contained in these datasets. Fig. 1 project the results of two different types of human evaluation into twodimensional plots, and it is interesting to study the different behavior of the systems that depend strongly on whether the tests are done on data from the same or from a different domain as the training data. The plot displays the relative performance of the systems for the directions German ↔ English according to s"
2008.eamt-1.6,2001.mtsummit-papers.39,0,0.0119217,"lemma information. The two representations are then combined and filters based on PoS sequences on both sides are used to obtain a set of candidates for the lexicon. A list of acceptable pairs of PoS sequences is generated by inspecting several hundred of the most frequently occurring PoS sequences and excluding those that either do not form a pair of linguistic phrases or where the interpretation on both sides is incompatible. Morphological classification is applied to these lexical entries to augment them with inflection classes, following the open lexicon interchange format (OLIF) standard [21]. Even if statistical alignment and linguistic preprocessing can lead us a long way towards the automatic creation of lexical entries, it is crucial to manually inspect and correct the results because rule-based MT systems have no other mechanism for preventing errors from incorrect lexical entries. In cases of technical terminology, the validation of the terminology requires both linguistic and technical competence and it may be necessary to distribute some steps over different groups of people. In order to facilitate this process, we have built up a web-based front end for lexical database m"
2008.eamt-1.6,W07-0732,0,0.0135969,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,W07-0728,0,0.0198984,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,vilar-etal-2006-error,0,0.026094,"SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if one or both of these approaches can be made to deliver significant improvements under fairly general conditions, the improvements will essentially only alleviate the problem of lexical coverage but will not touch some other well"
C90-1003,J87-1005,0,0.1059,"sl:donkey Y) , ] The first reading displayed again corresponds to the quantifier-raiscd interpretation, which paraphrases as: Situation s0 contains an individual Y, and the facts that Y is a donkey and that every way of making S1 true also makes $2 tree, where S1 contains the individual X and the facts that X is a man and X owns Y, and $2 contains the fact that X beats Y. . S :-: s O : [ s 0 : s l = = > s 2 , s 2 : o w n ( X , Y ) , s l :i (X), sl:man (X), S0 : 2 (Y), sO :donkey(Y)] ; S = sO: [ s 0 : s l = = > s 2 , s 2 : o w n ( X , Y ) , s2:i(Y),s2:donkey(Y),sl:i(X), sl:man(X)] 9 25 Sheiber [7] adopt such a scheme apparently on the grounds of greater perspicuity. In any case, the modifications that need to be made to our scheme are entirely trivial, requiring only the introduction of a modest amount of symbolic computation. Basically, the idea is to use operations which, instead of returning pieces of the final logical form incrementally and nondeterministically, return expression that will exhibit this nondeterministic behavior when evaluated later. The later evaluation will, of course, be as specified be the detinitions we have given. In short, we believe that the abstractions we"
C90-1003,C86-1156,1,\N,Missing
C90-1003,C69-7001,0,\N,Missing
C90-1003,C69-6902,0,\N,Missing
chen-etal-2008-improving,D07-1103,0,\N,Missing
chen-etal-2008-improving,steinberger-etal-2006-jrc,0,\N,Missing
chen-etal-2008-improving,D07-1005,0,\N,Missing
chen-etal-2008-improving,P02-1040,0,\N,Missing
chen-etal-2008-improving,P07-2045,0,\N,Missing
chen-etal-2008-improving,N03-1017,0,\N,Missing
chen-etal-2008-improving,J03-1002,0,\N,Missing
chen-etal-2008-improving,2005.mtsummit-papers.11,0,\N,Missing
chen-etal-2008-improving,W99-0602,0,\N,Missing
chen-etal-2008-improving,P03-1021,0,\N,Missing
J05-4001,J94-3001,1,0.63751,"e closed under the operations of set theory and concatenation. This means that, if you can write an algebraic expression that describes the language that you are interested in, then the computing of it will be straightforward and can be specified algebraically. Regular relations, which are modeled by finite-state transducers and are very closely related to finite-state machines of the standard kind, have just the power that is needed for many linguistic operations, particularly at the low end of the hierarchy— phonology, morphology, and spelling rules. In particular, as Ron and I pointed out (Kay and Kaplan 1994), a slight adjustment to the rules of engagement, as they are called in military circles these days, moves simple string-rewriting rules right from the top of the Chomsky hierarchy to the bottom or, at least, one step from the bottom. The rules I have in mind are of the form: α → β/γ δ meaning “replace α by β if there is γ on the left and δ on the right”. If we stipulate that no rule be allowed to rewrite any part of the string that is part of the output of a previous application of that same rule, and if the rules are ordered, then the set of them can be modeled by a finite-state transducer t"
J93-1006,P91-1035,0,0.0238637,"Missing"
J93-1006,P91-1022,0,0.884717,"Missing"
J93-1006,J90-2002,0,0.217285,"omputational Linguisti4s Volume 19, Number 1 able. Perhaps the best-known example of this approach is to be found in Sato and Nagao (1990). The method proposed there requires a database to be maintained of the syntactic structures of sentences together with the structures of the corresponding translations. This database is searched in the course of making a new translation for examples of previous sentences that are like the current one in ways that are relevant for the method. Another example is the completely automatic, statistical approach to translation taken by the research group at IBM (Brown et al. 1990), which takes a large corpus of text with aligned translations as its point of departure. It is widely recognized that one of the most important sources of information to which a translator can have access is a large body of previous translations. No dictionary or terminology bank can provide information of comparable value on topical matters of possibly intense though only transitory interest, or on recently coined terms in the target language, or on matters relating to house style. But such a body of data is useful only if, once a relevant example has been found in the source language, the c"
J93-1006,J90-1003,0,0.0457937,"e thresholds relevant to our algorithm can be precomputed at compile-time. The figure shown would be appropriate to pass 3 in our experiment. In the formula used, there are a few reasonable simplifications concerning the nature of the AST; however, a Monte-Carlo simulation that is exactly in accordance with our algorithm confirmed the depicted figure in every essential detail. 7 This discussion could also be cast in an information theoretic framework using the notion of ""mutual information"" (Fano 1961), estimating the variance of the degree of match in order to find a frequency-threshold (see Church and Hanks 1990). 126 Martin Kay and Martin R6scheisen Text-Translation Alignment .... i iiiii ii ,,i,~,kkNk~,:,:,k, i /////JIIfJi i iiiiiiiil, t r i P i i i i iiiiiiii\\\ "",~1 °8 t!!!!!!!![!!~\\\\\ / [ / [ / [ I ] I I J t I f 10 5 15 20 Frequency Figure 1 Likelihood that a word pair is a spurious match as a function of a word's frequency and its similarity with a word in the other text (maximum 0.94). associations are a d d e d to the ones inherited from the preceding pass. It is an obvious requirement of the m a p p i n g that lines of association should not cross. At the beginning of the relaxat"
J93-1006,P91-1023,0,0.937224,"ments, the process converges to the point when no new ones can be found; then it stops. In the next section, we describe the algorithm. In Section 3 we describe additions to the basic technique required to provide for morphology, that is, relatively superficial variations in the forms of words. In Section 4 we show the results of applying a program that embodies these techniques to an article from Scientific American and its German translation in Spektrum der Wissenschaft. In Section 5 we discuss other approaches to the alignment problem that were subsequently undertaken by other researchers (Gale and Church 1991; Brown, Lai, and Mercer 1991). Finally, in Section 6, we consider ways in which our present methods might be extended and improved. 122 Martin Kay and Martin R6scheisen Text-Translation Alignment 2. The Alignment Algorithm 2.1 Data Structures The principal data structures used in the algorithm are the following: Word-Sentence Index (WSI). One of these is prepared for each of the texts. It is a table with an entry for each different w o r d in the text showing the sentences in which that w o r d occurs. For the moment, we m a y take a w o r d as being simply a distinct sequence of letters. If"
J93-1006,C88-2142,0,0.100942,"Missing"
J93-1006,C90-3044,0,0.0126834,"o undertake statistical, and other kinds of empirical, studies of translation on a scale that was previously unthinkable. Alignment makes possible approaches to partially, or completely, automatic translation based on a large corpus of previous translations that have been deemed accept* Xerox PARC,3333 Coyote Hill Road, Palo Alto, CA 94306. t Department of Computer Science,TechnicalUniversity of Munich, 8000 Munich 40, Germany. (~) 1993 Associationfor Computational Linguistics Computational Linguisti4s Volume 19, Number 1 able. Perhaps the best-known example of this approach is to be found in Sato and Nagao (1990). The method proposed there requires a database to be maintained of the syntactic structures of sentences together with the structures of the corresponding translations. This database is searched in the course of making a new translation for examples of previous sentences that are like the current one in ways that are relevant for the method. Another example is the completely automatic, statistical approach to translation taken by the research group at IBM (Brown et al. 1990), which takes a large corpus of text with aligned translations as its point of departure. It is widely recognized that o"
J93-1006,J93-1004,0,\N,Missing
J94-2005,C67-1009,1,0.41809,"ws this problem to be solved straightforwardly by the general constraint mechanism. It might be advantageous for the ELI to encode very specific information about a lexical item and the empty nodes that it sponsors. For example, the ELI for a WH 298 Mark Johnson and Martin Kay Parsing and Empty Nodes item might specify that the traces it sponsors are coindexed with the WH item itself. Assuming that indices are just unbound variables (thus coindexing is unification and contraindexing is an inequality constraint), an interesting technical problem arises if the basic parsing engine uses a chart (Kay 1967, 1980). Because it is fundamental to such devices that the label on an edge is copied before it is used as a component of a larger phrase, the variables representing indices will be copied or renamed and the indices on the WH item and its sponsored trace will no longer be identical. However, it is important that the sharing of variables among the components of an ELI be respected when they come together in a phrase. One way of overcoming this problem is to associate a vector of variables with each edge, in which each variable that is shared between two or more edges is assigned a unique posit"
J94-2005,W89-0206,1,0.69684,"construct the V' nodes and could therefore use its subcategorization frame to determine how many to construct. However, this would require an analysis of the grammar that is beyond the scope of standard parsing procedures. Notice that the V trace from which the subcategorization frame is projected is incorporated into the structure only after all the of V ~nodes have been constructed. Finally, the number of VP nodes is not determined by the subcategorization frame. No amount of grammar analysis will allow a top-down parser to predict the number of adjuncts attached to VP. A head-first parser (Kay 1989; van Noord 1993) seems best adapted to the treatment of empty nodes. This mixed parsing strategy in effect predicts a head top-down and builds its complements and specifiers bottom-up. The trace of the verb would be identified immediately after the I gave had been recognized, since that trace is the 290 Mark Johnson and Martin Kay Parsing and Empty Nodes head of the complement of the I. But it is not clear how such a strategy would cope with empty nodes that do not stand in a head-to-head relationship, such as the trace associated with the adjoined NP. The construction of the NP a big picture"
J94-2005,J81-4003,0,0.0632442,"Missing"
J94-2005,C92-2066,0,0.0198154,"red by some lexical or morphological item that appears in the input. By sponsoring we mean that every empty node is associated with some nonempty lexical item, which we call its sponsor, and that the number of empty nodes that a single lexical token can sponsor is fixed by the lexicon, so that the set of all empty nodes to appear in the parse can be determined directly by a simple inspection of the lexical items in the input string. Sponsorship is closely related to lexicalization in TAGs and CFGs (Schabes 1990, 1992; Schabes, AbeillG and Joshi 1988; Schabes and Waters 1993; Vijay-Shanker and Schabes 1992). In a lexicalized grammar every node in the parse tree originates from some lexical entry, so parsing becomes a jigsaw puzzle-like problem of finding a consistent way of assembling the pieces of trees associated with each lexical item. Sponsoring is a weaker notion, in that only some of the constituent structure, namely the lexical items and empty nodes, are specified in lexical entries. This seems plausible in a framework in which general principles of grammar (e.g., X~ theory, Case theory, etc.) determine the overall structure of the parse tree. In addition, finding an appropriate associati"
J94-2005,C88-2121,0,0.0967409,"Missing"
J94-2005,C92-1034,0,0.0210612,"pty node be sponsored by some lexical or morphological item that appears in the input. By sponsoring we mean that every empty node is associated with some nonempty lexical item, which we call its sponsor, and that the number of empty nodes that a single lexical token can sponsor is fixed by the lexicon, so that the set of all empty nodes to appear in the parse can be determined directly by a simple inspection of the lexical items in the input string. Sponsorship is closely related to lexicalization in TAGs and CFGs (Schabes 1990, 1992; Schabes, AbeillG and Joshi 1988; Schabes and Waters 1993; Vijay-Shanker and Schabes 1992). In a lexicalized grammar every node in the parse tree originates from some lexical entry, so parsing becomes a jigsaw puzzle-like problem of finding a consistent way of assembling the pieces of trees associated with each lexical item. Sponsoring is a weaker notion, in that only some of the constituent structure, namely the lexical items and empty nodes, are specified in lexical entries. This seems plausible in a framework in which general principles of grammar (e.g., X~ theory, Case theory, etc.) determine the overall structure of the parse tree. In addition, finding an appropriate associati"
J94-2005,W89-0208,0,\N,Missing
J94-2005,C92-1028,0,\N,Missing
J94-2005,P93-1017,0,\N,Missing
J94-3001,C92-1025,1,0.356899,"f pairs); it suppresses the general mapping provided by P for the exceptional items, allowing outputs for them to come from E only. As another example, the finite list of formatives in a lexicon L can be combined with a regular phonology (perhaps with exceptions already folded in) by means of the composition Id(L) o P. This relation enshrines not only the phonological regularities of the language but its lexical inventory as well, and its corresponding transducer would perform phonological recognition and lexical lookup in a single sequence of transitions. This is the sort of arrangement that Karttunen et al. (1992) discuss. Finally, we know that many language classes are closed under finitestate transductions or composition with regular relations--the images of context-free languages, for example, are context-free. It might therefore prove advantageous to seek ways of composing phonology and syntax to produce a new system with the same formal properties as syntax alone. Acknowledgments We are particularly indebted to Danny Bobrow for helpful discussions in the early stages of the research on rewriting systems. Our understanding and analysis of two-level systems is based on very productive discussions wi"
J94-3001,E87-1002,1,0.120708,"small set of simple operations, each of which implements a simple mathematical fact about regular languages, regular relations, or both. Second, our more abstract perspective provides a general framework within which to treat other phonological formalisms, existing or yet to be devised. For example, two-level morphology (Koskenniemi 1983), which evolved from our early considerations of rewriting rules, relies for its analysis and implementation on the same algebraic techniques. We are also encouraged by initial successes in adapting these techniques to the autosegmental formalism described by Kay (1987). • . / . . . . . . 2. Rewriting Rules and Transducers Supposing for the moment that Rule 2 (N --* n) is optional, Figure 1 shows the transition diagram of a finite-state transducer that models it. A finite-state transducer has 333 Computational Linguistics N:n { Volume 20, Number 3 { [ 0 ~ ~ a:a ... n:n, N:N ... z:z Figure 1 Rule 2 as optional. N:n ~ other, n:n Figure 2 Rule 2 as obligatory. two tapes. A transition can be taken if the two symbols separated by the colon in its label are found at the current position on the corresponding tapes, and the current position advances across those tap"
J94-3001,J92-1003,0,0.0517836,"batch rule with finitely many subrules) that rewrites a string x to y just in case the pair (x, y) belongs to the relation. [] We remark that there are alternative but perhaps less intuitive proofs of this theorem framed only in terms of simple nonbatch rules. But this result cannot be established without making use of boundary-context rules. Without such rules we can only simulate a proper subclass of the regular relations, those that permit identity prefixes and suffixes of unbounded length to surround any nonidentity correspondences. It is interesting to note that for much the same reason, Ritchie (1992) also made crucial use of two-level boundary-context rules to prove that the relations denoted by Koskenniemi&apos;s (1985) two-level grammars are also coextensive with the regular relations. Moreover, putting Ritchie&apos;s result together with ours gives the following: Theorem Ordered rewriting grammars with boundaries and two-level constraint grammars with boundaries are equivalent in their expressive power. Although there may be aesthetic or explanatory differences between the two formal systems, empirical coverage by itself cannot be used to choose between them. 7. Two-Level Rule Systems Inspired i"
kennington-etal-2012-suffix,steinberger-etal-2006-jrc,0,\N,Missing
kennington-etal-2012-suffix,P02-1040,0,\N,Missing
kennington-etal-2012-suffix,P07-2045,0,\N,Missing
kennington-etal-2012-suffix,2005.mtsummit-papers.11,0,\N,Missing
N09-1015,chen-etal-2008-improving,1,0.422672,"stems which relied on carefully crafted rules. The most obvious is that it at dramatically reduces cost in human labor and it is able to reach many critical translation rules that are easily overlooked by human being. 128 SMT systems generally assemble translations by selecting phrases from a large candidate set. Unsupervised learning often introduces a considerable amount of noise into this set as a result of which the selection process becomes more longer and less effective. This paper provides one approach to these problems. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In"
N09-1015,P07-1092,0,0.0150356,"paper provides one approach to these problems. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In other words, they work with the union of the data from the different languages. In contrast, we work with the intersection of information acquired through a third language. The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently. 2 Noise in a phrase-based SMT system The phrases in a translation model are extracted heuristically from a word alignment between the parallel texts in two languages usi"
N09-1015,D07-1103,0,0.0782513,"advantages over earlier systems which relied on carefully crafted rules. The most obvious is that it at dramatically reduces cost in human labor and it is able to reach many critical translation rules that are easily overlooked by human being. 128 SMT systems generally assemble translations by selecting phrases from a large candidate set. Unsupervised learning often introduces a considerable amount of noise into this set as a result of which the selection process becomes more longer and less effective. This paper provides one approach to these problems. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained fr"
N09-1015,2005.mtsummit-papers.11,0,0.0222873,"for different model features in a SMT system, which are usually optimized for a given set of models with minimum error rate training (MERT) (Och, 2003) to achieve better translation performance. In other words, the weights obtained for a model do not necessarily apply to another model. Since the triangulated filtering method removes a part of the model, it is important to readjust the feature weights for the reduced phrase-table. 4 Experimental design All the text data used in our experiments are from Release v3 of “European Parliament Proceedings Parallel Corpus 1996-2006” (Europarl) corpus (Koehn, 2005). We mainly investigated translations from Spanish to English. There are enough structural differences in these two language to introduce some noise in the phrase table. French, Portuguese, Danish, German and Finnish were used as bridge languages. Portuguese is very similar to Spanish and French somewhat less so. Finnish is unrelated and fairly different typologically with Danish and German occupying the middle ground. In addition, we also present briefly the results on GermanEnglish translations with Dutch, Spanish and Danish 130 as bridges. For the Spanish-English pair, three translation mod"
N09-1015,D07-1005,0,0.0443042,"less effective. This paper provides one approach to these problems. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In other words, they work with the union of the data from the different languages. In contrast, we work with the intersection of information acquired through a third language. The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently. 2 Noise in a phrase-based SMT system The phrases in a translation model are extracted heuristically from a word alignment between the parallel tex"
N09-1015,2001.mtsummit-papers.46,0,0.058235,"ing techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In other words, they work with the union of the data from the different languages. In contrast, we work with the intersection of information acquired through a third language. The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently. 2 Noise in a phrase-based SMT system The phrases in a translation model are extracted heuristically from a word alignment between the parallel texts in two languages using machine learning techniques. The translation model featu"
N09-1015,P03-1021,0,0.0242448,"into a third language. If both phrases can be mapped to some phrases in the bridge language, but to different ones, we should remove it from the model. It is also possible that neither of the phrases appear in corresponding bridge models. In this case, we consider the bridge models insufficient for making the filtering decision and prefer to keep the pair in the table. The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system, which are usually optimized for a given set of models with minimum error rate training (MERT) (Och, 2003) to achieve better translation performance. In other words, the weights obtained for a model do not necessarily apply to another model. Since the triangulated filtering method removes a part of the model, it is important to readjust the feature weights for the reduced phrase-table. 4 Experimental design All the text data used in our experiments are from Release v3 of “European Parliament Proceedings Parallel Corpus 1996-2006” (Europarl) corpus (Koehn, 2005). We mainly investigated translations from Spanish to English. There are enough structural differences in these two language to introduce s"
N09-1015,2001.mtsummit-papers.68,0,0.0310232,"Missing"
N09-1015,W99-0602,0,0.171042,"Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In other words, they work with the union of the data from the different languages. In contrast, we work with the intersection of information acquired through a third language. The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently. 2 Noise in a phrase-based SMT system The phrases in a translation model are extracted heuristically from a word alignment between the parallel texts in two languages using machine learning techniques. The tra"
N09-1015,P02-1040,0,\N,Missing
N09-1015,P07-2045,0,\N,Missing
P96-1027,W89-0206,1,0.829687,"Missing"
P96-1027,C88-2128,0,0.753299,"indexing by string position. Interesting interactions occur between pairs of edges whose bit vectors have empty intersections, indicating that they cover disjoint sets of words. There can now be as many edges as bit-vectors and, not surprisingly, the computational complexity of the parsing process increases accordingly. Abstract Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases. 1 Charts Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation. In particular, we will be interested in the extent to which they bring to the generation process advantages comparable to those that make them attractive in parsing. Chart parsing is not a well defined notion. The usual conception of it involves at least four related ideas: Inactive edges. In context-free grammar, all phrases of a given category that cover a given part of the string are equivalent"
P96-1027,P89-1002,0,0.0848886,"Missing"
P96-1027,C92-2117,0,\N,Missing
P96-1027,P85-1008,0,\N,Missing
