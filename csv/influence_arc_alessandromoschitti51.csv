2020.acl-main.504,I17-1001,0,0.0544231,"Missing"
2020.acl-main.504,W19-4828,0,0.0358989,"Missing"
2020.acl-main.504,D18-1217,0,0.013761,"rather to improve inference throughput. Our apporach also shares some similarities with student-teacher (ST) approaches for self-training (Yarowsky, 1995; McClosky et al., 2006). Under this setting, a model is used both as a “teacher” (which makes predictions on unlabeled data to obtain automatic labels) and as a “student” (which learns both from gold standard and automatic labels). In recent years, many variants of ST have 2 Also known as auxiliary or diagnostic classifiers. been proposed, including treating teacher predictions as soft labels (Hinton et al., 2015), masking part of the label (Clark et al., 2018), or use multiple modules for the teacher (Zhou and Li, 2005; Ruder and Plank, 2018). Unlike classic ST approaches, we do not aim at improving the teacher models or creating efficient students; instead, we trained models to be used as sequential ranking components. This may be seen as a generalization of the ST approach, where the student needs to learn a simpler task than the teacher. However, our approach is significantly different from the traditional ST setting, which our preliminary investigation showed to be not very effective. 3 Preliminaries and Task Definition We first formalize the p"
2020.acl-main.504,P19-1285,0,0.0401061,"Missing"
2020.acl-main.504,N19-1423,0,0.565482,"lignment networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018). The use of compare and aggregate architectures has also been extensively evaluated (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). This family of approaches uses a shallow attention mechanism 5698 over the question and answer sentence embeddings. Finally, Tayyar Madabushi et al. (2018) exploited fine-grained question classification to further improve answer selection. Transformer models have been fine-tuned on several tasks that are closely related to AS2. For example, they were used for machine reading (Devlin et al., 2019; Yang et al., 2019a; Wang et al., 2019), ad-hoc document retrieval (Yang et al., 2019b; MacAvaney et al., 2019), and semantic understanding (Liu et al., 2019b) tasks to obtain significant improvement over previous neural methods. Recently, Garg et al. (2020) applied transformer models, obtaining an impressive boost of the state of the art for AS2 tasks. Reducing Transformer Complexity The high computational cost of transformer models prevents their use in many real-word applications. Some proposed solutions rely on leveraging knowledge distillation in the pre-training step, e.g., (Sanh et al."
2020.acl-main.504,P84-1044,0,0.106492,"Missing"
2020.acl-main.504,P19-1356,0,0.0192023,"th almost no impact on accuracy, as measured on two English Question Answering datasets. 1 Introduction Recent research has shown that transformer-based neural networks can greatly advance the state of the art over many natural language processing tasks. Efforts such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), XLNet (Dai et al., 2019), and others have led to major advancements in several NLP subfields. These models are able to approximate syntactic and semantic relations between words and their compounds by pre-training on copious amounts of unlabeled data (Clark et al., 2019; Jawahar et al., 2019). Then, they can easily be applied to different tasks by just fine-tuning them on training data from the target domain/task (Liu et al., 2019a; Peters et al., 2019). The impressive effectiveness of transformer-based neural networks can be partially attributed to their large number of parameters (ranging from 110 million for “base” models to over 8 billion (Shoeybi et al., 2019)); however, this also makes them rather expensive in terms of computation time and resources. Being aware of this problem, the research community has been developing techniques to prune unnecessary network parameters (La"
2020.acl-main.504,D19-1445,0,0.0469112,"Missing"
2020.acl-main.504,Q19-1026,0,0.0474012,". Therefore, we evaluated our approach (Sec. 4.3) on two datasets: ASNQ, which is publicly available, and our GPD dataset. We still leverage TRECQA and WikiQA to show that that our 5702 cascade system has comparable performance to state-of-the-art transformer models when no filtering is applied. ASNQ The Answer Sentence Natural Questions dataset (Garg et al., 2020) is a large collection (23M samples) of question-answer pairs, which is two orders of magnitude larger than most public AS2 datasets. It was obtained by extracting sentence candidates from the Google Natural Question (NQ) benchmark (Kwiatkowski et al., 2019). Samples in NQ consists of tuples hquestion, answerlong , answershort , labeli, where answerlong contains multiple sentences, answershort is fragment of a sentence, and label is a binary value indicating whether answerlong is correct. The positive samples were obtained by extracting sentences from answerlong that contain answershort ; all other sentences are labeled as negative. The original release of ANSQ7 only contains train and development splits; we further split the dev. set to both have dev. and test sets. GPD The General Purpose Dataset is part of our efforts to study large scale web"
2020.acl-main.504,N19-1112,0,0.275697,"ang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). This family of approaches uses a shallow attention mechanism 5698 over the question and answer sentence embeddings. Finally, Tayyar Madabushi et al. (2018) exploited fine-grained question classification to further improve answer selection. Transformer models have been fine-tuned on several tasks that are closely related to AS2. For example, they were used for machine reading (Devlin et al., 2019; Yang et al., 2019a; Wang et al., 2019), ad-hoc document retrieval (Yang et al., 2019b; MacAvaney et al., 2019), and semantic understanding (Liu et al., 2019b) tasks to obtain significant improvement over previous neural methods. Recently, Garg et al. (2020) applied transformer models, obtaining an impressive boost of the state of the art for AS2 tasks. Reducing Transformer Complexity The high computational cost of transformer models prevents their use in many real-word applications. Some proposed solutions rely on leveraging knowledge distillation in the pre-training step, e.g., (Sanh et al., 2019), or used parameter reduction techniques (Lan et al., 2019) to reduce inference cost. However, the effectiveness of these approaches varies depending o"
2020.acl-main.504,P19-1441,0,0.338729,"ang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). This family of approaches uses a shallow attention mechanism 5698 over the question and answer sentence embeddings. Finally, Tayyar Madabushi et al. (2018) exploited fine-grained question classification to further improve answer selection. Transformer models have been fine-tuned on several tasks that are closely related to AS2. For example, they were used for machine reading (Devlin et al., 2019; Yang et al., 2019a; Wang et al., 2019), ad-hoc document retrieval (Yang et al., 2019b; MacAvaney et al., 2019), and semantic understanding (Liu et al., 2019b) tasks to obtain significant improvement over previous neural methods. Recently, Garg et al. (2020) applied transformer models, obtaining an impressive boost of the state of the art for AS2 tasks. Reducing Transformer Complexity The high computational cost of transformer models prevents their use in many real-word applications. Some proposed solutions rely on leveraging knowledge distillation in the pre-training step, e.g., (Sanh et al., 2019), or used parameter reduction techniques (Lan et al., 2019) to reduce inference cost. However, the effectiveness of these approaches varies depending o"
2020.acl-main.504,2021.ccl-1.108,0,0.116211,"Missing"
2020.acl-main.504,P06-1043,0,0.0252755,"respect to the model architecture, our approach is similar to probing models2 (Adi et al., 2017; Liu et al., 2019a; Hupkes et al., 2018; Belinkov et al., 2017), as we train classification layers based on partial encoding on the input sequence. However, (i) our intermediate classifiers are integral part of the model, rather than being trained on frozen partial encodings, and (ii) we use these classifiers not to inspect model properties, but rather to improve inference throughput. Our apporach also shares some similarities with student-teacher (ST) approaches for self-training (Yarowsky, 1995; McClosky et al., 2006). Under this setting, a model is used both as a “teacher” (which makes predictions on unlabeled data to obtain automatic labels) and as a “student” (which learns both from gold standard and automatic labels). In recent years, many variants of ST have 2 Also known as auxiliary or diagnostic classifiers. been proposed, including treating teacher predictions as soft labels (Hinton et al., 2015), masking part of the label (Clark et al., 2018), or use multiple modules for the teacher (Zhou and Li, 2005; Ruder and Plank, 2018). Unlike classic ST approaches, we do not aim at improving the teacher mod"
2020.acl-main.504,W19-4302,0,0.107525,"rks can greatly advance the state of the art over many natural language processing tasks. Efforts such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), XLNet (Dai et al., 2019), and others have led to major advancements in several NLP subfields. These models are able to approximate syntactic and semantic relations between words and their compounds by pre-training on copious amounts of unlabeled data (Clark et al., 2019; Jawahar et al., 2019). Then, they can easily be applied to different tasks by just fine-tuning them on training data from the target domain/task (Liu et al., 2019a; Peters et al., 2019). The impressive effectiveness of transformer-based neural networks can be partially attributed to their large number of parameters (ranging from 110 million for “base” models to over 8 billion (Shoeybi et al., 2019)); however, this also makes them rather expensive in terms of computation time and resources. Being aware of this problem, the research community has been developing techniques to prune unnecessary network parameters (Lan et al., 2019; Sanh et al., 2019) or optimize the transformer architecture (Zhang et al., 2018; Xiao et al., 2019). In this paper, we propose a completely differen"
2020.acl-main.504,P18-1096,0,0.0132711,"s with student-teacher (ST) approaches for self-training (Yarowsky, 1995; McClosky et al., 2006). Under this setting, a model is used both as a “teacher” (which makes predictions on unlabeled data to obtain automatic labels) and as a “student” (which learns both from gold standard and automatic labels). In recent years, many variants of ST have 2 Also known as auxiliary or diagnostic classifiers. been proposed, including treating teacher predictions as soft labels (Hinton et al., 2015), masking part of the label (Clark et al., 2018), or use multiple modules for the teacher (Zhou and Li, 2005; Ruder and Plank, 2018). Unlike classic ST approaches, we do not aim at improving the teacher models or creating efficient students; instead, we trained models to be used as sequential ranking components. This may be seen as a generalization of the ST approach, where the student needs to learn a simpler task than the teacher. However, our approach is significantly different from the traditional ST setting, which our preliminary investigation showed to be not very effective. 3 Preliminaries and Task Definition We first formalize the problem of selecting the most likely element in a set as a reranking problem; then, w"
2020.acl-main.504,D17-1122,0,0.0634212,"Missing"
2020.acl-main.504,C18-1278,0,0.0829953,"odels. Answer Sentence Selection (AS2) In the last few years, several approaches have been proposed for AS2. For example, Severyn and Moschitti (2015) applied CNN to create question and answer representations, while others proposed interweighted alignment networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018). The use of compare and aggregate architectures has also been extensively evaluated (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). This family of approaches uses a shallow attention mechanism 5698 over the question and answer sentence embeddings. Finally, Tayyar Madabushi et al. (2018) exploited fine-grained question classification to further improve answer selection. Transformer models have been fine-tuned on several tasks that are closely related to AS2. For example, they were used for machine reading (Devlin et al., 2019; Yang et al., 2019a; Wang et al., 2019), ad-hoc document retrieval (Yang et al., 2019b; MacAvaney et al., 2019), and semantic understanding (Liu et al., 2019b) tasks to obtain significant improvement over previous neural methods. Recently, Garg et al. (2020) applied transformer models, obtaining an impressive boost of the state of the art for AS2 tasks."
2020.acl-main.504,N18-1115,0,0.0129057,"ness/efficiency trade-offs with three candidate generation approaches. While these methods are aligned with our approach, they target document retrieval, which is a very different setting. Further, they only used linear models or simple neural models. Agarwal et al. (2012) focused on AS2, but just applied linear models. Answer Sentence Selection (AS2) In the last few years, several approaches have been proposed for AS2. For example, Severyn and Moschitti (2015) applied CNN to create question and answer representations, while others proposed interweighted alignment networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018). The use of compare and aggregate architectures has also been extensively evaluated (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). This family of approaches uses a shallow attention mechanism 5698 over the question and answer sentence embeddings. Finally, Tayyar Madabushi et al. (2018) exploited fine-grained question classification to further improve answer selection. Transformer models have been fine-tuned on several tasks that are closely related to AS2. For example, they were used for machine reading (Devlin et al., 2019; Yang et al., 2019a; Wang et al., 2"
2020.acl-main.504,D07-1003,0,0.0349854,"periments We present three sets of experiments designed to evaluate CT. In the first (Section 5.3), we show that our proposed approach without any selection produces comparable or superior results with respect to the state of the art of AS2, thanks to its stability properties; in the second (Section 5.4), we compare our Cascade Transformer with a vanilla transformer, as well as a sequence of transformer models trained independently; finally, in the third (Section 5.5), we explore the tuning of the drop ratio, α. 5.1 Datasets TRECQA & WikiQA Traditional benchmarks used for AS2, such as TRECQA (Wang et al., 2007) and WikiQA (Yang et al., 2015), typically contain a limited number of candidates for each question. Therefore, while they are very useful to compare accuracy of AS2 systems with the state of the art, they do not enable testing large scale passage reranking, i.e., inference on hundreds or thousand of answer candidates. Therefore, we evaluated our approach (Sec. 4.3) on two datasets: ASNQ, which is publicly available, and our GPD dataset. We still leverage TRECQA and WikiQA to show that that our 5702 cascade system has comparable performance to state-of-the-art transformer models when no filter"
2020.acl-main.504,P95-1026,0,0.263646,"te results. With respect to the model architecture, our approach is similar to probing models2 (Adi et al., 2017; Liu et al., 2019a; Hupkes et al., 2018; Belinkov et al., 2017), as we train classification layers based on partial encoding on the input sequence. However, (i) our intermediate classifiers are integral part of the model, rather than being trained on frozen partial encodings, and (ii) we use these classifiers not to inspect model properties, but rather to improve inference throughput. Our apporach also shares some similarities with student-teacher (ST) approaches for self-training (Yarowsky, 1995; McClosky et al., 2006). Under this setting, a model is used both as a “teacher” (which makes predictions on unlabeled data to obtain automatic labels) and as a “student” (which learns both from gold standard and automatic labels). In recent years, many variants of ST have 2 Also known as auxiliary or diagnostic classifiers. been proposed, including treating teacher predictions as soft labels (Hinton et al., 2015), masking part of the label (Clark et al., 2018), or use multiple modules for the teacher (Zhou and Li, 2005; Ruder and Plank, 2018). Unlike classic ST approaches, we do not aim at i"
2020.acl-main.504,P18-1166,0,0.129596,"m on training data from the target domain/task (Liu et al., 2019a; Peters et al., 2019). The impressive effectiveness of transformer-based neural networks can be partially attributed to their large number of parameters (ranging from 110 million for “base” models to over 8 billion (Shoeybi et al., 2019)); however, this also makes them rather expensive in terms of computation time and resources. Being aware of this problem, the research community has been developing techniques to prune unnecessary network parameters (Lan et al., 2019; Sanh et al., 2019) or optimize the transformer architecture (Zhang et al., 2018; Xiao et al., 2019). In this paper, we propose a completely different approach for increasing the efficiency of transformer models, which is orthogonal to previous work, and thus can be applied in addition to any of the methods described above. Its main idea is that a large class of NLP problems requires choosing one correct candidate among many. For some applications, this often entails running the model over hundreds or thousands of instances. However, it is well-known that, in many cases, some candidates can be more easily excluded from the optimal solution (Land and Doig, 1960), i.e., the"
2020.acl-main.504,N19-4013,0,0.068627,"Missing"
2020.acl-main.504,P79-1022,0,0.282507,"Missing"
2020.acl-main.504,D15-1237,0,0.265912,"Missing"
2020.coling-main.457,P17-1171,0,0.22652,"eved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can already constitute a QA system. In contrast, MRC has been mainly developed to find answers in a paragraph or a text of limited size. Several models for adapting MRC to an end-to-end retrieval setting have been proposed, e.g., Chen et al. (2017a) and Kratzwald and Feuerriegel (2018), the deployment of MRC systems in production is challenged by two key factors: the lack of datasets for training MRC with realistic retrieval data, and the large volume of content needed to be processed, i.e., MRC cannot efficiently process a large amount of retrieved data. In contrast, AS2 research originated from the TREC competitions (Wang et al., 2007); thus, it has targeted large databases of unstructured text from the beginning of its development. Neural models have significantly contributed to AS2 with new techniques, e.g., Wang and Jiang (2016),"
2020.coling-main.457,P17-1152,0,0.190428,"eved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can already constitute a QA system. In contrast, MRC has been mainly developed to find answers in a paragraph or a text of limited size. Several models for adapting MRC to an end-to-end retrieval setting have been proposed, e.g., Chen et al. (2017a) and Kratzwald and Feuerriegel (2018), the deployment of MRC systems in production is challenged by two key factors: the lack of datasets for training MRC with realistic retrieval data, and the large volume of content needed to be processed, i.e., MRC cannot efficiently process a large amount of retrieved data. In contrast, AS2 research originated from the TREC competitions (Wang et al., 2007); thus, it has targeted large databases of unstructured text from the beginning of its development. Neural models have significantly contributed to AS2 with new techniques, e.g., Wang and Jiang (2016),"
2020.coling-main.457,N19-1423,0,0.0325051,"volume of content needed to be processed, i.e., MRC cannot efficiently process a large amount of retrieved data. In contrast, AS2 research originated from the TREC competitions (Wang et al., 2007); thus, it has targeted large databases of unstructured text from the beginning of its development. Neural models have significantly contributed to AS2 with new techniques, e.g., Wang and Jiang (2016), Qiao et al. (2019) and Nogueira and Cho (2019). Recently, new approaches for pre-training neural language models on large amount of data, e.g., ELMO (Peters et al., 2018), GPT (Radford et al., ), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), have led to major advancements in several NLP subfields. These pre-training techniques allow for creating models that automatically capture dependencies between the sentence compounds. Interestingly, the resulting models can be easily adapted to different tasks by fine-tuning them on the target training data. Unfortunately, all the models above (especially Transformer-based architectures) require a considerable number of parameters (up to 340 million for BERT Large). This poses three critical challenges for having ∗ work done at University of Trento prior to joini"
2020.coling-main.457,W16-2506,0,0.0181861,"sed to obtain the vector w ˆjc = [wjc ; rj ]. The two vectors, ri and rj are passed to the question-candidate encoder to create a pair representation. It is important to note that we keep the word embedding static during training, thus this operation does not cause much overhead during training as we do not need to back-propagate through it. For this model, we use the Numberbatch (Speer and Lowry-Duda, 2017) embeddings since they are more accurate than unsupervised word embeddings, such as Glove (Pennington et al., 2014), which may introduce noisy or non-common-sense relations. In particular, Faruqui et al. (2016) showed that unsupervised word embeddings tend to cluster according to the frequency of words of the dataset used for training them. For this reason, Speer and Lowry-Duda (2017) adopted retrofitting. This technique aims at reducing the distance between word embeddings of entities that are related in a knowledge base, i.e., ConcepNet. 4.1.2 Question-Candidate encoder Similarly to Severyn and Moschitti (2016), we encode the question and candidate independently using two single layers of CNN with a kernel size of 5 and global max pooling, producing two embeddings for the question qe and the candi"
2020.coling-main.457,D15-1181,0,0.0342097,"writing questions after reading the source of the answers. Additionally, Table 2 shows that the combination of the two features, word-overlap (WO), and reciprocal rank (RR), gives a strong baseline for all the datasets in consideration. This simple rule-based model ranks candidates according to the lexical overlap between question and candidates, and, in the case when two sentences have the same amount of overlapping words, it uses the reciprocal rank (RR) as a discriminator. 3 Related Work Despite the importance of global structure, most state-of-the-art models (Severyn and Moschitti, 2016; He et al., 2015; Madabushi et al., 2018; Tay et al., 2018b; Garg et al., 2019) do not take the global structure of candidates into account. They use a point-wise approach to maximize the score of positive candidates, i.e., those containing the answer, and minimize the score of those not containing the answer. Most models treat ranking as a binary classification problem. Nevertheless, other methods have been studied, e.g., contrastive pair-wise and, more recently, list-wise approaches. In the case of contrastive pair-wise training (Rao et al., 2016), given a question q, the loss maximizes the score of the mod"
2020.coling-main.457,P18-1031,0,0.0207768,"of the original paper, but in contrast with it, we keep the embedding static as we empirically found that it leads to similar results while having a lower number of trainable parameters. For the RNN and the LSTM, we used the same hidden size as the input, i.e., the double of the size of the convolutional operation hidden layer. For the Bidirectional variations, i.e., BiRNN and BiLSTM, we set the hidden size as half of the input size in each direction, resulting in a comparable number of parameters. All the models were trained for three epochs using slanted triangular learning rate scheduling (Howard and Ruder, 2018) without early stopping. In the case of the point-wise models, we used Adam optimizer with a maximum learning rate set at 2e-3, whereas for the list-wise approaches, we used a learning rate of 2e-4. All the experiments are performed on an Nvidia GTX 1080 ti GPU and an Intel Core I9-7900X processor. 5.3 State-of-the-art Results Table 3 shows the state-of-the-art results with respect to the WikiQA dataset. The first block reports the performance of the baselines; these models are computed using the simple features described in Section 2.1, i.e., the reciprocal rank (RR), the lexical overlap (WO)"
2020.coling-main.457,D18-1055,0,0.153843,"earch engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can already constitute a QA system. In contrast, MRC has been mainly developed to find answers in a paragraph or a text of limited size. Several models for adapting MRC to an end-to-end retrieval setting have been proposed, e.g., Chen et al. (2017a) and Kratzwald and Feuerriegel (2018), the deployment of MRC systems in production is challenged by two key factors: the lack of datasets for training MRC with realistic retrieval data, and the large volume of content needed to be processed, i.e., MRC cannot efficiently process a large amount of retrieved data. In contrast, AS2 research originated from the TREC competitions (Wang et al., 2007); thus, it has targeted large databases of unstructured text from the beginning of its development. Neural models have significantly contributed to AS2 with new techniques, e.g., Wang and Jiang (2016), Qiao et al. (2019) and Nogueira and Cho"
2020.coling-main.457,Q19-1026,0,0.0476977,"andidate pairs. The latter are joint representations of the question and the answer obtained by the question-answer encoder. For the attention mechanism, we propose Cosinet, a network that uses a sort of static attention, given by the cosine similarity between the embedding representation of the question and answer words. We show that this solution is very efficient and does not cause almost any drop compared to the use of standard attention. We tested our models on four different datasets, the well-known WikiQA dataset, the adaptation of SQuAD (Rajpurkar et al., 2016), and Natural Questions (Kwiatkowski et al., 2019) datasets to the AS2 task. Our comparative experiments show that (i) our models achieve better results than other efficient approaches, but (ii) the lack of contextual embeddings prevents the model from learning more complex functions, leading to lower results than the very expensive solutions. We partially solve this problem by using our joint model, which significantly improves the accuracy of efficient methods. Despite this adds a small overhead during training and testing, it is crucial to capture the structure of data. For example, the results on WikiQA show that BiRNN added to the Cosine"
2020.coling-main.457,D19-1610,0,0.0545584,"tion or as a feature concatenated to the word embeddings of Convolutional Neural Network (CNN). Subsequent approaches, e.g., (Wang and Jiang, 2016; Bian et al., 2017), use a word-level attention mechanism to identify the semantic overlap between each word in the question and each word in the answer candidate. Despite obtaining better results than previous approaches, the computational cost of performing word-level attention and the aggregation steps to leverage the information extracted by the attention mechanism increases the computational cost of previous methods. More recent models, e.g., (Lai et al., 2019; Garg et al., 2019; Yoon et al., 2018), leverage contextualized word representation, e.g., pre-trained using BERT, ELMo, RoBERTa, etc. These approaches achieve state-of-the-art results for AS2, but they require significant computational power for both pre-training, fine-tuning, and testing on the final task. 5214 4 Efficient Model for AS2 To build an efficient yet accurate model for AS2, it is crucial to leverage all the strong signals present in the dataset without increasing the complexity of the model itself. For this reason, we design a model as follows: (i) We build an efficient encoder"
2020.coling-main.457,C18-1278,0,0.381822,"s after reading the source of the answers. Additionally, Table 2 shows that the combination of the two features, word-overlap (WO), and reciprocal rank (RR), gives a strong baseline for all the datasets in consideration. This simple rule-based model ranks candidates according to the lexical overlap between question and candidates, and, in the case when two sentences have the same amount of overlapping words, it uses the reciprocal rank (RR) as a discriminator. 3 Related Work Despite the importance of global structure, most state-of-the-art models (Severyn and Moschitti, 2016; He et al., 2015; Madabushi et al., 2018; Tay et al., 2018b; Garg et al., 2019) do not take the global structure of candidates into account. They use a point-wise approach to maximize the score of positive candidates, i.e., those containing the answer, and minimize the score of those not containing the answer. Most models treat ranking as a binary classification problem. Nevertheless, other methods have been studied, e.g., contrastive pair-wise and, more recently, list-wise approaches. In the case of contrastive pair-wise training (Rao et al., 2016), given a question q, the loss maximizes the score of the model for a positive questi"
2020.coling-main.457,D14-1162,0,0.0893264,"[wiq ; ri ]. A similar procedure is applied to the candidate to find rj = maxi (ri,j ), which is used to obtain the vector w ˆjc = [wjc ; rj ]. The two vectors, ri and rj are passed to the question-candidate encoder to create a pair representation. It is important to note that we keep the word embedding static during training, thus this operation does not cause much overhead during training as we do not need to back-propagate through it. For this model, we use the Numberbatch (Speer and Lowry-Duda, 2017) embeddings since they are more accurate than unsupervised word embeddings, such as Glove (Pennington et al., 2014), which may introduce noisy or non-common-sense relations. In particular, Faruqui et al. (2016) showed that unsupervised word embeddings tend to cluster according to the frequency of words of the dataset used for training them. For this reason, Speer and Lowry-Duda (2017) adopted retrofitting. This technique aims at reducing the distance between word embeddings of entities that are related in a knowledge base, i.e., ConcepNet. 4.1.2 Question-Candidate encoder Similarly to Severyn and Moschitti (2016), we encode the question and candidate independently using two single layers of CNN with a kern"
2020.coling-main.457,N18-1202,0,0.0145646,"ng MRC with realistic retrieval data, and the large volume of content needed to be processed, i.e., MRC cannot efficiently process a large amount of retrieved data. In contrast, AS2 research originated from the TREC competitions (Wang et al., 2007); thus, it has targeted large databases of unstructured text from the beginning of its development. Neural models have significantly contributed to AS2 with new techniques, e.g., Wang and Jiang (2016), Qiao et al. (2019) and Nogueira and Cho (2019). Recently, new approaches for pre-training neural language models on large amount of data, e.g., ELMO (Peters et al., 2018), GPT (Radford et al., ), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), have led to major advancements in several NLP subfields. These pre-training techniques allow for creating models that automatically capture dependencies between the sentence compounds. Interestingly, the resulting models can be easily adapted to different tasks by fine-tuning them on the target training data. Unfortunately, all the models above (especially Transformer-based architectures) require a considerable number of parameters (up to 340 million for BERT Large). This poses three critical challenges for havin"
2020.coling-main.457,D16-1264,0,0.297996,"e cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the ∼ 18 minutes required by a standard BERT-base fine-tuning. 1 Introduction In recent years, there has been a renewed interest in Question Answering (QA) led by industrial needs, e.g., the development of personal assistants and academic research on neural networks. Regarding the latter, AS2 (Wang et al., 2007; Yang et al., 2015) and Machine Reading Comprehension (MRC) (Richardson et al., 2013; Rajpurkar et al., 2016) have been largely explored. AS2 consists of selecting sentences that are answers to a target question from documents or paragraphs retrieved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can already constitute a QA system. In contrast, MRC has been mainly developed to find answer"
2020.coling-main.457,D13-1020,0,0.017438,"highest accuracy among the cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the ∼ 18 minutes required by a standard BERT-base fine-tuning. 1 Introduction In recent years, there has been a renewed interest in Question Answering (QA) led by industrial needs, e.g., the development of personal assistants and academic research on neural networks. Regarding the latter, AS2 (Wang et al., 2007; Yang et al., 2015) and Machine Reading Comprehension (MRC) (Richardson et al., 2013; Rajpurkar et al., 2016) have been largely explored. AS2 consists of selecting sentences that are answers to a target question from documents or paragraphs retrieved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can already constitute a QA system. In contrast, MRC has been mainly"
2020.coling-main.457,2020.acl-main.504,1,0.839144,"small overhead during training and testing, it is crucial to capture the structure of data. For example, the results on WikiQA show that BiRNN added to the Cosinet improves of ∼ 4 points previous baselines. 5212 Finally, our word-relatedness encoder can replace the standard attention to enhance the speed of the fast attention-based approaches, resulting in a fast and accurate network in the class of fast methods. Our research will gain more and more important also in the light of improving the efficiency of large architecture using our models for sequential re-ranking (Matsubara et al., 2020; Soldaini and Moschitti, 2020). 2 AS2: Answer Sentence Selection The task of Answer Sentence Selection (AS2) can be formalized as follows: given a question q and a set of answer sentence candidates C = {c1 , c2 , ..., cn }, assign a score si for each candidate ci such that the sentence receiving the highest score is the one that most likely contains the answer, i.e., the answer sentence. It is interesting to note that AS2 can be, in fact, modeled as a re-ranking task. Although re-ranking is a structured output problem, most state-of-the-art approaches treat it as pointwise classification, i.e., classifying answer sentences"
2020.coling-main.457,S17-2008,0,0.0168823,"e most similar word in the other text. The value is concatenated to the word embedding of the question, i.e., w ˆiq = [wiq ; ri ]. A similar procedure is applied to the candidate to find rj = maxi (ri,j ), which is used to obtain the vector w ˆjc = [wjc ; rj ]. The two vectors, ri and rj are passed to the question-candidate encoder to create a pair representation. It is important to note that we keep the word embedding static during training, thus this operation does not cause much overhead during training as we do not need to back-propagate through it. For this model, we use the Numberbatch (Speer and Lowry-Duda, 2017) embeddings since they are more accurate than unsupervised word embeddings, such as Glove (Pennington et al., 2014), which may introduce noisy or non-common-sense relations. In particular, Faruqui et al. (2016) showed that unsupervised word embeddings tend to cluster according to the frequency of words of the dataset used for training them. For this reason, Speer and Lowry-Duda (2017) adopted retrofitting. This technique aims at reducing the distance between word embeddings of entities that are related in a knowledge base, i.e., ConcepNet. 4.1.2 Question-Candidate encoder Similarly to Severyn"
2020.coling-main.457,P19-1355,0,0.0130597,"at the top of the Nielsen ratings (an accomplishment later matched by The Andy Griffith Show and Seinfeld ). I Love Lucy is still syndicated in dozens of languages across the world Table 1: An example of question/answer-candidate from WikiQA. In green the answer to the question. such models in production: firstly, it requires powerful GPUs to achieve an acceptable service latency; secondly, although the classification of candidates can be parallelized, the necessary number of GPUs will prohibitively increase the operational cost and creating substantial environmental issues, as pointed out in Strubell et al. (2019); last, transformer-based architectures require many resources for pre-training, e.g., both data and compute power (TPUs). These resources may not be available for low resource languages or domain-specific applications. In this paper, we study and propose solutions to design accurate AS2 models, still preserving high efficiency. We first note that (i) the primary source of inefficiency is, unfortunately, the contextual embedding, e.g., language models produced by Transformer networks or other methods such as ELMo. These introduce at least one order of magnitude more parameters in the AS2 model"
2020.coling-main.457,D07-1003,0,0.219645,"ginal rank together with an effective word-relatedness encoder, we achieve the highest accuracy among the cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the ∼ 18 minutes required by a standard BERT-base fine-tuning. 1 Introduction In recent years, there has been a renewed interest in Question Answering (QA) led by industrial needs, e.g., the development of personal assistants and academic research on neural networks. Regarding the latter, AS2 (Wang et al., 2007; Yang et al., 2015) and Machine Reading Comprehension (MRC) (Richardson et al., 2013; Rajpurkar et al., 2016) have been largely explored. AS2 consists of selecting sentences that are answers to a target question from documents or paragraphs retrieved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic s"
2020.coling-main.457,W18-5446,0,0.0226473,"Missing"
2020.coling-main.457,D15-1237,0,0.528159,"with an effective word-relatedness encoder, we achieve the highest accuracy among the cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the ∼ 18 minutes required by a standard BERT-base fine-tuning. 1 Introduction In recent years, there has been a renewed interest in Question Answering (QA) led by industrial needs, e.g., the development of personal assistants and academic research on neural networks. Regarding the latter, AS2 (Wang et al., 2007; Yang et al., 2015) and Machine Reading Comprehension (MRC) (Richardson et al., 2013; Rajpurkar et al., 2016) have been largely explored. AS2 consists of selecting sentences that are answers to a target question from documents or paragraphs retrieved from the web by a search engine. MRC regards the extraction of an exact text span from a document answering the question, where the document is usually provided with the target question. Even though MRC is gaining more and more popularity, AS2 is more suitable for a production scenario, where a combination of a retrieval engine and an automatic sentence selector can"
2020.coling-main.457,N18-1142,0,0.0755781,"the word embeddings of Convolutional Neural Network (CNN). Subsequent approaches, e.g., (Wang and Jiang, 2016; Bian et al., 2017), use a word-level attention mechanism to identify the semantic overlap between each word in the question and each word in the answer candidate. Despite obtaining better results than previous approaches, the computational cost of performing word-level attention and the aggregation steps to leverage the information extracted by the attention mechanism increases the computational cost of previous methods. More recent models, e.g., (Lai et al., 2019; Garg et al., 2019; Yoon et al., 2018), leverage contextualized word representation, e.g., pre-trained using BERT, ELMo, RoBERTa, etc. These approaches achieve state-of-the-art results for AS2, but they require significant computational power for both pre-training, fine-tuning, and testing on the final task. 5214 4 Efficient Model for AS2 To build an efficient yet accurate model for AS2, it is crucial to leverage all the strong signals present in the dataset without increasing the complexity of the model itself. For this reason, we design a model as follows: (i) We build an efficient encoder to capture the question-candidate pair’"
2021.acl-long.252,2020.coling-main.457,1,0.877017,"Missing"
2021.acl-long.252,P17-1171,0,0.184577,"record producer. Walsh was awarded with the Vocal Group Hall of Fame in 2001. Table 1: A claim verification example from FEVER. Introduction Automated Question Answering (QA) research has received a renewed attention thanks to the diffusion of Virtual Assistants. Among the different types of methods to implement QA systems, we focus on Answer Sentence Selection (AS2) research, originated from TREC-QA track (Voorhees and Tice, 1999), as it proposes efficient models that are more suitable for a production setting, e.g., they are more efficient than those developed in machine reading (MR) work (Chen et al., 2017). Garg et al. (2020) proposed the TANDA approach based on pre-trained Transformer models, obtaining impressive improvement over the state of the art for AS2, measured on the two most used datasets, WikiQA (Yang et al., 2015) and TRECQA (Wang et al., 2007). However, TANDA was applied only to pointwise rerankers (PR), e.g., simple binary classifiers. Bonadiman and Moschitti ∗ Work done while the author was an intern at Amazon Alexa (2020) tried to improve this model by jointly modeling all answer candidates with listwise methods, e.g., (Bian et al., 2017). Unfortunately, merging the embeddings f"
2021.acl-long.252,P19-1221,0,0.0235181,"unction for training such networks is constituted by the 3253 contribution of all elements of its ranked items. The closest work to our research is by Bonadiman and Moschitti (2020), who designed several joint models. These improved early neural networks based on CNN and LSTM for AS2, but failed to improve the state of the art using pre-trained Transformer models. 2.2 Joint Models in Question Answering MR is a popular QA task that identifies an answer string in a paragraph or a text of limited size for a question. Its application to retrieval scenario has also been studied (Chen et al., 2017; Hu et al., 2019; Kratzwald and Feuerriegel, 2018). However, the large volume of retrieved content makes their use not practical yet. Moreover, the joint modeling aspect of MR regards sentences from the same paragraphs. Jin et al. (2020) use the relation between candidates in Multi-task learning approach for AS2. However, they do not exploit transformer models, thus their results are rather below the state of the art. In contrast with the work above, our modeling is driven by an answer support strategy, where the pieces of information are taken from different documents. This makes our model even more unique;"
2021.acl-long.252,D18-1055,0,0.0351321,"Missing"
2021.acl-long.252,2020.lrec-1.676,0,0.0317696,"er-weighted alignment networks (Shen et al., 2017), and pre-trained Transformer models, which are the state of the art. Garg et al. (2020) proposed TANDA, which is the current most accurate model on WikiQA and TREC-QA. Pairwise reranking: The method considers binary classifiers of the form χ(q, ci , cj ) for determining the partial rank between ci and cj , then the scoring function p(q, ci ) is obtained by summing up all the contributions with respect P to the target candidate t = ci , e.g., p(q, ci ) = j χ(q, ci , cj ). There has been a large body of work preceding Transformer models, e.g., (Laskar et al., 2020; Tayyar Madabushi et al., 2018; Rao et al., 2016). However, these methods are largely outperformed by the pointwise TANDA model. Listwise reranking: This approach, e.g., (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates. The loss function for training such networks is constituted by the 3253 contribution of all elements of its ranked items. The closest work to our research is by Bonadiman and Moschitti (2020), who designed several joint models. These improved early neural networks based on CNN and"
2021.acl-long.252,2021.ccl-1.108,0,0.0823016,"Missing"
2021.acl-long.252,2020.acl-main.655,0,0.208815,"the veracity of the claim. In contrast, Ev2 is neutral as it describes who Joe Walsh is but does not contribute to establish the induction. We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale. In this paper, we design joint models for AS2 based on the assumption that, given q and a target answer candidate t, the other answer candidates, (c1 , ..ck ) can provide positive, negative, or neutral support to decide the correctness of t. Our first approach exploits Fact Checking research: we adapted a state-of-the-art FEVER system, KGAT (Liu et al., 2020), for AS2. We defined a claim as 3252 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3252–3262 August 1–6, 2021. ©2021 Association for Computational Linguistics a pair constituted of the question and one target answer, while considering all the other answers as evidences. We re-trained and rebuilt all its embeddings for the AS2 task. Our second method, Answer Support-based Reranker (ASR), is completely new, it is based on the representation of the pair, (q, t), generated b"
2021.acl-long.252,S19-2149,0,0.0269976,"ork above, our modeling is driven by an answer support strategy, where the pieces of information are taken from different documents. This makes our model even more unique; it allows us to design innovative joint models, which are still not designed in any MR systems. 2.3 Fact Verification for Question Answering Fact verification has become a social need given the massive amount of information generated daily. The problem is, therefore, becoming increasingly important in NLP context (Mihaylova et al., 2018). In QA, answer verification is directly relevant due to its nature of content delivery (Mihaylova et al., 2019). The problem has been explored in MR setting (Wang et al., 2018). Zhang et al. (2020a) also proposed to fact check for product questions using additional associated evidence sentences. The latter are retrieved based on similarity scores computed with both TF-IDF and sentence-embeddings from pre-trained BERT models. While the process is technically sound, the retrieval of evidence is an expensive process, which is prohibitive to scale in production. We instead address this problem by leveraging the top answer candidates. 3 Baseline Models for AS2 In this section, we describe our baseline model"
2021.acl-long.252,D17-1122,0,0.0553964,"Missing"
2021.acl-long.252,C18-1278,0,0.015172,"ks (Shen et al., 2017), and pre-trained Transformer models, which are the state of the art. Garg et al. (2020) proposed TANDA, which is the current most accurate model on WikiQA and TREC-QA. Pairwise reranking: The method considers binary classifiers of the form χ(q, ci , cj ) for determining the partial rank between ci and cj , then the scoring function p(q, ci ) is obtained by summing up all the contributions with respect P to the target candidate t = ci , e.g., p(q, ci ) = j χ(q, ci , cj ). There has been a large body of work preceding Transformer models, e.g., (Laskar et al., 2020; Tayyar Madabushi et al., 2018; Rao et al., 2016). However, these methods are largely outperformed by the pointwise TANDA model. Listwise reranking: This approach, e.g., (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates. The loss function for training such networks is constituted by the 3253 contribution of all elements of its ranked items. The closest work to our research is by Bonadiman and Moschitti (2020), who designed several joint models. These improved early neural networks based on CNN and LSTM for AS2, but failed to imp"
2021.acl-long.252,N18-1074,0,0.0568648,"Missing"
2021.acl-long.252,W18-5501,0,0.0382764,"Missing"
2021.acl-long.252,D07-1003,0,0.449767,"sistants. Among the different types of methods to implement QA systems, we focus on Answer Sentence Selection (AS2) research, originated from TREC-QA track (Voorhees and Tice, 1999), as it proposes efficient models that are more suitable for a production setting, e.g., they are more efficient than those developed in machine reading (MR) work (Chen et al., 2017). Garg et al. (2020) proposed the TANDA approach based on pre-trained Transformer models, obtaining impressive improvement over the state of the art for AS2, measured on the two most used datasets, WikiQA (Yang et al., 2015) and TRECQA (Wang et al., 2007). However, TANDA was applied only to pointwise rerankers (PR), e.g., simple binary classifiers. Bonadiman and Moschitti ∗ Work done while the author was an intern at Amazon Alexa (2020) tried to improve this model by jointly modeling all answer candidates with listwise methods, e.g., (Bian et al., 2017). Unfortunately, merging the embeddings from all candidates with standard approaches, e.g., CNN or LSTM, did not improve over TANDA. A more structured approach to building joint models over sentences can instead be observed in Fact Verification Systems, e.g., the methods developed in the FEVER c"
2021.acl-long.252,P18-1178,0,0.0203048,"the pieces of information are taken from different documents. This makes our model even more unique; it allows us to design innovative joint models, which are still not designed in any MR systems. 2.3 Fact Verification for Question Answering Fact verification has become a social need given the massive amount of information generated daily. The problem is, therefore, becoming increasingly important in NLP context (Mihaylova et al., 2018). In QA, answer verification is directly relevant due to its nature of content delivery (Mihaylova et al., 2019). The problem has been explored in MR setting (Wang et al., 2018). Zhang et al. (2020a) also proposed to fact check for product questions using additional associated evidence sentences. The latter are retrieved based on similarity scores computed with both TF-IDF and sentence-embeddings from pre-trained BERT models. While the process is technically sound, the retrieval of evidence is an expensive process, which is prohibitive to scale in production. We instead address this problem by leveraging the top answer candidates. 3 Baseline Models for AS2 In this section, we describe our baseline models, which are constituted by pointwise, pairwise, and listwise str"
2021.acl-long.252,D15-1237,0,0.0776292,"Missing"
2021.acl-long.252,C00-2137,0,0.352395,"TREC-QA Table 5: Results on WikiQA and TREC-QA, using RoBERTa Large Transformer. • PR replicates the MAP and MRR of the stateof-the-art reranker by Garg et al. (2020) on WikiQA. • Joint Model Pairwise differs from ASR as it concatenates the embeddings of the (q, ci ), instead of using max-pooling, and does not use any Answer Support Classifier (ASC). Still, it exploits the idea of aggregating the information of all pairs (q, ci ) with respect to a target answer t, which proves to be effective, as the model improves on PR over all measures and datasets. WikiQA • We perform randomization test (Yeh, 2000) to verify if the models significantly differ in terms of prediction outcome. We use 100,000 trials for each calculation. The results confirm the statistically significant difference between ASR and all the baselines, with p < 0.05 for WikiQA, and between ASR and all models (i.e., including also KGAT) on WQA. 5.5 Official State of the art As the state of the art for AS2 is obtained using RoBERTa Large, we trained KGAT and ASR using this pre-trained language model. Table 5 also reports the comparison with PR, which is the official state of the art. Again, our PR replicates the results of Garg e"
2021.acl-long.252,2020.emnlp-main.188,0,0.0259259,"mation are taken from different documents. This makes our model even more unique; it allows us to design innovative joint models, which are still not designed in any MR systems. 2.3 Fact Verification for Question Answering Fact verification has become a social need given the massive amount of information generated daily. The problem is, therefore, becoming increasingly important in NLP context (Mihaylova et al., 2018). In QA, answer verification is directly relevant due to its nature of content delivery (Mihaylova et al., 2019). The problem has been explored in MR setting (Wang et al., 2018). Zhang et al. (2020a) also proposed to fact check for product questions using additional associated evidence sentences. The latter are retrieved based on similarity scores computed with both TF-IDF and sentence-embeddings from pre-trained BERT models. While the process is technically sound, the retrieval of evidence is an expensive process, which is prohibitive to scale in production. We instead address this problem by leveraging the top answer candidates. 3 Baseline Models for AS2 In this section, we describe our baseline models, which are constituted by pointwise, pairwise, and listwise strategies. 3.1 Pointwi"
2021.acl-short.28,U19-1008,0,0.0221712,"er summarization datasets (Hermann et al., 2015; Grusky et al., 2018; Koupaee and Wang, 2018; Ladhak et al., 2020) offer a summary in the form of a key points list (i.e., highlights). In this paper, we focus on coherent paragraph summarization datasets. ∗ Co-first author Work done while at Amazon 1 The dataset and human evaluation are available at https://registry.opendata.aws/wikisum. Automatic evaluation of summarization systems, e.g., by using the ROUGE metric, is challenging (Lloret et al., 2018) and is often inconsistent with human evaluation (Liu and Liu, 2008; Cohan and Goharian, 2016; Tay et al., 2019; Huang et al., 2020). To understand – and later improve – the quality of summarization systems, it is necessary to conduct a human evaluation. A human evaluation’s quality depends on the ease of reading and understanding of the measured text: a simple text does not require annotators with unique expertise, can be evaluated faster, and is easier to annotate correctly. However, existing coherent-paragraph summarization datasets consist of academic papers and cannot be considered easy to read. Evaluating such summarization samples requires unique expertise, takes time, and comes at a high cost."
2021.eacl-main.261,2020.acl-tutorials.8,0,0.403926,"Missing"
2021.eacl-main.261,D19-1243,0,0.0568148,"Missing"
2021.eacl-main.261,2020.tacl-1.5,0,0.316963,"webpages as independent candidate answers for a given question. For any webpage containing potential answer candidates for a question, AS2 models first extract individual sentences, then independently estimate their likelihood of being correct answers; this approach enable highly efficient processing of entire documents. However, under this framework, context information from the entire webpage (global context), which could be crucial for selecting correct answers, is ignored. Conversely, current systems in Machine Reading (MR) (Huang et al., 2019; Kwiatkowski et al., 2019a; Lee et al., 2019; Joshi et al., 2020a) uses a much larger context from the retrieved documents. MR models receive a question and one or more passages retrieved through a search engine as input; they then select one or more spans from the input passages to return as answer. While being potentially more accurate, MR models typically have higher computational requirements (and thus higher latency) than AS2 models. That is because MR models need to process passages in their entirety before an answer can be extracted; conversely, AS2 systems break down paragraphs in candidate sentences, and evaluate them all at once in parallel. Ther"
2021.eacl-main.261,Q19-1026,0,0.169268,"-domain QA typically consider sentences from webpages as independent candidate answers for a given question. For any webpage containing potential answer candidates for a question, AS2 models first extract individual sentences, then independently estimate their likelihood of being correct answers; this approach enable highly efficient processing of entire documents. However, under this framework, context information from the entire webpage (global context), which could be crucial for selecting correct answers, is ignored. Conversely, current systems in Machine Reading (MR) (Huang et al., 2019; Kwiatkowski et al., 2019a; Lee et al., 2019; Joshi et al., 2020a) uses a much larger context from the retrieved documents. MR models receive a question and one or more passages retrieved through a search engine as input; they then select one or more spans from the input passages to return as answer. While being potentially more accurate, MR models typically have higher computational requirements (and thus higher latency) than AS2 models. That is because MR models need to process passages in their entirety before an answer can be extracted; conversely, AS2 systems break down paragraphs in candidate sentences, and eval"
2021.eacl-main.261,2021.ccl-1.108,0,0.0848816,"Missing"
2021.eacl-main.261,P18-1160,0,0.0189024,"2. 3005 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3005–3010 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Early neural models for retrieval-based QA focused on incorporated neighboring sentences (local context) to improve performance. For example, Tan et al. (2017) proposed a neural architecture based on gated recurrent units to encode question, answer, and local context; their approach, while effective at the time, shows a significant gap to the current state-of-the-art models (Garg et al., 2020). Min et al. (2018) studied neural efficient models for MR by optimizing answer candidate extraction. More recently, researchers have focused in including source document information in transformer models. For example, Joshi et al. (2020b) proposed a contextualized model for MR that augments named entities in candidate passages with snippets extracted from Wikipedia pages. Their approach, while interesting, is limited to entitiesbased context, and specific to Wikipedia and MR domain. For AS2, Lauriola and Moschitti (2021) proposed a model that uses local context as defined by the preceding and following sentence"
2021.eacl-main.261,2020.acl-main.504,1,0.595353,"assages to return as answer. While being potentially more accurate, MR models typically have higher computational requirements (and thus higher latency) than AS2 models. That is because MR models need to process passages in their entirety before an answer can be extracted; conversely, AS2 systems break down paragraphs in candidate sentences, and evaluate them all at once in parallel. Therefore, in many practical applications, MR models are only used to examine 10 to 50 candidate passages; in contrast, AS2 approaches can potentially process hundreds of documents, e.g., (Matsubara et al., 2020; Soldaini and Moschitti, 2020). In this work, we study techniques that can combine the efficacy of MR models with the efficiency of AS2 approaches, while keeping a single sentence as target answer, as in related AS2 works1 . In particular, we focus our efforts on improving accuracy of AS2 systems without affecting their latency. ∗ Work was conducted while the author was an intern at Amazon Alexa. 1 MR systems have a different aim than AS2. 3005 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3005–3010 April 19 - 23, 2021. ©2021 Association for Computational"
2021.eacl-main.266,W16-2347,0,0.087995,"etting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages. 1 Introduction Online machine translation (MT) services require industrial-scale training data, i.e., significantly large and high-quality parallel sentences, to build accurate models. Exploiting the web for multilingual content has become a usual strategy in collecting large-scale parallel sentences for MT (Uszkoreit et al., 2010; Smith et al., 2013; Buck and Koehn, 2016a). Structural Translation Recognition for Acquiring Natural Data (STRAND) (Resnik and Smith, 2003) is a standard pipeline to extract parallel data from the web, consisting in three steps: (i) bilingual document alignment for an input set of documents, (ii) sentence alignment for each aligned document pair, and (iii) sentence filtering for nontranslation or boilerplate cleaning. The first step of identifying bilingual documents is technically challenging and made more complicated by the presence of large and noisy documents from web data. In the WMT-16 Bilingual Document Alignment Shared Task"
2021.eacl-main.266,W16-2365,0,0.0758928,"etting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages. 1 Introduction Online machine translation (MT) services require industrial-scale training data, i.e., significantly large and high-quality parallel sentences, to build accurate models. Exploiting the web for multilingual content has become a usual strategy in collecting large-scale parallel sentences for MT (Uszkoreit et al., 2010; Smith et al., 2013; Buck and Koehn, 2016a). Structural Translation Recognition for Acquiring Natural Data (STRAND) (Resnik and Smith, 2003) is a standard pipeline to extract parallel data from the web, consisting in three steps: (i) bilingual document alignment for an input set of documents, (ii) sentence alignment for each aligned document pair, and (iii) sentence filtering for nontranslation or boilerplate cleaning. The first step of identifying bilingual documents is technically challenging and made more complicated by the presence of large and noisy documents from web data. In the WMT-16 Bilingual Document Alignment Shared Task"
2021.eacl-main.266,W16-2366,0,0.0559729,"Missing"
2021.eacl-main.266,2020.aacl-main.62,0,0.0481505,"Missing"
2021.eacl-main.266,W16-2369,0,0.0548464,"Missing"
2021.eacl-main.266,2020.clssts-1.5,0,0.0173056,"h millions of documents and processing up to 28 languages, including low-resourced languages. In the remainder of this paper, we summarize the previous work regarding document alignment in Section 2. We then describe the proposed system and our experiments in sections 3 and 4, respectively. Finally, we derive the conclusions of the paper in Section 5. 2 Related Work Aligning multilingual documents is the key required processing in most multilingual text processing pipelines, including cross-lingual information retrieval (Steinberger et al., 2002; Pouliquen et al., 2004; Vulic and Moens, 2015; Jiang et al., 2020) and parallel data extraction. In the context of creating parallel training data for MT, the problem has been studied in the literature for comparable corpora (Munteanu et al., 2004; Vu et al., 2009; Pal et al., 2014) and web-structured parallel data extraction (Resnik, 1999; Uszkoreit et al., 2010; Buck and Koehn, 2016a). We focus on the latter in this paper. WMT-16 Bilingual Document Alignment Shared Task is a recent shared-task focusing on identifying bilingual documents from crawled websites (Buck and Koehn, 2016b). The top 3 systems are YODA (Dara and Lin, 2016), NOVALINCS (Gomes and Pere"
2021.eacl-main.266,2005.mtsummit-papers.11,0,0.270455,"Missing"
2021.eacl-main.266,N04-1034,0,0.183287,"gnment in Section 2. We then describe the proposed system and our experiments in sections 3 and 4, respectively. Finally, we derive the conclusions of the paper in Section 5. 2 Related Work Aligning multilingual documents is the key required processing in most multilingual text processing pipelines, including cross-lingual information retrieval (Steinberger et al., 2002; Pouliquen et al., 2004; Vulic and Moens, 2015; Jiang et al., 2020) and parallel data extraction. In the context of creating parallel training data for MT, the problem has been studied in the literature for comparable corpora (Munteanu et al., 2004; Vu et al., 2009; Pal et al., 2014) and web-structured parallel data extraction (Resnik, 1999; Uszkoreit et al., 2010; Buck and Koehn, 2016a). We focus on the latter in this paper. WMT-16 Bilingual Document Alignment Shared Task is a recent shared-task focusing on identifying bilingual documents from crawled websites (Buck and Koehn, 2016b). The top 3 systems are YODA (Dara and Lin, 2016), NOVALINCS (Gomes and Pereira Lopes, 2016), and UEDIN1 COSINE (Buck and Koehn, 2016b). The first two require costly features, such as (i) ngram comparison after translating all non-English text into English"
2021.eacl-main.266,J03-1002,0,0.0316485,"Missing"
2021.eacl-main.266,W14-1009,0,0.0312437,"e proposed system and our experiments in sections 3 and 4, respectively. Finally, we derive the conclusions of the paper in Section 5. 2 Related Work Aligning multilingual documents is the key required processing in most multilingual text processing pipelines, including cross-lingual information retrieval (Steinberger et al., 2002; Pouliquen et al., 2004; Vulic and Moens, 2015; Jiang et al., 2020) and parallel data extraction. In the context of creating parallel training data for MT, the problem has been studied in the literature for comparable corpora (Munteanu et al., 2004; Vu et al., 2009; Pal et al., 2014) and web-structured parallel data extraction (Resnik, 1999; Uszkoreit et al., 2010; Buck and Koehn, 2016a). We focus on the latter in this paper. WMT-16 Bilingual Document Alignment Shared Task is a recent shared-task focusing on identifying bilingual documents from crawled websites (Buck and Koehn, 2016b). The top 3 systems are YODA (Dara and Lin, 2016), NOVALINCS (Gomes and Pereira Lopes, 2016), and UEDIN1 COSINE (Buck and Koehn, 2016b). The first two require costly features, such as (i) ngram comparison after translating all non-English text into English (Dara and Lin, 2016), and (ii) phras"
2021.eacl-main.266,C04-1138,0,0.152153,"Missing"
2021.eacl-main.266,P99-1068,0,0.415241,"ectively. Finally, we derive the conclusions of the paper in Section 5. 2 Related Work Aligning multilingual documents is the key required processing in most multilingual text processing pipelines, including cross-lingual information retrieval (Steinberger et al., 2002; Pouliquen et al., 2004; Vulic and Moens, 2015; Jiang et al., 2020) and parallel data extraction. In the context of creating parallel training data for MT, the problem has been studied in the literature for comparable corpora (Munteanu et al., 2004; Vu et al., 2009; Pal et al., 2014) and web-structured parallel data extraction (Resnik, 1999; Uszkoreit et al., 2010; Buck and Koehn, 2016a). We focus on the latter in this paper. WMT-16 Bilingual Document Alignment Shared Task is a recent shared-task focusing on identifying bilingual documents from crawled websites (Buck and Koehn, 2016b). The top 3 systems are YODA (Dara and Lin, 2016), NOVALINCS (Gomes and Pereira Lopes, 2016), and UEDIN1 COSINE (Buck and Koehn, 2016b). The first two require costly features, such as (i) ngram comparison after translating all non-English text into English (Dara and Lin, 2016), and (ii) phrase-table of statistical MT (SMT) as a dictionary (Gomes and"
2021.eacl-main.266,J03-3002,0,0.550614,"Missing"
2021.eacl-main.266,W18-6488,0,0.0607664,"Missing"
2021.eacl-main.266,P13-1135,0,0.0243077,"A in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages. 1 Introduction Online machine translation (MT) services require industrial-scale training data, i.e., significantly large and high-quality parallel sentences, to build accurate models. Exploiting the web for multilingual content has become a usual strategy in collecting large-scale parallel sentences for MT (Uszkoreit et al., 2010; Smith et al., 2013; Buck and Koehn, 2016a). Structural Translation Recognition for Acquiring Natural Data (STRAND) (Resnik and Smith, 2003) is a standard pipeline to extract parallel data from the web, consisting in three steps: (i) bilingual document alignment for an input set of documents, (ii) sentence alignment for each aligned document pair, and (iii) sentence filtering for nontranslation or boilerplate cleaning. The first step of identifying bilingual documents is technically challenging and made more complicated by the presence of large and noisy documents from web data. In the WMT-16 Bilingual Document"
2021.eacl-main.266,C10-1124,0,0.190982,"ine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages. 1 Introduction Online machine translation (MT) services require industrial-scale training data, i.e., significantly large and high-quality parallel sentences, to build accurate models. Exploiting the web for multilingual content has become a usual strategy in collecting large-scale parallel sentences for MT (Uszkoreit et al., 2010; Smith et al., 2013; Buck and Koehn, 2016a). Structural Translation Recognition for Acquiring Natural Data (STRAND) (Resnik and Smith, 2003) is a standard pipeline to extract parallel data from the web, consisting in three steps: (i) bilingual document alignment for an input set of documents, (ii) sentence alignment for each aligned document pair, and (iii) sentence filtering for nontranslation or boilerplate cleaning. The first step of identifying bilingual documents is technically challenging and made more complicated by the presence of large and noisy documents from web data. In the WMT-16"
2021.eacl-main.266,E09-1096,1,0.717943,"then describe the proposed system and our experiments in sections 3 and 4, respectively. Finally, we derive the conclusions of the paper in Section 5. 2 Related Work Aligning multilingual documents is the key required processing in most multilingual text processing pipelines, including cross-lingual information retrieval (Steinberger et al., 2002; Pouliquen et al., 2004; Vulic and Moens, 2015; Jiang et al., 2020) and parallel data extraction. In the context of creating parallel training data for MT, the problem has been studied in the literature for comparable corpora (Munteanu et al., 2004; Vu et al., 2009; Pal et al., 2014) and web-structured parallel data extraction (Resnik, 1999; Uszkoreit et al., 2010; Buck and Koehn, 2016a). We focus on the latter in this paper. WMT-16 Bilingual Document Alignment Shared Task is a recent shared-task focusing on identifying bilingual documents from crawled websites (Buck and Koehn, 2016b). The top 3 systems are YODA (Dara and Lin, 2016), NOVALINCS (Gomes and Pereira Lopes, 2016), and UEDIN1 COSINE (Buck and Koehn, 2016b). The first two require costly features, such as (i) ngram comparison after translating all non-English text into English (Dara and Lin, 20"
2021.eacl-main.266,D17-1319,0,0.0592462,"Missing"
2021.emnlp-main.583,P17-1171,0,0.0260405,"ng CNNs (Severyn and Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015) have been supplemented with datasets such as ASNQ (Garg et al., 2020) having several million QA pairs. Open Domain QA (ODQA) (Chen et al., 2017; Chen and Yih, 2020) systems involve a combination of a retriever and a reader (Semnani and Pandey, 2020) trained independently (Yang et al., 2017) or jointly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agichtein, 2016; Sun et al., 2018; Xiong et al., 2019; Lu et al., 2019). Numerous research works have proposed different techniques for improving the performance on ODQA (Min et al., 2019; Asai et al., 2019; Wang et al., 2019; Qi et al., 2019). Filtering Ill-formed Questions E"
2021.emnlp-main.583,2020.acl-tutorials.8,0,0.016425,"d Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015) have been supplemented with datasets such as ASNQ (Garg et al., 2020) having several million QA pairs. Open Domain QA (ODQA) (Chen et al., 2017; Chen and Yih, 2020) systems involve a combination of a retriever and a reader (Semnani and Pandey, 2020) trained independently (Yang et al., 2017) or jointly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agichtein, 2016; Sun et al., 2018; Xiong et al., 2019; Lu et al., 2019). Numerous research works have proposed different techniques for improving the performance on ODQA (Min et al., 2019; Asai et al., 2019; Wang et al., 2019; Qi et al., 2019). Filtering Ill-formed Questions Evaluating wellformedn"
2021.emnlp-main.583,N19-1423,0,0.0347419,"on, etc. To ensure that our question filters are learning the capability of the QA system and not these artifacts, we consider datasets from industrial scenarios (where questions are real customer queries) like ASNQ, AQAD 7 and WikiQA in addition to SQuAD. 5.2 Models For each of the three academic datasets, we use two transformer based models (12 and 24 layer) as M: state-of-the-art RoBERTa-Base and RoBERTaLarge trained with TANDA for WikiQA 2 (Garg et al., 2020); RoBERTa-Base and RoBERTa-LargeMNLI fine-tuned on ASNQ 2 (Garg et al., 2020); and, BERT-Base and BERT-Large fine-tuned on SQuAD1.1 (Devlin et al., 2019). For AQAD, we use ELECTRA-Base trained using TANDA (Garg et al., 2020) after an initial transfer on ASNQ as M. For the question filter F, we use two different transformer based models (RoBERTa-Base, Large) for each of the four datasets. For WikiQA, ASNQ and SQuAD1.1, the RoBERTa-Base F is used for the 12-layer M and the RoBERTa-Large F is used for the 24-layer M. For AQAD we train both the RoBERTa-Base and RoBERTa-Large F for the single ELECTRA-Base M. All experimental details are presented in Appendix B, C for reproducibility. 5.3 Baselines To demonstrate efficacy of our question filters, we"
2021.emnlp-main.583,P13-1158,0,0.0402801,"ly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agichtein, 2016; Sun et al., 2018; Xiong et al., 2019; Lu et al., 2019). Numerous research works have proposed different techniques for improving the performance on ODQA (Min et al., 2019; Asai et al., 2019; Wang et al., 2019; Qi et al., 2019). Filtering Ill-formed Questions Evaluating wellformedness and intelligibility of queries has been a popular research topic for QA systems. Faruqui 7330 and Das annotate the Paralex dataset (Fader et al., 2013) on the well-formedness of the questions. The majority of research efforts have been aimed at reformulating user queries to elicit the best possible answer from the QA system (Yang et al., 2014; Buck et al., 2017; Chu et al., 2019). A complementary line of work uses hate speech detection techniques (Gupta et al., 2020) to filter questions that incite hate on the basis of race, religion, etc. Answer Verification QA systems sometimes use an answer validation component in addition to the system threshold, which analyzes the answer produced by the system and decides whether to answer or abstain. T"
2021.emnlp-main.583,D18-1091,0,0.11984,"lin2019) and transformer-based QA models (Sanh guistically complex questions have a lower probet al., 2019; Soldaini and Moschitti, 2020). ability to be answered. Language complexity corAn alternate solution for improving QA system relates with syntactic, semantic and lexical properefficiency aims to discard questions that will most ties, which have been shown to be well captured by probably be incorrectly answered by the system, us- pre-trained language models (LMs) (Jawahar et al., ing automatic classifiers. For example, Fader et al. 2019). Thus, the final answer extractor will be af(2013); Faruqui and Das (2018) aim to capture the fected by said complexity, suggesting that we can grammaticality and well-formedness of questions. predict which questions are likely to be unanswered However, these methods do not take the specific just using their surface forms. 7329 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7329–7346 c November 7–11, 2021. 2021 Association for Computational Linguistics Third, pre-training transformer-based LMs on huge amounts of web data enables them to implicitly capture the frequency/popularity of general phrases1 , among which entiti"
2021.emnlp-main.583,2021.eacl-main.261,1,0.797546,"he development of large-scale QA datasets like SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), etc. with increasingly challenging types of questions. For the task of AS2, initial efforts embedded the question and candidates using CNNs (Severyn and Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015) have been supplemented with datasets such as ASNQ (Garg et al., 2020) having several million QA pairs. Open Domain QA (ODQA) (Chen et al., 2017; Chen and Yih, 2020) systems involve a combination of a retriever and a reader (Semnani and Pandey, 2020) trained independently (Yang et al., 2017) or jointly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agic"
2021.emnlp-main.583,2020.findings-emnlp.372,0,0.220937,"rch (Tan et al., 2019), etc. Towards reducing compute of the answer model, the following techniques have been explored: multi-stage ranking using progressively larger models (Matsubara et al., 2020), using intermediate representations for early elimination of negative candidates (Soldaini and Moschitti, 2020; Xin et al., 2020), combining separate encoding of question and answer with shallow DNNs (Chen et al., 2020), and the most popular being knowledge distillation (Hinton et al., 2015) to train smaller transformer models with low inference latencies (DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), MobileBERT (Sun et al., 2020), etc.) Query Performance Prediction Pre-retrieval query difficulty prediction has been previously ex- 3 Preliminaries and Problem Setting plored in Information Retrieval (Carmel and YomTov, 2010). Previous works (He and Ounis, 2004; We first provide details of QA systems and explain the cost-saving opportunity space when they operMothe and Tanguy, 2005; He et al., 2008; Zhao ate at a given Precision (or answer score threshold). et al., 2008; Hauff, 2010) target p(a|q, f ), ground truth probability of an answer a to be correct, given a question q and a feature se"
2021.emnlp-main.583,2020.acl-main.503,0,0.338607,"on, etc. Answer Verification QA systems sometimes use an answer validation component in addition to the system threshold, which analyzes the answer produced by the system and decides whether to answer or abstain. These systems often use external entity knowledge (Magnini et al., 2002; Ko et al., 2007; Gondek et al., 2012) for basing their decision to verify the correctness of the answer. Recently Wang et al. (2020) propose to add a new MR model to reflect on the predictions of the original MR model to decide whether the produced answer is correct or not. Other efforts (Rodriguez et al., 2019; Kamath et al., 2020; Jia and Xie, 2020; Zhang et al., 2021) have trained calibrators for verifying if the question should be answered or not. All these works are fundamentally different from our question filtering approach since they operate jointly on the question and generated answer, thereby requiring the entire computation to be performed by the QA system before making a decision. Our work operates only on the question text to preemptively decide whether to filter it or not. Thus the primary goal of these existing works is to improve the precision of the answering model by not answering when not confident, w"
2021.emnlp-main.583,N07-1066,0,0.060698,"ts have been aimed at reformulating user queries to elicit the best possible answer from the QA system (Yang et al., 2014; Buck et al., 2017; Chu et al., 2019). A complementary line of work uses hate speech detection techniques (Gupta et al., 2020) to filter questions that incite hate on the basis of race, religion, etc. Answer Verification QA systems sometimes use an answer validation component in addition to the system threshold, which analyzes the answer produced by the system and decides whether to answer or abstain. These systems often use external entity knowledge (Magnini et al., 2002; Ko et al., 2007; Gondek et al., 2012) for basing their decision to verify the correctness of the answer. Recently Wang et al. (2020) propose to add a new MR model to reflect on the predictions of the original MR model to decide whether the produced answer is correct or not. Other efforts (Rodriguez et al., 2019; Kamath et al., 2020; Jia and Xie, 2020; Zhang et al., 2021) have trained calibrators for verifying if the question should be answered or not. All these works are fundamentally different from our question filtering approach since they operate jointly on the question and generated answer, thereby requi"
2021.emnlp-main.583,Q19-1026,0,0.0148338,"ch LMs can capture well. 2 Code will be released soon at https://github. com/alexa/wqa-question-filtering 2 Related Work Question Answering Prior efforts on QA have been broadly categorized into two fronts: tackling MR, and AS2. For the former, recently pre-trained transformer models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020), etc. have achieved SOTA performance, sometimes even exceeding the human performance. Progress on this front has also seen the development of large-scale QA datasets like SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), etc. with increasingly challenging types of questions. For the task of AS2, initial efforts embedded the question and candidates using CNNs (Severyn and Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al., 2007) and WikiQA (Yang et al."
2021.emnlp-main.583,2020.acl-main.537,0,0.0166858,"et (Faruqui and Das, 2018) 8 . We denote the resulting filter by FW . For the second baseline, we train a question classifier which predicts whether M will correctly answer a question. This idea has been studied in very recent contemporary works (Varshney et al., 2020; Chakravarti and Sil, 2021) but for answer verification (not for efficiency). We fine-tune RoBERTa-Base, Large for each dataset to predict whether the target M correctly answers the question or not. We denote this filter by FC . We exclude comparisons with early exiting strategies (Soldaini and Moschitti, 2020; Xin et al., 2020; Liu et al., 2020) that adaptively reduce the number of transformer layers per sample and aim to improve efficiency of M instead of Ω. Inference batching strategy with multiple samples cannot exploit this efficiency benefit directly, thus these works report efficiency gains through abstract concepts such as FLOPs (Floating Point Operations per Second) using an inference batch-size=1, which is not practical. The efficiency gains from our approach are tangible, since filtering questions can scale down the required number of GPU-compute instances. Furthermore, ideas from these works can easily be combined with our"
2021.emnlp-main.583,2021.ccl-1.108,0,0.0614846,"Missing"
2021.emnlp-main.583,P02-1054,0,0.314318,"rity of research efforts have been aimed at reformulating user queries to elicit the best possible answer from the QA system (Yang et al., 2014; Buck et al., 2017; Chu et al., 2019). A complementary line of work uses hate speech detection techniques (Gupta et al., 2020) to filter questions that incite hate on the basis of race, religion, etc. Answer Verification QA systems sometimes use an answer validation component in addition to the system threshold, which analyzes the answer produced by the system and decides whether to answer or abstain. These systems often use external entity knowledge (Magnini et al., 2002; Ko et al., 2007; Gondek et al., 2012) for basing their decision to verify the correctness of the answer. Recently Wang et al. (2020) propose to add a new MR model to reflect on the predictions of the original MR model to decide whether the produced answer is correct or not. Other efforts (Rodriguez et al., 2019; Kamath et al., 2020; Jia and Xie, 2020; Zhang et al., 2021) have trained calibrators for verifying if the question should be answered or not. All these works are fundamentally different from our question filtering approach since they operate jointly on the question and generated answ"
2021.emnlp-main.583,D19-1261,0,0.0175638,"QA pairs. Open Domain QA (ODQA) (Chen et al., 2017; Chen and Yih, 2020) systems involve a combination of a retriever and a reader (Semnani and Pandey, 2020) trained independently (Yang et al., 2017) or jointly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agichtein, 2016; Sun et al., 2018; Xiong et al., 2019; Lu et al., 2019). Numerous research works have proposed different techniques for improving the performance on ODQA (Min et al., 2019; Asai et al., 2019; Wang et al., 2019; Qi et al., 2019). Filtering Ill-formed Questions Evaluating wellformedness and intelligibility of queries has been a popular research topic for QA systems. Faruqui 7330 and Das annotate the Paralex dataset (Fader et al., 2013) on the well-formedness of the questions. The majority of research efforts have been aimed at reformulating user queries to elicit the best possible answer from the QA system (Yang et al., 2014; Buck et al., 2017; Chu et al., 2019). A complementary line of work uses hate speech detection techniques (Gupta et al., 2020) to filter questions that incite hate on the basis of race, religion,"
2021.emnlp-main.583,D16-1264,0,0.331788,"ords, not necessarily specific to a grammatical theory, which LMs can capture well. 2 Code will be released soon at https://github. com/alexa/wqa-question-filtering 2 Related Work Question Answering Prior efforts on QA have been broadly categorized into two fronts: tackling MR, and AS2. For the former, recently pre-trained transformer models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020), etc. have achieved SOTA performance, sometimes even exceeding the human performance. Progress on this front has also seen the development of large-scale QA datasets like SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), etc. with increasingly challenging types of questions. For the task of AS2, initial efforts embedded the question and candidates using CNNs (Severyn and Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small dat"
2021.emnlp-main.583,D17-1122,0,0.0565783,"Missing"
2021.emnlp-main.583,2020.acl-main.504,1,0.928172,"illions of users. Optimizing the efficiency cause Transformer-based models are used for anof such systems is vital to reduce their operational swer extraction in most research areas in QA with costs. Recently, there has been a large body of re- unstructured text, e.g., Machine Reading(MR) (Rasearch devoted towards reducing the compute com- jpurkar et al., 2016), and Answer Sentence Selecplexity of retrieval (Gallagher et al., 2019; Tan et al., tion(AS2) (Garg et al., 2020). Second, more lin2019) and transformer-based QA models (Sanh guistically complex questions have a lower probet al., 2019; Soldaini and Moschitti, 2020). ability to be answered. Language complexity corAn alternate solution for improving QA system relates with syntactic, semantic and lexical properefficiency aims to discard questions that will most ties, which have been shown to be well captured by probably be incorrectly answered by the system, us- pre-trained language models (LMs) (Jawahar et al., ing automatic classifiers. For example, Fader et al. 2019). Thus, the final answer extractor will be af(2013); Faruqui and Das (2018) aim to capture the fected by said complexity, suggesting that we can grammaticality and well-formedness of questio"
2021.emnlp-main.583,D18-1455,0,0.0187563,"d Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015) have been supplemented with datasets such as ASNQ (Garg et al., 2020) having several million QA pairs. Open Domain QA (ODQA) (Chen et al., 2017; Chen and Yih, 2020) systems involve a combination of a retriever and a reader (Semnani and Pandey, 2020) trained independently (Yang et al., 2017) or jointly (Yang et al., 2019). Efforts in ODQA transitioned from using knowledge bases for answering questions to using external text sources and web articles (Savenkov and Agichtein, 2016; Sun et al., 2018; Xiong et al., 2019; Lu et al., 2019). Numerous research works have proposed different techniques for improving the performance on ODQA (Min et al., 2019; Asai et al., 2019; Wang et al., 2019; Qi et al., 2019). Filtering Ill-formed Questions Evaluating wellformedness and intelligibility of queries has been a popular research topic for QA systems. Faruqui 7330 and Das annotate the Paralex dataset (Fader et al., 2013) on the well-formedness of the questions. The majority of research efforts have been aimed at reformulating user queries to elicit the best possible answer from the QA system (Yang"
2021.emnlp-main.583,2020.acl-main.195,0,0.0211622,"ards reducing compute of the answer model, the following techniques have been explored: multi-stage ranking using progressively larger models (Matsubara et al., 2020), using intermediate representations for early elimination of negative candidates (Soldaini and Moschitti, 2020; Xin et al., 2020), combining separate encoding of question and answer with shallow DNNs (Chen et al., 2020), and the most popular being knowledge distillation (Hinton et al., 2015) to train smaller transformer models with low inference latencies (DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), MobileBERT (Sun et al., 2020), etc.) Query Performance Prediction Pre-retrieval query difficulty prediction has been previously ex- 3 Preliminaries and Problem Setting plored in Information Retrieval (Carmel and YomTov, 2010). Previous works (He and Ounis, 2004; We first provide details of QA systems and explain the cost-saving opportunity space when they operMothe and Tanguy, 2005; He et al., 2008; Zhao ate at a given Precision (or answer score threshold). et al., 2008; Hauff, 2010) target p(a|q, f ), ground truth probability of an answer a to be correct, given a question q and a feature set f in input using sim- 3.1 QA"
2021.emnlp-main.583,D19-1527,0,0.0270974,"when not confident, while our work aims to improve efficiency of the QA system and save runtime compute cost. Figure 1: A real-world QA system having a retrieval (R), candidate extraction (S) and answering component (M). Our proposed question filter (highlighted by the red box) preemptively removes the questions which will fail the threshold τ1 of M. Efficient QA Several works on improving the efficiency of the retrieval involve using a cascade of rerankers to quickly identify good documents (Wang et al., 2011, 2016; Gallagher et al., 2019), nonmetric matching functions for efficient search (Tan et al., 2019), etc. Towards reducing compute of the answer model, the following techniques have been explored: multi-stage ranking using progressively larger models (Matsubara et al., 2020), using intermediate representations for early elimination of negative candidates (Soldaini and Moschitti, 2020; Xin et al., 2020), combining separate encoding of question and answer with shallow DNNs (Chen et al., 2020), and the most popular being knowledge distillation (Hinton et al., 2015) to train smaller transformer models with low inference latencies (DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), Mo"
2021.emnlp-main.583,N19-4013,0,0.0388498,"Missing"
2021.emnlp-main.583,D15-1237,0,0.0705996,"Missing"
2021.emnlp-main.583,D18-1259,0,0.0222833,"grammatical theory, which LMs can capture well. 2 Code will be released soon at https://github. com/alexa/wqa-question-filtering 2 Related Work Question Answering Prior efforts on QA have been broadly categorized into two fronts: tackling MR, and AS2. For the former, recently pre-trained transformer models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020), etc. have achieved SOTA performance, sometimes even exceeding the human performance. Progress on this front has also seen the development of large-scale QA datasets like SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), etc. with increasingly challenging types of questions. For the task of AS2, initial efforts embedded the question and candidates using CNNs (Severyn and Moschitti, 2015), weight aligned networks (Shen et al., 2017; Tran et al., 2018; Tay et al., 2018) and compare-aggregate architectures (Wang and Jiang, 2016; Bian et al., 2017; Yoon et al., 2019). Recent progress has stemmed from the application of transformer models for performing AS2 (Garg et al., 2020; Han et al., 2021; Lauriola and Moschitti, 2021). On the data front, small datasets like TrecQA (Wang et al."
2021.emnlp-main.583,2021.findings-acl.172,0,0.126778,"sometimes use an answer validation component in addition to the system threshold, which analyzes the answer produced by the system and decides whether to answer or abstain. These systems often use external entity knowledge (Magnini et al., 2002; Ko et al., 2007; Gondek et al., 2012) for basing their decision to verify the correctness of the answer. Recently Wang et al. (2020) propose to add a new MR model to reflect on the predictions of the original MR model to decide whether the produced answer is correct or not. Other efforts (Rodriguez et al., 2019; Kamath et al., 2020; Jia and Xie, 2020; Zhang et al., 2021) have trained calibrators for verifying if the question should be answered or not. All these works are fundamentally different from our question filtering approach since they operate jointly on the question and generated answer, thereby requiring the entire computation to be performed by the QA system before making a decision. Our work operates only on the question text to preemptively decide whether to filter it or not. Thus the primary goal of these existing works is to improve the precision of the answering model by not answering when not confident, while our work aims to improve efficiency"
2021.findings-acl.374,P18-1060,0,0.104289,"sing same loss with learning rate 5E −6 . Evaluation We used accuracy as our primary metric for all our experiments and models. This is computed as the average number of questions a model answers correctly; for a selector S, it is equivalent to Precision at 1. For S, we also report Hit Rate at 5, which is the fraction of queries with at least one good candidate ranked five or less. Beside human evaluation, we also experimented with automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) for GenQA. Such metrics have found little success in evaluating QA tasks (Chaganty et al., 2018; Chen et al., 2019), so we investigate whether that is the case for AS2 as well. 4.2 Results How to Fine-tune GenQA? As described in Section 4.1, we tested two GenQA variants: one uses a UnifiedQA T5 (U QAT5) (Khashabi et al., 2020) as base model, while the other leverages BARTLarge (Lewis et al., 2020a). Of the datasets used in this work, MSNLG and WQA are large enough for fine-tuning GenQA. Therefore, based on preliminary results, we tested four different strategies for training U QAT5: fine tuning on (i) WQA or (ii) MSNLG alone, (iii) combine the two datasets by alternating mini-batches du"
2021.findings-acl.374,D19-5817,0,0.12313,"rning rate 5E −6 . Evaluation We used accuracy as our primary metric for all our experiments and models. This is computed as the average number of questions a model answers correctly; for a selector S, it is equivalent to Precision at 1. For S, we also report Hit Rate at 5, which is the fraction of queries with at least one good candidate ranked five or less. Beside human evaluation, we also experimented with automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) for GenQA. Such metrics have found little success in evaluating QA tasks (Chaganty et al., 2018; Chen et al., 2019), so we investigate whether that is the case for AS2 as well. 4.2 Results How to Fine-tune GenQA? As described in Section 4.1, we tested two GenQA variants: one uses a UnifiedQA T5 (U QAT5) (Khashabi et al., 2020) as base model, while the other leverages BARTLarge (Lewis et al., 2020a). Of the datasets used in this work, MSNLG and WQA are large enough for fine-tuning GenQA. Therefore, based on preliminary results, we tested four different strategies for training U QAT5: fine tuning on (i) WQA or (ii) MSNLG alone, (iii) combine the two datasets by alternating mini-batches during training, or (i"
2021.findings-acl.374,2020.emnlp-main.413,0,0.0199769,"esired information is not a natural-sounding response to the query. Table 1 shows an example of our system: given the question, Q, and a list of candidates, Ck = {c1 , . . . , c5 } sorted by a state-of-the-art AS2 system, we use a sequence-tosequence model to produce an answer G given Q and Ck as input. This approach, which we refer to as GenQA, addresses the limitations of AS2 systems by composing concise answers which may contain information from multiple sources. Recent works have shown that large, transformerbased conditional generative models can be used to significantly improve parsing (Chen et al., 2020; Rongali et al., 2020), retrieval (De Cao et al., 2020; Pradeep et al., 2021), and classification tasks (Raffel et al., 2019). Our approach builds on top of this line of work by designing and testing generative models for AS2-based QA systems. In recent years, the use of generative approaches has been evaluated for other QA tasks, such as machine reading (MR) (Izacard and Grave, 2021; Lewis et al., 2020b) and question-based summarization (QS) (Iida et al., 2019; Goodwin et al., 2020; Deng et al., 2020). However, while related, these efforts are fundamentally different from the experimental se"
2021.findings-acl.374,2020.findings-emnlp.289,0,0.0136921,"ave shown that large, transformerbased conditional generative models can be used to significantly improve parsing (Chen et al., 2020; Rongali et al., 2020), retrieval (De Cao et al., 2020; Pradeep et al., 2021), and classification tasks (Raffel et al., 2019). Our approach builds on top of this line of work by designing and testing generative models for AS2-based QA systems. In recent years, the use of generative approaches has been evaluated for other QA tasks, such as machine reading (MR) (Izacard and Grave, 2021; Lewis et al., 2020b) and question-based summarization (QS) (Iida et al., 2019; Goodwin et al., 2020; Deng et al., 2020). However, while related, these efforts are fundamentally different from the experimental setting described in this paper. Given a question, generative MR models are used to extract a short span (1-5 tokens) from a passage that could be used to construct an answer to a question. In contrast, AS2 returns a complete sentence that could be directly returned to a user. QS systems are designed to create a general summary given a question and one or more related documents. Unlike QS, AS2-based QA systems need to provide specific answers; thus, the presence of even a small amount"
2021.findings-acl.374,2021.eacl-main.261,1,0.742642,"tion Q and five answer candidates c1 , . . . , c5 from WikiQA (Yang et al., 2015) ranked by an AS2 system. Answer G generated by our best system is significantly more natural and concise than any extracted candidates. Introduction Question answering systems are a core component of many commercial applications, ranging from task-based dialog systems to general purpose virtual assistants, e.g., Google Home, Amazon Alexa, and Siri. Among the many approaches for QA, AS2 has attracted significant attention in the last few years (Tymoshenko and Moschitti, 2018; Tian et al., 2020; Garg et al., 2020; Han et al., 2021). Under this framework, for a given question, a retrieval system is first used to obtain and rank a set of supporting passages; then, an AS2 model is used to estimate the likelihood of each sentence extracted from passages to be a correct answer, returning the one with the highest probability. This approach is favored in virtual assistant systems because full sentences are more likely to include the ∗ This work was completed while the author was an intern at Amazon Alexa. right context and sound natural, both of which are characteristics users value (Berdasco et al., 2019). AS2 models have sho"
2021.findings-acl.374,2021.eacl-main.74,0,0.0248033,"systems by composing concise answers which may contain information from multiple sources. Recent works have shown that large, transformerbased conditional generative models can be used to significantly improve parsing (Chen et al., 2020; Rongali et al., 2020), retrieval (De Cao et al., 2020; Pradeep et al., 2021), and classification tasks (Raffel et al., 2019). Our approach builds on top of this line of work by designing and testing generative models for AS2-based QA systems. In recent years, the use of generative approaches has been evaluated for other QA tasks, such as machine reading (MR) (Izacard and Grave, 2021; Lewis et al., 2020b) and question-based summarization (QS) (Iida et al., 2019; Goodwin et al., 2020; Deng et al., 2020). However, while related, these efforts are fundamentally different from the experimental setting described in this paper. Given a question, generative MR models are used to extract a short span (1-5 tokens) from a passage that could be used to construct an answer to a question. In contrast, AS2 returns a complete sentence that could be directly returned to a user. QS systems are designed to create a general summary given a question and one or more related documents. Unlike"
2021.findings-acl.374,2020.findings-emnlp.171,0,0.0572639,"e use beam search with a beam size of four and a maximum output length of 100 tokens. 4 Experiments In this section, we first report on our experimental setup, then we show the results on fine-tuning GenQA, and finally, we report on the comparative results between AS2 and GenQA. 4.1 Setup 3 The public version of WQA will be released in the shortterm future. Please search for a publication with title WQA: A Dataset for Web-based Question Answering Tasks on arXiv. Models and Parameterization Our GenQA model is based on the T5 (Raffel et al., 2019) vari4278 ant of the UnifiedQA (U QAT5) model by Khashabi et al. (2020). We use the Large version of U QAT5, which has 770M parameters for all of our experiments. We compute training loss as the mean of the cross-entropy between the softmax probabilities over the output vocabulary and the one-hot encoded target answer. We fine-tune U QAT5 with a learning rate of 5E −5 . We also experiment with the Large variant of BART (Lewis et al., 2020a), which is comprised of 400M parameters. This model was trained using same loss with learning rate 5E −6 . Evaluation We used accuracy as our primary metric for all our experiments and models. This is computed as the average nu"
2021.findings-acl.374,Q19-1026,0,0.0117796,"correct; (ii) natural-sounding; and (iii) required no additional information to be understood. All QA pairs were single annotated, as we determined sufficient agreement for this task in previous campaigns. WikiQA by Yang et al. (2015) contains queries from Bing search logs and candidate answer sentences extracted from a relevant Wikipedia page. For evaluation, we used the dev. and test sets, which contain 126 and 243 unique questions and we reannotated all of the resulting 569 QA pairs.2 Answer Sentence Natural Questions (ASNQ) introduced by Garg et al. (2020) was derived from the NQ dataset (Kwiatkowski et al., 2019) and consists of the questions which have a short answer span within a single sentence in a long answer span. The sentences containing the short answer are marked as correct and the other sentences in the document are marked as incorrect. We use the dev. and test splits introduced by Soldaini and Moschitti (2020) which contain 1,336 questions each. We re-annotated a total of 5,344 QA pairs. WQA is an internal AS2 dataset created from a non-representative sample of questions asked by 1 Our models, source code, and annotated data are available at: https://github.com/alexa/ wqa-cascade-transforme"
2021.findings-acl.374,2020.acl-main.703,0,0.51893,"ise answers which may contain information from multiple sources. Recent works have shown that large, transformerbased conditional generative models can be used to significantly improve parsing (Chen et al., 2020; Rongali et al., 2020), retrieval (De Cao et al., 2020; Pradeep et al., 2021), and classification tasks (Raffel et al., 2019). Our approach builds on top of this line of work by designing and testing generative models for AS2-based QA systems. In recent years, the use of generative approaches has been evaluated for other QA tasks, such as machine reading (MR) (Izacard and Grave, 2021; Lewis et al., 2020b) and question-based summarization (QS) (Iida et al., 2019; Goodwin et al., 2020; Deng et al., 2020). However, while related, these efforts are fundamentally different from the experimental setting described in this paper. Given a question, generative MR models are used to extract a short span (1-5 tokens) from a passage that could be used to construct an answer to a question. In contrast, AS2 returns a complete sentence that could be directly returned to a user. QS systems are designed to create a general summary given a question and one or more related documents. Unlike QS, AS2-based QA sys"
2021.findings-acl.374,W04-1013,0,0.0851732,"Lewis et al., 2020a), which is comprised of 400M parameters. This model was trained using same loss with learning rate 5E −6 . Evaluation We used accuracy as our primary metric for all our experiments and models. This is computed as the average number of questions a model answers correctly; for a selector S, it is equivalent to Precision at 1. For S, we also report Hit Rate at 5, which is the fraction of queries with at least one good candidate ranked five or less. Beside human evaluation, we also experimented with automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) for GenQA. Such metrics have found little success in evaluating QA tasks (Chaganty et al., 2018; Chen et al., 2019), so we investigate whether that is the case for AS2 as well. 4.2 Results How to Fine-tune GenQA? As described in Section 4.1, we tested two GenQA variants: one uses a UnifiedQA T5 (U QAT5) (Khashabi et al., 2020) as base model, while the other leverages BARTLarge (Lewis et al., 2020a). Of the datasets used in this work, MSNLG and WQA are large enough for fine-tuning GenQA. Therefore, based on preliminary results, we tested four different strategies for training U QAT5: fine tuni"
2021.findings-acl.374,2021.ccl-1.108,0,0.0314643,"Missing"
2021.findings-acl.374,P02-1040,0,0.111251,"ent with the Large variant of BART (Lewis et al., 2020a), which is comprised of 400M parameters. This model was trained using same loss with learning rate 5E −6 . Evaluation We used accuracy as our primary metric for all our experiments and models. This is computed as the average number of questions a model answers correctly; for a selector S, it is equivalent to Precision at 1. For S, we also report Hit Rate at 5, which is the fraction of queries with at least one good candidate ranked five or less. Beside human evaluation, we also experimented with automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) for GenQA. Such metrics have found little success in evaluating QA tasks (Chaganty et al., 2018; Chen et al., 2019), so we investigate whether that is the case for AS2 as well. 4.2 Results How to Fine-tune GenQA? As described in Section 4.1, we tested two GenQA variants: one uses a UnifiedQA T5 (U QAT5) (Khashabi et al., 2020) as base model, while the other leverages BARTLarge (Lewis et al., 2020a). Of the datasets used in this work, MSNLG and WQA are large enough for fine-tuning GenQA. Therefore, based on preliminary results, we tested four different strategies for tr"
2021.findings-acl.374,2020.emnlp-main.437,0,0.0388586,"Missing"
2021.findings-acl.374,2020.acl-main.504,1,0.833711,"cted from a relevant Wikipedia page. For evaluation, we used the dev. and test sets, which contain 126 and 243 unique questions and we reannotated all of the resulting 569 QA pairs.2 Answer Sentence Natural Questions (ASNQ) introduced by Garg et al. (2020) was derived from the NQ dataset (Kwiatkowski et al., 2019) and consists of the questions which have a short answer span within a single sentence in a long answer span. The sentences containing the short answer are marked as correct and the other sentences in the document are marked as incorrect. We use the dev. and test splits introduced by Soldaini and Moschitti (2020) which contain 1,336 questions each. We re-annotated a total of 5,344 QA pairs. WQA is an internal AS2 dataset created from a non-representative sample of questions asked by 1 Our models, source code, and annotated data are available at: https://github.com/alexa/ wqa-cascade-transformers. 2 Due to time and annotation constraints, we were only able to annotate results for 100 queries from each of the dev. and test sets for our U QAT5 model 4277 users of a virtual personal assistant in 20193 . For each question, we retrieved 500 pages from an index containing over 100M web documents. We then ran"
2021.findings-acl.374,D18-1240,1,0.900069,"Missing"
2021.findings-acl.374,D15-1237,0,0.101865,"Missing"
2021.findings-acl.426,P17-1152,0,0.0301967,"struct graphs with evidences as nodes and use deep graph neural ER networks to propagate knowledge; DREAM (Zhong et al., 2020) reasons on a graph built on top of a semantic role labeler output; TransformerXH (Zhao et al., 2020) propagates knowledge between [CLS] tokens of different evidence pieces; CorefBERT (Ye et al., 2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced sequential inference model (ESIM) (Chen et al., 2017) to do ER. The top-scoring published approach, DOMLIN++ (Stammbach and Ash, 2020), simply textconcatenates evidence pieces and uses a RoBERTabased classifier, thus supporting our thesis that simple models can be very effective. On the other hand, they use additional DocIR components and data (MultiNLI (Williams et al., 2018) corpus) for fine-tuning. To the best of our knowledge, their code/output are not available online yet2 , so we cannot compare to them directly at the moment. Baselines. The baselines in the above works, apart from the previous SOTA systems, consist in applying a transforme"
2021.findings-acl.426,W18-5516,0,0.0202988,"#SUP #REF #NEI TRAIN 80,035 29,775 35,639 DEV 6,666 6,666 6,666 TEST 6,666 6,666 6,666 Table 3: FEVER dataset statistics. # denotes the number of claims in a given class. Table 2: Input data for the baselines. Eipage is the name of the Ei source Wikipedia page. [SEP] and [CLS] are the standard “separator” and “classification” tokens used in BERTlike models. &lt;psep&gt; is delimiter separating page name from the evidence text. We use “. ”, while Liu et al. (2020) use [SEP] [CLS] is p = sof tmax(Wmp hmp ), Wmp ∈ R3×hdim . It is inspired by the max pooling evidence aggregation procedure employed by (Hanselowski et al., 2018; Zhou et al., 2019). WgtSum: encodes each (C, Ei ) pair with a transformer, computes the weighted sum PK [CLS] [CLS] hws = ∈ Rhdim , αi = i=1 αi hi [CLS] sof tmaxi (Wws hi ), Wws ∈ R1×hdim . The weight αi is intended to reflect the relative importance of Ei . The output is p = [CLS] sof tmax(W hws ), W ∈ R3×hdim . WgtSum is similar to the Zhou et al. (2019)’s attention baseline in the sense that we aggregate pieces of evidence representations via a weighted summation. However, differently from us, they obtain the weights by computing attention between the claim and the evidence hidden states."
2021.findings-acl.426,P18-1031,0,0.0276029,"tps://github.com/iKernels/ reasoning-baselines. We use the pre-trained BERT and RoBERTa LRMs from the transformers5 library, namely bert-base-cased, roberta-base and roberta-large. Training setup. We train for three epochs, with an evaluation checkpoint every 500 and 2500 training steps for global and local models correspondingly, thus having 14 checkpoints in total. We use K = 5 evidence pieces per claim. For all the models the batch size/number of gradient accumulation steps are 8/8 and 2/32 with base and large LRMs, respectively. We use Adam optimizer with slanted triangular learning rate (Howard and Ruder, 2018), cut f rac = 0.1, ratio of 326 . When experimenting with roberta-base we tried 5 https://github.com/huggingface/ transformers 6 Standard values suggested in (Howard and Ruder, 2018) learning rates [1e-5; 5e-5] with the step of 1e-5 and observed no noticeable difference between the rates in the range [2e-5; 5e-5]. Additional details are available in the appendix. FEVER metrics. The primary shared task metric is FEVER7 . It takes the correctness of the evidence set provided with the claim label8 into account. The evidence set must contain all sentences belonging to at least one evidence9 associ"
2021.findings-acl.426,2020.acl-main.655,0,0.13817,"osing complex models that are less accurate than our baselines can most likely mislead the research community, thus knowing our baselines can help to lead research in this area in the right directions. Additionally, our baselines are strong, simple to use, and easily reproducible, enabling fast comparison with innovative inference models. 2 Related work SOTA approaches. Most recent approaches encode claim and evidence texts using Transformerbased language representation models (LRM), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and others. GEAR (Zhou et al., 2019) and KGAT (Liu et al., 2020) construct graphs with evidences as nodes and use deep graph neural ER networks to propagate knowledge; DREAM (Zhong et al., 2020) reasons on a graph built on top of a semantic role labeler output; TransformerXH (Zhao et al., 2020) propagates knowledge between [CLS] tokens of different evidence pieces; CorefBERT (Ye et al., 2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced sequential inference model (ESI"
2021.findings-acl.426,D19-1341,0,0.0224121,"marginally outperform Concat with roberta-large. We also trained Concat with roberta-large setting K=1 both for training and predicting, i.e., using only top evidence piece retrieved by ES. The resulting LA of 79.57 is only approximately one point behind that of Concat (Line 16) and KGAT (Line 7). This suggests that good performance can be obtained on the FEVER dataset even without joint reasoning over multiple Ei -s, and that there is still room for further improvement for the systems able to reason upon multiple evidence pieces. Also this could be partially attributed to the observation by Schuster et al. (2019) who showed that FEVER claims contain certain linguistic biases and BERT model fine-tuned on the claim texts only significantly outperforms the majority baseline. Schuster et al. (2019) proposed a debiased symmetric test set, but its instances are claim-evidence pairs. This means that K = 1, and thus we did not evaluate our baselines on it as with K = 1 they all become equivalent to Local. Comparison to the state of the art. Tab. 5 compares the performance of MaxPool and WgtSum to that of the SOTA systems as of June 1st, 2021. Our simple baselines outperform all the other systems on DEV, but w"
2021.findings-acl.426,W18-5501,0,0.0646431,"Missing"
2021.findings-acl.426,N18-1101,0,0.0303354,"2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced sequential inference model (ESIM) (Chen et al., 2017) to do ER. The top-scoring published approach, DOMLIN++ (Stammbach and Ash, 2020), simply textconcatenates evidence pieces and uses a RoBERTabased classifier, thus supporting our thesis that simple models can be very effective. On the other hand, they use additional DocIR components and data (MultiNLI (Williams et al., 2018) corpus) for fine-tuning. To the best of our knowledge, their code/output are not available online yet2 , so we cannot compare to them directly at the moment. Baselines. The baselines in the above works, apart from the previous SOTA systems, consist in applying a transformer-based classifier to (i) concatenation of C and all Ei , i = 1..K (Zhou et al., 2019; Zhong et al., 2020; Zhao et al., 2020); or (ii) separate (C, Ei ) pairs, i = 1..K, and aggregating the results heuristically (Zhou et al., 2019). The latter also considered max-pooling and weighted2 We could try to re-implement their pipel"
2021.findings-acl.426,2020.emnlp-main.582,0,0.151643,"models. 2 Related work SOTA approaches. Most recent approaches encode claim and evidence texts using Transformerbased language representation models (LRM), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and others. GEAR (Zhou et al., 2019) and KGAT (Liu et al., 2020) construct graphs with evidences as nodes and use deep graph neural ER networks to propagate knowledge; DREAM (Zhong et al., 2020) reasons on a graph built on top of a semantic role labeler output; TransformerXH (Zhao et al., 2020) propagates knowledge between [CLS] tokens of different evidence pieces; CorefBERT (Ye et al., 2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced sequential inference model (ESIM) (Chen et al., 2017) to do ER. The top-scoring published approach, DOMLIN++ (Stammbach and Ash, 2020), simply textconcatenates evidence pieces and uses a RoBERTabased classifier, thus supporting our thesis that simple models can be very effective. On the other hand, they use additional DocIR components and data (MultiNLI (Willi"
2021.findings-acl.426,2020.acl-main.549,0,0.104844,"baselines can help to lead research in this area in the right directions. Additionally, our baselines are strong, simple to use, and easily reproducible, enabling fast comparison with innovative inference models. 2 Related work SOTA approaches. Most recent approaches encode claim and evidence texts using Transformerbased language representation models (LRM), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and others. GEAR (Zhou et al., 2019) and KGAT (Liu et al., 2020) construct graphs with evidences as nodes and use deep graph neural ER networks to propagate knowledge; DREAM (Zhong et al., 2020) reasons on a graph built on top of a semantic role labeler output; TransformerXH (Zhao et al., 2020) propagates knowledge between [CLS] tokens of different evidence pieces; CorefBERT (Ye et al., 2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced sequential inference model (ESIM) (Chen et al., 2017) to do ER. The top-scoring published approach, DOMLIN++ (Stammbach and Ash, 2020), simply textconcatenates e"
2021.findings-acl.426,P19-1085,0,0.165298,"baselines. For example, proposing complex models that are less accurate than our baselines can most likely mislead the research community, thus knowing our baselines can help to lead research in this area in the right directions. Additionally, our baselines are strong, simple to use, and easily reproducible, enabling fast comparison with innovative inference models. 2 Related work SOTA approaches. Most recent approaches encode claim and evidence texts using Transformerbased language representation models (LRM), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and others. GEAR (Zhou et al., 2019) and KGAT (Liu et al., 2020) construct graphs with evidences as nodes and use deep graph neural ER networks to propagate knowledge; DREAM (Zhong et al., 2020) reasons on a graph built on top of a semantic role labeler output; TransformerXH (Zhao et al., 2020) propagates knowledge between [CLS] tokens of different evidence pieces; CorefBERT (Ye et al., 2020) trains a BERT-based LRM, which employs an additional objective modeling coreference knowledge, and use it within KGAT architecture. The winners of the original FEVER competition (Nie et al., 2019) used older LRMs and a modified enhanced seq"
2021.findings-emnlp.363,2020.acl-main.740,0,0.0354964,"Missing"
2021.findings-emnlp.363,L18-1566,0,0.0186363,". The entire process is exemplified by Figure 2. AVA as an Automatic Labeler AVA is designed to classify an answer to a question as correct or incorrect like an AS2 model does, but it exploits the semantic similarity between t and r, conditioned by q. We studied multiple configurations to optimize Weakly Supervised Data Creation Distant su- AVA for our task of generating weakly supervision. pervision has gained success in creating weakly In our experiments, we use the best setting we labeled data for both relation extraction (Mintz found in (Vu and Moschitti, 2021), which uses a et al., 2009; Jiang et al., 2018; Qin et al., 2018) and Transformer-based approach with Peer-Attention, machine reading (Joshi et al.; Koˇciský et al., 2018), to model the interaction among q, t, and r. using curated entity relation database. Unlike othWe built AVA using a dataset of 245 questions, ers, we use abundant Web data and reference an- each having roughly 100 annotated answers. The swers to create weakly label data. We also argue number of correct and incorrect answers are 5.3K that we are the first to address this research in AS2 and 20.7K, respectively. This generates approxicontext. mately 500K point-wise traini"
2021.findings-emnlp.363,P17-1147,0,0.0655455,"Missing"
2021.findings-emnlp.363,Q18-1023,0,0.0721276,"Missing"
2021.findings-emnlp.363,Q19-1026,0,0.0417319,"Missing"
2021.findings-emnlp.363,2021.naacl-main.412,1,0.887968,"perates in two stages: (i) collecting answer candidates from Web documents, and (ii) automatically assigning them correct or incorrect labels. More specifically, we build a large index of more than 100MM Web documents from Common Crawl’s crawls. Given a question-reference pair, the question is used as a query to retrieve a set of relevant documents from the index. Then, we extract sentences from those documents to build a large pool of answer candidates, which are finally scored by an automatic evaluator based on the provided reference. We use the AVA approach, which we recently introduced in Vu and Moschitti (2021) for automatic evaluation of AS2. We show that RWS complements the original data (question/answer pairs) by measuring the improvement over the state-of-the-art AS2 models on WikiQA and TREC-QA datasets. The experimental results suggest that the weakly supervised data produced by RWS adds new supervision capacity to the original dataset, enabling models to advance the state of the art. In a nutshell, our contributions include: (i) a pipeline for processing large-scale data, which generates labeled question-answer pairs using publicly available Web data, i.e., Common Crawl; and (ii) a large auto"
2021.findings-emnlp.363,D07-1003,0,0.0606501,"nfigurations. We use HuggingFace’s Transformer library (Wolf et al., 2020) and set the learning-rates to 1e−6 and 1e−5 for the transfer and adapt stages of TANDA, respectively, across all experiments. The other hyper-parameters are set to default values. Specifically, all experiments share the same hyper-parameter setting, including the default random seed of the transformers library (i.e., 42). We also performed the experiments with 5 random seeds and averaged the results. 4.2 WQA Split Train Dev Test Train Dev Test Train Dev Test Datasets TREC-QA is a traditional benchmark for the AS2 task (Wang et al., 2007). We use the standard split used in previous work, e.g., (Tan et al., 2015; Rao et al., 2016; Garg et al., 2020). WikiQA The dataset, introduced by Yang et al. (2015), consists of questions from Bing query logs and answers extracted from a user-clicked Wikipedia page returned by Bing. We follow the standard setting used in previous work, e.g., (Yoon et al., 2019; Tay et al., 2017; Garg et al., 2020). Web-based Question Answering (WQA)1 . We built the dataset as part of the effort to improve understanding and benchmarking in open-domain QA systems. The creation process includes the following st"
2021.findings-emnlp.363,D15-1237,0,0.0531496,"Missing"
2021.findings-emnlp.363,2021.ccl-1.108,0,0.0639316,"Missing"
2021.findings-emnlp.363,P09-1113,0,0.210087,"Missing"
2021.findings-emnlp.363,P18-1199,0,0.0198696,"is exemplified by Figure 2. AVA as an Automatic Labeler AVA is designed to classify an answer to a question as correct or incorrect like an AS2 model does, but it exploits the semantic similarity between t and r, conditioned by q. We studied multiple configurations to optimize Weakly Supervised Data Creation Distant su- AVA for our task of generating weakly supervision. pervision has gained success in creating weakly In our experiments, we use the best setting we labeled data for both relation extraction (Mintz found in (Vu and Moschitti, 2021), which uses a et al., 2009; Jiang et al., 2018; Qin et al., 2018) and Transformer-based approach with Peer-Attention, machine reading (Joshi et al.; Koˇciský et al., 2018), to model the interaction among q, t, and r. using curated entity relation database. Unlike othWe built AVA using a dataset of 245 questions, ers, we use abundant Web data and reference an- each having roughly 100 annotated answers. The swers to create weakly label data. We also argue number of correct and incorrect answers are 5.3K that we are the first to address this research in AS2 and 20.7K, respectively. This generates approxicontext. mately 500K point-wise training examples for AVA"
2021.findings-emnlp.363,D17-1122,0,0.378323,"Missing"
2021.naacl-main.263,Q17-1010,0,0.0233741,"Table 1: Comparison of clustering models: completely unsupervised, using supervised instance similarity function, and our supervised clustering on the test set of IC&OOS by Larson et al. (2019); disjoint scenario. Models: We experiment with two variants of our Neural model for Supervised Clustering (NSC), based on two ways to encode question pairs outlined in Sec. 5.1: (i) NSC-CNN, using word and word overlap embeddings, and (ii) NSC-BERT, using BERT embeddings of question pairs. For NSC-CNN, we employ fastText7 word embeddings, in dimension 300, pre-trained for English language on Wikipedia (Bojanowski et al., 2017). We set the max length of questions to 50 and pad the shorter questions on the right. The size of the hidden layer is set to 13 of the size of the input layer. The convolution filter width varies from 1 to 3. For NSC-BERT, we use BERTBASE model, which we train for 3 epochs for fine-tuning on the question pair classification task. Since the training samples vary in size, we clip the gradients to have their L∞ norm less than or equal to 1. This is to prevent the updates being dominated by the samples of bigger size. Evaluation: We follow the evaluation setting of Haponchyk et al. (2018). Thus,"
2021.naacl-main.263,W10-4305,0,0.0207839,"onvolution filter width varies from 1 to 3. For NSC-BERT, we use BERTBASE model, which we train for 3 epochs for fine-tuning on the question pair classification task. Since the training samples vary in size, we clip the gradients to have their L∞ norm less than or equal to 1. This is to prevent the updates being dominated by the samples of bigger size. Evaluation: We follow the evaluation setting of Haponchyk et al. (2018). Thus, we compute (i) clustering F1 measure, based on assigning each cluster to the most frequent (gold/output) cluster, (ii) coreference resolution CEAFe score (Luo, 2005; Cai and Strube, 2010). Parameterization: We use dev. set for tuning the loss parameter r, which takes values from {0.1, 0.5, 1.0}, and selecting the best epoch with respect to clustering F1. Baselines We consider a number of baselines based on the pairwise query similarities. We experiment with the following sources of pairwise signals: (i) tf-idf scores, (ii) outputs of the binary question pair classifier, which we train in two modalities, CNN and BERT. We group the pairwise signals into a clustering output using spectral clustering algorithm (Ng et al., 2001) (implementation from smile8 library), which we run on"
2021.naacl-main.263,D16-1164,0,0.0284848,"the complexity associated with adapting the methods from previous work to neural frameworks. For example, using ILP (Roth and Yih, 2004) for clustering inference in SPIGOT (Peng et al., 2018), which facilitates the backpropagation through argmax based on a projection onto the feasible set of structured outputs, would inevitably require reducing the com1 https://github.com/iKernels/intent-qa putational overhead (Miyauchi et al., 2018). On the line of research of question clustering, Wen et al. (2001) proposed to cluster queries with respect to a group of web pages frequently selected by users. Deepak (2016) describes a k-means like algorithm, MiXKmeans, that can cluster threads in Community Question Answering websites. These methods are unsupervised and, thus, are likely sensitive to setting the optimal number of clusters or to a heuristic adopted for the clustering criterion. Also among the classification approaches, there are semi-supervised and mixed classification methods which advance on the use of vast amounts of unlabelled queries. Li et al. (2008) classify unlabeled queries using their proximity to labeled queries in a click graph. Beitzel et al. (2007) classify queries from logs into to"
2021.naacl-main.263,N19-1423,0,0.0229939,"her the items of unseen clusters, we use one set of intent classes for training and another set for evaluation, which is constituted by a completely different set of intent classes and questions. This way, we retain the queries from one third of intent classes, i.e., 50, from the training part, the dev. queries from another third of intent classes, and the test queries from the remaining one third4 of classes, and use them as new training, dev. and test parts, respectively. Additionally, it should be noted that the original dataset contains OOS queries, which we keep all. (2) We exploit BERT (Devlin et al., 2019) embeddings: φ(xi , xj ) = 12 (bert_emb(xi , xj ) + bert_emb(xj , xi )), where bert_emb for a pair of 3 We use the Small variant of the dataset. 4 questions comes from the final hidden layer repreThe split of the classes into three sets is done randomly sentations, i.e., [CLS] token from the BERT model. without reference to the 10 original topic domains. 3367 Thus, our new split can be also used to analyze OOS queries, which might not be put in any semantically meaningful cluster, as well as unseen intent items, for which, we know that their natural clusters (original categories) exist. Data s"
2021.naacl-main.263,P15-1030,0,0.0299766,"ng tasks. 2 Related Work This paper touches two main research areas: structured prediction, in particular with neural models, and intent clustering, which are described below. Structured prediction has shown powerful machine learning algorithms for solving NLP tasks requiring complex output, e.g., syntactic parsing (Smith, 2011), coreference resolution (Yu and Joachims, 2009; Fernandes et al., 2014). This work has mainly regarded traditional frameworks, e.g., SVMs, CRF, perceptron. Only little work has been devoted to the integration of the above theory in neural networks (LeCun et al., 2006; Durrett and Klein, 2015; Weiss et al., 2015; Kiperwasser and Goldberg, 2016; Peng et al., 2018; Milidiú and Rocha, 2018; Xu et al., 2018; Wang et al., 2019), and, to the best of our knowledge, none to supervised clustering. This is partially due to the fact that local solutions have usually produced optimal results. For example, in case of supervised clustering, it is difficult to design a loss function that captures the global information about the clusters. Work in neural coreference resolution, e.g., (Lee et al., 2017), uses simple losses, which deliver state-of-the-art results but do not strictly take into accou"
2021.naacl-main.263,D18-1254,1,0.897475,"hat h loss ∆(y, y ˆ, h) neural model optimizes the objective E defined in Eq. 2. We use the loss ∆ of Yu and Joachims ˆ in (2009) based on computing edge mistakes in h, which negative edge penalties are scaled with an r-parameter. 4.2 Differentiable scoring function For optimizing E in Eq. 2, we define a scoring function that decomposes over the edges of h: X sθ (x, y, h) = netθ (e). (3) e=(xi ,xj )∈h e=(xi ,xj )∈h where φ(e) is a feature representation for edge e, describing a pair of elements of x. Graph structures h are incorporated as latent variables into the latent formulation of LSSVM. Haponchyk et al. (2018) adapt the latent structured perceptron (LSP) by Fernandes et al. (2014) to the graph structures h and apply the approach to question pairs, (qi , qj ), to cluster sets of questions into different user intents; we compare to these methods. 4 Neural Structured Output Clustering We propose a model for optimizing a structural clustering loss with neural networks. 4.1 Global max-margin objective As a standard practice in structured prediction, our goal is to train a model with a scoring function sθ such that the correct clustering y is scored higher than incorrect clusterings y ˆ. LSSVM optimizes"
2021.naacl-main.263,Q16-1023,0,0.366163,"wo main research areas: structured prediction, in particular with neural models, and intent clustering, which are described below. Structured prediction has shown powerful machine learning algorithms for solving NLP tasks requiring complex output, e.g., syntactic parsing (Smith, 2011), coreference resolution (Yu and Joachims, 2009; Fernandes et al., 2014). This work has mainly regarded traditional frameworks, e.g., SVMs, CRF, perceptron. Only little work has been devoted to the integration of the above theory in neural networks (LeCun et al., 2006; Durrett and Klein, 2015; Weiss et al., 2015; Kiperwasser and Goldberg, 2016; Peng et al., 2018; Milidiú and Rocha, 2018; Xu et al., 2018; Wang et al., 2019), and, to the best of our knowledge, none to supervised clustering. This is partially due to the fact that local solutions have usually produced optimal results. For example, in case of supervised clustering, it is difficult to design a loss function that captures the global information about the clusters. Work in neural coreference resolution, e.g., (Lee et al., 2017), uses simple losses, which deliver state-of-the-art results but do not strictly take into account the cluster structure. Secondly, this is also due"
2021.naacl-main.263,D19-1131,0,0.0382663,"Missing"
2021.naacl-main.263,D17-1018,0,0.159639,"en devoted to the integration of the above theory in neural networks (LeCun et al., 2006; Durrett and Klein, 2015; Weiss et al., 2015; Kiperwasser and Goldberg, 2016; Peng et al., 2018; Milidiú and Rocha, 2018; Xu et al., 2018; Wang et al., 2019), and, to the best of our knowledge, none to supervised clustering. This is partially due to the fact that local solutions have usually produced optimal results. For example, in case of supervised clustering, it is difficult to design a loss function that captures the global information about the clusters. Work in neural coreference resolution, e.g., (Lee et al., 2017), uses simple losses, which deliver state-of-the-art results but do not strictly take into account the cluster structure. Secondly, this is also due to the complexity associated with adapting the methods from previous work to neural frameworks. For example, using ILP (Roth and Yih, 2004) for clustering inference in SPIGOT (Peng et al., 2018), which facilitates the backpropagation through argmax based on a projection onto the feasible set of structured outputs, would inevitably require reducing the com1 https://github.com/iKernels/intent-qa putational overhead (Miyauchi et al., 2018). On the li"
2021.naacl-main.263,P19-1548,0,0.120202,"ese methods are unsupervised and, thus, are likely sensitive to setting the optimal number of clusters or to a heuristic adopted for the clustering criterion. Also among the classification approaches, there are semi-supervised and mixed classification methods which advance on the use of vast amounts of unlabelled queries. Li et al. (2008) classify unlabeled queries using their proximity to labeled queries in a click graph. Beitzel et al. (2007) classify queries from logs into topics using supervised or unsupervised methods. The following classification approaches address new emerging intents. Lin and Xu (2019) enable a neural model to detect unknown intents as outliers using a novelty detection technique. This model, however, does not have the capability to distinguish between different unknown intents. Xia et al. (2018) devise a capsule neural network able to discriminate between different emerging intents. Its zero-shot learning ability critically depends on the definition of a similarity between existing and new intents. Our approach does not hold any explicit representation of intents. The recent work by Lin et al. (2020) proposes a deep intent clustering model which takes advantage of labeled"
2021.naacl-main.263,H05-1004,0,0.00764392,"ayer. The convolution filter width varies from 1 to 3. For NSC-BERT, we use BERTBASE model, which we train for 3 epochs for fine-tuning on the question pair classification task. Since the training samples vary in size, we clip the gradients to have their L∞ norm less than or equal to 1. This is to prevent the updates being dominated by the samples of bigger size. Evaluation: We follow the evaluation setting of Haponchyk et al. (2018). Thus, we compute (i) clustering F1 measure, based on assigning each cluster to the most frequent (gold/output) cluster, (ii) coreference resolution CEAFe score (Luo, 2005; Cai and Strube, 2010). Parameterization: We use dev. set for tuning the loss parameter r, which takes values from {0.1, 0.5, 1.0}, and selecting the best epoch with respect to clustering F1. Baselines We consider a number of baselines based on the pairwise query similarities. We experiment with the following sources of pairwise signals: (i) tf-idf scores, (ii) outputs of the binary question pair classifier, which we train in two modalities, CNN and BERT. We group the pairwise signals into a clustering output using spectral clustering algorithm (Ng et al., 2001) (implementation from smile8 li"
2021.naacl-main.263,P18-1173,0,0.0429148,"Missing"
2021.naacl-main.263,W04-2401,0,0.0502027,"e to supervised clustering. This is partially due to the fact that local solutions have usually produced optimal results. For example, in case of supervised clustering, it is difficult to design a loss function that captures the global information about the clusters. Work in neural coreference resolution, e.g., (Lee et al., 2017), uses simple losses, which deliver state-of-the-art results but do not strictly take into account the cluster structure. Secondly, this is also due to the complexity associated with adapting the methods from previous work to neural frameworks. For example, using ILP (Roth and Yih, 2004) for clustering inference in SPIGOT (Peng et al., 2018), which facilitates the backpropagation through argmax based on a projection onto the feasible set of structured outputs, would inevitably require reducing the com1 https://github.com/iKernels/intent-qa putational overhead (Miyauchi et al., 2018). On the line of research of question clustering, Wen et al. (2001) proposed to cluster queries with respect to a group of web pages frequently selected by users. Deepak (2016) describes a k-means like algorithm, MiXKmeans, that can cluster threads in Community Question Answering websites. These me"
2021.naacl-main.263,D18-1348,0,0.061402,"-supervised and mixed classification methods which advance on the use of vast amounts of unlabelled queries. Li et al. (2008) classify unlabeled queries using their proximity to labeled queries in a click graph. Beitzel et al. (2007) classify queries from logs into topics using supervised or unsupervised methods. The following classification approaches address new emerging intents. Lin and Xu (2019) enable a neural model to detect unknown intents as outliers using a novelty detection technique. This model, however, does not have the capability to distinguish between different unknown intents. Xia et al. (2018) devise a capsule neural network able to discriminate between different emerging intents. Its zero-shot learning ability critically depends on the definition of a similarity between existing and new intents. Our approach does not hold any explicit representation of intents. The recent work by Lin et al. (2020) proposes a deep intent clustering model which takes advantage of labeled data for discovering new user intents, but it requires the indication of the exact number of output clusters. Finally, Zhang et al. (2021) propose Deep Aligned Clustering, a semisupervised method, to discover new in"
2021.naacl-main.263,P15-1032,0,0.0313977,"This paper touches two main research areas: structured prediction, in particular with neural models, and intent clustering, which are described below. Structured prediction has shown powerful machine learning algorithms for solving NLP tasks requiring complex output, e.g., syntactic parsing (Smith, 2011), coreference resolution (Yu and Joachims, 2009; Fernandes et al., 2014). This work has mainly regarded traditional frameworks, e.g., SVMs, CRF, perceptron. Only little work has been devoted to the integration of the above theory in neural networks (LeCun et al., 2006; Durrett and Klein, 2015; Weiss et al., 2015; Kiperwasser and Goldberg, 2016; Peng et al., 2018; Milidiú and Rocha, 2018; Xu et al., 2018; Wang et al., 2019), and, to the best of our knowledge, none to supervised clustering. This is partially due to the fact that local solutions have usually produced optimal results. For example, in case of supervised clustering, it is difficult to design a loss function that captures the global information about the clusters. Work in neural coreference resolution, e.g., (Lee et al., 2017), uses simple losses, which deliver state-of-the-art results but do not strictly take into account the cluster struc"
2021.naacl-main.412,magnini-etal-2002-towards,0,0.224883,"Missing"
2021.naacl-main.412,W19-2310,0,0.0412494,"Missing"
2021.naacl-main.412,2021.findings-emnlp.363,1,0.836926,"Missing"
2021.naacl-main.412,2021.ccl-1.108,0,0.0946731,"Missing"
2021.naacl-main.412,P17-1103,0,0.0606401,"Missing"
2021.naacl-main.412,W19-5302,0,0.0339397,"Missing"
2021.naacl-main.412,P11-1076,0,0.0310145,"s mentioned in the introduction MT unsupervised metrics, e.g., BLEU score or BERTScore, are not either a solution or a reasonable baseline for automatic QA evaluation. They could be used as features for our models, but we designed several supervised approaches based on pre-trained Transformer models, which subsume these MT features. A remotely related research effort for automatizing answer evaluation concerns student essays. Short answer grading (SAG), or short answer scoring, involves the automatic grading of students’ answers, typically written in free text, for a given prompt or question (Mohler et al., 2011). This task has been studied in (Mitchell et al., 2002; Pulman and Sukkarieh, 2005) for educational applications. Neural-based systems have also been recently proposed to improve the models (Riordan et al., 2017; Wang et al., 2019). Despite the conceptual similarity, i.e., evaluating an answer, the problem setting for the task is fundamentally different. Specifically, SAG is prompt-centric; thus, the learning objective is to score accurately other different answer variants for a particular question by building models trained on previously known variants (Wang et al., 2019). Besides, the answer"
2021.naacl-main.412,P02-1040,0,0.109684,"Missing"
2021.naacl-main.412,W05-0202,0,0.141955,"BERTScore, are not either a solution or a reasonable baseline for automatic QA evaluation. They could be used as features for our models, but we designed several supervised approaches based on pre-trained Transformer models, which subsume these MT features. A remotely related research effort for automatizing answer evaluation concerns student essays. Short answer grading (SAG), or short answer scoring, involves the automatic grading of students’ answers, typically written in free text, for a given prompt or question (Mohler et al., 2011). This task has been studied in (Mitchell et al., 2002; Pulman and Sukkarieh, 2005) for educational applications. Neural-based systems have also been recently proposed to improve the models (Riordan et al., 2017; Wang et al., 2019). Despite the conceptual similarity, i.e., evaluating an answer, the problem setting for the task is fundamentally different. Specifically, SAG is prompt-centric; thus, the learning objective is to score accurately other different answer variants for a particular question by building models trained on previously known variants (Wang et al., 2019). Besides, the answers, while written in free text, are not typically complete sentences. Therefore, the"
2021.naacl-main.412,W17-5017,0,0.0287145,"ls, but we designed several supervised approaches based on pre-trained Transformer models, which subsume these MT features. A remotely related research effort for automatizing answer evaluation concerns student essays. Short answer grading (SAG), or short answer scoring, involves the automatic grading of students’ answers, typically written in free text, for a given prompt or question (Mohler et al., 2011). This task has been studied in (Mitchell et al., 2002; Pulman and Sukkarieh, 2005) for educational applications. Neural-based systems have also been recently proposed to improve the models (Riordan et al., 2017; Wang et al., 2019). Despite the conceptual similarity, i.e., evaluating an answer, the problem setting for the task is fundamentally different. Specifically, SAG is prompt-centric; thus, the learning objective is to score accurately other different answer variants for a particular question by building models trained on previously known variants (Wang et al., 2019). Besides, the answers, while written in free text, are not typically complete sentences. Therefore, the SAG design aims to capture sufficient content covered in the reference responses for a question. On the contrary, AVA is design"
2021.naacl-main.412,D17-1122,0,0.120242,"Missing"
2021.naacl-main.412,D19-6119,0,0.0148975,"veral supervised approaches based on pre-trained Transformer models, which subsume these MT features. A remotely related research effort for automatizing answer evaluation concerns student essays. Short answer grading (SAG), or short answer scoring, involves the automatic grading of students’ answers, typically written in free text, for a given prompt or question (Mohler et al., 2011). This task has been studied in (Mitchell et al., 2002; Pulman and Sukkarieh, 2005) for educational applications. Neural-based systems have also been recently proposed to improve the models (Riordan et al., 2017; Wang et al., 2019). Despite the conceptual similarity, i.e., evaluating an answer, the problem setting for the task is fundamentally different. Specifically, SAG is prompt-centric; thus, the learning objective is to score accurately other different answer variants for a particular question by building models trained on previously known variants (Wang et al., 2019). Besides, the answers, while written in free text, are not typically complete sentences. Therefore, the SAG design aims to capture sufficient content covered in the reference responses for a question. On the contrary, AVA is designed to operate in an"
boyanov-etal-2017-building,P02-1040,0,\N,Missing
boyanov-etal-2017-building,N13-1090,0,\N,Missing
boyanov-etal-2017-building,P15-1152,0,\N,Missing
boyanov-etal-2017-building,S15-2047,1,\N,Missing
boyanov-etal-2017-building,D16-1230,0,\N,Missing
boyanov-etal-2017-building,S16-1136,1,\N,Missing
boyanov-etal-2017-building,Q17-1010,0,\N,Missing
boyanov-etal-2017-building,P16-2075,1,\N,Missing
boyanov-etal-2017-building,C16-2001,1,\N,Missing
boyanov-etal-2017-building,W17-5534,0,\N,Missing
boyanov-etal-2017-building,D16-1165,1,\N,Missing
C08-1121,P95-1017,0,0.0873369,")))) 3.2 Binding Kernels The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a"
C08-1121,P05-1018,0,0.0198924,",d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge about the structure of names certainly plays a role in recognizing that BJ Habibie is a possible antecedent for Mr. Habibie. Information about coreference relations–i.e., which noun phrases are mentions of the same entity–has been shown to be beneficial in a great number of NLP tasks, including information extraction (McCarthy and Lehnert 1995), text planning (Barzilay and Lapata 2005) and summarization (Steinberger et al. 2007). However, the performance of coreference resolvers on (1) a. John thinks that Peter hates him. b. John hopes that Jane is speaking only to unrestricted text is still quite low. One reason himself. for this is that coreference resolution requires a c. It’s lonely here. great deal of information, ranging from string matching to syntactic constraints to semantic d. It had been raining all day. knowledge to discourse salience information to The need to capture such information suggests c 2008. Licensed under the Creative Commons a role for kernel method"
C08-1121,W05-0406,0,0.0127941,"Missing"
C08-1121,P02-1034,0,0.663339,"corefAttribution-Noncommercial-Share Alike 3.0 Unported lierence resolution. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct clas"
C08-1121,P06-1117,1,0.56365,"0 Unported lierence resolution. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our"
C08-1121,J95-2003,0,0.0306172,"lexical information (as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identification, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed up"
C08-1121,P06-1079,0,0.016425,"and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish from other words, we explicitly mark up in the structured feature the pronoun and the antecedent candidate under consideration, by appending a string tag “ANA” and “CANDI” in their respective nodes, i.e. “NN-CANDI” for “man” and “PRP-ANA” for “him”. (b) To reduce the data sparseness, the leaf nod"
C08-1121,C96-1021,0,0.0600413,"n in the path. (S-I (NP-I (PRP-I It)) (VP (VBX have) (NP)) (.)) or, in a similar fashion, (S (NP (PRP it)) (VP (VBZ ’s) (NP (NP (NN time)) (PP (IN for) (NP (PRP$ their) (JJ biannual) (NN powwow)))))) would just be represented as the ST: (S-I (NP-I (PRP-I it)) (VP (BE VBZ) (NP-PRD (NN time)))) 3.2 Binding Kernels The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) i"
C08-1121,J94-4002,0,0.871698,"tion. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed, it may only be definable in a graded sense (Sturt 2003; Yang et al. 2006)), witness examples like (1b). Parallelism effects are a good example of structural information inducing preferences rather than constraints. Recognizing that It in examples such as (1c,d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge abo"
C08-1121,H05-1083,0,0.0761939,"f pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish from other words, we exp"
C08-1121,W04-2403,1,0.845958,"to all possible child sequences of the tree nodes, i.e. a string kernel combined with a STK. 2.3 Kernel Engineering The Kernels of previous section are basic functions that can be applied to feature vectors, strings and 962 trees. In order to make them effective for a specific task, e.g. for coreference resolution: (a) we can combine them with additive or multiplicative operators and (b) we can design specific data objects (vectors, sequences and tree structures) for the target tasks. It is worth noting that a basic kernel applied to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framewor"
C08-1121,W06-2909,1,0.595127,"ces of the tree nodes, i.e. a string kernel combined with a STK. 2.3 Kernel Engineering The Kernels of previous section are basic functions that can be applied to feature vectors, strings and 962 trees. In order to make them effective for a specific task, e.g. for coreference resolution: (a) we can combine them with additive or multiplicative operators and (b) we can design specific data objects (vectors, sequences and tree structures) for the target tasks. It is worth noting that a basic kernel applied to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing i"
C08-1121,P07-1098,1,0.614863,"poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our developed kernels produce high accuracy for both dis"
C08-1121,P02-1014,0,0.478169,"ernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing instances consist of a pair (anaphor, antecedent). During training, a positive instance is created for each anaphor encountered by pairing the anaphor with its closest antecedent; each of the non-coreferential mentions between anaphor and antecedent is used to produce a negative instance. During resolution, every mention to be resolved is paired with each preceding antecedent candidate to form a testing instance. This instance is presented to the classifier which then returns a class label with a confidence value indicating the likelihood that the cand"
C08-1121,J01-4004,0,0.917128,"d to an innovative view of a structure yields a new kernel (e.g. Moschitti and Bejan (2004); Moschitti et al. (2006)), as we show below: Let K(t1 , t2 ) = φ(t1 ) · φ(t2 ) be a basic kernel, where t1 and t2 are two trees. If we map t1 and t2 into two new structures s1 and s2 with a mapping φM (·), we obtain: K(s1 , s2 ) = φ(s1 ) · φ(s2 ) = φ(φM (t1 )) · φ(φM (t2 )) = φ′ (t1 ) · φ′ (t2 )=K′ (t1 , t2 ), which is a noticeably different kernel induced by the mapping φ′ = φ ◦ φM . 3 Kernels for Coreference Resolution In this paper we follow the standard learning approach to coreference developed by Soon et al. (2001) and also used the few variants in Ng and Cardie (2002). In this framework, training and testing instances consist of a pair (anaphor, antecedent). During training, a positive instance is created for each anaphor encountered by pairing the anaphor with its closest antecedent; each of the non-coreferential mentions between anaphor and antecedent is used to produce a negative instance. During resolution, every mention to be resolved is paired with each preceding antecedent candidate to form a testing instance. This instance is presented to the classifier which then returns a class label with a c"
C08-1121,J00-4003,1,0.915842,"(as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identification, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution. 1 Introduction full common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed,"
C08-1121,P06-1006,1,0.913917,"ll common sense reasoning (Sidner 1979; Hobbs 1978, 1979; Grosz et al. 1995; Vieira and Poesio 2000; Mitkov 2002). Much of this information won’t be available to robust coreference resolvers until better methods are found to represent and encode common sense knowledge; but part of the problem is also the need for better methods to encode information that is in part structural, in part lexical. Enforcing binding constraints –e.g., ruling out Peter as antecedent of him in (1a) requires recognizing that the anaphor occurs in a particular type of construction (Chomsky 1981; Lappin and Leass 1994; Yang et al. 2006) whose exact definition however has not yet been agreed upon by linguists (indeed, it may only be definable in a graded sense (Sturt 2003; Yang et al. 2006)), witness examples like (1b). Parallelism effects are a good example of structural information inducing preferences rather than constraints. Recognizing that It in examples such as (1c,d) are expletives requires a combination of structural information and lexical information (Lappin and Leass 1994; Evans 2001). But some sort of structure also underlies our interpretation of other types of coreference: e.g., knowledge about the structure of"
C08-1121,P04-1017,1,0.839828,"s The resolution of pronominal anaphora heavily relies on the syntactic information and relationships between the anaphor and the antecedent candidates, including binding and other constraints, but also context-induced preferences in sub-clauses. Some researchers (Lappin and Leass 1994; Kennedy and Boguraev 1996) use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach (Aone and Bennett 1995; Yang et al. 2004; Luo and Zitouni 2005). However, such a solution has limitations, since the syntactic features have to be selected and defined manually, and it is still partly an open question which syntactic properties should be considered in anaphora resolution. We follow (Yang et al. 2006; Iida et al. 2006) in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate. Given a sentence like “The Figure 3 graphically shows such tree highlighted with dash lines. More in detail we operate the following tree transformation: (a) To distinguish f"
C08-1121,P06-1051,1,0.82065,"on. Kernel functions make it poscense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sible to capture the similarity between structures 961 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961–968 Manchester, August 2008 without explicitly enumerating all the substructures, and have therefore been shown to be a viable approach to feature engineering for natural language processing for any task in which structural information plays a role, e.g. (Collins and Duffy 2002; Zelenko et al. 2003; Giuglea and Moschitti 2006; Zanzotto and Moschitti 2006; Moschitti et al. 2007). Indeed, they have already been used in NLP to encode the type of structural information that plays a role in binding constraints (Yang et al. 2006); however, the methods used in this previous work do not make it possible to exploit the full power of kernel functions. In this work, we extend the use of kernel functions for coreference by designing and testing kernels for three subtasks of the coreference task: • Binding constraints • Expletive detection • Aliasing and developing distinct classifiers for each of these tasks. We show that our developed kernels produce hi"
C08-1121,J08-1001,0,\N,Missing
C10-1059,W06-1651,0,0.695308,"ch tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic and semantic analysis becomes more clear. For the task of subjective expression detection, Choi et al. (2006) and Breck et al. (2007) used syntactic features in a sequence model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2"
C10-1059,W02-1001,0,0.174029,"A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD They [called ]DSE him a [liar]ESE A0 A1 A2 call.01 Figure 1: Syntactic and shallow semantic structure. 3.2 Base Sequence Labeling Model To solve the first subtask, we implemented a standard sequence labeler for subjective expression markup, similar to the approach by Breck et al. (2007). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002). The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). It is important to note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler was used to generate the candidate set for the reranker. To generate reranking training data, we carried out a 5-fold hold-out procedure: We split the training set into 5 pieces, 521 trained a sequence label"
C10-1059,W08-2123,1,0.892449,"Missing"
C10-1059,P09-2079,0,0.0607521,"Missing"
C10-1059,W06-0301,0,0.355539,"e repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Inter"
C10-1059,D07-1114,0,0.03973,"Breck et al. (2007) used syntactic features in a sequence model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has"
C10-1059,W04-2705,0,0.0606724,"x – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. 3.1 Syntactic and Semantic Structures We used the syntactic–semantic parser by Johansson and Nugues (2008) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD They [called ]DSE him a [liar]ES"
C10-1059,J05-1004,0,0.00659733,"features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. 3.1 Syntactic and Semantic Structures We used the syntactic–semantic parser by Johansson and Nugues (2008) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMO"
C10-1059,W02-1011,0,0.0123042,"ed a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler. Similarly, the recall is boosted by almost 11 points for the holder extraction (3 points in F-measure) by modeling the interaction of opinion expressions with respect to holders. 2 Related Work Since the most significant body of work in subjectivity analysis has been dedicated to coarsegrained tasks such as document polarity classification, most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as part-of-speech tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic"
C10-1059,ruppenhofer-etal-2008-finding,0,0.045649,"sed on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Interaction over Syntactic and Semantic Structure Previous systems for opinion expression markup have typically used simple feature sets which have allowed the use of efficient off-the-shelf sequence labeling methods based on Viterbi se"
C10-1059,D09-1018,0,0.178321,"has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Interaction over Syntactic and Semantic Structure Previous systems for opinion expression markup have typically used simple feature sets which have allowed the use of efficient off-the-shelf sequence labeling methods based on Viterbi search (Choi et al., 2006; Breck et al., 2007). This is not possible in our case since we would like to extract structural, relational features that involve pairs of opinion expressions and may apply over an arbitrarily long distance in the sentence. While it is possible that search algorithms for exact or approximate inference can be constructured fo"
C10-1059,W06-1640,0,0.0200935,"published results, which have been precision-oriented and scored quite low on recall. We analyzed the impact of the syntactic and semantic features and saw that the best model is the one that makes use of both types of features. The most effective features we have found are purely structural, i.e. based on tree fragments in a syntactic or semantic tree. Features involving words did not seem to have the same impact. There are multiple opportunities for future work in this area. An important issue that we have left open is the coreference problem for holder extraction, which has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to incorporate complex, high-level linguistic structure such as discourse representations (Somasundaran et al., 2009); it is clear that these structures are very relevant for explaining the way humans organize their expressions of opinions rhetorically. However, theoretical depth does not necessarily guarantee practical applicability, and the challenge is as usual to find a middle ground that balances our goals: explanatory power in theory, significant performance gains in practice, computational tractability, and robustness in difficult circumstances. 6 Ackn"
C10-1059,E99-1023,0,0.0560222,"Missing"
C10-1059,P99-1032,0,0.0264183,"Missing"
C10-1059,H05-1044,0,0.0433488,"1 A2 call.01 Figure 1: Syntactic and shallow semantic structure. 3.2 Base Sequence Labeling Model To solve the first subtask, we implemented a standard sequence labeler for subjective expression markup, similar to the approach by Breck et al. (2007). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002). The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). It is important to note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler was used to generate the candidate set for the reranker. To generate reranking training data, we carried out a 5-fold hold-out procedure: We split the training set into 5 pieces, 521 trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. 3.3 Base Opinion Holder Extractor For every opinion expression, we extracted opinion holders, i.e. mentions of the entity holding the opinion deno"
C10-1059,D09-1159,0,0.0306229,"e model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectiv"
C10-1059,W03-1017,0,0.139931,"ute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler. Similarly, the recall is boosted by almost 11 points for the holder extraction (3 points in F-measure) by modeling the interaction of opinion expressions with respect to holders. 2 Related Work Since the most significant body of work in subjectivity analysis has been dedicated to coarsegrained tasks such as document polarity classification, most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as part-of-speech tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic and semantic analysis becomes mo"
C10-2104,W05-0601,1,0.893251,"Missing"
C10-2104,magnini-etal-2006-cab,0,0.0269896,"ning models like previous work on NER, the composite kernel not only captures most of the flat features but also efficiently exploits structured features. More interestingly, this kernel yields significant improvement when applied to two corpora of two different languages. The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the state-of-the-art. 2 2.1 Italian corpus and the well-known CoNLL 2003 English shared task corpus. The EVALITA 2009 Italian dataset is based on I-CAB, the Italian Content Annotation Bank (Magnini et al., 2006), annotated with four entity types: Person (PER), Organization (ORG), Geo-Political Entity (GPE) and Location (LOC). The training data, taken from the local newspaper “L’Adige”, consists of 525 news stories which belong to five categories: News Stories, Cultural News, Economic News, Sports News and Local News. Test data, on the other hand, consist of completely new data, taken from the same newspaper and consists of 180 news stories. The CoNLL 2003 English dataset is created within the shared task of CoNLL-2003 (Sang and Meulder, 2003). It is a collection of news wire articles from the Reuters"
C10-2104,P04-1043,1,0.951356,"Missing"
C10-2104,H05-1091,0,0.410254,"Missing"
C10-2104,W02-2004,0,0.0903869,"Missing"
C10-2104,C02-1025,0,0.0358435,"Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The adva"
C10-2104,D09-1143,1,0.887061,"Missing"
C10-2104,W03-0423,0,0.0190266,"yed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The advantage is that we derived it by using the aver"
C10-2104,M98-1028,0,0.0172663,"e Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many c"
C10-2104,P02-1034,0,0.312481,"erence on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences. For this purpose, we apply tree kernels to a tree struc"
C10-2104,D09-1112,1,0.82235,"Missing"
C10-2104,W03-0425,0,0.0442864,"nd locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and"
C10-2104,W03-0419,0,0.0260008,"Missing"
C10-2104,N04-1023,0,0.249552,"Missing"
C10-2104,P03-1002,0,0.111166,"Missing"
C10-2104,P08-1067,0,0.0308094,"wo input strings. Introduction Reranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Be"
C10-2104,W09-1119,0,\N,Missing
C10-2104,P02-1062,0,\N,Missing
C10-5001,W10-2926,1,0.863458,"Missing"
C10-5001,C10-2104,1,0.883222,"Missing"
C10-5001,D09-1143,1,0.800389,"Missing"
C10-5001,D09-1112,1,0.811472,"Missing"
C10-5001,J08-2003,1,0.879792,"Missing"
C10-5001,P08-2029,1,0.901527,"Missing"
C10-5001,P08-4003,1,0.902937,"Missing"
C10-5001,N09-2022,1,0.896847,"Missing"
C10-5001,P07-1098,1,0.869819,"Missing"
C10-5001,S07-1062,1,0.874356,"Missing"
C10-5001,P06-1051,1,0.892435,"Missing"
C10-5001,P06-1117,1,0.886746,"Missing"
C10-5001,E06-1015,1,0.893286,"Missing"
C10-5001,W06-2909,1,0.869458,"Missing"
C10-5001,W05-0601,1,0.889457,"Missing"
C10-5001,W04-2403,1,0.839537,"Missing"
C10-5001,W03-1012,0,0.0809386,"Missing"
C10-5001,P04-1054,0,0.141252,"Missing"
C10-5001,W04-3222,0,0.0576831,"Missing"
C10-5001,P05-1024,0,0.0449042,"Missing"
C10-5001,H05-1091,0,0.126541,"Missing"
C10-5001,P05-1052,0,0.0613293,"Missing"
C10-5001,P06-1104,0,0.0458844,"Missing"
C10-5001,D07-1076,0,0.0386586,"Missing"
C10-5001,I08-2119,0,0.0381849,"Missing"
C10-5001,E09-1066,1,\N,Missing
C10-5001,N10-1146,1,\N,Missing
C10-5001,P04-1043,1,\N,Missing
C10-5001,P08-1091,1,\N,Missing
C12-2040,W10-2903,0,0.0545519,"We ran several experiments to evaluate the accuracy of our approach for automatic generation and selection of correct SQL queries from questions. We experimented with a well-known dataset GeoQuery developed in the context of semantic parsing. To generate the set of possible SQL queries we applied our algorithm described in Section 3 to the GEOQUERIES corpus. We considered the full GeoQuery annotation (GEO880) but we used the subset of 700 pairs (henceforth GEO700) since they are translated by (Popescu et al., 2003) from Prolog data to SQL queries. Additionally, to compare with latest systems (Clarke et al., 2010; Liang et al., 2011), which used a subset of 500 pairs, hereafter GEO500, we annotated the remaining 180 pairs as they were included in GEO500. The latter was randomly split by (Clarke et al., 2010) in 250 pairs for training and 250 pairs for testing. The data is slightly easier since the number of logical symbols per word are limited to an average of 13 logical symbols. It is worth noting that even if we manually annotated missing questions with their answering SQL queries, we only used them for extracting the answer from the database and evaluate the pair correctness (so we do not really us"
C12-2040,de-marneffe-etal-2006-generating,0,0.0334068,"Missing"
C12-2040,W05-0602,0,0.0307611,"pairs of questions and answers concerning a target DB (no SQL query is needed) to train our model. KEYWORDS: Natural Language Interface to Databases, Semantic Parsing, Reranking. Proceedings of COLING 2012: Posters, pages 401–410, COLING 2012, Mumbai, December 2012. 401 Figure 1: A DBMS catalog containing GEOQUERY database 1 Introduction In the last decade, a variety of approaches have been developed to automatically convert natural language questions into machine-readable instructions. A considerable amount of research work has tackled such problem along the line of semantic parsing, e.g., (Ge and Mooney, 2005; Wong and Mooney, 2006) defined algorithms for mapping natural language questions to logical forms, (Minock et al., 2008) made use of a specific semantic grammar and (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) applied lambda calculus to the meaning representation of the questions. In the perspective of question answering (QA) targeting the information of databases (DBs), the automatic system only needs to execute one or more Structured Query Language (SQL) queries that retrieve the answer to the posed natural language question. In our recent work (Giordani and Moschitti, 2009a,b, 2"
C12-2040,giordani-moschitti-2010-corpora,1,0.923417,"Missing"
C12-2040,P06-1115,0,0.0250681,"Missing"
C12-2040,D10-1119,0,0.085202,"Missing"
C12-2040,P11-1060,0,0.0493223,"ments to evaluate the accuracy of our approach for automatic generation and selection of correct SQL queries from questions. We experimented with a well-known dataset GeoQuery developed in the context of semantic parsing. To generate the set of possible SQL queries we applied our algorithm described in Section 3 to the GEOQUERIES corpus. We considered the full GeoQuery annotation (GEO880) but we used the subset of 700 pairs (henceforth GEO700) since they are translated by (Popescu et al., 2003) from Prolog data to SQL queries. Additionally, to compare with latest systems (Clarke et al., 2010; Liang et al., 2011), which used a subset of 500 pairs, hereafter GEO500, we annotated the remaining 180 pairs as they were included in GEO500. The latter was randomly split by (Clarke et al., 2010) in 250 pairs for training and 250 pairs for testing. The data is slightly easier since the number of logical symbols per word are limited to an average of 13 logical symbols. It is worth noting that even if we manually annotated missing questions with their answering SQL queries, we only used them for extracting the answer from the database and evaluate the pair correctness (so we do not really use the SQL queries). T"
C12-2040,D08-1082,0,0.0311031,"Missing"
C12-2040,P12-1080,1,0.858631,"Missing"
C12-2040,W06-2909,1,0.33867,"Missing"
C12-2040,N06-1056,0,0.0127828,"nd answers concerning a target DB (no SQL query is needed) to train our model. KEYWORDS: Natural Language Interface to Databases, Semantic Parsing, Reranking. Proceedings of COLING 2012: Posters, pages 401–410, COLING 2012, Mumbai, December 2012. 401 Figure 1: A DBMS catalog containing GEOQUERY database 1 Introduction In the last decade, a variety of approaches have been developed to automatically convert natural language questions into machine-readable instructions. A considerable amount of research work has tackled such problem along the line of semantic parsing, e.g., (Ge and Mooney, 2005; Wong and Mooney, 2006) defined algorithms for mapping natural language questions to logical forms, (Minock et al., 2008) made use of a specific semantic grammar and (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) applied lambda calculus to the meaning representation of the questions. In the perspective of question answering (QA) targeting the information of databases (DBs), the automatic system only needs to execute one or more Structured Query Language (SQL) queries that retrieve the answer to the posed natural language question. In our recent work (Giordani and Moschitti, 2009a,b, 2010, 2012), we have show"
C12-2040,P07-1121,0,0.0200414,"ages 401–410, COLING 2012, Mumbai, December 2012. 401 Figure 1: A DBMS catalog containing GEOQUERY database 1 Introduction In the last decade, a variety of approaches have been developed to automatically convert natural language questions into machine-readable instructions. A considerable amount of research work has tackled such problem along the line of semantic parsing, e.g., (Ge and Mooney, 2005; Wong and Mooney, 2006) defined algorithms for mapping natural language questions to logical forms, (Minock et al., 2008) made use of a specific semantic grammar and (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) applied lambda calculus to the meaning representation of the questions. In the perspective of question answering (QA) targeting the information of databases (DBs), the automatic system only needs to execute one or more Structured Query Language (SQL) queries that retrieve the answer to the posed natural language question. In our recent work (Giordani and Moschitti, 2009a,b, 2010, 2012), we have shown that machine learning algorithms, exploiting syntactic representations of both questions and queries, can be used to automatically associate a question with the corresponding SQL queries. One lim"
C14-1020,P02-1034,0,0.423722,"itional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using"
C14-1020,P06-2034,0,0.0263295,"derable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a"
C14-1020,J02-3001,0,0.163552,"cognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of conc"
C14-1020,D12-1083,1,0.768831,"low trees. A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example, the first clause “along my route tell me the next steak house” is elaborated by the second clause “that is within a mile”. The relations by which clauses in a text are linked are called coherence relations (e.g., Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide additional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson, 1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is. 3.3 New features In order to compare to the structured representation, we also devoted significant effort towards engineering a set of features to be used in a flat feature-vector representation; they can be used in isolation or in combination with the kernel-based a"
C14-1020,P06-1115,0,0.033166,"eranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a local model, where the hyp"
C14-1020,H05-1064,0,0.0355336,"ated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly bas"
C14-1020,P05-1024,0,0.0308755,"o be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures f"
C14-1020,J08-2001,1,0.787599,"Missing"
C14-1020,H94-1053,0,0.881717,"{price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be fo"
C14-1020,W06-2909,1,0.869136,"of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-gr"
C14-1020,H91-1020,0,0.794759,"Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 193 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 193–202, Dublin, Ireland, August 23-29 2014. Finally, a database query is formed from the list of labels and values, and is then executed against the database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed. {$and [{cuisine:&quot;lebanese&quot;}, {city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied"
C14-1020,H89-1026,0,0.078377,"city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of"
C16-1163,C16-1237,1,0.746099,"Missing"
C16-1163,P15-2114,0,0.0146485,". Table 1: A re-ranking example: we report the Google rank (G), the gold standard relevance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the"
C16-1163,P08-1019,0,0.0283351,"s our learning-to-rank approach. Section 4 describes the application of LSTMs in TK-based ranking models. Section 5 describes our text selection strategies. Section 6 discusses our experiments and the obtained results. Finally, Section 7 concludes the paper. 2 Related Work Question ranking in cQA has been central in the research community practically since the begining of cQA system design. Beside “standard” similarity measures, different characterizations and models have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a col"
C16-1163,P15-1097,1,0.787031,"ranslation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in th"
C16-1163,S16-1172,1,0.882623,"uestions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text select"
C16-1163,S16-1126,0,0.0424839,"s is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in the month of June. Just wanna know some good places to visit. G GS R Retrieved Questions 1 -1 8 The Qatar banana island will be transfered by the end of 2013 to 5 stars resort called Anantara. Has anyone seen this island? Where is it? Is it near to Corniche? 2 +1 2 Is there"
C16-1163,W01-0515,0,0.030947,"gure 1: Representation of two questions as syntactic trees. Related nodes are enriched with REL links. 3.4 Feature Vectors We combine the kernel above with an RBF kernel applied to feature vectors composed of similarity features. These are computed between the original and the related question and the Google rank. Such text similarity features (sim) are 20 similarities sim(qo , qs ) using word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also add a structural similarity obtained by comparing the syntactic trees of the questions of an example pair using the partial tree kernel, i.e., T K(t(qo , qs ), t(qs , qo )). Note that the operands of the kernel function are members of the same pair. The ranking-based feature (rank) is computed using the ranking generated by the baseline Google search engine system. Each candidate question is located in one position in the range [1, . . . , 10]. We exploit this information as the inverse of the position. 4 Long Short-Term Memory Networks for TK-based Reranking A"
C16-1163,S15-2047,1,0.587169,"vance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the weights learned by the attention model for selecting important text seg"
C16-1163,N16-1152,1,0.849591,"in Section 2, several neural approaches have been successfully applied to QA tasks. Unfortunately, question retrieval in cQA is heavily affected by a large amount of noise and a rather different domain, which make it difficult to effectively use out-of-domain embeddings to pre-train neural networks. This probably prevented the participants to SemEval tasks from achieving satisfactory results with such models (Nakov et al., 2016). In this work, we also tried to exploit neural models using their top-level representations for the (qo , qs ) pair and fed them into the TK classifier as proposed by Tymoshenko et al. (2016), but this simple combination proved to be ineffective as well. In contrast, neural embeddings and weights can be useful for selecting better representations for TK models. In the reminder of this section, we present LSTM networks for question retrieval and our approach for incorporating them into TK-based rerankers. We approach question ranking as a classification task: given a pair (qo , qs ), we need to classify qs as relevant or irrelevant. In order to evaluate the neural classifiers on our ranking task, we can rank candidates, qs , according to their posterior probability. Among the diffe"
C16-1163,P06-1051,1,0.693051,"ining examples, αi are weights, yi are the example labels, φ(qoi , qsi ) is the representation of pairs of the original and candidate questions. This leads to the following scoring function: r(qo , qs ) = n X αi yi φ(qo , qs ) · φ(qoi , qsi ) = i=1 n X  αi yi K hqo , qs i, hqoi , qsi i , i=1 where the kernel, K(·, ·), intends to capture the similarity between pairs of objects constituted by the original and retrieved questions. The definition of effective Ks for QA and other relational learning tasks, e.g., textual entailment and paraphrasing, has been studied in a large body of work, e.g., (Zanzotto and Moschitti, 2006; Filice et al., 2015b). Given the high similarity between question ranking in cQA and passage ranking in QA, we opted for the state-of-the-art model proposed by Severyn and Moschitti (2012). It should be noted that we apply TK models to pairs of questions rather than questions with their passages. Figure 1 displays an example of the structure we used for representing the original question, qo and the seventh candidate question, qs , in Table 1. The graph is composed by two macro-trees, one for each question, which in turn are constituted by the syntactic trees of the sentences composing the t"
C16-1163,P11-1066,0,0.015238,"have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, wit"
C16-1163,S16-1138,1,\N,Missing
C16-1237,S16-1138,1,0.881056,"Missing"
C16-1237,P15-2114,0,0.171147,"09) computed their similarity function on the syntactic-tree representations of the questions. The more substructures the trees have in common, the more similar their associated questions are. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topics distribution to retrieve similar historical questions. The recent boom in neural network approaches has also impacted question retrieval. dos Santos et al. (2015) applied convolutional neural networks to retrieve semantically-equivalent questions’ subjects. When dealing with whole questions —subject and (generally long) body—, they had to aggregate a bag-of-words neural network to boost the model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al."
C16-1237,P08-1019,0,0.125266,"r model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are like"
C16-1237,P15-1097,1,0.531448,"hat generate question/answer pairs and use the learned topics distribution to retrieve similar historical questions. The recent boom in neural network approaches has also impacted question retrieval. dos Santos et al. (2015) applied convolutional neural networks to retrieve semantically-equivalent questions’ subjects. When dealing with whole questions —subject and (generally long) body—, they had to aggregate a bag-of-words neural network to boost the model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similar"
C16-1237,S16-1172,1,0.800539,"fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they use. Franco-Salvador et al. (2016) rely heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statistically-significant differences were observed in the performance of these three systems. To the best of our knowledge, so far the only other work exploring text selection to improve"
C16-1237,S16-1126,0,0.451216,"sed on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they use. Franco-Salvador et al. (2016) rely heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statistically-significant differences were observed in the performance of these th"
C16-1237,S15-2035,0,0.0138226,"le questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they"
C16-1237,D15-1068,1,0.877252,"Missing"
C16-1237,N03-1017,0,0.0194065,"ao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are likely to address similar questions! The questions in the so-generated pairs compose their “parallel” corpus. Wang et al. (2009) computed their similarity function on the syntactic-tree representations of the questions. The more substructures the trees have in"
C16-1237,W01-0515,0,0.484216,"Missing"
C16-1237,E06-1015,1,0.568992,"ns on top of all the Irrelevant ones, regardless of the order within both subsets (Cao et al., 2008; dos Santos et al., 2015; Jeon et al., 2005b). We adopt the same architecture as the recently-proposed, most successful, models on this kind of setting (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). We apply a kernel approach to solve a binary classification problem, f : Q × D → {Relevant, Irrelevant}, and sort the forum questions d ∈ D according to their classification score against q: f (q, d). We opt for a tree kernel applied to parse-tree representations (Moschitti, 2006b; Sun et al., 2011), as it performs well in ranking both passages (Severyn and Moschitti, 2012) and questions (Barr´on-Cede˜no et al., 2016; Da San Martino et al., 2016; Filice et al., 2016). Additionally, the nodes of the parse trees of the pairs (q, d) are marked with a REL tag when there is at least a lexical match between the phrases of the questions (c.f. (Filice et al., 2015b) for details). The approach for dealing with a pair of trees, is to compose kernels on single trees K T (x1 , x2 ): K((qi , di ), (qj , dj )) = K T (t(qi , di )), t(qj , dj ))) + K T (t(di , qi )), t(dj , qj ))), ("
C16-1237,S15-2047,1,0.911316,"Missing"
C16-1237,S15-2036,1,0.865019,"Missing"
C16-1237,C16-1163,1,0.744397,"Missing"
C16-1237,P08-1082,0,0.0407781,"2 Related Work Community question answering poses various challenges: answer and question ranking, and question de-duplication are three examples. We now review the related literature with focus on question ranking. One of the first approaches to answer ranking relied completely on the website’s metadata (Jeon et al., 2006), such as an author’s reputation and click counts. Agichtein et al. (2008) explored a graphbased model of contributors relationships together with both content- and usage-based features. These approaches depend heavily on the forum’s meta-data and social features. Still, as Surdeanu et al. (2008) stress, relying on this kind of data causes the model portability to be difficult; a drawback that disappears when focusing on the content of the questions and answers only. Therefore, our model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of wh"
C16-1237,S15-2038,0,0.0124471,"he model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first ("
C16-1237,P11-1066,0,0.485679,"le threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are likely to address similar questions! The questions in the so-generated pairs compose their “parallel”"
C16-1237,P15-1025,0,0.0200524,"utation and click counts. Agichtein et al. (2008) explored a graphbased model of contributors relationships together with both content- and usage-based features. These approaches depend heavily on the forum’s meta-data and social features. Still, as Surdeanu et al. (2008) stress, relying on this kind of data causes the model portability to be difficult; a drawback that disappears when focusing on the content of the questions and answers only. Therefore, our model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questi"
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
C18-1185,W03-0422,0,0.0432218,"Missing"
C18-1185,W03-0423,0,0.116276,"n progressive NER, analyzing the performance of both models. Our main contribution is twofold, we show that the Seq2Seq model is: (i) effective for the NER task, and (ii) more robust in the progressive NER setting as it does not forget previously learned knowledge about seen NE categories. 2 Related Work In this section, we report related works on current state-of-the-art models for standard NER task, as well as the related works in the Transfer Learning and Progressive Learning areas. 2.1 Neural Models for NER Early works on NER focused on engineering useful linguistic features for the task (Chieu and Ng, 2003; Carreras et al., 2003; Florian et al., 2003). The recent advances in the deep learning brought about new methods and produced a higher performance for the task. Current state-of-the-art models on standard NER are mostly neural models, in particular, Recurrent Neural Networks. These models incorporate word (sometimes also character) level embeddings and/or additional morphological word features for recognizing NE. Examples include CRF (Conditional Random Fields) over BLSTM (Bidirectional LSTM) model proposed by Lample et al. (2016) and BLSTM with Convolutional character feature extractor prop"
C18-1185,Q16-1026,0,0.395286,"ally provided with more available labeled data. Hence, methods that can utilize such data in new domains are rather useful. Additionally, the possibility to only use the models trained on such data is very important as it avoids the NER designer to deliver most training data to the user for adaptation in new domains. The setting above describes a TL problem, where the domain remains unchanged but the output space of the task changes: a graphical illustration of our progressive task is shown in Fig. 1. Since the current state-of-the-art of NER is provided by neural models (Lample et al., 2016; Chiu and Nichols, 2016), we focus on them. In particular, we implement a baseline model with a bidirectional LSTM (BLSTM) as it is effective and does not require a heavy feature engineering for the task. BLSTM achieves competitive performance in comparisons with many other more complex state-of-theart models, being much less complex. A drawback of such model is that, although its prediction takes the ∗ The main part of this work was carried out when the author was at the Unviversity of Trento. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommo"
C18-1185,D14-1179,0,0.0332429,"Missing"
C18-1185,W03-0425,0,0.13271,"e of both models. Our main contribution is twofold, we show that the Seq2Seq model is: (i) effective for the NER task, and (ii) more robust in the progressive NER setting as it does not forget previously learned knowledge about seen NE categories. 2 Related Work In this section, we report related works on current state-of-the-art models for standard NER task, as well as the related works in the Transfer Learning and Progressive Learning areas. 2.1 Neural Models for NER Early works on NER focused on engineering useful linguistic features for the task (Chieu and Ng, 2003; Carreras et al., 2003; Florian et al., 2003). The recent advances in the deep learning brought about new methods and produced a higher performance for the task. Current state-of-the-art models on standard NER are mostly neural models, in particular, Recurrent Neural Networks. These models incorporate word (sometimes also character) level embeddings and/or additional morphological word features for recognizing NE. Examples include CRF (Conditional Random Fields) over BLSTM (Bidirectional LSTM) model proposed by Lample et al. (2016) and BLSTM with Convolutional character feature extractor proposed by Chiu and Nichols (2016). We implement"
C18-1185,P15-1046,0,0.0315795,"the data for training and testing has the same feature space and distribution. TF is a technique that emerges to relax this assumption, allowing previously-learned knowledge to be used for a new task or in a new domain (Pan and Yang, 2010). Neural networksbased TL has proven to be very effective for image recognition (Donahue et al., 2014; Razavian et al., 2014). As for NLP, Mou et al. (2016) showed that TL can also be applied successfully on semantically equivalent NLP tasks. TL was applied on NER too, to transfer model and features on NER dataset with different NE entities (Qu et al., 2016; Kim et al., 2015). Our learning paradigm falls in a more specific TL category – Progressive Learning. The Progressive Learning technique is adopted in multi-class classification by Venkatesan and Er (2016). They remodeled a network architecture (a single layer feed-forward network) by increasing the number of new neurons and interconnections while encountering unseen class labels in the dataset. The effectiveness of this technique is shown on several classic classification datasets. Our task is a sequence labeling task, where the independence assumption of output labels in a sequence does not hold. Hence, it c"
C18-1185,N16-1030,0,0.69549,"common NEs are typically provided with more available labeled data. Hence, methods that can utilize such data in new domains are rather useful. Additionally, the possibility to only use the models trained on such data is very important as it avoids the NER designer to deliver most training data to the user for adaptation in new domains. The setting above describes a TL problem, where the domain remains unchanged but the output space of the task changes: a graphical illustration of our progressive task is shown in Fig. 1. Since the current state-of-the-art of NER is provided by neural models (Lample et al., 2016; Chiu and Nichols, 2016), we focus on them. In particular, we implement a baseline model with a bidirectional LSTM (BLSTM) as it is effective and does not require a heavy feature engineering for the task. BLSTM achieves competitive performance in comparisons with many other more complex state-of-theart models, being much less complex. A drawback of such model is that, although its prediction takes the ∗ The main part of this work was carried out when the author was at the Unviversity of Trento. This work is licensed under a Creative Commons Attribution 4.0 International License. License detai"
C18-1185,D15-1166,0,0.0475907,"extractor proposed by Chiu and Nichols (2016). We implement a BLSTM as the baseline model for pro2182 gressive NER since it is an effective model that does not require a heavy feature engineering for the task. The performance of BLSTM model are reported in different works (Lample et al., 2016; Chiu and Nichols, 2016): this also makes it easier for us to conduct a comprehensive comparison with other works. 2.2 Seq2Seq Models Although not previously used for NER task, Seq2Seq models have seen a great success in various applications, such as Neural Machine Translation by (Bahdanau et al., 2014; Luong et al., 2015), Speech Recognition (Lu et al., 2016; Zhang et al., 2017), Question Answering (Yin et al., 2016; Zhou et al., 2015) and so on. The idea of Seq2Seq model is to map a variable length source sequence to a fixed length vector representation by the encoder, and then remap it back to a variable length target sequence by the decoder. The Seq2Seq model was proposed by Cho et al. (2014) to learn phrase representations to be used in statistical Machine Translation. They reported empirical improvement of log-linear model performance while incorporating conditional probability of phrase pairs computed by"
C18-1185,D16-1046,0,0.0297623,"the input word sequence and the output label sequence because the source and target sequences have a one-to-one mapping in the NER task. 2.3 Transfer Learning and Progressive Learning Most machine learning methods assume the data for training and testing has the same feature space and distribution. TF is a technique that emerges to relax this assumption, allowing previously-learned knowledge to be used for a new task or in a new domain (Pan and Yang, 2010). Neural networksbased TL has proven to be very effective for image recognition (Donahue et al., 2014; Razavian et al., 2014). As for NLP, Mou et al. (2016) showed that TL can also be applied successfully on semantically equivalent NLP tasks. TL was applied on NER too, to transfer model and features on NER dataset with different NE entities (Qu et al., 2016; Kim et al., 2015). Our learning paradigm falls in a more specific TL category – Progressive Learning. The Progressive Learning technique is adopted in multi-class classification by Venkatesan and Er (2016). They remodeled a network architecture (a single layer feed-forward network) by increasing the number of new neurons and interconnections while encountering unseen class labels in the datas"
C18-1185,D16-1087,0,0.0239044,"g methods assume the data for training and testing has the same feature space and distribution. TF is a technique that emerges to relax this assumption, allowing previously-learned knowledge to be used for a new task or in a new domain (Pan and Yang, 2010). Neural networksbased TL has proven to be very effective for image recognition (Donahue et al., 2014; Razavian et al., 2014). As for NLP, Mou et al. (2016) showed that TL can also be applied successfully on semantically equivalent NLP tasks. TL was applied on NER too, to transfer model and features on NER dataset with different NE entities (Qu et al., 2016; Kim et al., 2015). Our learning paradigm falls in a more specific TL category – Progressive Learning. The Progressive Learning technique is adopted in multi-class classification by Venkatesan and Er (2016). They remodeled a network architecture (a single layer feed-forward network) by increasing the number of new neurons and interconnections while encountering unseen class labels in the dataset. The effectiveness of this technique is shown on several classic classification datasets. Our task is a sequence labeling task, where the independence assumption of output labels in a sequence does no"
C18-1185,W16-0106,0,0.0179664,"182 gressive NER since it is an effective model that does not require a heavy feature engineering for the task. The performance of BLSTM model are reported in different works (Lample et al., 2016; Chiu and Nichols, 2016): this also makes it easier for us to conduct a comprehensive comparison with other works. 2.2 Seq2Seq Models Although not previously used for NER task, Seq2Seq models have seen a great success in various applications, such as Neural Machine Translation by (Bahdanau et al., 2014; Luong et al., 2015), Speech Recognition (Lu et al., 2016; Zhang et al., 2017), Question Answering (Yin et al., 2016; Zhou et al., 2015) and so on. The idea of Seq2Seq model is to map a variable length source sequence to a fixed length vector representation by the encoder, and then remap it back to a variable length target sequence by the decoder. The Seq2Seq model was proposed by Cho et al. (2014) to learn phrase representations to be used in statistical Machine Translation. They reported empirical improvement of log-linear model performance while incorporating conditional probability of phrase pairs computed by the Seq2Seq model as additional features. Sutskever et al. (2014) also proposed a more general"
C18-1185,P15-2117,0,0.0310874,"ince it is an effective model that does not require a heavy feature engineering for the task. The performance of BLSTM model are reported in different works (Lample et al., 2016; Chiu and Nichols, 2016): this also makes it easier for us to conduct a comprehensive comparison with other works. 2.2 Seq2Seq Models Although not previously used for NER task, Seq2Seq models have seen a great success in various applications, such as Neural Machine Translation by (Bahdanau et al., 2014; Luong et al., 2015), Speech Recognition (Lu et al., 2016; Zhang et al., 2017), Question Answering (Yin et al., 2016; Zhou et al., 2015) and so on. The idea of Seq2Seq model is to map a variable length source sequence to a fixed length vector representation by the encoder, and then remap it back to a variable length target sequence by the decoder. The Seq2Seq model was proposed by Cho et al. (2014) to learn phrase representations to be used in statistical Machine Translation. They reported empirical improvement of log-linear model performance while incorporating conditional probability of phrase pairs computed by the Seq2Seq model as additional features. Sutskever et al. (2014) also proposed a more general end-to-end approach"
coppola-moschitti-2010-general,S07-1018,0,\N,Missing
coppola-moschitti-2010-general,J08-2003,1,\N,Missing
coppola-moschitti-2010-general,W05-0407,1,\N,Missing
coppola-moschitti-2010-general,W04-3212,0,\N,Missing
coppola-moschitti-2010-general,N09-2022,1,\N,Missing
coppola-moschitti-2010-general,W05-0630,1,\N,Missing
coppola-moschitti-2010-general,W09-0505,1,\N,Missing
coppola-moschitti-2010-general,P98-1013,0,\N,Missing
coppola-moschitti-2010-general,C98-1013,0,\N,Missing
coppola-moschitti-2010-general,W05-0620,0,\N,Missing
coppola-moschitti-2010-general,P02-1034,0,\N,Missing
coppola-moschitti-2010-general,J02-3001,0,\N,Missing
coppola-moschitti-2010-general,J05-1004,0,\N,Missing
coppola-moschitti-2010-general,E06-1015,1,\N,Missing
coppola-moschitti-2010-general,W04-2412,0,\N,Missing
D09-1012,P03-1004,0,0.716984,"ven very accurate models generally fail in providing useful feedback for improving our understanding of the problems at study. Moreover, the computational burden induced by high dimensional kernels makes the application of SVMs to large corpora still more problematic. In (Pighin and Moschitti, 2009), we proposed a feature extraction algorithm for Tree Kernel (TK) spaces, which selects the most relevant features (tree fragments) according to the gradient components (weight vector) of the hyperplane learnt by an SVM, in line with current research, e.g. (Rakotomamonjy, 2003; Weston et al., 2003; Kudo and Matsumoto, 2003). In particular, we provided algorithmic solutions to deal with the huge dimensionality and, consequently, high computational complexity of the fragment space. Our experimental results showed that our approach reduces learning and classification processing time leaving the accuracy unchanged. In this paper, we present a new version of such algorithm which, under the same parameterization, is almost three times as fast while producing the same results. Most importantly, we explored tree fragment spaces for two interesting natural language tasks: Semantic Role Labeling (SRL) and Question Classif"
D09-1012,P05-1024,0,0.0914859,"ties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will detail the outcome of our"
D09-1012,P07-1098,1,0.956725,"xploit the SVM optimizer to select the most relevant features instead of a relevance assessment measure that moves from different statistical assumptions than the learning algorithm. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. In (Moschitti, 2006; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Moschitti et al., 2007), the SST kernel along with other tree and combined kernels are employed for question classification and semantic role labeling with interesting results. and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Mul"
D09-1012,W05-0620,0,0.111749,"Missing"
D09-1012,J08-2003,1,0.917757,"theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will detail the outcome of our experiments, and Section"
D09-1012,A00-2018,0,0.0113084,"roach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. In (Moschitti, 2006; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Moschitti et al., 2007), the SST kernel along with other tree and combined kernels are employed for question classification and semantic role labeling with interesting results. and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM). 5 Experiments 5.1.1 Setup (A0) NP NNP (A1) NP VB Mary bought ⇒ D NN a cat VB-P NP VB-P bought D-B bought a -1: BC NP-B D NN a cat +1: BC,A1 -1: A0,A2,A3,A4,A5 Figure 2: Examples o"
D09-1012,P02-1034,0,0.83769,"ents on semantic role labeling and question classification illustrate the above claims. 1 Introduction The last decade has seen a massive use of Support Vector Machines (SVMs) for carrying out NLP tasks. Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Se"
D09-1012,P04-1054,0,0.203975,"he last decade has seen a massive use of Support Vector Machines (SVMs) for carrying out NLP tasks. Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization"
D09-1012,J05-1004,0,0.0411768,"Missing"
D09-1012,P08-1091,1,0.841399,"2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will detail the outcome of our experiments, and Section 6 will discuss some r"
D09-1012,W09-1106,1,0.725322,"K-Irst, HLT Alessandro Moschitti University of Trento, DISI Via di Sommarive, 18 I-38100 Povo (TN) Italy Via di Sommarive, 14 I-38100 Povo (TN) Italy pighin@fbk.eu moschitti@disi.unitn.it Abstract by the the implicit nature of the kernel space, which prevents to directly observe the most relevant features. As a consequence, even very accurate models generally fail in providing useful feedback for improving our understanding of the problems at study. Moreover, the computational burden induced by high dimensional kernels makes the application of SVMs to large corpora still more problematic. In (Pighin and Moschitti, 2009), we proposed a feature extraction algorithm for Tree Kernel (TK) spaces, which selects the most relevant features (tree fragments) according to the gradient components (weight vector) of the hyperplane learnt by an SVM, in line with current research, e.g. (Rakotomamonjy, 2003; Weston et al., 2003; Kudo and Matsumoto, 2003). In particular, we provided algorithmic solutions to deal with the huge dimensionality and, consequently, high computational complexity of the fragment space. Our experimental results showed that our approach reduces learning and classification processing time leaving the a"
D09-1012,J02-3001,0,0.0277981,"rgument nodes. All the most relevant fragments encode the minimum sub-tree encompassing the predicate and the argument node. This kind of structured feature subsumes several features traditionally employed for explicit SRL models: the Path (i.e. the sequence of nodes connecting the predicate and the candidate argument node), Phrase Type (i.e. the label of the candidate argument node), Predicate POS (i.e. the POS of the predicate word), Position (i.e. whether the argument is to the left or to the right of the predicate) and Governing Category (i.e. the label of the common ancestor) defined in (Gildea and Jurafsky, 2002). The linearized model for BC contains about 160 thousand fragments. Of these, about 70 and 33 thousand encompass the candidate argument or the predicate node, respectively. About 16 thousand fragments contain both. 5.2 Class ABBR DESC ENTY HUM LOC NUM Overall 89 1,164 1,269 1,231 834 896 9 138 94 65 81 113 Accuracy SST SSTℓ 80.0 96.0 63.9 88.1 77.6 80.4 86.2 87.5 94.5 63.5 87.2 77.9 80.8 86.6 Table 2: Number of positive training (Tr+ ) and test (Te+ ) examples in the QA dataset. Accuracy of the non-linearized (SST) and linearized (SSTℓ) binary classifiers is F1 measure. Overall accuracy is th"
D09-1012,W03-1012,0,0.140186,"out NLP tasks. Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subje"
D09-1012,P05-1050,0,0.0585944,", their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will deta"
D09-1012,W04-3222,0,0.161612,"ive use of Support Vector Machines (SVMs) for carrying out NLP tasks. Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Se"
D09-1012,H05-1018,0,0.110576,"Machines (SVMs) for carrying out NLP tasks. Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications. More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (Collins and Duffy, 2002), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Toutanova et al., 2004), (Kazama and Torisawa, 2005), (Shen et al., 2003), (Gliozzo et al., 2005), (Kudo et al., 2005), (Moschitti et al., 2008), (Diab et al., 2008). Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced 111 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111–120, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion category. The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous"
D09-1012,P03-1054,0,0.00448364,"ies. We adopted the question taxonomy known as coarse grained, which has been described in (Zhang and Lee, 2003) and (Li and Roth, 2006), consisting of six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LOC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates). For each question, we generate the full parse of the sentence and use it to train SST and (linearized) SSTℓ models. The automatic parses are obtained with the Stanford parser3 (Klein and Manning, 2003). We actually have only 5,483 sentences in our training set, due to parsing issues with a few of them. QC Fragment space. Tables from 4 to 9 list the top fragments identified for each class 4 . As expected, for all the categories the domain lexical information is very relevant. For example, film, color, book, novel and sport for ENTY or city, country, state and capital for LOC. Of the six classes, ENTY (Table 6) is mostly characterized by lexical features. Interestingly, function words, which would have been eliminated by a pure Information Retrieval approach (i.e. by means of 2 http://l2r.cs."
D09-1112,W05-0601,1,0.501549,"Missing"
D09-1112,P02-1034,0,0.833297,"U hypotheses, which are re-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The la"
D09-1112,W06-2909,1,0.902359,"ine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to generate correlation between concepts and word structures. Figures 2(a), 2(b) and 3 describe the structural association between the concept and the word 1080 (a) FLAT Tree (b) MULTILEVEL Tree Figure 2: Examples of st"
D09-1112,P04-1054,0,0.0372046,"The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we u"
D09-1112,P07-1098,1,0.842965,"or structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschi"
D09-1112,E09-1024,1,0.941097,"possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art"
D09-1112,W09-0505,1,0.909585,"possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art"
D09-1112,J08-2003,1,0.944424,"he generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the senten"
D09-1112,E06-1015,1,0.941864,"e-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined wit"
D09-1112,hahn-etal-2008-comparison,0,0.0496457,"able by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art in SLU. Learning curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer fea"
D09-1112,N01-1025,0,0.0418612,"Section 4. The initial annotation to be re-ranked is the list of the ten best hypotheses output by an FST model. We point out that, on the large Media dataset the processing time is considerably high2 so we could not run all the models. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST again with SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always set to 10. The CRF model we compare with was trained with the CRF++ tool, available at http://crfpp.sourceforge.net/. The model is equivalent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic categories in a window of [-2, +2] with respect to the current token"
D09-1112,W03-1012,0,0.189001,"nce kernel is used to evaluate the number of common ngrams between si and sj . Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function 1 2 as follows: let ek be the pair sk , sk we evaluate the kernel: KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (2) − SK(s11 , s22 ) − SK(s21 , s12 ) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et a"
D09-1112,P03-1004,0,0.0318646,"curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm d"
D09-1112,N04-1023,0,0.0904065,"t sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function 1 2 as follows: let ek be the pair sk , sk we evaluate the kernel: KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (2) − SK(s11 , s22 ) − SK(s21 , s12 ) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence"
D09-1112,P05-1024,0,0.107111,"Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style"
D09-1112,W04-3222,0,0.0642139,"s organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as"
D09-1112,W04-2403,1,0.91097,"Missing"
D09-1112,P08-2029,1,0.82344,"ucing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the sentence: How may I help you ? sample s"
D09-1143,W05-0601,1,0.778199,"nt tree or of the path linking two entities of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provid"
D09-1143,H05-1091,0,0.860771,"ng relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of"
D09-1143,P04-1053,0,0.0120628,"vious work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be use"
D09-1143,P03-1054,0,0.00259989,"ata portion includes 348 documents and 4400 relation instances. It defines seven entity types and seven relation types. Every relation is assigned one of the seven types: Physical, Person/Social, Employment/Membership/Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse. For sake of space, we do not explain these relationships here, nevertheless, they are explicitly described in the ACE document guidelines. There are 4400 positive and 38,696 negative examples when generating pairs of entity mentions as potential relations. Documents are parsed using Stanford Parser (Klein and Manning, 2003) to produce parse trees. Potential relations are generated by iterating all pairs of entity mentions in the same sentence. Entity information, namely entity type, is integrated into parse trees. To train and test our binary relation classifier, we used SVMs. Here, relation detection is formulated as a multiclass classification problem. The one vs. rest strategy is employed by selecting the instance with largest margin as the final answer. For experimentation, we use 5-fold cross-validation with the Tree Kernel Tools (Moschitti, 2004) (available at http://disi.unitn.it/˜moschitt/Tree-Kernel.htm"
D09-1143,J93-2004,0,0.0366907,"Missing"
D09-1143,A00-2030,0,0.0308941,"performance than previous sequence and dependency models for RE. 1 The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning m"
D09-1143,W04-2403,1,0.85905,"task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit"
D09-1143,P07-1098,1,0.48532,"ernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the thr"
D09-1143,J08-2003,1,0.717886,"ic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the three available tree kernels: ST, SST or PT."
D09-1143,P04-1043,1,0.911584,"alculation since the size of a complete parse tree may be very large (up to 300 nodes in the Penn Treebank (Marcus et al., 1993)); second, there is ambiguity on the target pairs of NEs, i.e. different NEs associated with different relations are described by the same parse tree. Therefore, it is necessary to identify the portion of the parse tree that best represent the useful syntactic information. Let e1 and e2 be two entity mentions in the same sentence such that they are in a relationship R. For the constituent parse tree, we used the pathenclosed tree (PET), which was firstly proposed in (Moschitti, 2004) for Semantic Role Labeling and then adapted by (Zhang et al., 2005) for relation extraction. It is the smallest common subtree including the two entities of a relation. The dashed frame in Figure 2.a surrounds PET associated with the two mentions, officials and Washington. Moreover, to improve the representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washingto"
D09-1143,P04-1054,0,0.963726,"in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where t"
D09-1143,P06-1117,1,0.379513,"Missing"
D09-1143,C02-1151,0,0.098702,"Missing"
D09-1143,I08-2119,0,0.419818,"1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of com"
D09-1143,W02-1010,0,0.685626,"action (RE) is defined in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel ("
D09-1143,I05-1034,0,0.599525,"es in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the"
D09-1143,P06-1104,0,0.669457,"es over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution tree kernels for dependency trees is worthwhile also considering that, to the best of our knowledge, these models have not been previously used for relation extraction1 task. Additionally, sequence kernels should be included in such global st"
D09-1143,P05-1052,0,0.738863,"ubtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution t"
D09-1143,P05-1053,0,0.286051,"on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syn"
D09-1143,D07-1076,0,0.704331,"ptured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest"
D11-1066,W05-0601,1,0.863421,"Missing"
D11-1066,H05-1091,0,0.0862146,"Missing"
D11-1066,P07-1073,0,0.0212609,"Missing"
D11-1066,A00-2018,0,0.0135374,"[when][hit][by][electrons][,][a][phosphor][gives] Tools: for SVM learning, we used the SVMLight[off][electromagnetic][energy][in][this][form] TK software4 , which includes structural kernels in PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] SVMLight (Joachims, 1999)5 . For generating con[dt][nn] 3 Additionally, we use constituency trees (CTs), see 2 From here the name syntactic tree kernels 716 Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4 Available at http://dit.unitn.it/∼moschitt 5 http://svmlight.joachims.org stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that"
D11-1066,P06-2010,0,0.0474861,"Missing"
D11-1066,P02-1034,0,0.0610751,"ing three different kernels: • Sequence Kernels (SK); we implemented the 1 discontinuous string kernels described in (ShaweTaylor and Cristianini, 2004). This allows for representing a string of symbols in terms of its possible substrings with gaps, i.e. an arbitrary number of symbols can be skipped during the generation of a substring. The symbols we used in the sequential descriptions of questions are words and part-of-speech tags (in two separate sequences). Consequently, all possible multiwords with gaps are features of the implicitly generated vector space. • Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) applied to constituency parse trees. This generates all possible tree fragments as features with the conditions that sibling nodes from the original trees cannot be separated. In other words, substructures are composed by atomic building blocks corresponding to nodes along with all their direct children. These, in case of a syntactic parse tree, are complete production rules of the associated parser grammar2 . • Partial Tree Kernel (PTK) (Moschitti, 2006) applied to both constituency and dependency parse trees. This generates all possible tree fragments, as above, but sibling nodes can be sep"
D11-1066,P04-1054,0,0.155675,"Missing"
D11-1066,W04-3233,0,0.0718242,"Missing"
D11-1066,P03-1003,0,0.031502,"reviously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by exper"
D11-1066,P05-1050,0,0.0605612,"Missing"
D11-1066,W08-2123,0,0.0740335,"Missing"
D11-1066,W07-1206,0,0.023133,"n annuity is an investment or retirement fund that pays out this often (answer: yearly) Even though the clue is nearly identical to (3), the clue does not provide a definition for the answer yearly, although at first glance we may have been misled. The source of complexity is given by the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and PO"
D11-1066,H05-1018,0,0.0419426,"Missing"
D11-1066,P03-1004,0,0.093103,"Missing"
D11-1066,P05-1024,0,0.0648996,"Missing"
D11-1066,C02-1150,0,0.510942,"Missing"
D11-1066,W04-2705,0,0.247345,"e 2 and dependency structures converted into the dependency trees (DTs), e.g. shown in Figure 3. Note that, the POS-tags are central nodes, the grammatical relation label is added as a father node and all the relations with the other nodes are described by means of the connecting edges. Words are considered additional children of the POS-tag nodes (in this case the connecting edge just serves to add a lexical feature to the target POS-tag node). Finally, we also use predicate argument structures generated by verbal and nominal relations according to PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). Given the target sentence, the set of its predicates are extracted and converted into a forest, then a fake root node, PAS, is used to connect these trees. For example, Figure 4 illustrates a Predicate Argument Structures Set (PASS) encoding two relations, give and hit, as well as the nominalization energy along with all their arguments. 4 Experiments on Definition Question Classification In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions. We build several classifiers based on SVMs and kernel methods. Each classifier"
D11-1066,W05-0630,1,0.864591,"Missing"
D11-1066,P07-1098,1,0.630672,"the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and POS tag sequences, dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e.g. (Collins and Duffy, 713 2002; Shawe-Taylor and Cristianini, 2004; Moschitti, 2006). The extensive empirical analysis of several advanced mod"
D11-1066,J08-2003,1,0.882524,"Missing"
D11-1066,D09-1143,1,0.876937,"Missing"
D11-1066,J05-1004,0,0.262489,"finition Jeopardy! question: Figure 2 and dependency structures converted into the dependency trees (DTs), e.g. shown in Figure 3. Note that, the POS-tags are central nodes, the grammatical relation label is added as a father node and all the relations with the other nodes are described by means of the connecting edges. Words are considered additional children of the POS-tag nodes (in this case the connecting edge just serves to add a lexical feature to the target POS-tag node). Finally, we also use predicate argument structures generated by verbal and nominal relations according to PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). Given the target sentence, the set of its predicates are extracted and converted into a forest, then a fake root node, PAS, is used to connect these trees. For example, Figure 4 illustrates a Predicate Argument Structures Set (PASS) encoding two relations, give and hit, as well as the nominalization energy along with all their arguments. 4 Experiments on Definition Question Classification In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions. We build several classifiers based on SVMs a"
D11-1066,P05-1027,0,0.0409807,"Missing"
D11-1066,D07-1002,0,0.0310238,"rk Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved"
D11-1066,W03-1012,0,0.08161,"Missing"
D11-1066,C04-1189,0,0.0266264,"the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lap"
D11-1066,P08-1082,0,0.0261139,"Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The re"
D11-1066,C02-1119,0,0.0667303,"Missing"
D11-1066,W03-1208,0,0.12756,"omplexity is given by the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and POS tag sequences, dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e.g. (Collins and Duffy, 713 2002; Shawe-Taylor and Cristianini, 2004; Moschitti, 2006). The extensive empirical analysi"
D11-1066,W06-2902,0,0.0654666,"Missing"
D11-1066,W04-3222,0,0.0727528,"Missing"
D11-1066,C08-1121,1,0.905995,"Missing"
D11-1066,P06-1006,0,0.0626542,"Missing"
D11-1066,W02-1010,0,0.0866442,"Missing"
D11-1066,I05-1034,0,0.0321659,"Missing"
D11-1066,N06-1037,0,0.0640236,"Missing"
D11-1066,P06-1136,0,\N,Missing
D11-1066,J80-1003,0,\N,Missing
D11-1096,W05-0601,1,0.814389,". PTK is more general than the STK as if we only consider the contribution of shared subsequences containing all children of nodes, we implement the STK kernel. The computational complexity of PTK is O(pρ2 |NT1 ||NT2 |) (Moschitti, 2006a), where p is the largest subsequence of children that we want consider and ρ is the maximal outdegree observed in the two trees. However the average running time again tends to be linear for natural language syntactic trees (Moschitti, 2006a). 2.3 Lexical Semantic Kernel Given two text fragments d1 and d2 ∈ D (the text fragment set), a general lexical kernel (Basili et al., 2005) defines their similarity as: K(d1 , d2 ) = X w1 ∈d1 ,w2 ∈d2 (ω1 ω2 ) × σ(w1 , w2 ) (1) where ω1 and ω2 are the weights of the words (features) w1 and w2 in the documents d1 and d2 , respectively, and σ is a term similarity function, e.g. (Pedersen et al., 2004b; Sahlgren, 2006; Corley and Mihalcea, 2005; Mihalcea et al., 2005). Technically, any σ can be used, provided that the resulting Gram matrix, G = K(d1 , d2 ) ∀d1 , d2 ∈ D is positive semi-definite (Shawe-Taylor and Cristianini, 2004) (D is typically the training text set). We determine the term similarity function through distributional"
D11-1096,H05-1091,0,0.0608426,", 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lower than 1, depending on the semantic similarity of the corresponding leave"
D11-1096,A00-2018,0,0.0533177,"Missing"
D11-1096,P06-2010,0,0.0389822,"Missing"
D11-1096,P02-1034,0,0.796148,"rsen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) i"
D11-1096,W05-1203,0,0.0465447,"consider and ρ is the maximal outdegree observed in the two trees. However the average running time again tends to be linear for natural language syntactic trees (Moschitti, 2006a). 2.3 Lexical Semantic Kernel Given two text fragments d1 and d2 ∈ D (the text fragment set), a general lexical kernel (Basili et al., 2005) defines their similarity as: K(d1 , d2 ) = X w1 ∈d1 ,w2 ∈d2 (ω1 ω2 ) × σ(w1 , w2 ) (1) where ω1 and ω2 are the weights of the words (features) w1 and w2 in the documents d1 and d2 , respectively, and σ is a term similarity function, e.g. (Pedersen et al., 2004b; Sahlgren, 2006; Corley and Mihalcea, 2005; Mihalcea et al., 2005). Technically, any σ can be used, provided that the resulting Gram matrix, G = K(d1 , d2 ) ∀d1 , d2 ∈ D is positive semi-definite (Shawe-Taylor and Cristianini, 2004) (D is typically the training text set). We determine the term similarity function through distributional analysis (Pado and Lapata, 2007), according to the idea that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis, (Harris, 1964)). The contexts are words appearing in a n-window with target words: such a space models a generic notion of se"
D11-1096,H92-1046,0,0.190264,"of training data is usually scarce. This requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in (Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Pedersen et al., 2004a; Bloehdorn and Moschitti, 2007b; Davis et al., 2007) or in semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature enginee"
D11-1096,P04-1054,0,0.313426,"Missing"
D11-1096,W04-3233,0,0.0623579,"Missing"
D11-1096,D09-1002,0,0.0122799,"Missing"
D11-1096,P06-1117,1,0.855736,"Missing"
D11-1096,P05-1050,0,0.0125296,"red in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies"
D11-1096,O97-1002,0,0.0807278,"-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels. 1 Introduction A central topic in Natural Language Processing is the design of lexical and syntactic features suitable for the target application. The selection of effective patterns composed of syntactic dependencies and lexical constraints is typically a complex task. Additionally, the availability of training data is usually scarce. This requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in (Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Pedersen et al., 2004a; Bloehdorn and Moschitti, 2007b; Davis et al., 2007) or in semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focu"
D11-1096,C10-1059,1,0.788535,"Missing"
D11-1096,W10-2910,1,0.715725,"Missing"
D11-1096,W08-2123,0,0.0548567,"Missing"
D11-1096,C08-1050,0,0.092654,"Missing"
D11-1096,P03-1004,0,0.0162247,"itsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragment"
D11-1096,P05-1024,0,0.0278,"; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lo"
D11-1096,C02-1150,0,0.133231,"ally, to build the matrix M, POS tagging is first applied to build rows with pairs hlemma, ::POSi, or lemma::POS in brief. The contexts of such items are the columns of M and are short windows of size [−3, +3], centered on the items. This allows for better capturing syntactic properties of words. The most frequent 20,000 items are selected along with their 20k contexts. The entries of M are the point-wise mutual information between them. The SVD reduction is then applied to M, with a dimensionality cut of l = 250. The second approach uses the similarity based on word list (WL) as provided in (Li and Roth, 2002). Models: SVM-LightTK is applied to the different tree representations discussed in Section 4. Since PTK and SPTK are typically used in our experiments, to have a more compact acronym for each model, we associate the latter with the name of the structure, i.e. this indicates that PTK is applied to it. Then the presence of the subscript W L and LSA indicates that SPTK is applied along with the corresponding similarity, e.g. LCTW L is the SPTK kernel applied to LCT structure, using WL similarity. We experiment with multi-classification, which we model through one-vs-all scheme by selecting the c"
D11-1096,N10-1146,1,0.822543,"rees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lower than 1, depending on the semantic similarity of the corresponding leaves in the syntactic fragments. Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question ClassificaProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1034–1046, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tion (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al., 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a). Additionally, to our knowledge, there is no previous study that clearly describes how dependency structures should be converted in trees to be fully and effectively exploitable by convolution kernels. Indeed, although the work in ("
D11-1096,H05-1052,0,0.178003,"n semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005;"
D11-1096,P07-1098,1,0.895631,"Missing"
D11-1096,J08-2003,1,0.903654,"Missing"
D11-1096,P04-1043,1,0.905891,"Missing"
D11-1096,E06-1015,1,0.523335,"rence on Empirical Methods in Natural Language Processing, pages 1034–1046, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tion (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al., 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a). Additionally, to our knowledge, there is no previous study that clearly describes how dependency structures should be converted in trees to be fully and effectively exploitable by convolution kernels. Indeed, although the work in (Culotta and Sorensen, 2004) defines a dependency tree also using node similarity, it is not a convolution kernel: this results in a much poorer feature space. In this paper, we propose a study of convolution kernels for dependency structures aiming at jointly modeling syntactic and lexical semantic similarity. More precisely, we define several dependency trees ex"
D11-1096,J07-2002,0,0.0473797,"eir similarity as: K(d1 , d2 ) = X w1 ∈d1 ,w2 ∈d2 (ω1 ω2 ) × σ(w1 , w2 ) (1) where ω1 and ω2 are the weights of the words (features) w1 and w2 in the documents d1 and d2 , respectively, and σ is a term similarity function, e.g. (Pedersen et al., 2004b; Sahlgren, 2006; Corley and Mihalcea, 2005; Mihalcea et al., 2005). Technically, any σ can be used, provided that the resulting Gram matrix, G = K(d1 , d2 ) ∀d1 , d2 ∈ D is positive semi-definite (Shawe-Taylor and Cristianini, 2004) (D is typically the training text set). We determine the term similarity function through distributional analysis (Pado and Lapata, 2007), according to the idea that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis, (Harris, 1964)). The contexts are words appearing in a n-window with target words: such a space models a generic notion of semantic relatedness, i.e. two words close in the space are likely to be either in paradigmatic or syntagmatic relation as in (Sahlgren, 2006). The original word-by-word context matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U , S, and V so that S"
D11-1096,N04-3012,0,0.173068,"ification confirms the benefit of semantic smoothing for dependency kernels. 1 Introduction A central topic in Natural Language Processing is the design of lexical and syntactic features suitable for the target application. The selection of effective patterns composed of syntactic dependencies and lexical constraints is typically a complex task. Additionally, the availability of training data is usually scarce. This requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in (Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Pedersen et al., 2004a; Bloehdorn and Moschitti, 2007b; Davis et al., 2007) or in semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two st"
D11-1096,J98-1004,0,0.339096,"tic role classification confirms the benefit of semantic smoothing for dependency kernels. 1 Introduction A central topic in Natural Language Processing is the design of lexical and syntactic features suitable for the target application. The selection of effective patterns composed of syntactic dependencies and lexical constraints is typically a complex task. Additionally, the availability of training data is usually scarce. This requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in (Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Pedersen et al., 2004a; Bloehdorn and Moschitti, 2007b; Davis et al., 2007) or in semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechani"
D11-1096,W03-1012,0,0.0643693,"h similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surfac"
D11-1096,W06-2902,0,0.036538,"; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lower than 1, depending on th"
D11-1096,W04-3222,0,0.0364774,", e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they ha"
D11-1096,C08-1121,1,0.389195,"orm lower than models based on manually engineered features for SRL tasks, e.g., (Moschitti, 2004; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti, 2006b; Che et al., 2006; Moschitti et al., 2008). Thus for the first time in an SRL task, a general tree kernel reaches the same accuracy of heavy manual feature design. This also suggests an improvement when used in combinations with manual feature vectors. • Relation Extraction and Pronominal Coreference, whose state-of-the-art for some tasks is achieved with the simple STK-CT (see (Zhang et al., 2006) and (Yang et al., 2006; Versley et al., 2008), respectively). • In word sense disambiguation tasks, SPTK can generalize context according to syntactic and semantic constraints (selectional restrictions) making very effective distributional semantic approaches. • In Opinion Mining SPTK will allow to match sentiment words within their corresponding syntactic counterparts and improve the stateof-the-art (Johansson and Moschitti, 2010b; Johansson and Moschitti, 2010a). • Experiments on Recognizing Textual Entailment (RTE) tasks, the use of SSTK (instead of STK-CT) improved the state-of-the-art (Mehdad et al., 2010). SPTK may provide further"
D11-1096,P94-1019,0,0.0214503,"usually scarce. This requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in (Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Pedersen et al., 2004a; Bloehdorn and Moschitti, 2007b; Davis et al., 2007) or in semisupervised settings, e.g. (Chapelle et al., 2006). A semantic similarity can be defined at structural level over a graph, e.g. (Freeman, 1977; Bunke and Shearer, 1998; Brandes, 2001; Zhao et al., 2009), as well as combining structural and lexical similarity 1034 over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; F¨urstenau and Lapata, 2009; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or"
D11-1096,P06-1006,0,0.0103659,"els (TK) alone perform lower than models based on manually engineered features for SRL tasks, e.g., (Moschitti, 2004; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti, 2006b; Che et al., 2006; Moschitti et al., 2008). Thus for the first time in an SRL task, a general tree kernel reaches the same accuracy of heavy manual feature design. This also suggests an improvement when used in combinations with manual feature vectors. • Relation Extraction and Pronominal Coreference, whose state-of-the-art for some tasks is achieved with the simple STK-CT (see (Zhang et al., 2006) and (Yang et al., 2006; Versley et al., 2008), respectively). • In word sense disambiguation tasks, SPTK can generalize context according to syntactic and semantic constraints (selectional restrictions) making very effective distributional semantic approaches. • In Opinion Mining SPTK will allow to match sentiment words within their corresponding syntactic counterparts and improve the stateof-the-art (Johansson and Moschitti, 2010b; Johansson and Moschitti, 2010a). • Experiments on Recognizing Textual Entailment (RTE) tasks, the use of SSTK (instead of STK-CT) improved the state-of-the-art (Mehdad et al., 2010). SP"
D11-1096,C00-2137,0,0.156371,"Missing"
D11-1096,W02-1010,0,0.0367616,"09; Navigli and Lapata, 2010). On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lower than 1, depending on the semantic similarity"
D11-1096,N06-1037,0,0.176688,"ious work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Cancedda et al., 2003; Daum´e III and Marcu, 2004; Toutanova et al., 2004; Shen et al., 2003; Gliozzo et al., 2005; Kudo et al., 2005; Titov and Henderson, 2006; Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms). This implies matching scores lower than 1, depending on the semantic similarity of the corresponding leaves in the syntactic fr"
D11-1096,J06-1003,0,\N,Missing
D13-1044,P02-1034,0,0.0237854,"ion modules basically assume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy. For example, the great IBM Watson system (Ferrucci et al., 2010) uses a learning to rank algorithm fed with hundreds of features. The extraction of some of the latter requires articulated rules/algorithms, which, in terms of complexity, are very similar to those constituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in det"
D13-1044,damljanovic-etal-2010-identification,0,0.0666763,"Expected Answer Type (EAT) → named entity types. EAT HUM LOCATION ENTITY DATE QUANTITY CURRENCY Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with F C tag; (iii) build a set of negative trees by labeling any other constituent node with F C; (iii) we train the F C node classifier with tree kernels. At the test time, we try to label each constituent node with F C generating a set of candidate trees. Finally, we select the tr"
D13-1044,N10-1145,0,0.712495,"IN) that were manually curated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least one correct answer sentence (ALL). The latter set represents a more noisy setting, since many answer sentences are marked erroneously as correct as they simply match a regular expression. Table 3 summarizes the data used for training and testing. Table 4 compares our kernel-based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping betwee"
D13-1044,C02-1150,0,0.317355,"constituent associated with the highest SVM score. Question classification. Our question classification model is simpler than before: we use an SVM multiclassifier with tree kernels to automatically extract the question class. To build a multi-class classifier we train a binary SVM for each of the classes and apply a one-vs-all strategy to obtain the predicted 461 Named Entity types Person Location Organization, Person, Misc Date, Time, Number Number, Percentage Money, Number class. We use constituency trees as our input representation. Our question taxonomy is derived from the UIUIC dataset (Li and Roth, 2002) which defines 6 coarse and 50 fine grain classes. In particular, our set of question categories is formed by adopting 3 coarse classes: HUM (human), LOC (location), ENTY (entities) and replacing the NUM (numeric) coarse class with 3 fine-grain classes: CURRENCY, DATE, QUANTITY2 . This set of question categories is sufficient to capture the coarse semantic answer type of the candidate answers found in TREC. Also using fewer question classes results in a more accurate multi-class classifier. Semantic tagging. Question focus word specifies the lexical answer type capturing the target information"
D13-1044,P08-2029,1,0.764855,"apply a na¨ıve majority voting scheme to select a single best answer from a set of extracted answer candidates. This step has a dramatic impact on the final performance of the answer extraction system resulting in a large drop of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e., performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the ou"
D13-1044,E09-1066,1,0.248261,"er Selection. We apply a na¨ıve majority voting scheme to select a single best answer from a set of extracted answer candidates. This step has a dramatic impact on the final performance of the answer extraction system resulting in a large drop of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e., performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on str"
D13-1044,E06-1050,0,0.0241324,"rk chunks containing candidate answer (here the correct answer John Chapman). higher probability to be correct answers following a mapping defined in Table 1. Table 1: Expected Answer Type (EAT) → named entity types. EAT HUM LOCATION ENTITY DATE QUANTITY CURRENCY Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with F C tag; (iii) build a set of negative trees by labeling any other constituent node with F C; (iii) w"
D13-1044,quarteroni-etal-2012-evaluating,0,0.044585,"ned in Table 1. Table 1: Expected Answer Type (EAT) → named entity types. EAT HUM LOCATION ENTITY DATE QUANTITY CURRENCY Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with F C tag; (iii) build a set of negative trees by labeling any other constituent node with F C; (iii) we train the F C node classifier with tree kernels. At the test time, we try to label each constituent node with F C generating a set of candidate trees."
D13-1044,P11-1138,0,0.00945032,"ing. Our structural model relies heavily on the ability of NER to identify the relevant entities in the candidate sentence that can be further linked to the focus word of the question. While our answer extraction model is working on all the NP chunks, the semantic tags from NER serve as a strong cue for the classifier that a given chunk has a high probability of containing an answer. Typical off-the-shelf NER taggers have good precision and low recall, s.t. many entities as potential answers are missed. In this respect, a high recall entity linking system, e.g., linking to wikipedia entities (Ratinov et al., 2011), is required to boost the quality of candidates considered for answer extraction. Finally, improving the accuracy of question and focus classifiers would allow for having more accurate input representations fed to the learning algorithm. Answer Extraction. Our answer extraction model acts as a high recall system, while it suffers from low precision in extracting answers for many incorrect sentences. Improving the precision without sacrificing the recall would ease the successive task of best answer selection, since having less incorrect answer candidates would result in a better final perform"
D13-1044,W13-3509,1,0.921869,"nstituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links b"
D13-1044,P13-2125,1,0.559805,"nstituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links b"
D13-1044,C10-1131,0,0.625593,"rated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least one correct answer sentence (ALL). The latter set represents a more noisy setting, since many answer sentences are marked erroneously as correct as they simply match a regular expression. Table 3 summarizes the data used for training and testing. Table 4 compares our kernel-based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 4"
D13-1044,D07-1003,0,0.0910129,"et of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based approaches does not target answer extraction. One of the best models for answer sentence selection has been proposed in (Wang et al., 2007). They use the paradigm of quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. (Wang and Manning, 2010) develop a probabilistic model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of structured output learning with latent vari"
D13-1044,N13-1106,0,0.620938,"Missing"
D13-1044,P06-1051,1,0.296887,"nvolved model, i.e., performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based appro"
D14-1027,W14-3352,1,0.726645,"Missing"
D14-1027,W07-0718,0,0.384691,"Missing"
D14-1027,W11-2103,0,0.0593209,"ourse parser can be downloaded from http://alt.qcri.org/tools/ 216 In particular, let r and r0 be the references for the pairs ht1 , t2 i and ht01 , t02 i, we can redefine all the members of Eq. 1, e.g., K(t1 , t01 ) becomes K(ht1 , ri, ht01 , r0 i) = PTK(φM (t1 , r), φM (t01 , r0 )) + PTK(φM (r, t1 ), φM (r0 , t01 )), In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (τ ), which was official at WMT12. Table 1 presents the τ scores for all metric variants introduced in this paper: for the individual language pairs"
D14-1027,W05-0904,0,0.114217,"ndation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav"
D14-1027,W12-3102,0,0.167189,"Missing"
D14-1027,W12-3129,0,0.108627,".org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio"
D14-1027,W10-1750,1,0.877508,"Missing"
D14-1027,P07-1098,1,0.765065,"ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are"
D14-1027,W08-0331,0,0.225781,"y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the fe"
D14-1027,W07-0738,1,0.935253,"Missing"
D14-1027,P14-1065,1,0.878614,"Missing"
D14-1027,P02-1040,0,0.0928706,"ranslations Francisco Guzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges"
D14-1027,D12-1083,1,0.785191,"relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feat"
D14-1027,W07-0707,0,0.511133,"Missing"
D14-1027,P13-1048,1,0.81161,"L "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feature space. Consideri"
D14-1027,C14-1020,1,0.886934,"Missing"
D14-1027,D14-1050,1,0.886979,"Missing"
D14-1027,W13-3509,1,0.930152,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,P13-2125,1,0.924292,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,2006.amta-papers.25,0,0.625255,"uzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some ev"
D14-1027,W11-2113,0,0.0478224,"re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically."
D14-1027,N03-1033,0,0.00879476,"o-REL .-REL &apos;&apos;-REL to think . "" to think . "" relation propagation direction DIS:ELABORATION Bag-of-words relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0"
D14-1027,D12-1097,0,0.0991997,"elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy"
D14-1027,P06-1051,1,\N,Missing
D14-1050,N09-1003,0,0.0327503,"Missing"
D14-1050,J92-4003,0,0.375881,"reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic similarity, while also combining them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the rerankin"
D14-1050,S13-2060,0,0.012357,"a baseline, we picked the best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100."
D14-1050,W10-2802,0,0.024579,"best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100. In our experiments, we will"
D14-1050,H94-1053,0,0.260291,"e list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with respect to the gold standard"
D14-1050,D11-1096,1,0.95073,"SKS) (b) SKS with Brown Clusters Figure 1: CSL structures: standard and with Brown Clusters. Another relevant line of research are the semantic kernels, i.e., kernels that use lexical similarity between features. One of the first that applyed LSA was (Cristianini et al., 2002), whereas (Bloehdorn et al., 2006; Basili et al., 2006) used WordNet. Semantic structural kernels of the type we use in this paper were first introduced in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b). The most advanced model based on tree kernels, which we also use in this paper, is the Smoothed PTK (Croce et al., 2011). We further apply a semantic kernel (SK), namely the Smoothed Partial Tree Kernel (Croce et al., 2011), which uses the lexical similarity between the tree nodes, while computing the substructure space. This is the first time that SKs are applied to reranking hypotheses. This (i) makes the global sentence structure along with concepts available to the learning algorithm, and (ii) enables computing the similarity between lexicals in syntactic patterns that are enriched by concepts. We tested our models on the Restaurant domain. Our results show that: (i) The basic CRF parser, which uses semi-Ma"
D14-1050,W06-2909,1,0.683466,"d Brown clusters, and another one using semantic similarity among the words composing the structure. The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers. 1 {$and [{cuisine:&quot;lebanese&quot;},{city:&quot;doha&quot;}, {price:&quot;low&quot;},{amenity:&quot;carry out&quot;}]} The state-of-the-art of CSL is represented by conditional models for sequence labeling such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) trained with simple morphological and lexical features. The basic CRF model was improved by means of reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic"
D14-1050,C10-5001,1,0.846723,"ng them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word se"
D14-1050,P12-4002,1,0.857346,"vative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word sequences, often re"
D14-1050,H89-1026,0,0.0214012,"hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with resp"
D14-1050,P10-1023,0,0.0382136,"Missing"
D14-1050,J07-2002,0,0.0577582,"Missing"
D14-1050,N03-1033,0,0.00778762,"the hypothesis. 3.3 Semantic structures Tree kernels allow us to compute structural similarities between two trees; thus, we engineered a special structure for the CSL task. In order to capture the structural dependencies between the semantic tags,1 we use a basic tree (see for example Figure 1a), where the words of a sentence are tagged with their semantic tags. 4 Experiments The experiments aim at investigating the role of feature vectors, PTK, SK and BCs in reranking. We first describe the experimental setting and then we move into the analysis of the results. 2 We use the Stanford tagger (Toutanova et al., 2003). For instance, if the output sequence is Other-RatingOther-Amenity the 3-gram patterns would be: S-OtherRating, Other-Rating-Other, Rating-Other-Amenity, and Other-Amenity-E. 3 1 They are associated with the following IDs: 0-Other, 1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6Hours, 7-Location, and 8-Price. 438 semi-CRF Reranker Train 6,922 7,000 Devel. 739 3,695 Test 1,521 7,605 Total 9,182 39,782 N F1 2 87.76 5 92.63 10 95.23 100 98.72 Table 2: Oracle F1 score for N -best lists. Table 1: Number of instances and pairs used to train the semi-CRF and rerankers, respectively. 4.1 1 83"
D14-1050,N04-3012,0,0.201946,"Missing"
D14-1050,H91-1020,0,0.482894,"probability to be globally correct as estimated using local classifiers or global classifiers that only use local features. Then, a reranker, typically a meta-classifier, tries to select the best hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whethe"
D14-1050,C14-1020,1,0.793271,"Missing"
D14-1050,C10-5000,0,\N,Missing
D14-1219,P13-1048,1,0.800791,"ected by coherence relations (e.g., E LABORA TION , C AUSE ). Discourse units connected by a relation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the"
D14-1219,P05-1022,0,0.016711,"tuent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and multi-sentential discourse parsing. However, their model does not conSummary Same-Unit Elaboration On the Big Board, Crawford & Co., Atlanta, (CFD) begins trading today. Figure 9: An error made by our reranker. sider long range dependencies between DT constituents, which are encoded by our kernels. Regarding the latter, our work is surely inspired by (Collins and Duffy, 2002), which uses TK for syntactic parsing reranking or in general discriminative reranking, e.g., (Collins and Koo, 2005; Charniak and Johnson, 2005; Dinarelli et al., 2011). However, such excellent studies do not regard discourse parsing, and in absolute they achieved lower improvements than our methods. 8 Conclusions and Future Work In this paper, we have presented a discriminative approach for reranking discourse trees generated by an existing discourse parser. Our reranker uses tree kernels in SVM preference ranking framework to effectively capture the long range structural dependencies between the constituents of a discourse tree. We have shown the reranking performance for sentence-level discourse parsing using the standard tree ker"
D14-1219,P02-1034,0,0.73226,"Reranking models can make the global structural information available to the system as follows: first, a base parser produces several DT hypotheses; and then a classifier exploits the entire information in each hypothesis, e.g., the complete DT with its dependencies, for selecting the best DT. Designing features capturing such global properties is however not trivial as it requires the selection of important DT fragments. This means selecting subtree patterns from an exponential feature space. An alternative approach is to implicitly generate the whole feature space using tree kernels (TKs) (Collins and Duffy, 2002; Moschitti, 2006). In this paper, we present reranking models for discourse parsing based on Support Vector Machines (SVMs) and TKs. The latter allows us to represent structured data using the substructure space thus capturing structural dependencies between DT constituents, which is essential for effective discourse parsing. Specifically, we made the following contributions. First, we extend the 2049 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049–2060, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistic"
D14-1219,J05-1003,0,0.0325215,"he label of a DT constituent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and multi-sentential discourse parsing. However, their model does not conSummary Same-Unit Elaboration On the Big Board, Crawford & Co., Atlanta, (CFD) begins trading today. Figure 9: An error made by our reranker. sider long range dependencies between DT constituents, which are encoded by our kernels. Regarding the latter, our work is surely inspired by (Collins and Duffy, 2002), which uses TK for syntactic parsing reranking or in general discriminative reranking, e.g., (Collins and Koo, 2005; Charniak and Johnson, 2005; Dinarelli et al., 2011). However, such excellent studies do not regard discourse parsing, and in absolute they achieved lower improvements than our methods. 8 Conclusions and Future Work In this paper, we have presented a discriminative approach for reranking discourse trees generated by an existing discourse parser. Our reranker uses tree kernels in SVM preference ranking framework to effectively capture the long range structural dependencies between the constituents of a discourse tree. We have shown the reranking performance for sentence-level discourse parsing"
D14-1219,J00-3005,0,0.27392,", similar short parenthesized texts are also used to elaborate as in Senate Majority Leader George Mitchell (D., Maine), where the text (D., Maine) (i.e., Democrat from state Maine) elaborates its preceding text. This confuses our reranker. We also found error examples where the reranker failed to distinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorpo"
D14-1219,W06-2909,1,0.803843,"ncy of the algorithm did not limit us to produce k-best parses for larger k, it was not a priority in this work. 5 Kernels for Reranking Discourse Trees In Section 3, we described D ISC TK, which essentially can be used for any classification task involving discourse trees. For example, given a DT, we can use D ISC TK to classify it as correct vs. incorrect. However, such classification is not completely aligned to our purpose, since our goal is to select the best (i.e., the most correct) DT from k candidate DTs; i.e., a ranking task. We adopt a preference reranking technique as described in (Moschitti et al., 2006; Dinarelli et al., 2011). 5.1 Preference Reranker Preference reranking (PR) uses a classifier C of pairs of hypotheses hhi , hj i, which decides if hi (i.e., a candidate DT in our case) is better than hj . We generate positive and negative examples to train the classifier using the following approach. The pairs hh1 , hi i constitute positive examples, where h1 has the highest f -score accuracy on the Relation metric (to be described in Section 6) with respect to the gold standard among the candidate hypotheses, and vice versa, hhi , h1 i are considered as negative examples. At test time, C cl"
D14-1219,C10-5001,1,0.934708,"ole fragment space. A TK function T1 and T2 is defined as: P overP T K(T1 , T2 ) = n1 ∈NT n2 ∈NT2 ∆(n1 , n2 ), 1 where NT1 and NT2 are the sets of the nodes of T1 and T2 , respectively, and ∆(n1 , n2 ) is equal to the number of common fragments rooted in the n1 and n2 nodes.2 The computation of ∆ function depends on the shape of fragments, conversely, a different ∆ determines the richness of the kernel space and thus different tree kernels. In the following, we briefly describe two existing and well-known tree kernels. Please see several tutorials on kernels (Moschitti, 2013; Moschitti, 2012; Moschitti, 2010) for more details.3 Syntactic Tree Kernels (S TK) produce fragments such that each of their nodes includes all or none of its children. Figure 2 shows a tree T and its three fragments (do not consider the single nodes) in the S TK space on the left and right of the ar2 To get a similarity score between 0 and 1, it is common to apply a normalization in the kernel space, T K(T1 ,T2 ) i.e. √ . 3 T K(T1 ,T1 )×T K(T2 ,T2 ) c g b e b g c e a g b c a e a b b c e e Figure 3: A tree with its P TK fragments. node. The maximum out-degree of a tree is the highest index of all the nodes of the scendants (s"
D14-1219,P12-1007,0,0.204028,"lation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the text spans from e3 to e6 (linked by C AUSE and E LAB ORATION ) compose the statement to be attribu"
D14-1219,P12-4002,1,0.901157,"onsidering the whole fragment space. A TK function T1 and T2 is defined as: P overP T K(T1 , T2 ) = n1 ∈NT n2 ∈NT2 ∆(n1 , n2 ), 1 where NT1 and NT2 are the sets of the nodes of T1 and T2 , respectively, and ∆(n1 , n2 ) is equal to the number of common fragments rooted in the n1 and n2 nodes.2 The computation of ∆ function depends on the shape of fragments, conversely, a different ∆ determines the richness of the kernel space and thus different tree kernels. In the following, we briefly describe two existing and well-known tree kernels. Please see several tutorials on kernels (Moschitti, 2013; Moschitti, 2012; Moschitti, 2010) for more details.3 Syntactic Tree Kernels (S TK) produce fragments such that each of their nodes includes all or none of its children. Figure 2 shows a tree T and its three fragments (do not consider the single nodes) in the S TK space on the left and right of the ar2 To get a similarity score between 0 and 1, it is common to apply a normalization in the kernel space, T K(T1 ,T2 ) i.e. √ . 3 T K(T1 ,T1 )×T K(T2 ,T2 ) c g b e b g c e a g b c a e a b b c e e Figure 3: A tree with its P TK fragments. node. The maximum out-degree of a tree is the highest index of all the nodes o"
D14-1219,W05-1506,0,0.0347966,"eously. More specifically, each cell in the dynamic programming tables (i.e., A, B and C) should now contain k entries (sorted by their probabilities), and for each such entry there should be a back-pointer that keeps track of the decoding path. The algorithm works in polynomial time. For n discourse units and M number of relations, the 1-best parsing algorithm has a time complexity of O(n3 M ) and a space complexity of O(n2 ), where the k-best version has a time and space complexities of O(n3 M k 2 log k) and O(n2 k), respectively. There are cleverer ways to reduce the complexity (e.g., see (Huang and Chiang, 2005) for three such ways). However, since the efficiency of the algorithm did not limit us to produce k-best parses for larger k, it was not a priority in this work. 5 Kernels for Reranking Discourse Trees In Section 3, we described D ISC TK, which essentially can be used for any classification task involving discourse trees. For example, given a DT, we can use D ISC TK to classify it as correct vs. incorrect. However, such classification is not completely aligned to our purpose, since our goal is to select the best (i.e., the most correct) DT from k candidate DTs; i.e., a ranking task. We adopt a"
D14-1219,D09-1012,1,0.758801,"des connecting two EDUs (i.e., SPANs in Figure 4), number of nodes connecting two relational nodes, number of nodes connecting a relational node and an EDU, number of nodes that connects a relational node as left child and an EDU as right child, and vice versa. Relation Features. We encode the relations in the DT as bag-of-relations (i.e., frequency count). This will allow us to assess the impact of a flat representation of the DT. Note that more important relational features would be the subtree patterns extracted from the DT. However, they are already generated by TKs in a simpler way. See (Pighin and Moschitti, 2009; Pighin and Moschitti, 2010) for a way to extract the most relevant features from a model learned in the kernel space. 6 Experiments Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art. To achieve this, we (i) compute the oracle accuracy of the kbest parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking performance using the best kernel for different number of hypotheses, and (iv) show the relative importance of features"
D14-1219,W10-2926,1,0.890972,"., SPANs in Figure 4), number of nodes connecting two relational nodes, number of nodes connecting a relational node and an EDU, number of nodes that connects a relational node as left child and an EDU as right child, and vice versa. Relation Features. We encode the relations in the DT as bag-of-relations (i.e., frequency count). This will allow us to assess the impact of a flat representation of the DT. Note that more important relational features would be the subtree patterns extracted from the DT. However, they are already generated by TKs in a simpler way. See (Pighin and Moschitti, 2009; Pighin and Moschitti, 2010) for a way to extract the most relevant features from a model learned in the kernel space. 6 Experiments Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art. To achieve this, we (i) compute the oracle accuracy of the kbest parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking performance using the best kernel for different number of hypotheses, and (iv) show the relative importance of features coming from different source"
D14-1219,D12-1083,1,0.776641,"relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the text spans from e3 to e6 (linked by C AUSE and E LAB ORATION ) compose the statement to be attributed. Such errors occur because existing systems"
D14-1219,N03-1030,0,0.0831879,"mocrat from state Maine) elaborates its preceding text. This confuses our reranker. We also found error examples where the reranker failed to distinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorporating rich linguistic features, which include lexical semantics and discourse production rules. Joty et al. (2013) achieved the best prior results by (i) jointly modelin"
D14-1219,H05-1033,0,0.00843381,"tinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorporating rich linguistic features, which include lexical semantics and discourse production rules. Joty et al. (2013) achieved the best prior results by (i) jointly modeling the structure and the label of a DT constituent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and"
D14-1219,C10-5000,0,\N,Missing
D15-1068,S15-2047,1,0.877035,"Missing"
D15-1068,S12-1059,0,0.0177823,"Missing"
D15-1068,S15-2036,1,0.541482,"Missing"
D15-1068,P15-2113,1,0.19975,"Missing"
D15-1068,P04-1035,0,0.00602109,"og sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ ∈ [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1 , v2 , · · · , vn , s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi , s) with capacity siG , and to the sink node t by adding an edge w(vi , t) with capacity siB . Finally,"
D15-1068,I11-1164,0,0.0214203,"ssification in Community Question Answering ˜ Giovanni Da San Martino, Simone Filice, Shafiq Joty, Alberto Barr´on-Cedeno, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov, Qatar Computing Research Institute, HBKU {sjoty,albarron,gmartino,sfilice, lmarquez,amoschitti,pnakov}@qf.org.qa Abstract As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat"
D15-1068,W04-2401,0,0.398636,"he CQA-QL dataset: after merging Bad and Potential into Bad. The Task 1 Train 2,600 16,541 8,069 8,472 3 http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee http://www.qatarliving.com/forum http://alt.qcri.org/semeval2015/task3/ 574 3.1 ci and cj have the same label; assigning 0 to xijS means that ci and cj do not have the same label. The same interpretation holds for the other possible classes (in this case only Different).4 Let ciG be the cost of classifying ci as Good, cijS be the cost of assigning the same labels to ci and cj , etc. Following (Roth and Yih, 2004), these costs are obtained from local classifiers by taking log probabilities, i.e., ciG = − log siG , cijS = − log sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The"
D15-1068,S15-2035,0,0.084801,"ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in"
D15-1068,W03-0402,0,0.128215,"Missing"
D15-1068,P15-2117,0,0.0999102,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,W01-0515,0,0.0287716,"he comment-pair variables are consistent: xijD = xiG ⊕ xjG , ∀i, j 1 ≤ i < j ≤ n. λ ∈ [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); Integer Linear Programming Approach Here we follow the inference with clas"
D15-1068,S15-2037,0,0.0777175,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,D07-1002,0,\N,Missing
D15-1068,N10-1145,0,\N,Missing
D15-1068,P07-1098,1,\N,Missing
D15-1068,C10-1131,0,\N,Missing
D15-1068,N13-1106,0,\N,Missing
D15-1068,S15-2038,0,\N,Missing
D15-1068,P08-1082,0,\N,Missing
D15-1068,W13-3509,1,\N,Missing
D15-1068,D13-1044,1,\N,Missing
D17-1093,W03-0402,0,0.151,"p1 is ranked higher than p2 , and negative otherwise. One approach for producing training data is to form pairs both using hp1 , p2 i and hp2 , p1 i, thus generating both positive and negative examples. However, since these are clearly redundant as formed by the same members, it is more efficient training with a reduced set of examples such that members are not swapped. Algorithm 1 describes how we generate a more compact set of positive (E+ ) and negative (E− ) training examples for a specific Q. Given a pair of examples, hp1 , p2 i and hp01 , p02 i, we used the following preference kernel (Shen and Joshi, 2003): Figure 2: CNN architecture to compute the similarity between question and answer. the NN model described in (Tymoshenko et al., 2016a) and depicted in Fig. 2. It includes two main components (i) two sentence encoders that map input documents i into fixed size m-dimensional vectors xsi , and (ii) a feed forward NN that computes the similarity between the two sentences in the input. We use a sentence model built with a convolution operation followed by a k-max pooling layer with k = 1. The sentence vectors, xsi , are concatenated together and given in input to standard NN layers, which are con"
D17-1093,S16-1172,1,0.89828,"Missing"
D17-1093,N16-1152,1,0.470172,"res derived from Q/AP text (Tymoshenko et al., 1 897 http://alt.qcri.org/semeval2016/ Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 897–902 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Figure 1: Shallow chunk-based tree representation of a question in the Q/AP pair: Q: “Who wrote white Christmas?”, AP: “White Christmas is an Irving Berlin song”. the proposed hybrid kernel consistently outperforms standard reranking models in all settings. 2 2013; Severyn and Moschitti, 2015) and further advanced in (Tymoshenko et al., 2016a,b). The following subsections provide a brief overview of these models. Answer Sentence/Comment Selection We focus on two question answering subtasks: answer sentence selection task (AST) and the comment selection task from cQA. AST consists in selecting correct answer sentences (i.e., an AP composed of only one sentence) for a question Q from a set of candidate sentences, S = {s1 , ..., sN }. In factoid question answering, Q typically asks for an entity or a fact, e.g., time location and date. S is typically a result of socalled primary search, a result of fast-recall/lowprecision search fo"
D17-1093,D15-1237,0,0.296846,"ranking fashion. Therefore, we propose a hybrid preference-pointwise kernel, which consists in (i) a standard reranking kernel based on CTKs applied to the Q/AP structural representations; and (ii) a classification kernel based on the embeddings learned by neural networks. The intuition about the hybrid models is to add CNN layer vectors, not their difference, to the preference CTK. That is, CNN layers are still used as they were used in a classification setting whereas CTKs follow the standard SVMRank approach. We tested our proposed models on the answer sentence selection benchmark, WikiQA (Yang et al., 2015), and the benchmark from cQA SemEval-2016 Task 3.A1 corpus. We show that Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs. However, the embeddings are constructed according to a classification function, which is not directly exploitable in the preference ranking algorithm of SVMs. In this work, we propose a new hybrid app"
D17-1093,C02-1150,0,0.384165,"trictions, we present only high-level details below. A shallow chunk-based representations of a text contains lemma nodes at leaf level and their partof-speech (POS) tag nodes at the preterminal level. The latter are further grouped under the chunk and sentence nodes. A constituency tree representation is an ordinary constituency parse tree. In all representations, we mark lemmas that occur in both Q and AP by prepending the REL tag to the labels of the corresponding preterminal nodes and their parents. Moreover, in the AST setting, often question and focus classification information is used (Li and Roth, 2002), thus we enrich our representation with the question class and focus information, when is available. Additionally, we mark AP chunks containing named entities that match the expected answer type of the question by prepending REL-FOCUS&lt;QC&gt; to them. Here, the &lt; QC &gt; placeholder is substituted with the actual question class. Fig. 1 illustrates a shallow chunk-based syntactic structure enriched with relational tags. CTK and CNN models 3.2 Our baselines are the standalone CTK and CNN models originally proposed in (Severyn et al., Convolutional Neural Networks A number of NN-based models have been"
D17-1093,Q16-1019,0,0.201384,"Missing"
D17-1093,P14-5010,0,0.00978888,"TRAIN. Then, we merged WikiQA DEV and TEST sets, split the resulting set into 5 subsets, and use i-th subset to test the model trained in i-th fold (i=1,..,5). Table 2 reports the performance of the models. Here, Rank corresponds to the traditional reranking model described by Eq. 2 in Sec. 4. Hybrid refers to our new reranking/classification kernels described by Eq. 3. V means that the model uses a kernel applied to the embedding feature vectors only. T specifies that the model employs strucText Preprocessing: we used the Illinois chunker (Punyakanok and Roth, 2001) and the Stanford CoreNLP (Manning et al., 2014) toolkit, v3.6.0. When experimenting with SemEval-2016, we perform preprocessing as in (Tymoshenko et al., 2016a), e.g., we truncate all the comments to 2000 symbols and sentences to 70 words. CTKs: we trained our models with SVM-LightTK4 using the partial tree kernel (PTK) and the subset tree kernel (STK). We use PTK for WikiQA and STK for SemEval as suggested in our previous work (Tymoshenko et al., 2016a) with default 2 http://alt.qcri.org/semeval2016/ task3/index.php?id=description-of-tasks 3 http://www.qatarliving.com/forum 4 http://disi.unitn.it/moschitti/ Tree-Kernel.htm 900 Rank:T Clas"
D17-1093,S16-1083,1,0.886494,"Missing"
D17-1093,S15-2036,1,0.879518,"Missing"
D17-1093,W13-3509,1,0.934127,"Missing"
D18-1131,S16-1081,0,0.0416144,"Missing"
D18-1131,S16-1138,1,0.88442,"Missing"
D18-1131,E17-2115,1,0.903967,"Missing"
D18-1131,S16-1172,1,0.90167,"Missing"
D18-1131,S17-2053,1,0.887,"Missing"
D18-1131,P16-2075,1,0.90714,"Missing"
D18-1131,S16-1137,1,0.908531,"Missing"
D18-1131,D18-1452,1,0.847801,"Missing"
D18-1131,D16-1165,1,0.920046,"Missing"
D18-1131,K17-1024,1,0.866923,"and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and (iii) domain adaptation component, as shown in Figure 1. The encoder E maps a sequence of word"
D18-1131,S16-1083,1,0.91625,"Missing"
D18-1131,C18-1181,0,0.0587596,"Missing"
D18-1131,N16-1153,1,0.91865,"Missing"
D18-1131,D14-1162,0,0.0812127,"0 Adaptation — — Classification Wasserstein AskUbuntu source and the target domains. SuperUser Experiments and Evaluation Experimental Setup Baselines We compare our ADA model to the following baselines: (a) direct transfer, which directly applies models learned from the source to the target domain without any adaptation; and (b) the standard unsupervised BM25 (Robertson and Zaragoza, 2009) scoring provided in search engines such as Apache Lucene (McCandless et al., 2010). Models We use a bi-LSTM (Hochreiter and Schmidhuber, 1997) encoder that operates on 300dimensional GloVe word embeddings (Pennington et al., 2014), which we train on the combined data from all domains. We keep word embeddings fixed in our experiments. For the adversarial component, we use a multi-layer perceptron. Evaluation Metrics As our datasets may contain some duplicate question pairs, which were not discovered and thus not annotated, we end up having false negatives. Metrics such as MAP and MRR are not suitable in this situation. Instead, we use AUC (area under the curve) to evaluate how well the model ranks positive pairs vs. negative ones. AUC quantifies how well the true positive rate (tpr) grows at various false positive rates"
D18-1131,P17-1001,0,0.0385569,"ss-entropy loss:   sigmoid W> (v1 v2 ) + b • we study when transfer learning performs well and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and"
D18-1131,D15-1166,0,0.0536517,"Missing"
D18-1131,P15-2114,0,0.187747,"Missing"
D18-1131,P18-2046,1,0.894892,"Missing"
D18-1131,P07-1108,0,0.0320584,"Missing"
D18-1131,Q17-1036,0,0.0265715,"v2 ) + b • we study when transfer learning performs well and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and (iii) domain adaptation component, as"
D18-1131,P11-1066,0,0.122868,"Missing"
D18-1133,N16-1108,0,0.0228294,"model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms. 1 2 Introduction Modeling a match between pieces of text is at the core of many NLP tasks. Recently, manual feature engineering methods have been shadowed by neural network approaches. These networks model the interaction of two pieces of text, or word-toword interactions across sentences, using sophisticated attention mechanisms (Wang et al., 2016a; Santos et al., 2016) and compare-aggregate frameworks (He and Lin, 2016; Wang et al., 2017). Architectural complexity is tied to longer training times 1 . Meaningful features may take long time to emerge by only leveraging word representations and the training data of the task at hand. This is especially problematic with little data, as it often happens in question answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This work was partially carried out when the author was at the University of Trent"
D18-1133,P14-1062,0,0.0510604,"2018. 2018 Association for Computational Linguistics intermediate results are then aggregated into a fixed size vector to quantify the final match. In this work, we take some elements of the traditional QA research, i.e., semantic features, and use them to model relationships between sentence pairs, in the context of a neural network, which is less complex than attentive and compareaggregate counterparts. swer selection system places the highest number of correct answers at the top of a candidate answer list. In this paper, we use convolutional neural networks, referred to as CNNs (Kim, 2014; Kalchbrenner et al., 2014), to (i) classify a question into a category, (ii) identify the focus word in a question, and (iii) build a question and answer representations for QA. 3 4.1 Question Analysis Question Analysis is an important part of a QA system (Lally et al., 2012) and can give us syntactic and semantic clues that greatly help in scoring answer passages, and in identifying the final answer. Leveraging a relatively small number of annotated examples, we can automatically extract question properties that may be exploited by a QA model to increase the accuracy of its answers. We use classifiers to extract the q"
D18-1133,D14-1181,0,0.00407495,"ovember 4, 2018. 2018 Association for Computational Linguistics intermediate results are then aggregated into a fixed size vector to quantify the final match. In this work, we take some elements of the traditional QA research, i.e., semantic features, and use them to model relationships between sentence pairs, in the context of a neural network, which is less complex than attentive and compareaggregate counterparts. swer selection system places the highest number of correct answers at the top of a candidate answer list. In this paper, we use convolutional neural networks, referred to as CNNs (Kim, 2014; Kalchbrenner et al., 2014), to (i) classify a question into a category, (ii) identify the focus word in a question, and (iii) build a question and answer representations for QA. 3 4.1 Question Analysis Question Analysis is an important part of a QA system (Lally et al., 2012) and can give us syntactic and semantic clues that greatly help in scoring answer passages, and in identifying the final answer. Leveraging a relatively small number of annotated examples, we can automatically extract question properties that may be exploited by a QA model to increase the accuracy of its answers. We use"
D18-1133,D16-1244,0,0.0901088,"Missing"
D18-1133,W13-3509,1,0.836212,"on answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This work was partially carried out when the author was at the University of Trento 1 http://dawn.cs.stanford.edu/ benchmark/ † Related Work Traditional work on QA makes heavily use of syntactic and semantic features (Hickl et al., 2007; Ferrucci et al., 2010). A different direction consists in using structural kernels on text encoded as trees (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Recently, deep learning methods have been very successful in NLP tasks. Words and sentences are mapped into low dimensional representations using convolutional (Krizhevsky et al., 2012) and recurrent networks (Schuster and Paliwal, 1997), and then adoperated for classification. Complex networks for such a task include attentive networks and compare-aggregate networks. Attentive Networks (Bahdanau et al., 2015; Parikh et al., 2016; Yin et al., 2016) build a sentence representation by also considering the other sentence, weighting th"
D18-1133,D17-1122,0,0.026194,"Missing"
D18-1133,N16-1152,1,0.834662,"Missing"
D18-1133,P16-1122,0,0.0184715,"is greatly boosts the accuracy of our reference network, while keeping the model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms. 1 2 Introduction Modeling a match between pieces of text is at the core of many NLP tasks. Recently, manual feature engineering methods have been shadowed by neural network approaches. These networks model the interaction of two pieces of text, or word-toword interactions across sentences, using sophisticated attention mechanisms (Wang et al., 2016a; Santos et al., 2016) and compare-aggregate frameworks (He and Lin, 2016; Wang et al., 2017). Architectural complexity is tied to longer training times 1 . Meaningful features may take long time to emerge by only leveraging word representations and the training data of the task at hand. This is especially problematic with little data, as it often happens in question answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This wor"
D18-1133,D07-1003,0,0.062907,"roaches. These networks model the interaction of two pieces of text, or word-toword interactions across sentences, using sophisticated attention mechanisms (Wang et al., 2016a; Santos et al., 2016) and compare-aggregate frameworks (He and Lin, 2016; Wang et al., 2017). Architectural complexity is tied to longer training times 1 . Meaningful features may take long time to emerge by only leveraging word representations and the training data of the task at hand. This is especially problematic with little data, as it often happens in question answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This work was partially carried out when the author was at the University of Trento 1 http://dawn.cs.stanford.edu/ benchmark/ † Related Work Traditional work on QA makes heavily use of syntactic and semantic features (Hickl et al., 2007; Ferrucci et al., 2010). A different direction consists in using structural kernels on text encoded as trees (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Tymoshenko et al., 2014; Tymoshenko and Moschitti,"
D18-1133,C16-1127,0,0.0161828,"is greatly boosts the accuracy of our reference network, while keeping the model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms. 1 2 Introduction Modeling a match between pieces of text is at the core of many NLP tasks. Recently, manual feature engineering methods have been shadowed by neural network approaches. These networks model the interaction of two pieces of text, or word-toword interactions across sentences, using sophisticated attention mechanisms (Wang et al., 2016a; Santos et al., 2016) and compare-aggregate frameworks (He and Lin, 2016; Wang et al., 2017). Architectural complexity is tied to longer training times 1 . Meaningful features may take long time to emerge by only leveraging word representations and the training data of the task at hand. This is especially problematic with little data, as it often happens in question answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This wor"
D18-1133,D15-1237,0,0.0592438,"Missing"
D18-1133,P13-1171,0,0.0651249,"Missing"
D18-1133,Q16-1019,0,0.0288743,"n using structural kernels on text encoded as trees (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Recently, deep learning methods have been very successful in NLP tasks. Words and sentences are mapped into low dimensional representations using convolutional (Krizhevsky et al., 2012) and recurrent networks (Schuster and Paliwal, 1997), and then adoperated for classification. Complex networks for such a task include attentive networks and compare-aggregate networks. Attentive Networks (Bahdanau et al., 2015; Parikh et al., 2016; Yin et al., 2016) build a sentence representation by also considering the other sentence, weighting the contribution of its parts with the so-called attention mechanism. Compare-Aggregate Networks (Wang and Jiang, 2017) apply several decompositions to each sentence in a pair. The resulting vectors are compared or composed with multiple functions, and possibly some attention mechanisms. All the 1070 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1070–1076 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics intermediate"
D18-1133,E14-1070,1,0.860032,"e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015). Thus, effective word representations are crucial in neural network models to get state-of-the-art performance. ∗ Now at Google This work was partially carried out when the author was at the University of Trento 1 http://dawn.cs.stanford.edu/ benchmark/ † Related Work Traditional work on QA makes heavily use of syntactic and semantic features (Hickl et al., 2007; Ferrucci et al., 2010). A different direction consists in using structural kernels on text encoded as trees (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Recently, deep learning methods have been very successful in NLP tasks. Words and sentences are mapped into low dimensional representations using convolutional (Krizhevsky et al., 2012) and recurrent networks (Schuster and Paliwal, 1997), and then adoperated for classification. Complex networks for such a task include attentive networks and compare-aggregate networks. Attentive Networks (Bahdanau et al., 2015; Parikh et al., 2016; Yin et al., 2016) build a sentence representation by also considering the other sentence, weighting the contribution of its parts"
D18-1240,P13-4021,0,0.0785041,"Missing"
D18-1240,S16-1138,1,0.895377,"Missing"
D18-1240,Q18-1018,0,0.0755968,"a logistic regression meta-classifier on them (META models). We build the meta-classifiers on the outputs of the standalone system BASE and TKs, namely PTK and SST. The “Ensemble” section of Table 4 shows that meta-system combinations mostly outperform the standalone kernels. In general, combining cross-pair and intra-pair similarities (with kernel sum or meta-classifiers) provides state-of-the-art results without using deep learning. Additionally, the outcome is deterministic, while the DNN accuracy may vary depending on the type of the hardware used or the random initialization parameters (Crane, 2018). 5.5 Comparison with the state of the art Tables 5, 6 and 7 report the performance of the most recent state-of-the-art systems on WikiQA, TREC13 and SemEval in comparison with our best results. We discuss them with respect to the different datasets. WikiQA. As already mentioned earlier, WikiQA contains many questions without correct answer (see Tab. 3). When evaluated on the full data, even the oracle system will achieve at most 38.38 points of MAP. Moreover, as originally observed in (Wang et al., 2007), the questions that do not have either correct answers or incorrect answers are not usefu"
D18-1240,N10-1145,0,0.37542,"ortantly, our approach can outperform much more complex algorithms based on neural networks. 1 Introduction Answer sentence selection (AS) is an important subtask of open-domain Question Answering (QA). Its input are a question Q and a set of candidate answer passages A = {A1 , A2 , ..., AN }, which may, for example, be the output of a search engine. The objective consists in selecting Ai , i ∈ {1, ..., N } that contain correct answers. Pre-deep learning renaissance approaches to AS typically addressed the task by modeling Q-to-A (intra-pair) similarities (Yih et al., 2013; Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). Q-to-A similarity and alignment are indeed crucial, but, in practice, it is very difficult to automatically extract meaningful relations between Q and A. For example, consider two positive Q/A pairs in Table 1. If we want to learn a model based only on the intra-pair Qto-A matches, simple lexical matching (marked with italics) will not be enough. One would need to conduct more complex processing and identify ∗ Professor at the University of Trento. Alessandro Moschitti∗ Amazon Manhattan Beach, CA 90266, USA amosch@amazon.com that movie and film are synonyms, and that"
D18-1240,C02-1150,0,0.282619,"dings trained on two different corpora, which result in two features, and GloVe trained on one corpus. 2 We have opted to use summation in this case to follow the earlier work. 3 WikiQA and TREC13 are the factoid AS datasets, as their questions ask for a specific fact, e.g. date or a name. 4 For example, the PERson named entity type matches the HUMan EAT. More specifically, we employ the following NER-to-EAT matching rules: PERson, ORGanization → HUMan; LOCation → LOCation; DATE, TIME, MONEY, PERCENTAGE, DURATION, NUMBER, SET → NUM; ORGanization, PERson, MISCellanious → ENTiTY. We employ the (Li and Roth, 2002) coarse-grained EAT taxonomy and Stanford CoreNLP (Manning et al., 2014) entity types. Figure 2: Shallow syntactic representation of A1 from the running example in Table 1 (i) cosine similarity applied to the BoW representations of T1 and T2 in terms of word lemmas, bi-, three-, four-grams (computed twice with and without stopwords); POS-tags; dependency triplets; (ii) longest common string subsequence measure w. and w/out stopwords; (iii) Jaccard similarity metric applied to one-, two-, four, three-grams w. and w/out stopwords; (iv) word n-gram containment measure on uni- and bi-grams w. and"
D18-1240,P14-5010,0,0.00986221,"and GloVe trained on one corpus. 2 We have opted to use summation in this case to follow the earlier work. 3 WikiQA and TREC13 are the factoid AS datasets, as their questions ask for a specific fact, e.g. date or a name. 4 For example, the PERson named entity type matches the HUMan EAT. More specifically, we employ the following NER-to-EAT matching rules: PERson, ORGanization → HUMan; LOCation → LOCation; DATE, TIME, MONEY, PERCENTAGE, DURATION, NUMBER, SET → NUM; ORGanization, PERson, MISCellanious → ENTiTY. We employ the (Li and Roth, 2002) coarse-grained EAT taxonomy and Stanford CoreNLP (Manning et al., 2014) entity types. Figure 2: Shallow syntactic representation of A1 from the running example in Table 1 (i) cosine similarity applied to the BoW representations of T1 and T2 in terms of word lemmas, bi-, three-, four-grams (computed twice with and without stopwords); POS-tags; dependency triplets; (ii) longest common string subsequence measure w. and w/out stopwords; (iii) Jaccard similarity metric applied to one-, two-, four, three-grams w. and w/out stopwords; (iv) word n-gram containment measure on uni- and bi-grams w. and w/out stopwords (Broder, 1997); (v) greedy string tiling (Wise, 1996) wi"
D18-1240,S15-2036,1,0.899176,"Missing"
D18-1240,D14-1162,0,0.088478,"), in total, in the intra-pair setting. The respective cross-pair kernels are a composite kernel summing 23 products of cosine kernels applied to 23 different (Q1 , Q2 ) and (A1 , A2 ) bag-of-ngram representations. 3.3.2 Similarity features , Embedding-based similarities (E). We represent an input text as an average of embeddings of its lemmas from pre-trained word embedding models. Then, the embedding feaE ture fmodel (T1 , T2 ) is the cosine kernel applied to the embedding-based representations of T1 and T2 . We use two pretrained embeddings: Word2Vec (Mikolov et al., 2013) and 2165 GloVe (Pennington et al., 2014), resulting in three1 embedding-based features (see Sec. 5 for more technical details). 3.3.3 Tree-kernel based similarities Following the framework defined in (Severyn et al., 2013; Tymoshenko et al., 2016a), we represent T1 and T2 as syntactico-semantic structures and use TKs as semantic similarity metrics. When computing KCP with TK as similarities, in Eq.2, we employ summation instead of multiplication2 . More specifically, we represent T1 and T2 as (i) constituency trees and apply subset TK (SST); or (ii) shallow chunk-based trees, similar to the one presented in Figure 2, and apply parti"
D18-1240,D13-1044,1,0.868559,"Missing"
D18-1240,W13-3509,1,0.92361,"at if two H/T pairs hH1 , T1 i and hH2 , T2 i share the same T-to-H “rewrite rules”, they are likely to share the same label. Based on this idea, they proposed an algorithm applying TKs to (H1 , H2 ) and (T1 , T2 ) syntactic tree representations, enriched with H-toT intra-pair rewrite rule information. More concretely, such algorithm aligns the constituents of H with T and then marks them with symbols directly in the trees. This way the alignment information can be matched by tree kernels applied to cross-pair members. Then, a line of work on AS, started by Severyn and Moschitti (2012, 2013); Severyn et al. (2013), was inspired by a similar idea of incorporating “rewrite rules” directly into the tree representations of Q1 /A1 and Q2 /A2 . They represent Q and A as syntactic trees enhanced with Q-to-A relational information, and apply TKs (Moschitti, 2006) to (Q1 , Q2 ) and (A1 , A2 ). Thus they model cross-pair similarity, and learn important patterns occurring in Q and A separately. As shown in (Tymoshenko et al., 2016a), this approach is competitive with convolutional neural networks (CNNs). 2163 In our approach, instead of using only one TK, we employ a number of different word-based kernels, most o"
D18-1240,D17-1122,0,0.071515,"Missing"
D18-1240,N16-1152,1,0.705834,"Q2 contain the same pattern who ... in the movie ..., and their respective answers contain film ... starring .... If we know that P1 = (Q1 , A1 ) is a positive AS example and want to classify P2 = (Q2 , A2 ), then high Q2 to-Q1 and A2 -to-A1 cross-pair similarities can suggest that P1 and P2 are likely to have the same label. This idea, for example, was exploited by Severyn and Moschitti (2012), whose system measures syntactic-semantic similarities directly between structural syntactic tree representations of Q1 /Q2 and A1 /A2 . This model still exhibited state-of-the-art performance in 2016 (Tymoshenko et al., 2016a). Deep neural networks (DNNs) also naturally use such cross-pair similarity when modeling two input texts, and then further combine it with intrapair similarity, for example, by means of attention mechanisms (Shen et al., 2017), compareaggregate architectures (Bian et al., 2017; Wang and Jiang, 2017), or fully-connected layers (Severyn and Moschitti, 2015, 2016; Rao et al., 2016). In this work, we observe that: (i) the high accuracy of the kernel model by Severyn and Moschitti (2012) was due not only to the use of syntactic structures, but also to the use of cross-pair similarities; and (ii)"
D18-1240,D17-1093,1,0.789897,"SUMBASE;SST . Simple metaclassifiers, summing the output of the BASE and 2167 PTK or SST systems, respectively. 5.3 Toolkits We trained the models using scikit-learn8 by Pedregosa et al. (2012) using the SVC version of SVM with precomputed KIP and KCP kernel matrices and default parameters. We trained the ensemble model using the scikit LogisticRegression classifier implementation with the default parameters. We used spaCy library9 and scikit to obtain bag-of-n-gram representations for the B similarity features, and to compute B- and E- base gram matrices. We used the RelTextRank framework10 (Tymoshenko et al., 2017b) to generate the structural representations for the TK similarity features and to extract the strong baseline feature vectors from Sec. 4. We used KeLP (Filice et al.) to compute the TK gram matrices. Regarding the Embedding-based similarities (E), we obtain three similarity features by using three word embedding models to generate the representations of the input texts, T1 and T2 , namely GloVe vectors trained on common crawl data11 , Word2Vec vectors pre-trained on Google News12 , and another Word2Vec vectors model13 pre-trained on Aquaint14 plus Wikipedia. 5.4 Results and discussion Table"
D18-1240,P17-4014,1,0.811742,"SUMBASE;SST . Simple metaclassifiers, summing the output of the BASE and 2167 PTK or SST systems, respectively. 5.3 Toolkits We trained the models using scikit-learn8 by Pedregosa et al. (2012) using the SVC version of SVM with precomputed KIP and KCP kernel matrices and default parameters. We trained the ensemble model using the scikit LogisticRegression classifier implementation with the default parameters. We used spaCy library9 and scikit to obtain bag-of-n-gram representations for the B similarity features, and to compute B- and E- base gram matrices. We used the RelTextRank framework10 (Tymoshenko et al., 2017b) to generate the structural representations for the TK similarity features and to extract the strong baseline feature vectors from Sec. 4. We used KeLP (Filice et al.) to compute the TK gram matrices. Regarding the Embedding-based similarities (E), we obtain three similarity features by using three word embedding models to generate the representations of the input texts, T1 and T2 , namely GloVe vectors trained on common crawl data11 , Word2Vec vectors pre-trained on Google News12 , and another Word2Vec vectors model13 pre-trained on Aquaint14 plus Wikipedia. 5.4 Results and discussion Table"
D18-1240,P16-1122,0,0.0185865,"d by means of the backpropagation algorithm on the training Q/A pairs. Thus, obviously, classifying a new Q/A pair is partially equivalent to performing the implicit cross-pair Qto-Q and A-to-A comparison. Additionally, the DNN approaches model the Q-to-A relatedness explicitly in a variety of ways, e.g., by: (i) using a Q-to-A transformation matrix and simple Q-to-A similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) relying on RNN and LSTM architectures (Wang and Nyberg, 2015; Shen et al., 2017), (iii) employing attention components (Yin et al., 2016; Shen et al., 2017; Wang et al., 2016a), (iv) decomposing input into similarity and dissimilarity matches (Wang et al., 2016b) or (v) using the compare-aggregate method (Wang and Jiang, 2017; Bian et al., 2017). We believe that the ability of DNNs to implicitly capture cross-pair relational matching, i.e., the capacity of learning from (Q1 , Q2 ) and (A1 , A2 ), is a very important factor to their high performance. This is of course paired with their ability to learn non-linear patterns and capture Q-toA relatedness by means of attention mechanisms. It should be noted that the latter are typically hard-coded in kernel models as l"
D18-1240,P15-2116,0,0.108587,"Missing"
D18-1240,C10-1131,0,0.0867041,"ed as follows. We describe the kernels incorporating intra- and crosspair matches in Sec. 3.2, list the simple crossand intra-pair features in Sec. 3.3, describe strong hand-crafted baseline features in Sec. 4, and report the experimental results in Sec. 5. 2 Related work Early approaches to AS typically focused on modeling intra-pair Q-to-A alignment similarities. For example, Yih et al. (2013) proposed a latent alignment model that employed lexical-semantical Q-to-A alignments, Wang et al. (2007) modeled syntactic alignments with probabilistic quasisynchronous grammar, and Heilman and Smith (2010); Yao et al. (2013); Wang and Manning (2010) employed Tree Edit Distance-based Q-toA alignments. Originally, the idea of cross-pair similarity was proposed by Zanzotto and Moschitti (2006) and applied to the recognizing textual entailment task, which consists in detecting whether a text T entails a hypothesis H. They assumed that if two H/T pairs hH1 , T1 i and hH2 , T2 i share the same T-to-H “rewrite rules”, they are likely to share the same label. Based on this idea, they proposed an algorithm applying TKs to (H1 , H2 ) and (T1 , T2 ) syntactic tree representations, enriched with H-toT intr"
D18-1240,D07-1003,0,0.34336,"selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks. 1 Introduction Answer sentence selection (AS) is an important subtask of open-domain Question Answering (QA). Its input are a question Q and a set of candidate answer passages A = {A1 , A2 , ..., AN }, which may, for example, be the output of a search engine. The objective consists in selecting Ai , i ∈ {1, ..., N } that contain correct answers. Pre-deep learning renaissance approaches to AS typically addressed the task by modeling Q-to-A (intra-pair) similarities (Yih et al., 2013; Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). Q-to-A similarity and alignment are indeed crucial, but, in practice, it is very difficult to automatically extract meaningful relations between Q and A. For example, consider two positive Q/A pairs in Table 1. If we want to learn a model based only on the intra-pair Qto-A matches, simple lexical matching (marked with italics) will not be enough. One would need to conduct more complex processing and identify ∗ Professor at the University of Trento. Alessandro Moschitti∗ Amazon Manhattan Beach, CA 90266, USA amosch@amazon.com that movie and fi"
D18-1240,C16-1127,0,0.0397276,"d by means of the backpropagation algorithm on the training Q/A pairs. Thus, obviously, classifying a new Q/A pair is partially equivalent to performing the implicit cross-pair Qto-Q and A-to-A comparison. Additionally, the DNN approaches model the Q-to-A relatedness explicitly in a variety of ways, e.g., by: (i) using a Q-to-A transformation matrix and simple Q-to-A similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) relying on RNN and LSTM architectures (Wang and Nyberg, 2015; Shen et al., 2017), (iii) employing attention components (Yin et al., 2016; Shen et al., 2017; Wang et al., 2016a), (iv) decomposing input into similarity and dissimilarity matches (Wang et al., 2016b) or (v) using the compare-aggregate method (Wang and Jiang, 2017; Bian et al., 2017). We believe that the ability of DNNs to implicitly capture cross-pair relational matching, i.e., the capacity of learning from (Q1 , Q2 ) and (A1 , A2 ), is a very important factor to their high performance. This is of course paired with their ability to learn non-linear patterns and capture Q-toA relatedness by means of attention mechanisms. It should be noted that the latter are typically hard-coded in kernel models as l"
D18-1240,D15-1237,0,0.527432,"Missing"
D18-1240,N13-1106,0,0.126402,"Missing"
D18-1240,P13-1171,0,0.535124,"n answer sentence selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks. 1 Introduction Answer sentence selection (AS) is an important subtask of open-domain Question Answering (QA). Its input are a question Q and a set of candidate answer passages A = {A1 , A2 , ..., AN }, which may, for example, be the output of a search engine. The objective consists in selecting Ai , i ∈ {1, ..., N } that contain correct answers. Pre-deep learning renaissance approaches to AS typically addressed the task by modeling Q-to-A (intra-pair) similarities (Yih et al., 2013; Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). Q-to-A similarity and alignment are indeed crucial, but, in practice, it is very difficult to automatically extract meaningful relations between Q and A. For example, consider two positive Q/A pairs in Table 1. If we want to learn a model based only on the intra-pair Qto-A matches, simple lexical matching (marked with italics) will not be enough. One would need to conduct more complex processing and identify ∗ Professor at the University of Trento. Alessandro Moschitti∗ Amazon Manhattan Beach, CA 90266, USA amosch@amazon.co"
D18-1240,Q16-1019,0,0.0478563,"Missing"
D18-1240,P06-1051,1,0.435404,"ong hand-crafted baseline features in Sec. 4, and report the experimental results in Sec. 5. 2 Related work Early approaches to AS typically focused on modeling intra-pair Q-to-A alignment similarities. For example, Yih et al. (2013) proposed a latent alignment model that employed lexical-semantical Q-to-A alignments, Wang et al. (2007) modeled syntactic alignments with probabilistic quasisynchronous grammar, and Heilman and Smith (2010); Yao et al. (2013); Wang and Manning (2010) employed Tree Edit Distance-based Q-toA alignments. Originally, the idea of cross-pair similarity was proposed by Zanzotto and Moschitti (2006) and applied to the recognizing textual entailment task, which consists in detecting whether a text T entails a hypothesis H. They assumed that if two H/T pairs hH1 , T1 i and hH2 , T2 i share the same T-to-H “rewrite rules”, they are likely to share the same label. Based on this idea, they proposed an algorithm applying TKs to (H1 , H2 ) and (T1 , T2 ) syntactic tree representations, enriched with H-toT intra-pair rewrite rule information. More concretely, such algorithm aligns the constituents of H with T and then marks them with symbols directly in the trees. This way the alignment informat"
D18-1254,S12-1051,0,0.0597921,"ated by the major dialog managers (Apple’s SiriKit, Amazon’s Alexa, Microsoft’s Luis, Google’s Dialogflow, and Snips.ai) against a rather small set of relatively generic intents (e.g., GetWeather). This involved a considerable effort on aligning different system outputs. To our knowledge, no previous work has been dedicated to automatizing this task, mainly because the underlying problem, semantic question paraphrasing, is very challenging. However, recent initiatives for automatic question duplicate detection1 , question relatedness (Nakov et al., 2016, 2017) and semantic textual similarity (Agirre et al., 2012; Cer et al., 2017) have shown that current technology achieves good accuracy in matching short text expressing similar semantics. In this paper, we propose a model for automatically clustering questions into user intent categories, which can help the design of dialog systems. Our approach aims at overcoming the difficulty of providing a unique definition of intent from either a theoretical or practical perspective. Collaborating with stakeholders, we observe that it is very challenging to capture the intent property that can optimize dialogue/chatbot engineering work in terms of design and ef"
D18-1254,S13-1004,0,0.0366648,"Missing"
D18-1254,S16-1138,1,0.829498,"Missing"
D18-1254,W10-4305,0,0.0973474,"is the number of output clusters. This number is exactly the standard clustering purity by Zhao and Karypis (2002). Since the purity is known to favor the clustering outputs with the large number of clusters, we interchange the roles of output and gold clusters, which gives us the clustering k 1 X Recall = maxi |ˆ ci ∩ cj |, N (10) j=1 where k is the number of gold standard clusters. We then compute F1 from the above measures. The defined majority-class based clustering measure allows assigning more than one cluster to the same gold cluster. The coreference resolution metric CEAF (Luo, 2005; Cai and Strube, 2010) solves this issue by finding one-to-one alignment between the clusters in the output and in the ground truth, based on which the final score is computed. We use CEAFe , the variant with the entity-based similarity, as an alternative evaluation measure.7 Note that, although we split the data into samples, all the clustering measures we use, the majority-based, defined by equations 9 and 10, and CEAF, are computed over the whole test sets (not by averaging scores separately for each sample). We evaluate the results as well in terms of the classification scores relative to the correctness of the"
D18-1254,S17-2001,0,0.020336,"log managers (Apple’s SiriKit, Amazon’s Alexa, Microsoft’s Luis, Google’s Dialogflow, and Snips.ai) against a rather small set of relatively generic intents (e.g., GetWeather). This involved a considerable effort on aligning different system outputs. To our knowledge, no previous work has been dedicated to automatizing this task, mainly because the underlying problem, semantic question paraphrasing, is very challenging. However, recent initiatives for automatic question duplicate detection1 , question relatedness (Nakov et al., 2016, 2017) and semantic textual similarity (Agirre et al., 2012; Cer et al., 2017) have shown that current technology achieves good accuracy in matching short text expressing similar semantics. In this paper, we propose a model for automatically clustering questions into user intent categories, which can help the design of dialog systems. Our approach aims at overcoming the difficulty of providing a unique definition of intent from either a theoretical or practical perspective. Collaborating with stakeholders, we observe that it is very challenging to capture the intent property that can optimize dialogue/chatbot engineering work in terms of design and effort. The important"
D18-1254,D16-1164,0,0.0897107,"e same group of web pages that are frequently selected by users. Jeon et al. (2005) use machine translation to estimate word translation probabilities and retrieve similar questions from question archives. Li et al. (2008) try to infer the intent of unlabeled queries according to the proximity with respect to the labeled queries in a click graph. Beitzel et al. (2007) propose to automatically classify web queries from logs into a set of topics by using a combination of different techniques, either supervised or unsupervised. The extracted topics are further used 2311 for efficient web search. Deepak (2016) presents MiXKmeans, a variation of k-means algorithm, suited for clustering threads present on forums and Community Question Answering websites. However, most techniques use unsupervised clustering to group similar questions/queries, without modeling intents. In contrast, our study relies on supervised clustering to learn intent-based similarity. Finally, our work is related to a large body of research on dialog acts (Stolcke et al., 2000; Kim et al., 2010; Chen et al., 2018): our low-level intent labels (Table 1) can be seen as very finegrained dialog acts (Core and Allen, 1997; Bunt et al.,"
D18-1254,S16-1172,1,0.838879,"is supposed to produce a better w, which we test in our experiments described in Section 5. Alternatively, we employ the same structural model with another learning algorithm – latent structured perceptron (LSP) by Sun et al. (2009); Fernandes et al. (2014). 3.2 Pairwise question similarity classifier Our intent clustering algorithm relies on the pairwise similarity between questions (edge score). To provide an accurate estimation of questionquestion similarity, we build upon an extensive state-of-the-art research in semantic similarity for short text, more specifically, on our previous work (Filice et al., 2016, 2017; Barr´on-Cede˜no et al., 2016; Da San Martino et al., 2016) solutions/features shown effective in the shared tasks by Nakov et al. (2016, 2017); Agirre et al. (2012, 2013). In such work, the classifier is trained with SVMs, which learn a classification function f : Q × Q → {0, 1} on duplicate vs. non-duplicate pairs of questions belonging to the question set Q. The classifier score is used to decide if two questions in the dataset qi and qj are duplicate or not. We represent question pairs as vectors of similarity features derived between two questions. Feature Vectors are built for que"
D18-1254,S17-2053,1,0.880251,"Missing"
D18-1254,D10-1084,0,0.0783493,"combination of different techniques, either supervised or unsupervised. The extracted topics are further used 2311 for efficient web search. Deepak (2016) presents MiXKmeans, a variation of k-means algorithm, suited for clustering threads present on forums and Community Question Answering websites. However, most techniques use unsupervised clustering to group similar questions/queries, without modeling intents. In contrast, our study relies on supervised clustering to learn intent-based similarity. Finally, our work is related to a large body of research on dialog acts (Stolcke et al., 2000; Kim et al., 2010; Chen et al., 2018): our low-level intent labels (Table 1) can be seen as very finegrained dialog acts (Core and Allen, 1997; Bunt et al., 2010; Oraby et al., 2017). However, our paper’s objective is different as our goal is not to rigidly define intents and then exploit them to derive a semantic interpretation. We focus on two contributions: first, we aim at providing a tool to help implementing dialog managers such that the designer can more easily create categories from precomputed clusters. Note that having in mind hundreds of questions to create intent category from scratch is clearly an"
D18-1254,P16-2002,0,0.0306796,"2 Related Work Intent is a key concept for building dialog systems and is therefore a central research topic in the area. In particular, recent general-purpose dialog systems have to rely on extensive intent modeling to be able to correctly analyze a wide variety of user queries. This has led to a considerable amount of research on data-driven intent modeling. In particular, Xu et al. (2013) represent query’s intent as trees and employ a procedure for mapping an NL query into a tree-structured intent. The problem of this approach is that a new set of intent trees is required for new domains. Kim et al. (2016); Celikyilmaz et al. (2011) use semisupervised approaches with large amounts of unlabeled data to improve the accuracy in mapping user queries into intents. However, they still require a small amount of labeled data in order to learn a given intent. Chen et al. (2016a) train a Convolutional Deep Neural Network to jointly learn the representations of human intents and associated utterances. Chen et al. (2016b) propose feature-enriched matrix factorization to model open domain intents. This leverages knowledge from Wikipedia and Freebase to acquire information from unexplored domains according t"
D18-1254,H05-1004,0,0.00967673,"red, and kˆ is the number of output clusters. This number is exactly the standard clustering purity by Zhao and Karypis (2002). Since the purity is known to favor the clustering outputs with the large number of clusters, we interchange the roles of output and gold clusters, which gives us the clustering k 1 X Recall = maxi |ˆ ci ∩ cj |, N (10) j=1 where k is the number of gold standard clusters. We then compute F1 from the above measures. The defined majority-class based clustering measure allows assigning more than one cluster to the same gold cluster. The coreference resolution metric CEAF (Luo, 2005; Cai and Strube, 2010) solves this issue by finding one-to-one alignment between the clusters in the output and in the ground truth, based on which the final score is computed. We use CEAFe , the variant with the entity-based similarity, as an alternative evaluation measure.7 Note that, although we split the data into samples, all the clustering measures we use, the majority-based, defined by equations 9 and 10, and CEAF, are computed over the whole test sets (not by averaging scores separately for each sample). We evaluate the results as well in terms of the classification scores relative to"
D18-1254,W01-0515,0,0.080823,"is used to decide if two questions in the dataset qi and qj are duplicate or not. We represent question pairs as vectors of similarity features derived between two questions. Feature Vectors are built for questions pairs, (qi , qj ), using a set of text similarity features that capture the relations between two questions. More specifically, we compute 20 similarity features sim(qi , qj ) using word n-grams (n = [1, . . . , 4]), after stopword removal, greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. 4 Building Intent clusters In this study, we rely on two datasets, providing some important insights on question semantics. None of them, however, fully annotates intents explicitly. Below we describe our approach for converting these resources into intent corpora, relying on an automatic procedure followed by a manual post-annotation step. Our intent corpora as well as the larger raw question clusters collections are available to the research community.3 4.1 Quora corpus The original Quora task requires detecting whether two questions are semantically duplicate or not"
D18-1254,S17-2003,1,0.868615,"Missing"
D18-1254,J00-3003,0,0.193159,"Missing"
D18-1254,I13-1063,0,0.0199301,"sters). These unseen intents are clearly problematic for dialogue management and cannot be covered by traditional approaches (e.g., our unsupervised clustering baselines show much lower performance on singleton clusters). 2 Related Work Intent is a key concept for building dialog systems and is therefore a central research topic in the area. In particular, recent general-purpose dialog systems have to rely on extensive intent modeling to be able to correctly analyze a wide variety of user queries. This has led to a considerable amount of research on data-driven intent modeling. In particular, Xu et al. (2013) represent query’s intent as trees and employ a procedure for mapping an NL query into a tree-structured intent. The problem of this approach is that a new set of intent trees is required for new domains. Kim et al. (2016); Celikyilmaz et al. (2011) use semisupervised approaches with large amounts of unlabeled data to improve the accuracy in mapping user queries into intents. However, they still require a small amount of labeled data in order to learn a given intent. Chen et al. (2016a) train a Convolutional Deep Neural Network to jointly learn the representations of human intents and associat"
E06-1015,kingsbury-palmer-2002-treebank,0,\N,Missing
E06-1015,W04-3201,0,\N,Missing
E06-1015,J93-2004,0,\N,Missing
E06-1015,W03-1012,0,\N,Missing
E06-1015,P04-1043,1,\N,Missing
E06-1015,P04-1054,0,\N,Missing
E06-1015,P02-1034,0,\N,Missing
E06-1015,J02-3001,0,\N,Missing
E06-1015,P02-1031,0,\N,Missing
E06-1015,H05-1018,0,\N,Missing
E09-1024,P06-1051,1,0.85368,"Missing"
E09-1024,P02-1034,0,0.421577,"substructures called partial trees fragments (PTFs). These can be generated by the application of partial production rules of the grammar, consequently [VP [V]] and [VP [NP]] are valid PTFs. Figure 1(b) shows that the number of PTFs derived from the same tree as before is still higher (i.e. 30 PTs). 3.4 nc(n1 ) ∆(n1 , n2 ) = Y (σ + ∆(cjn1 , cjn2 )) (1) j=1 where σ ∈ {0, 1}, nc(n1 ) is the number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, nc(n1 ) = nc(n2 ). ∆(n1 , n2 ) evaluates the number of STFs common to n1 and n2 as proved in (Collins and Duffy, 2002). Moreover, a decay factor λ can be added by modifying steps (2) and (3) as follows2 : 2. ∆(n1 , n2 ) = λ, Qnc(n ) 3. ∆(n1 , n2 ) = λ j=1 1 (σ + ∆(cjn1 , cjn2 )). The computational complexity of Eq. 1 is O(|NT1 |× |NT2 |) but as shown in (Moschitti, 2006), the average running time tends to be linear, i.e. O(|NT1 |+ |NT2 |), for natural language syntactic trees. Counting Shared SubTrees The main idea of tree kernels is to compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. To evaluate the above kernels between two T1 an"
E09-1024,N01-1025,0,0.123821,"xperiments (CER) using FST and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA. Split Training based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST as described in Section 2.1. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti). For λ (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively. 4.3 STK 18.5 18.5 18.5 Monolithic Training SVM PCT PTK SK STK PTK 19.3 19.1 24.2 28.3 19.3 19.0 29.4 23.7 19.3 19.1 31.5 30.0 WOZ RR-A RR-B RR-C STK 20.0 19.0 19.0 SVM PTK 18.0 19.0 18.4 SK 16.1 19.0 16.6 STK 28.4 26.3 27.1 PCT PTK 29.8 30.0 26.2 SK 27.8 25.6 30.3 Table 5: Results of experiments, in terms of Concept Error Rate (CER), on the LUNA WOZ c"
E09-1024,W04-2403,1,0.906112,"d concepts. In the last decade two major approaches have been proposed to find this correlation: (i) generative models, whose parameters refer to the joint Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the gene"
E09-1024,W06-2909,1,0.887769,"kernel: eral than Eq. 1. Indeed, if we only consider the contribution of the longest child sequence from KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (3) node pairs that have the same children, we imple− SK(s11 , s22 ) − SK(s21 , s12 ) ment the STK kernel. This schema, consisting in summing four differ3.7 Re-ranking models using sequences ent kernels, has been already applied in (Collins and Duffy, 2002) for syntactic parsing re-ranking, The FST generates the m most likely concept anwhere the basic kernel was a tree kernel instead of notations. These are used to build annotation SK and in (Moschitti et al., 2006), where, to repairs, si , sj , which are positive instances if si j rank Semantic Role Labeling annotations, a tree has a lower concept annotation error than s , with kernel was used on a semantic tree similar to the respect to the manual annotation in the corpus. i one introduced in the next section. Thus, a trained binary classifier can decide if s j is more accurate than s . Each candidate anno3.8 Re-ranking models using trees tation si is described by a word sequence where Since the aim in concept annotation re-ranking is each word is followed by its concept annotation. to exploit innovati"
E09-1024,J08-2003,1,0.896247,"Missing"
E09-1066,P08-2003,0,0.0132962,"fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. 578 PAS PAS A1 rel A0 R-A0 rel A1 Disorder characterize fear that causes anxiety (a) (b) Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety"
E09-1066,P02-1034,0,0.508539,"es and processing errors. In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece"
E09-1066,P04-1054,0,0.0671308,"been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic infor"
E09-1066,W04-2403,1,0.895303,"(SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown"
E09-1066,P08-2029,1,0.553097,"structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. 578 PAS PAS A1 rel A0 R-A0 rel A1 Disorder characterize fear that causes anxiety (a) (b) Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense f"
E09-1066,W06-2909,1,0.880963,"may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves"
E09-1066,P07-1098,1,0.82333,"representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/an"
E09-1066,E06-1015,1,0.267383,"articular, we first show that our SRK is far more efficient and effective than SSTK. Then, we study the impact of the above kernels as well as sequence kernels based on words and Part of Speech Tags and tree kernels for the classification of question/answer text pairs. SRK vs. SSTK A comparison between SSTK and SRK suggests the following points: first, although the computational complexity of SRK is larger than the one of SSTK, we will show in the experiment section that the running time (for both training and testing) is much lower. The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). Second, although SRK uses trees with only three levels, in Eq.1, the function σ (defined to give 1 or 0 if the heads match or not) can be substituted by any kernel function. Thus, σ can recursively be an SRK (and evaluate Nested PASs (Moschitti et al., 2007)) or any other potential kernel (over the arguments). The very interesting aspect is that the efficient algorithm that we provide (Eqs 2, 3 and 4) can be accordingly modified to efficiently evaluate new kernels obtained with the σ substitutio"
E09-1066,W07-1401,0,0.0156159,"cument representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing"
E09-1066,P06-1117,1,0.806349,"Missing"
E09-1066,C02-1042,0,0.0123889,"urnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabil"
E09-1066,A00-2008,0,0.150335,"et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. However, the relatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relational learning from questions an"
E09-1066,H05-1018,0,0.0140114,". Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff"
E09-1066,kingsbury-palmer-2002-treebank,0,0.212027,"oreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. However, the relatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relational learning from questions and answers. We designed sequence kernels for words and Part of Speech Tags which capture basic lexical semantics and basic syntactic information. Then, we design a novel shallow semantic kernel which is"
E09-1066,P03-1004,0,0.0331532,"In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009."
E09-1066,P05-1024,0,0.0487225,"oise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form"
E09-1066,D07-1002,0,0.0428187,"fficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) ex"
E09-1066,W03-1012,0,0.01156,"ort Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Lingu"
E09-1066,P08-1082,0,0.054778,"ured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were propose"
E09-1066,W04-3222,0,0.0545695,"features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument St"
E09-1066,P06-1051,1,0.822344,"Missing"
E09-1066,N06-1037,0,0.0438915,"inguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and"
E09-1066,P06-1136,0,\N,Missing
E09-1066,W05-0620,0,\N,Missing
E14-1070,D13-1044,1,0.830462,"Q/AP are needed. In this respect, our previous work, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2009), has shown that tree kernels for NLP, e.g., (Moschitti, 2006), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines. Our more recent work, (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction (Severyn and Moschitti, 2013). Introduction Past work in TREC QA, e.g. (Voorhees, 2001), and more recent work (Ferrucci et al., 2010) in QA has shown that, to achieve human performance, semantic resources, e.g., Wikipedia1 , must be utilized by QA systems. This requires the design of rules or machine learning features that exploit such knowledge by also satisfying syntactic constraints, e.g., the semantic type of the answer must match the question focus words. The engineering of such rules for open domain QA is typically very costly. For instance, for automatically deriving the correctness of the answer passage in the fol"
E14-1070,W13-3509,1,0.334721,"er(Quaker Oats Co.,Stokely-Van Camp), took over(Y, Z)→own(Z,Y), and carry out logic unification and resolution. Therefore, approaches that can automatically generate patterns (i.e., features) from syntactic and semantic representations of the Q/AP are needed. In this respect, our previous work, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2009), has shown that tree kernels for NLP, e.g., (Moschitti, 2006), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines. Our more recent work, (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction (Severyn and Moschitti, 2013). Introduction Past work in TREC QA, e.g. (Voorhees, 2001), and more recent work (Ferrucci et al., 2010) in QA has shown that, to achieve human performance, semantic resources, e.g., Wikipedia1 , must be utilized by QA systems. This requires the design of rules or machine learning features that exploit such knowledge by also satisfying syntactic co"
E14-1070,P08-2029,1,0.843873,"for Passage Reranking Kateryna Tymoshenko Alessandro Moschitti Trento RISE Qatar Computing Research Instit. 38123 Povo (TN), Italy 5825 Doha, Qatar k.tymoshenko@trentorise.eu amoschitti@qf.org.qa Abstract is(Quaker Oats Co.,company), own(Stokely-Van Camp,Gatorade), took over(Quaker Oats Co.,Stokely-Van Camp), took over(Y, Z)→own(Z,Y), and carry out logic unification and resolution. Therefore, approaches that can automatically generate patterns (i.e., features) from syntactic and semantic representations of the Q/AP are needed. In this respect, our previous work, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2009), has shown that tree kernels for NLP, e.g., (Moschitti, 2006), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines. Our more recent work, (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction (Severyn and Moschitti, 2013). Introduction Past work in TREC QA, e.g. (Voorhees, 2001), and more recent work (Ferru"
E14-1070,P07-1098,1,0.827414,"in Syntactic Structures for Passage Reranking Kateryna Tymoshenko Alessandro Moschitti Trento RISE Qatar Computing Research Instit. 38123 Povo (TN), Italy 5825 Doha, Qatar k.tymoshenko@trentorise.eu amoschitti@qf.org.qa Abstract is(Quaker Oats Co.,company), own(Stokely-Van Camp,Gatorade), took over(Quaker Oats Co.,Stokely-Van Camp), took over(Y, Z)→own(Z,Y), and carry out logic unification and resolution. Therefore, approaches that can automatically generate patterns (i.e., features) from syntactic and semantic representations of the Q/AP are needed. In this respect, our previous work, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2009), has shown that tree kernels for NLP, e.g., (Moschitti, 2006), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines. Our more recent work, (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction (Severyn and Moschitti, 2013). Introduction Past work in TREC QA, e.g. (Voorhees, 20"
E14-1070,E09-1066,1,0.844506,"ymoshenko Alessandro Moschitti Trento RISE Qatar Computing Research Instit. 38123 Povo (TN), Italy 5825 Doha, Qatar k.tymoshenko@trentorise.eu amoschitti@qf.org.qa Abstract is(Quaker Oats Co.,company), own(Stokely-Van Camp,Gatorade), took over(Quaker Oats Co.,Stokely-Van Camp), took over(Y, Z)→own(Z,Y), and carry out logic unification and resolution. Therefore, approaches that can automatically generate patterns (i.e., features) from syntactic and semantic representations of the Q/AP are needed. In this respect, our previous work, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2009), has shown that tree kernels for NLP, e.g., (Moschitti, 2006), can exploit syntactic patterns for answer passage reranking significantly improving search engine baselines. Our more recent work, (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a), has shown that using automatically produced semantic labels in shallow syntactic trees, such as question category and question focus, can further improve passage reranking and answer extraction (Severyn and Moschitti, 2013). Introduction Past work in TREC QA, e.g. (Voorhees, 2001), and more recent work (Ferrucci et al., 2010)"
E17-2023,P14-1005,0,0.160178,"Missing"
E17-2023,D13-1057,0,0.375982,"tive feature selection approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being at the same time much more efficient. 1 Introduction Recent research on CR has shown effective applications of structured prediction, e.g., the latent structured perceptron (LSP) by Fernandes et al. (2014) obtained the top rank in the CoNLL-2012 Shared Task (Pradhan et al., 2012). There has been an exploration of LSP variants (Chang et al., 2011; Bj¨orkelund and Kuhn, 2014; Lassalle and Denis, 2015), and also of SGD-like methods (Chang et al., 2013; Peng et al., 2015; Kummerfeld et al., 2015). Surprisingly, no study was devoted to LSSVM by Yu and Joachims (2009), which offers theoretical guarantees on reducing the error upper-bound. The major advantage of such a theory is the possibility to stop the optimization process, carried out using the Concave-Convex Procedure (CCCP) by Yuille and Rangarajan (2003), 1 conll.cemantix.org/2012/software.html 143 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 143–149, c Valencia, Spain, April 3-7, 2017. 2017 A"
E17-2023,D15-1032,0,0.0281158,"roving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being at the same time much more efficient. 1 Introduction Recent research on CR has shown effective applications of structured prediction, e.g., the latent structured perceptron (LSP) by Fernandes et al. (2014) obtained the top rank in the CoNLL-2012 Shared Task (Pradhan et al., 2012). There has been an exploration of LSP variants (Chang et al., 2011; Bj¨orkelund and Kuhn, 2014; Lassalle and Denis, 2015), and also of SGD-like methods (Chang et al., 2013; Peng et al., 2015; Kummerfeld et al., 2015). Surprisingly, no study was devoted to LSSVM by Yu and Joachims (2009), which offers theoretical guarantees on reducing the error upper-bound. The major advantage of such a theory is the possibility to stop the optimization process, carried out using the Concave-Convex Procedure (CCCP) by Yuille and Rangarajan (2003), 1 conll.cemantix.org/2012/software.html 143 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 143–149, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Mode"
E17-2023,Q15-1029,0,0.50668,"ally very expensive. To overcome such inefficiency, Yu and Joachims (2009) proposed LSSVM performing inference on undirected (latent) graphs built on document mentions using Kruskal’s spanning algorithm. Fernandes et al. (2014) specialized the latent structured perceptron proposed by Sun et al. (2009) for solving CR tasks (LSP). This is based on (i) the Minimum Spanning Tree algorithm on the directed mention graph and (ii) the structured perceptron, updated on a per-document basis. The same approach, referred to as antecedent trees, is included in the generalized latent structure framework of Martschat and Strube (2015). The authors report that the mention-ranking approach, which uses the LSP inference and mention-based updates2 , produces slightly better results. It should be noted that the LSP inference is equivalent to the best-left-link inference of Chang et al. (2013), who coupled it with SGD updates on a per-mention basis. Chang et al. (2011, 2012, 2013); Peng et al. (2015) reformulated the bestleft-link in terms of Integer Linear Programming inference. Bj¨orkelund and Kuhn (2014) experimented with updates both on a per-mention and document basis to enable inference with non-local features. Lassalle an"
E17-2023,K15-1002,0,0.107742,"on approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being at the same time much more efficient. 1 Introduction Recent research on CR has shown effective applications of structured prediction, e.g., the latent structured perceptron (LSP) by Fernandes et al. (2014) obtained the top rank in the CoNLL-2012 Shared Task (Pradhan et al., 2012). There has been an exploration of LSP variants (Chang et al., 2011; Bj¨orkelund and Kuhn, 2014; Lassalle and Denis, 2015), and also of SGD-like methods (Chang et al., 2013; Peng et al., 2015; Kummerfeld et al., 2015). Surprisingly, no study was devoted to LSSVM by Yu and Joachims (2009), which offers theoretical guarantees on reducing the error upper-bound. The major advantage of such a theory is the possibility to stop the optimization process, carried out using the Concave-Convex Procedure (CCCP) by Yuille and Rangarajan (2003), 1 conll.cemantix.org/2012/software.html 143 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 143–149, c Valencia, Spain, April 3-7, 2017. 2017 Association for Comp"
E17-2115,N10-1145,0,\N,Missing
E17-2115,P11-1066,0,\N,Missing
E17-2115,C10-1131,0,\N,Missing
E17-2115,D07-1003,0,\N,Missing
E17-2115,N13-1106,0,\N,Missing
E17-2115,P08-1019,0,\N,Missing
E17-2115,P15-2116,0,\N,Missing
E17-2115,N15-1092,0,\N,Missing
E17-2115,S16-1138,1,\N,Missing
E17-2115,S16-1083,1,\N,Missing
E17-2115,S16-1172,1,\N,Missing
E17-2115,N16-1152,1,\N,Missing
E17-2115,P16-2075,0,\N,Missing
giordani-moschitti-2010-corpora,A00-2018,0,\N,Missing
giordani-moschitti-2010-corpora,N06-1056,0,\N,Missing
giordani-moschitti-2010-corpora,P06-1115,0,\N,Missing
giordani-moschitti-2010-corpora,W05-0602,0,\N,Missing
I11-1082,H05-1091,0,0.177563,"instances from the whole corpus. The multiple pieces of evidence for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firstly used DS from Wikipedia"
I11-1082,P07-1073,0,0.208948,"for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firstly used DS from Wikipedia for SLRE by exploiting state-of-the-art models based on Support V"
I11-1082,P11-2048,1,0.385439,"Missing"
I11-1082,D09-1143,1,0.688397,"are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firstly used DS from Wikipedia for SLRE by exploiting state-of-the-art models based on Support Vector Machines (SVMs)"
I11-1082,P04-1054,0,0.194469,"ate data and (ii) applied to extract relation instances from the whole corpus. The multiple pieces of evidence for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitt"
I11-1082,C10-2104,1,0.894885,"Missing"
I11-1082,doddington-etal-2004-automatic,0,0.321606,"tant Supervision (DS) provides a promising solution to the problem. In this paper, we study DS for designing endto-end systems of sentence-level RE. In particular, we propose a joint model between Web data derived with DS and manually annotated data from ACE. The results show (i) an improvement on the previous state-of-the-art in ACE, which provides important evidence of the benefit of DS; and (ii) a rather good accuracy on extracting 52 types of relations from Web data, which suggests the applicability of DS for general RE. 1 Introduction Automatic Relation Extraction (RE) as defined in ACE (Doddington et al., 2004) achieves the highest accuracy when supervised approaches are applied, e.g., (Zelenko et al., 2002). Unfortunately, they require labeled data and tend to be domaindependent as different domains involve different relations. Distant supervision (DS), e.g., using Wikipedia (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010), can be applied for automatically acquiring relation types and their training data. The main idea behind DS is to exploit (i) relation repositories, e.g., the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text of the page associated"
I11-1082,P04-1053,0,0.029585,"inally Section 6 summarizes the conclusions. 2 Related Work The extraction of relational data from text has drawn popularity for its potential application in a broad range of tasks. It refers to the automated extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised methods (Hasegawa et al., 2004; Banko et al., 2007). Early work on Relation Extraction has mostly employed kernel-based approaches (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005). Structural kernels on parse trees were proposed in (Collins and Duffy, 2001) for parse reranking and (Culotta and Sorensen, 2004) extended them for RE using augmented dependency trees. Recent literature has shown that efficient and appropriate kernels can be used to solve the RE problem, exploiting constituency trees (Zhang et al., 2006) and their combination with dependency trees (Nguyen 3 Resourc"
I11-1082,P10-1030,0,0.224304,"Missing"
I11-1082,W02-1010,0,0.214357,"igning endto-end systems of sentence-level RE. In particular, we propose a joint model between Web data derived with DS and manually annotated data from ACE. The results show (i) an improvement on the previous state-of-the-art in ACE, which provides important evidence of the benefit of DS; and (ii) a rather good accuracy on extracting 52 types of relations from Web data, which suggests the applicability of DS for general RE. 1 Introduction Automatic Relation Extraction (RE) as defined in ACE (Doddington et al., 2004) achieves the highest accuracy when supervised approaches are applied, e.g., (Zelenko et al., 2002). Unfortunately, they require labeled data and tend to be domaindependent as different domains involve different relations. Distant supervision (DS), e.g., using Wikipedia (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010), can be applied for automatically acquiring relation types and their training data. The main idea behind DS is to exploit (i) relation repositories, e.g., the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text of the page associated with x to produce the training sentences, which are supposed to express instances of RT (x). 1 Thi"
I11-1082,P04-3022,0,0.149965,"extract relation instances from the whole corpus. The multiple pieces of evidence for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firs"
I11-1082,I05-1034,0,0.0822824,"corpus. The multiple pieces of evidence for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firstly used DS from Wikipedia for SLRE by exploit"
I11-1082,P03-1054,0,0.00362669,"e.g. Person, Organization, Location, Time, Numbers, do not provide much selective information. For this purpose, we also provide adapted kernels by simply removing the category label in the nodes of the trees and in the sequences. This data transformation corresponds to define different kernel functions (Cristianini and Shawe-Taylor, 2000). 4.2 Joint Model for Distant and Direct Supervision An interesting test of the quality of our DS data can be carried out by using it for ACE RE experiments. This way, we can use the gold and well 736 as candidate relations. We employed the Stanford Parser (Klein and Manning, 2003) to produce parse trees. The candidate relations are generated by iterating all pairs of entity mentions in the same sentence. Regarding the DS data extraction (see Table 2), we used two PCs, one with Intel X5270 3.50GHz CPU, 32GB RAM, another with 3.40GHz CPU and 8GB RAM to run the Algorithm 3.1. We processed about 25,000 Wikipedia documents per day per machine. When we added the generation of structures and features, the whole procedure required one day to process 5,000 Wikipedia documents (per machine). Thus, it took about 10 days to create the dataset and the computational learning files."
I11-1082,P06-1104,0,0.201932,"pieces of evidence for each relation instance are then exploited to recover from errors of the automatic extractors. Additionally, a recent approach, i.e., (Hoffmann et al., 2010), has shown that DS can be also applied at level of Wikipedia article: given a target Infobox template, all its attributes1 can be extracted from a given document matching such template. Sentence-level RE (SLRE) has been typically modeled with the traditional supervised approach, e.g., using the data manually annotated in ACE (Culotta and Sorensen, 2004; Kambhatla, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Zhang et al., 2006; Bunescu and Mooney, 2007; Nguyen et al., 2009). The resulting extractors are very valuable as they find rare relation instances that might be expressed in only one document. For example, the relation President(Barrack Obama, United States) can be extracted from thousands of documents thus there is a large chance of acquiring it. In contrast, President(Eneko Agirre, SIGLEX) is probably expressed in very few documents (if not just one sentence), increasing the complexity for obtaining it. In (Nguyen and Moschitti, 2011), we firstly used DS from Wikipedia for SLRE by exploiting state-of-the-art"
I11-1082,P05-1053,0,0.0100044,"ur RE models, including the joint ACE-Wikipedia model, Section 5 reports on all experiments with our models and finally Section 6 summarizes the conclusions. 2 Related Work The extraction of relational data from text has drawn popularity for its potential application in a broad range of tasks. It refers to the automated extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised methods (Hasegawa et al., 2004; Banko et al., 2007). Early work on Relation Extraction has mostly employed kernel-based approaches (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005). Structural kernels on parse trees were proposed in (Collins and Duffy, 2001) for parse reranking and (Culotta and Sorensen, 2004) extended them for RE using augmented dependency trees. Recent literature has shown that efficient and appropriate kernels can be used to solve the RE probl"
I11-1082,P09-1113,0,0.0528157,"e-of-the-art in ACE, which provides important evidence of the benefit of DS; and (ii) a rather good accuracy on extracting 52 types of relations from Web data, which suggests the applicability of DS for general RE. 1 Introduction Automatic Relation Extraction (RE) as defined in ACE (Doddington et al., 2004) achieves the highest accuracy when supervised approaches are applied, e.g., (Zelenko et al., 2002). Unfortunately, they require labeled data and tend to be domaindependent as different domains involve different relations. Distant supervision (DS), e.g., using Wikipedia (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010), can be applied for automatically acquiring relation types and their training data. The main idea behind DS is to exploit (i) relation repositories, e.g., the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text of the page associated with x to produce the training sentences, which are supposed to express instances of RT (x). 1 This is a simpler tasks as one of the two entity is fixed. 732 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 732–740, c Chiang Mai, Thailand, November 8 – 13, 2011. 201"
I11-1082,D07-1076,0,0.029965,"Missing"
I11-1082,P04-1043,1,0.742619,"GB RAM to run the Algorithm 3.1. We processed about 25,000 Wikipedia documents per day per machine. When we added the generation of structures and features, the whole procedure required one day to process 5,000 Wikipedia documents (per machine). Thus, it took about 10 days to create the dataset and the computational learning files. To train and test our binary relation classifier, we used SVMs, where relation detection is formulated as a multiclass classification problem. We employed one vs. rest, selecting the instance with largest margin as the final label. We used the Tree Kernel toolkit3 (Moschitti, 2004; Moschitti, 2006; Moschitti, 2008) as SVM platform to implement CK1 and CSK (see Section 4.1). The training phase with convolution kernels on syntactic parse tree and diverse sequence kernels on the large DS data took 3 days. For testing on ACE data, we applied 5-fold cross-validation and evaluated single classifiers with the average of Precision, Recall and F1 on the 5-folds. The overall accuracy is measured with the mean of the Micro-Average (All) over the 5-folds. For testing on Wikipedia, as DS data may be incorrect, we created a test set by sampling 200 articles from Freebase (these arti"
I13-1012,W05-0406,0,0.0343421,"ific semantic types. A number of linguistic studies focus on various syntactic, semantic and discourse clues that might help identify nominal constructions that cannot participate in coreference relations. Possible features include, among others, specific syntactic constructions for expletive pronouns, negation, modality and quantification (Karttunen, 1976). Several algorithms have been proposed recently, trying to tackle some of the addressed phenomena within a computational approach. Thus, a number of algorithms have been developed recently to identify expletive usages of “it” (Evans, 2001; Boyd et al., 2005; Bergsma and Yarowsky, 2011). While these approaches are potentially beneficial for mention detection in English, for other languages, neither theoretical nor computational studies are available at the moment. In this paper, we use tree kernels to extract relevant syntactic patterns automatically, without assuming any prior knowledge of the input language. In this paper, we propose a learning-based solution to the mention detection task. We use SVMs (Joachims, 1999) with syntactic tree kernels (Collins and Duffy, 2001; Moschitti, 2008; Moschitti, 2006) to classify parse tree nodes as ±mention"
I13-1012,W11-1916,0,0.214957,"Missing"
I13-1012,D08-1067,0,0.0287771,"Relation Extraction (RE) and Coreference Resolution (CR). If a toolkit cannot extract mentions reliably, it will obviously be unable to assign them to relations or entities. Many studies on RE and CR report evaluation figures on gold mentions: in such a setting, a system is supplied with correct mention boundaries and/or semantic classes or other relevant properties. It can, in theory, be argued that such a methodology provides better insights on performance of RE and CR algorithms per se. It has been demonstrated, however, that evaluation results on gold mentions are misleading: for example, Ng (2008) shows that unsupervised CR algorithms exhibit promising results on gold mentions, that are not mirrored in a more realistic evaluation on automatically detected mentions. The exact scope of the mention detection task varies considerably depending on the annotation 100 International Joint Conference on Natural Language Processing, pages 100–108, Nagoya, Japan, 14-18 October 2013. adapting their mention detection algorithm to the OntoNotes English data in a semi-automatic way, reporting mixed results. coreference resolution system. 2 Related Work 3 Until recently, most RE and CR toolkits have b"
I13-1012,doddington-etal-2004-automatic,0,0.0905188,"Missing"
I13-1012,W12-4501,1,0.934384,"ntions are annotated with their minimal and maximal span, allowing for relaxed matching between gold and automatically extracted boundaries. In such a setting, the mention detection task can be cast as a tagging problem, similar to the named entity recognition and classification task. A number of systems have followed this scenario, demonstrating reliable performance (Florian et al., 2004; Ittycheriah et al., 2003; Zitouni and Florian, 2008). In the past years, however, several corpora have been created from a more linguistic perspective: for example, the OntoNotes dataset (Hovy et al., 2006; Pradhan et al., 2012) provides annotation for unrestricted coreference. The guidelines differ significantly from the ACE scheme: mentions correspond to parse nodes and can be of any semantic type, the systems are expected to recover mention boundaries exactly. The OntoNotes mentions— unlike ACE ones–correspond to large NP structures (embedding NP nodes in gold parse trees), so a traditional approach (e.g., one of those mentioned above), which aims at identifying basic NP chunks, would not be applicable here. Therefore, any MD method for OntoNotes would rely on parsing. The OntoNotes corpus has been used for evalua"
I13-1012,J01-4004,0,0.29578,"Missing"
I13-1012,N04-1001,0,0.0774687,"Missing"
I13-1012,P08-4003,1,0.872366,"om the training and development data. For Arabic, we used gold PoS tags to classify pronouns into subtypes, person, number and gender. For Chinese, no such information is available, so we consulted several grammar sketches and lists of pronouns on the web. Finally, we extracted a list of gender affixes for Arabic along Incorporating TK-based Mention Detection into an end-to-end coreference resolution system For our experiments, we use BART – a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering (Versley et al., 2008). BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and domains. In our evaluation experiments, we follow a very simple model of coreference, namely, the mention-pair approach advocated by Soon et al. 4 Recall that all the layers, apart from the Arabic lemma, were computed using state-of-the-art preprocessing tools by the CoNLL organizers and do not contain gold information 104 Figure 3 shows the results of our simulation experiment on the development data. Each line on the figure corresponds to a single MD"
I13-1012,N06-2015,0,0.135168,"Missing"
I13-1012,D08-1063,0,0.064725,"Missing"
I13-1012,H05-1013,0,0.0575476,"Missing"
I13-1012,N03-2014,0,\N,Missing
I13-1012,J03-4003,0,\N,Missing
I13-1012,C69-7001,0,\N,Missing
I13-1012,C69-6902,0,\N,Missing
I13-1189,H05-1091,0,0.0634594,"els for intra/inter TERE, Sec. 5 lays out the experiments and, finally, Sec. 6 discusses the results deriving our conclusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitt"
I13-1189,P08-1090,0,0.0380133,"elational links between constituents was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of temporal relations, the typical approaches follow similar principles of the above RE methods. Early work was devoted to ordering events with respect to one another, e.g., (Chambers and Jurafsky, 2008), and detecting their typical durations, e.g., (Pan et al., 2006). The TempEval workshops (Verhagen et al., 2007) defined the task of (i) extracting temporal relations between events and time expressions and (ii) naming relations like BEFORE, AFTER or OVERLAP. We focus on the first part of the TempEval task, following (Filatova and Hovy, 2001; Boguraev and Ando, 2005; Hovy et al., 2012), where we used the the system and results associated with the latter paper as a baseline of this paper. (Mirroshandel et al., 2011) used syntactic tree kernels for event-time links in the same sentence. As we a"
I13-1189,P04-1054,0,0.0491356,"Sec. 5 lays out the experiments and, finally, Sec. 6 discusses the results deriving our conclusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Mosc"
I13-1189,D12-1062,0,0.183869,"me that: (i) a timestamp must be explicitly stated for each event/relation that we consider to be in a temporal relation; and (ii) every event/relation is associated with only one time expression whereas a temporal expression can be linked to one or more events or relations. TIME TOK TOK TOK TOK revenue gain last monday BOW TERM EVENT ... TIME TOK TOK gain monday Figure 2: E.g. features from STK with BOW tree et al., 2012) proposed to link events via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. Finally, a recent work explicitly tackling the ISTERE task is described in (Do et al., 2012). Their system was based on three classifiers: (i) a local classifier, which processes all pairs of events and time expressions in a document and decides which pairs are linked together; (ii) a classifier between pairs of events, which determines their relations: BEFORE, AFTER, OVERLAP and NO RELATION; and (iii) a joint model which, exploiting global constraints, can highly improve the overall ISTERE accuracy. We will further comment on this work in Sec. 6. 3 Baseline Models for TERE The analysis of previous work has shown that there is almost no models for ISTERE. Therefore to align with prio"
I13-1189,doddington-etal-2004-automatic,0,0.310544,"Missing"
I13-1189,C96-1079,0,0.409448,"Missing"
I13-1189,E12-1019,1,0.299343,"i.e., we carry out a classification task, where, for each possible pair of (event/relation, time) in a document, the classifier decides whether there exists a link between the two. In particular, we assume that the event mentions, relation mentions and time expressions are given to us by an external process. There is a large body of work on the above topics and they remain difficult problems, but we use human annotated mentions and expressions as input to our models since TERE itself is a relatively new problem in this context. Previous work in TempEval-2 (Verhagen et al., 2010) and our work (Hovy et al., 2012) have shown that accurate relation classifiers can be modeled with supervised approaches, provided that the expressions are limited to be in the same sentence. In contrast, there is almost no previous work on inter-sentence TERE (ISTERE), for three main reasons: • Across a document, the number of time-event pairs to consider is large, as they are quadratic in the number of time and event expressions. • There are almost no practically useful linguistic models that can be applied for capturing intersentence relations. • Defining inter-sentence features is complex: their non-optimal definition in"
I13-1189,P04-3022,0,0.0205008,"models for intra-sentence TERE also using structural kernels, Sec. 4 describes our new models for intra/inter TERE, Sec. 5 lays out the experiments and, finally, Sec. 6 discusses the results deriving our conclusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer"
I13-1189,P12-1010,0,0.0402162,"Missing"
I13-1189,P06-1095,0,0.103926,"Missing"
I13-1189,P08-2029,1,0.832165,"ey, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more computationally expensive solution based on enumerating relational links between constituents was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of temporal relations, the typical approaches follow similar principles of t"
I13-1189,P07-1098,1,0.740103,"l methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more computationally expensive solution based on enumerating relational links between constituents was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of tempora"
I13-1189,P04-1043,1,0.821021,"rofit from continuing operations of $4 million. We produce another STR associated with it, as shown in Fig. 5. This way, we model he, ti with a pair of trees hE, T i, where e ∈ E and t ∈ T (see Sec. 3). Accordingly, we define the kernel K(p1 , p2 ) over two pairs p1 = hE1 , T1 i and p2 = hE2 , T2 , i as: T K(p1 , p2 ) = PTK(E1 , E2 ) + PTK(T1 , T2 ). It should be noted that: (i) the additive combination of kernels is still a valid kernel and it corresponds to the merged fragment space of E and T trees; (ii) the kernel product can also be applied but it has shown poor results in previous work (Moschitti, 2004); and (iii) PTK allows for modeling structural features in two sentences located in different part of the document. Thus, the features will be pairs of tree fragments from E and T. It is worth noting that the pairs of BOW and POS trees used in (Hovy et al., 2012) cause PTK to be too slow for this setting (although they may achieve comparable accuracy). Additionally, the PTK can be combined with a linear kernel of manually engineered features using an additive operation. If ~xi is the vector representation of the manually engineered features of pi , the kernel combination of PTK and the linear"
I13-1189,E09-1066,1,0.662723,"Japan, 14-18 October 2013. tion arguments. We define methods to deal with time-event relations, where the text fragment indicating the time expression, e.g., the appointment took effect Nov. 13 of the example above, is separated from the main event, e.g., succession and stepping down. In particular, our representation is constituted by a pair of shallow syntactic trees (one for each sentence containing the relation arguments), where their nodes are enriched with semantic labels, i.e., EVENT and TIME. We rely on automatic feature engineering with structural kernels (see e.g., (Moschitti, 2008; Moschitti, 2009)) to feed the learning algorithm with meaningful patterns implicitly described by such a representation. Kernels are applied to our shallow syntactic representations of text resulting in a model robust to noise and easily adaptable to new domains and tasks, such as ISTERE. We tested our models on Machine Reading and TimeBank datasets over three different configurations: (i) relation arguments both within the same sentence, (ii) relation arguments in different sentences and (iii) relation arguments both, within and across, sentences. Our experiments demonstrate that such approach is very promis"
I13-1189,D09-1143,1,0.838616,"clusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more computationally expensive solution based on enume"
I13-1189,P06-1050,0,0.0296638,"006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of temporal relations, the typical approaches follow similar principles of the above RE methods. Early work was devoted to ordering events with respect to one another, e.g., (Chambers and Jurafsky, 2008), and detecting their typical durations, e.g., (Pan et al., 2006). The TempEval workshops (Verhagen et al., 2007) defined the task of (i) extracting temporal relations between events and time expressions and (ii) naming relations like BEFORE, AFTER or OVERLAP. We focus on the first part of the TempEval task, following (Filatova and Hovy, 2001; Boguraev and Ando, 2005; Hovy et al., 2012), where we used the the system and results associated with the latter paper as a baseline of this paper. (Mirroshandel et al., 2011) used syntactic tree kernels for event-time links in the same sentence. As we aim at exploring long-distance RE, we consider more robust represe"
I13-1189,D09-1012,1,0.846005,"Missing"
I13-1189,W10-2926,1,0.902608,"Missing"
I13-1189,W13-3509,1,0.82371,"i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more computationally expensive solution based on enumerating relational links between constituents was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of temporal relations, the typical approaches follow similar principles of the above RE methods. Early work was devoted to ordering events with respect to one another, e.g., (Chambers and Jurafsky, 2008), and detecting their typical durations, e.g., (Pan et al., 2006). The TempEval workshops (Verhagen et al., 2007) defined the task of (i) extracting temporal relations between events and time expressions and (ii) naming relations like BEFORE, AFTER or OVERLAP. We focus on the first part of the TempEval task, following (Filatova and Hovy, 2001; Bo"
I13-1189,P06-1051,1,0.80692,"her variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more computationally expensive solution based on enumerating relational links between constituents was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task. Some faster versions were provided in (Moschitti and Zanzotto, 2007; Zanzotto et al., 2010). More efficient solutions based on a shallow tree and relational tags were recently proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013). Regarding the more specific task of extraction of temporal relations, the typical approaches follow similar principles of the above RE methods. Early work was devoted to ordering events with respect to one another, e.g., (Chambers and Jurafsky, 2008), and detecting their typical durations, e.g., (Pan"
I13-1189,I05-1034,0,0.0233558,"nts and, finally, Sec. 6 discusses the results deriving our conclusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni"
I13-1189,P06-1104,0,0.027676,"c. 6 discusses the results deriving our conclusions. 2 Related Work The extraction of relations between entities has been a long-standing topic of research, with work spanning more than a couple of decades, e.g., ACE (Doddington et al., 2004) and MUC (Grishman and Sundheim, 1996). In particular, sentence-level Relation Extraction (RE) has been typically modeled with supervised approaches, using manually annotated data, such as ACE (Kambhatla, 2004). Most work has focused on kernel methods, i.e., string and tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhang et al., 2005; Zhang et al., 2006) or their combinations (Nguyen et al., 2009). From the kernel perspective, our approach to TERE is another variant of the general RE work using kernel: we use PTK applied to two-level shallow syntactic trees, which extracts a sort of hierarchical subsequences. This follows up our rather long research, e.g., tree kernels for modeling the relations between syntactic constituents embedded in pairs of text (i.e., question and answer passage) for answer re-ranking (Moschitti et al., 2007; Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Moschitti and Quarteroni, 2010). A more compu"
I13-1189,strassel-etal-2010-darpa,0,\N,Missing
I13-1189,S07-1014,0,\N,Missing
I13-1189,W01-1313,0,\N,Missing
I13-1189,S10-1010,0,\N,Missing
I13-1189,P08-1000,0,\N,Missing
J08-2003,P98-1013,0,0.0722972,"Missing"
J08-2003,W04-2412,0,0.0326685,"Missing"
J08-2003,W05-0620,0,0.302447,"Missing"
J08-2003,P04-1054,0,0.894351,"complexity is the adoption of syntactic tree kernels (Collins and Duffy 2002). For example, in Moschitti (2004), the predicate–argument relation is represented by means of the minimal subtree that includes both of them. The similarity between two instances is evaluated by a tree kernel function in terms of common substructures. Such an approach is in line with current research on kernels for natural language learning, for example, syntactic parsing re-ranking (Collins and Duffy 2002), relation extraction (Zelenko, Aone, and Richardella 2003), and named entity recognition (Cumby and Roth 2003; Culotta and Sorensen 2004). Furthermore, recent work (Haghighi, Toutanova, and Manning 2005; Punyakanok et al. 2005) has shown that, to achieve high labeling accuracy, joint inference should be applied on the whole predicate–argument structure. For this purpose, we need to extract features from the sentence syntactic parse tree that encodes the relationships governing complex semantic structures. This task is rather difﬁcult because we do not exactly know which syntactic clues effectively capture the relation between the predicate and its arguments. For example, to detect the interesting context, the modeling of syntax"
J08-2003,J02-3001,0,0.876438,"itkowski 2004), for which the most important features encoding predicate–argument relations are derived from (shallow or deep) syntactic information. The outcome of this research brings wide empirical evidence in favor of the linking theories between semantics and syntax, for example, Jackendoff (1990). Nevertheless, as no such theory provides a sound and complete treatment, the choice and design of syntactic features to represent semantic structures requires remarkable research effort and intuition. For example, earlier studies on feature design for semantic role labeling were carried out by Gildea and Jurafsky (2002) and Thompson, Levy, and Manning (2003). Since then, researchers have proposed several syntactic feature sets, where the more recent sets slightly enhanced the older ones. A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a viable way to alleviate the feature design complexity is the adoption of syntactic tree kernels (Collins and Duffy 2002). For example, in Moschitti (2004), the predicate–argument relation is represented by means of the minimal subtree that includes both of them. The similarity between two instances is eval"
J08-2003,W05-0623,0,0.0156739,"Missing"
J08-2003,H05-1018,0,0.28038,"h is mainly based on diverse natural language learning problems tackled by means of tree kernels. In Collins and Duffy (2002), the SST kernel was experimented with using the voted perceptron for the parse tree re-ranking task. A combination with the original PCFG model improved the syntactic parsing. Another interesting kernel for re-ranking was deﬁned in Toutanova, Markova, and Manning (2004). This represents parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree. It is worth noting that the PT kernel includes tree fragments identical to such paths. In Kazama and Torisawa (2005), an interesting algorithm that speeds up the average running time is presented. This algorithm looks for node pairs in which the rooted subtrees share many substructures (malicious nodes) and applies a transformation to the trees rooted in such nodes to make the kernel computation faster. The results show a several-hundred-fold speed increase with respect to the basic implementation. 201 Computational Linguistics Volume 34, Number 2 In Zelenko, Aone, and Richardella (2003), two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, for exampl"
J08-2003,P03-1004,0,0.130984,"t sets which constitute back-off models for important syntactic features. Using them, the learning algorithm generalizes 194 Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling better and produces a more accurate classiﬁer, especially when the amount of training data is scarce. Finally, once the learning algorithm using tree kernels has converged, we can identify the most important structured features of the generated model. One approach for such a reverse engineering process relies on the computation of the explicit feature space, at least for the highest-weighted features (Kudo and Matsumoto 2003). Once the most relevant fragments are available, they can be used to design novel effective attribute–value features (which in turn can be used to design more efﬁcient classiﬁers, e. g., with linear kernels) and inspire new linguistic theories. These points suggest that tree kernels should always be applied, at least for an initial study of the problem. Unfortunately, they suffer from two main limitations: (a) poor impact on boundary detection as, in this task, correct and incorrect arguments may share a large portion of the encoding trees (Moschitti 2004); and (b) more expensive running time"
J08-2003,W04-0803,0,0.0171574,"revised submission received: 1 May 2007; accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 2 and Kingsbury 2005), inspired by Levin’s verb classes. To annotate natural language sentences, such systems generally require (1) the detection of the target word embodying the predicate and (2) the detection and classiﬁcation of the word sequences constituting the predicate’s arguments. Previous work has shown that these steps can be carried out by applying machine learning techniques (Carreras and M`arquez 2004, 2005; Litkowski 2004), for which the most important features encoding predicate–argument relations are derived from (shallow or deep) syntactic information. The outcome of this research brings wide empirical evidence in favor of the linking theories between semantics and syntax, for example, Jackendoff (1990). Nevertheless, as no such theory provides a sound and complete treatment, the choice and design of syntactic features to represent semantic structures requires remarkable research effort and intuition. For example, earlier studies on feature design for semantic role labeling were carried out by Gildea and Jur"
J08-2003,J93-2004,0,0.0303277,"Missing"
J08-2003,P04-1043,1,0.891709,"quires remarkable research effort and intuition. For example, earlier studies on feature design for semantic role labeling were carried out by Gildea and Jurafsky (2002) and Thompson, Levy, and Manning (2003). Since then, researchers have proposed several syntactic feature sets, where the more recent sets slightly enhanced the older ones. A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a viable way to alleviate the feature design complexity is the adoption of syntactic tree kernels (Collins and Duffy 2002). For example, in Moschitti (2004), the predicate–argument relation is represented by means of the minimal subtree that includes both of them. The similarity between two instances is evaluated by a tree kernel function in terms of common substructures. Such an approach is in line with current research on kernels for natural language learning, for example, syntactic parsing re-ranking (Collins and Duffy 2002), relation extraction (Zelenko, Aone, and Richardella 2003), and named entity recognition (Cumby and Roth 2003; Culotta and Sorensen 2004). Furthermore, recent work (Haghighi, Toutanova, and Manning 2005; Punyakanok et al."
J08-2003,E06-1015,1,0.256007,"we provide a comprehensive study of the use of tree kernels for semantic role labeling. For this purpose, we deﬁne tree kernels based on the composition of two different feature functions: canonical mappings, which map sentence-parse trees in tree structures encoding semantic information, and feature extraction functions, which encode these trees in the actual feature space. The latter functions explode the canonical trees into all their substructures and, in the literature, are usually referred to as tree kernels. For instance, in Collins and Duffy (2002), Vishwanathan and Smola (2002), and Moschitti (2006a) different tree kernels extract different types of tree fragments. Given the heuristic nature of canonical mappings, we studied their properties by experimenting with them within support vector machines and with the data set provided by CoNLL shared tasks (Carreras and M`arquez 2005). The results show that carefully engineered tree kernels always boost the accuracy of the basic systems. Most importantly, in complex tasks such as the re-ranking of semantic role annotations, they provide an easy way to engineer new features which enhance the state-of-the-art in SRL. In the remainder of this ar"
J08-2003,W05-0407,1,0.931043,"uickly as the feature extractor module only requires the writing of a general procedure for subtree extraction. In contrast, traditional SRL systems use more than thirty features (e. g., Pradhan, Hacioglu, Krugler et al. 2005), each of which requires the writing of a dedicated procedure. Second, their combination with traditional attribute–value models produces more accurate systems, also when using the same machine learning algorithm in the combination, because the feature spaces are very different. Third, we can carry out feature engineering using kernel combinations and marking strategies (Moschitti et al. 2005a; Moschitti, Pighin, and Basili 2006). This allows us to boost the SRL accuracy in a relatively simple way. Next, tree kernels generate large tree fragment sets which constitute back-off models for important syntactic features. Using them, the learning algorithm generalizes 194 Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling better and produces a more accurate classiﬁer, especially when the amount of training data is scarce. Finally, once the learning algorithm using tree kernels has converged, we can identify the most important structured features of the generated model"
J08-2003,W05-0630,1,0.869915,"uickly as the feature extractor module only requires the writing of a general procedure for subtree extraction. In contrast, traditional SRL systems use more than thirty features (e. g., Pradhan, Hacioglu, Krugler et al. 2005), each of which requires the writing of a dedicated procedure. Second, their combination with traditional attribute–value models produces more accurate systems, also when using the same machine learning algorithm in the combination, because the feature spaces are very different. Third, we can carry out feature engineering using kernel combinations and marking strategies (Moschitti et al. 2005a; Moschitti, Pighin, and Basili 2006). This allows us to boost the SRL accuracy in a relatively simple way. Next, tree kernels generate large tree fragment sets which constitute back-off models for important syntactic features. Using them, the learning algorithm generalizes 194 Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling better and produces a more accurate classiﬁer, especially when the amount of training data is scarce. Finally, once the learning algorithm using tree kernels has converged, we can identify the most important structured features of the generated model"
J08-2003,W06-2607,1,0.842129,"Missing"
J08-2003,J05-1004,0,0.193615,"Missing"
J08-2003,W05-0634,0,0.0288306,"Missing"
J08-2003,P05-1072,0,0.0133556,"Missing"
J08-2003,N04-1030,0,0.24239,"constituents surrounding the argument node Temporal markers which are very distinctive of some roles 197 Computational Linguistics Volume 34, Number 2 overlapping tree nodes, namely, one dominating the other, are classiﬁed as positive boundaries. The simplest solution relies on the application of heuristics that take into account the whole predicate–argument structure to remove the incorrect labels (e. g., Moschitti et al. 2005a; Tjong Kim Sang et al. 2005). A much more complex solution consists in the application of some joint inference model to the whole predicate–argument structure, as in Pradhan et al. (2004). As an example, Haghighi, Toutanova, and Manning (2005) associate a posterior probability with each argument node role assignment, estimate the likelihood of the alternative labeling schemes, and employ a re-ranking mechanism to select the best annotation. Additionally, the most accurate systems participating in CoNLL 2005 shared task (Pradhan, Hacioglu, Ward et al. 2005; Punyakanok et al. 2005) use different syntactic views of the same input sentence. This allows the SRL system to recover from syntactic parser errors; for example, a prepositional phrase specifying the direct object of the pr"
J08-2003,W05-0625,0,0.0439545,"Missing"
J08-2003,W03-1012,0,0.451243,"Missing"
J08-2003,W05-0637,0,0.0993094,"Missing"
J08-2003,P05-1073,0,0.0519196,"Missing"
J08-2003,W04-3222,0,0.302742,"Missing"
J08-2003,W04-3212,0,0.12312,"Missing"
J08-2003,N06-1037,0,0.194298,"Missing"
J08-2003,W03-1006,0,\N,Missing
J08-2003,C98-1013,0,\N,Missing
J08-2003,P02-1034,0,\N,Missing
J11-3007,W10-2910,1,0.830566,"ical use of semantic parses for concrete tasks or real-world applications. That is, the question “Now that we have such a nice shallow semantic representation, how do we use it for a concrete (e.g., commercial) task?” remains unanswered. It is not easy to respond to this question as, at the moment, no industrial company is using SRL (or getting from it a resounding success). However, a discussion of previous work that has successfully exploited SRL—for example, for question answering (Moschitti et al. 2007; Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2008), for sentiment analysis (Johansson and Moschitti 2010), and for cross-document coreference resolution (Ponzetto and Strube 2006)—could have been attempted. Another potentially interesting chapter would have been a survey of machine learning approaches. Although the book wisely presents a well-assessed and restricted set of techniques, the proliﬁc SRL research has developed many other interesting methods, for example, in CoNLL (Carreras and M`arquez 2005; Surdeanu et al. 2008). Additionally, more evidence on the accuracy and speed achievable by the different SRL models on different corpora and tasks would have been useful for practitioners to esti"
J11-3007,W04-2705,0,0.0384206,"a sets. The authors’ technical description is minimal but precise and highlights the differences between resources and their annotation from theoretical and practical viewpoints: The chapter provides what is needed to understand the format and meaning of the data so that writing code for using it becomes straightforward. Advanced topics such as linking different resources to boost the accuracy of automatic SRL systems are also presented. One drawback of the chapter, which is a consequence of the limited available space, is the lack of a description of other important resources (e.g., NomBank [Meyers et al. 2004], and resources in languages other than English. Chapter 3. For a computational linguist, this is the most fascinating chapter. It presents the typical computational models used to design automatic SRL systems by illustrating the most effective pipeline architectures. These are typically composed of different modules performing different tasks; the ﬁltering, identiﬁcation, classiﬁcation, and joint inference stages are described in detail by proposing features and models that have proved to be effective during several years of research. Additionally, the chapter discusses important aspects of"
J11-3007,P04-1043,1,0.829768,"enhancing accurate basic systems to state-of-the-art shallow semantic parsers. The chapter concludes with valuable information for testing the quality of an SRL system, that is, a description of the most-used accuracy measures for different kinds of parsing paradigms. Typical aspects that impact system performance such as domain variability, combinations of different resources, and the use of unsupervised approaches are also illustrated. However, this chapter may still be considered incomplete as some architectures exploiting advanced machine learning techniques, for example, kernel methods (Moschitti 2004), are not reported. Chapter 4. After all the important topics of English SRL have been presented, this chapter is dedicated to discussing the problem of extending semantic parser models to other languages (e.g., Chinese). After a brief presentation of resources for other languages, which serves only the purpose of showing their availability, the chapter focuses on interesting topics such as semantic role projection and alignment. The former allows for automatically generating labeled data using annotation on one language and parallel corpora, whereas the latter aligns both annotated corpora an"
J11-3007,N06-1025,0,0.0330024,"t is, the question “Now that we have such a nice shallow semantic representation, how do we use it for a concrete (e.g., commercial) task?” remains unanswered. It is not easy to respond to this question as, at the moment, no industrial company is using SRL (or getting from it a resounding success). However, a discussion of previous work that has successfully exploited SRL—for example, for question answering (Moschitti et al. 2007; Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2008), for sentiment analysis (Johansson and Moschitti 2010), and for cross-document coreference resolution (Ponzetto and Strube 2006)—could have been attempted. Another potentially interesting chapter would have been a survey of machine learning approaches. Although the book wisely presents a well-assessed and restricted set of techniques, the proliﬁc SRL research has developed many other interesting methods, for example, in CoNLL (Carreras and M`arquez 2005; Surdeanu et al. 2008). Additionally, more evidence on the accuracy and speed achievable by the different SRL models on different corpora and tasks would have been useful for practitioners to estimate the expected performance of new systems in new application domains. F"
J11-3007,D07-1002,0,0.0356791,"ly, there are some missing or not completely described topics. One of them is the practical use of semantic parses for concrete tasks or real-world applications. That is, the question “Now that we have such a nice shallow semantic representation, how do we use it for a concrete (e.g., commercial) task?” remains unanswered. It is not easy to respond to this question as, at the moment, no industrial company is using SRL (or getting from it a resounding success). However, a discussion of previous work that has successfully exploited SRL—for example, for question answering (Moschitti et al. 2007; Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2008), for sentiment analysis (Johansson and Moschitti 2010), and for cross-document coreference resolution (Ponzetto and Strube 2006)—could have been attempted. Another potentially interesting chapter would have been a survey of machine learning approaches. Although the book wisely presents a well-assessed and restricted set of techniques, the proliﬁc SRL research has developed many other interesting methods, for example, in CoNLL (Carreras and M`arquez 2005; Surdeanu et al. 2008). Additionally, more evidence on the accuracy and speed achievable by the diff"
J11-3007,P08-1082,0,0.0215088,"ully exploited SRL—for example, for question answering (Moschitti et al. 2007; Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2008), for sentiment analysis (Johansson and Moschitti 2010), and for cross-document coreference resolution (Ponzetto and Strube 2006)—could have been attempted. Another potentially interesting chapter would have been a survey of machine learning approaches. Although the book wisely presents a well-assessed and restricted set of techniques, the proliﬁc SRL research has developed many other interesting methods, for example, in CoNLL (Carreras and M`arquez 2005; Surdeanu et al. 2008). Additionally, more evidence on the accuracy and speed achievable by the different SRL models on different corpora and tasks would have been useful for practitioners to estimate the expected performance of new systems in new application domains. Finally, a description of the available resources, classiﬁed with respect to the semantic role theory, the language, and the genre would have been very nice. It would have given a clear picture of the spread of SRL in the natural language processing or related ﬁelds, for example, semantic Web or data mining. Despite these points, this is a unique book"
J13-3002,P07-1056,0,0.0174502,"Missing"
J13-3002,W06-1651,0,0.808682,"Missing"
J13-3002,D08-1083,0,0.240148,"Missing"
J13-3002,P10-2050,0,0.678975,"the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006) built a joint model of opinion expression extraction and holder extraction and applied integer linear programming to carry out the optimization step. While the tasks of opinion expression detection and polarity classification of opinion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned them polarity values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a concep"
J13-3002,W02-1001,0,0.0225998,"are unable to handle overlapping opinion expressions, but they are fortunately rare. To exemplify, Figure 1 shows an example of a sentence and how it is processed by the sequence labeler. The ESE defenseless situation is encoded in IOB2 as two tags B-ESE and I-ESE. There are four input columns (words, lemmas, POS tags, subjectivity clues) and one output column (opinion expression tags in IOB2 encoding). The figure also shows the sliding window from which the feature extraction function can extract features when predicting an output tag (at the arrow). We trained the model using the method by Collins (2002), with a Viterbi decoder and the online Passive–Aggressive algorithm (Crammer et al. 2006) to estimate the model weights. The learning algorithm parameters were tuned on a development set. When searching for the best value of the C parameter, we varied it along a log scale from 479 Computational Linguistics HRW has denounced the defenseless situation of these prisoners HRW have denounce the defenseless situation of this prisoner Volume 39, Number 3 NNP VBZ VBN DT JJ NN IN DT NNS − − str/neg − − − − − weak/neg O O B−DSE O B−ESE I−ESE Figure 1 Sequence labeling example. 0.001 to 100, and the bes"
J13-3002,N09-1057,0,0.0109708,"nd Hatzivassiloglou 2003) or low-level grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce, and O’Hara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili 2004). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009) argued that a speaker’s attitude can be predicted from syntactic features such as the selection of a transitive or intransitive verb frame. In contrast to the early work, recent years have seen a shift towards more detailed problem formulations where the task is not only to find a piece of opinionated text, but also to extract a structured representation of the opinion. For instance, we may determine the person holding the opinion (the holder) and towards which entity or fact it is directed (the topic), whether it is positive or negative (the polarity), and the strength of the opinion (the in"
J13-3002,P08-1067,0,0.0286941,"only simple local features, to generate a relatively small hypothesis set; we then applied a classifier using interaction features to pick the final result. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. Investigating inference methods that take a less brute-force approach than plain reranking is thus another possible future direction. Interesting examples of such inference methods include forest reranking (Huang 2008) and loopy belief propagation (Smith and Eisner 2008). Nevertheless, although the development of such algorithms is a fascinating research problem, it will not necessarily result in a more usable system: Rerankers impose very few restrictions on feature expressivity and make it easy to trade accuracy for efficiency. We investigated the effect of machine learning features, as well as other design parameters such as the choice of machine learning method and the size of the hypothesis set. For the features, we analyzed the impact of using syntax and semantics and saw that the best models are thos"
J13-3002,P10-1060,0,0.0531441,"Missing"
J13-3002,W08-2123,1,0.897783,"phrase of the sentence is its holder or not. Separate classifiers were trained to extract holders for DSEs, ESEs, and OSEs. Hereafter, we describe the feature set used by the classifiers. Our walkthrough example is given by the sentence in Figure 1. Some features are derived from the syntactic and shallow semantic analysis of the sentence, shown in Figure 2 (Section 6.1 gives more details on this). S YNTACTIC PATH. Similarly to the path feature widely used in semantic role labeling (SRL), we extract a feature representing the path in the dependency tree between the expression and the holder (Johansson and Nugues 2008). For instance, the path from denounced to HRW in the example is VC↑SBJ↓. S HALLOW- SEMANTIC RELATION. If there is a direct shallow-semantic relation between the expression and the holder, we use a feature representing its semantic role, such as A0 between denounced and HRW. E XPRESSION HEAD WORD , POS, AND LEMMA. denounced, VBD, denounce for denounced; situation, NN, situation for defenseless situation. H EAD WORD AND POS OF HOLDER CANDIDATE. HRW, NNP for HRW. D OMINATING EXPRESSION TYPE. When locating the holder for the ESE defenseless situation, we extract a feature representing the fact th"
J13-3002,P09-2079,0,0.0664067,"Missing"
J13-3002,W06-0301,0,0.597559,"o natural language processing territory; the methods used here have been inspired by information extraction and semantic role labeling, combinatorial optimization, and structured machine learning. For such tasks, deeper representations of linguistic structure have seen more use than in the coarsegrained case. Syntactic and shallow-semantic relations have repeatedly proven useful for subtasks of opinion analysis that are relational in nature, above all for determining the holder or topic of a given opinion, in which case there is considerable similarity to tasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran, and Wiebe 2008). There has been no systematic research, however, on the role played by linguistic structure in the relations between opinions expressed in text, despite the fact that the opinion expressions in a sentence are not independent but organized rhetorically to achieve a communicative effect intended by the speaker. We therefore expect that the interplay between opinion expressions can be exploited to derive information useful for the analysis of opinions expressed in text. In this article, we start from this intuition and propose several novel features de"
J13-3002,D07-1114,0,0.250077,"Missing"
J13-3002,W04-2705,0,0.012106,"fiers); they did not directly use the local features ΦL . We normalized the scores over the k candidates so that their exponentials summed to 1. 6.1 Syntactic and Shallow Semantic Analysis The features used by the rerankers, as well as the opinion holder extractor in Section 4.2, are to a large extent derived from syntactic and semantic role structures. To extract them, we used the syntactic–semantic parser by Johansson and Nugues (2008), which annotates the sentences with dependency syntax (Mel’ˇcuk 1988) and shallow semantics in the PropBank (Palmer, Gildea, and Kingsbury 2005) and NomBank (Meyers et al. 2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al. 2008). The system includes a sense disambiguator that assigns PropBank or NomBank senses to the predicate words. Figure 2 shows an example of the structure of the annotation: The sentence HRW denounced the defenseless situation of these prisoners, where denounced is a DSE and defenseless situation is an ESE, has been annotated with dependency syntax (above the text) and semantic role structure (below the text). The predicate denounced, which is an instance of the PropBank frame denounce.01, has two semantic arguments:"
J13-3002,J05-1004,0,0.00732727,"Missing"
J13-3002,P04-1035,0,0.0774089,"these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for the extraction of opinion expressions and classifiers for opinion holder extraction and polarity labeling; Section 5 reports on the main contribution: the description of the interaction models and their features; finally, Section 7 presents the experimental results and Section 8 derives the conclusions. 2. Motivation and Related Work Intuiti"
J13-3002,W02-1011,0,0.0153832,"Missing"
J13-3002,H05-1043,0,0.220532,"there have been several publications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as defined by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline syste"
J13-3002,prasad-etal-2008-penn,0,0.015091,"categorization verbs dominating them. (Here, opinions are marked S and holders H.) Example (2) (a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the election as [fraud-tainted]S and [unfair]S . (b) [Bush]H [labeled]S North Korea, Iran and Iraq an “[axis of evil]S .” In addition, interaction is important when determining opinion polarity. Here, relations that influence polarity interpretation include coordination, verb–complement, as well as other types of discourse relations. In particular, the presence of a C OMPARISON discourse relation, such as contrast or concession (Prasad et al. 2008), may allow us to infer that opinion expressions have different polarities. In Example (3), we see how contrastive discourse connectives (underlined) are used when there are contrasting polarities in the surrounding opinion expressions. (Positive opinions are tagged ‘+’, negative ‘-’.) Example (3) (a) “[This is no blind violence but rather targeted violence]- ,” Annemie Neyts [said]- . “However, the movement [is more than that]+ .” (b) “[Trade alone will not save the world]- ,” Neyts [said]- , but it constitutes an [important]+ factor for economic development. The problems we focus on in this"
J13-3002,ruppenhofer-etal-2008-finding,0,0.450978,"Missing"
J13-3002,D08-1016,0,0.0154094,"relatively small hypothesis set; we then applied a classifier using interaction features to pick the final result. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. Investigating inference methods that take a less brute-force approach than plain reranking is thus another possible future direction. Interesting examples of such inference methods include forest reranking (Huang 2008) and loopy belief propagation (Smith and Eisner 2008). Nevertheless, although the development of such algorithms is a fascinating research problem, it will not necessarily result in a more usable system: Rerankers impose very few restrictions on feature expressivity and make it easy to trade accuracy for efficiency. We investigated the effect of machine learning features, as well as other design parameters such as the choice of machine learning method and the size of the hypothesis set. For the features, we analyzed the impact of using syntax and semantics and saw that the best models are those making use of both. The most effective features we"
J13-3002,D09-1018,0,0.0614763,"s and assigned them polarity values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a conceptual level, discourse-oriented approaches (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) applying interaction features for polarity classification are arguably the most related because they are driven by a vision similar to ours: Individual opinion expressions interplay in discourse and thus provide information about each other. On a practical level there are obvious differences, since our features are extracted from syntactic and shallow-semantic linguistic representations, which we argue are reflections of discourse structure, while they extract features directly from a discourse representation. It is doubtful whether automatic discourse representation extrac"
J13-3002,W06-1640,0,0.0164309,"dentifier has been able to generalize from the specific domains. It would still be relevant, however, to apply domain adaptation techniques (Blitzer, Dredze, and Pereira 2007). It could also be interesting to see how domain-specific opinion word lexicons could improve over the generic lexicon we used here; especially if such a lexicon were automatically constructed (Jijkoun, de Rijke, and Weerkamp 2010). There are multiple additional opportunities for future work in this area. An important issue that we have left open is the coreference problem for holder extraction, which has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to incorporate complex, high-level linguistic structure such as discourse representations (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011); it is clear that these structures are very relevant for explaining the way humans organize 505 Computational Linguistics Volume 39, Number 3 their expressions of opinions rhetorically. Theoretical depth does not necessarily guarantee practical applicability, however, and the challenge is as usual to find a middle ground that balances our goals: explanatory power in theory, significant perfor"
J13-3002,stoyanov-cardie-2008-annotating,0,0.00677489,"ntensity feature taking the values L OW, M EDIUM, H IGH, and E XTREME. The two sentences in Example (6) from the MPQA corpus show opinion expressions with polarities. Positive polarity is represented with a ‘+’ and negative with a ‘-’. Example (6) (a) We foresaw electoral [fraud]- but not [daylight robbery]- . (b) Join in this [wonderful]+ event and help Jameson Camp continue to provide the year-round support that gives kids a [chance to create dreams]+ . The corpus does not currently contain annotation of topics (evaluees) of opinions, although there have been efforts to add this separately (Stoyanov and Cardie 2008). 4. Baseline Systems for Fine-Grained Opinion Analysis The assessment of our reranking-based systems requires us to compare against strong baselines. We developed (1) a sequence labeler for opinion expression extraction similar to that by Breck, Choi, and Cardie (2007), (2) a set of classifiers to determine the opinion holder, and (3) a multiclass classifier that assigns polarity to a given opinion expression similar to that described by Wilson, Wiebe, and Hoffmann (2009). These tools were also used to generate the hypothesis sets for the rerankers described in Section 5. 4.1 Sequence Labeler"
J13-3002,W08-2121,1,0.617805,"Missing"
J13-3002,P08-1036,0,0.0485753,"blications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as defined by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for"
J13-3002,E99-1023,0,0.0325909,"Missing"
J13-3002,P99-1032,0,0.142381,"Missing"
J13-3002,N10-1121,0,0.38338,"g local features in a small contextual window. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Ros´e 476 Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis 2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for extracting holders, but did not consider their relations to the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006) built a joint model of opinion expression extraction and holder extraction and applied integer linear programming to carry out the optimization step. While the"
J13-3002,H05-1044,0,0.375829,"Missing"
J13-3002,D09-1159,0,0.127022,"r of publications (Choi, Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). Such systems do not use any features describing the interaction between opinions, and it would not be possible to add interaction features because a Viterbibased sequence labeler by construction is restricted to using local features in a small contextual window. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Ros´e 476 Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis 2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for extracting holders, but did not consider their relations to the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not comp"
J13-3002,W03-1017,0,0.241498,"cal uses, either as stand-alone applications or supporting other tools such as information retrieval or question answering systems. The research community initially focused on high-level tasks such as retrieving documents or passages expressing opinion, or classifying the polarity of a given text, and these coarse-grained problem formulations naturally led to the application of methods derived from standard retrieval or text categorization techniques. The models underlying these approaches have used very simple feature representations such as purely lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-level grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce, and O’Hara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili 2004). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009)"
J13-3002,I11-1038,0,0.450832,"ty values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a conceptual level, discourse-oriented approaches (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) applying interaction features for polarity classification are arguably the most related because they are driven by a vision similar to ours: Individual opinion expressions interplay in discourse and thus provide information about each other. On a practical level there are obvious differences, since our features are extracted from syntactic and shallow-semantic linguistic representations, which we argue are reflections of discourse structure, while they extract features directly from a discourse representation. It is doubtful whether automatic discourse representation extraction in text is cur"
J13-3002,H05-2017,0,\N,Missing
J13-3002,P10-1160,0,\N,Missing
J13-3002,J09-3003,0,\N,Missing
K17-1007,P14-1005,0,0.0386869,"Missing"
K17-1007,P16-1060,0,0.0354392,"Missing"
K17-1007,P16-1061,0,0.0260544,"Missing"
K17-1007,H05-1068,0,0.0499524,"essing for relatively easy cases (e.g., name1 Across all the systems, the two most different submissions are zhekova vs. li (34.10 MELA) and the two closest ones are chunyang vs. shou (95.85 MELA). 47 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 47–57, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics fernandes martschat bjorkelund key 60.64 57.67 57.41 fernandes 100 martschat 66.74 100 bjorkelund 67.07 64.22 100 collection of models and then picking the globally best one on the development data (Munson et al., 2005; Saha et al., 2011). Another possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-trainin"
K17-1007,W03-1015,0,0.0772841,"possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al., 2009; Ng and Cardie, 2003b; Ng and Cardie, 2003a; Kouchnir, 2004). Building upon these studies, Rahman and Ng (2011) combine different coreference algorithms in an ensemble-based approach. For each mention in the document, they run several models (mention-pair, mentionranking, entity-ranking) and heuristically merge their outputs. All these approaches, however, assume a very tight integration of individual components into the ensemble. Thus, they all assume the same set of mentions to be classified.2 Moreover, most algorithms can only make ensembles of rather similar components, for example, varying feature sets or pa"
K17-1007,doddington-etal-2004-automatic,0,0.0294211,"titioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the second-best coreference performance reported so far in the literature (MELA v08 score of 64.47). 1 Introduction Coreference resolution has been one of the key areas of NLP for several decades. Major modeling breakthroughs have been achieved, not surprisingly, following three successful shared tasks, such as MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004) and, most recently, CoNLL (Pradhan et al., 2011; Pradhan et al., 2012). As of today, several high-performing systems are available publicly and, in addition, novel algorithms are being proposed regularly, even if without any code release. Our study aims at making a good use of these resources through a novel ensemble resolution method. Coreference is a heterogeneous task that requires a combination of accurate and robust processing for relatively easy cases (e.g., name1 Across all the systems, the two most different submissions are zhekova vs. li (34.10 MELA) and the two closest ones are chun"
K17-1007,N03-1023,0,0.0794749,"possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al., 2009; Ng and Cardie, 2003b; Ng and Cardie, 2003a; Kouchnir, 2004). Building upon these studies, Rahman and Ng (2011) combine different coreference algorithms in an ensemble-based approach. For each mention in the document, they run several models (mention-pair, mentionranking, entity-ranking) and heuristically merge their outputs. All these approaches, however, assume a very tight integration of individual components into the ensemble. Thus, they all assume the same set of mentions to be classified.2 Moreover, most algorithms can only make ensembles of rather similar components, for example, varying feature sets or pa"
K17-1007,P13-1012,0,0.0574466,"Missing"
K17-1007,P05-1020,0,0.0318712,"shou (95.85 MELA). 47 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 47–57, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics fernandes martschat bjorkelund key 60.64 57.67 57.41 fernandes 100 martschat 66.74 100 bjorkelund 67.07 64.22 100 collection of models and then picking the globally best one on the development data (Munson et al., 2005; Saha et al., 2011). Another possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al., 2009; Ng and Cardie, 2003b; Ng and Cardie, 2003a; Kouchnir, 2004). Building upon these studies, Rahman and Ng (2011) combine different coreference"
K17-1007,P17-1094,1,0.847624,"Missing"
K17-1007,W11-1901,0,0.171614,"ather different, each of them being only slightly closer to each other than to the gold key.1 This suggests that a meta-algorithm could merge their outputs in an intelligent way, combining the correct decisions of individual systems to arrive at a superior partition. Although several coreference resolution toolkits exist for over a decade, to our knowledge, there have been no attempts at trying to merge their outputs. The very few ensemble methods reported in the literature focus on combining several resolution strategies within the same system. Following the success of the CoNLL shared task (Pradhan et al., 2011; Pradhan et al., 2012), however, multiple complex approaches have been investigated, with very different underlying models. This means that a re-implementation of all these algorithms within a single system requires a considerable engineering effort. In the present study, we combine the final outputs of the individual systems, without making any assumptions on their specifications. This means that our approach is completely modular, allowing to combine third-party software as black boxes. The present study aims at finding a partition This paper presents a collaborative partitioning algorithm—"
K17-1007,E17-2023,1,0.861446,"Missing"
K17-1007,W12-4501,1,0.928907,"of them being only slightly closer to each other than to the gold key.1 This suggests that a meta-algorithm could merge their outputs in an intelligent way, combining the correct decisions of individual systems to arrive at a superior partition. Although several coreference resolution toolkits exist for over a decade, to our knowledge, there have been no attempts at trying to merge their outputs. The very few ensemble methods reported in the literature focus on combining several resolution strategies within the same system. Following the success of the CoNLL shared task (Pradhan et al., 2011; Pradhan et al., 2012), however, multiple complex approaches have been investigated, with very different underlying models. This means that a re-implementation of all these algorithms within a single system requires a considerable engineering effort. In the present study, we combine the final outputs of the individual systems, without making any assumptions on their specifications. This means that our approach is completely modular, allowing to combine third-party software as black boxes. The present study aims at finding a partition This paper presents a collaborative partitioning algorithm—a novel ensemblebased a"
K17-1007,P14-2006,0,0.0493618,"Missing"
K17-1007,N06-2015,0,0.100738,"Missing"
K17-1007,I11-1011,1,0.831174,"easy cases (e.g., name1 Across all the systems, the two most different submissions are zhekova vs. li (34.10 MELA) and the two closest ones are chunyang vs. shou (95.85 MELA). 47 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 47–57, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics fernandes martschat bjorkelund key 60.64 57.67 57.41 fernandes 100 martschat 66.74 100 bjorkelund 67.07 64.22 100 collection of models and then picking the globally best one on the development data (Munson et al., 2005; Saha et al., 2011). Another possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al"
K17-1007,P04-2010,0,0.0606879,"would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al., 2009; Ng and Cardie, 2003b; Ng and Cardie, 2003a; Kouchnir, 2004). Building upon these studies, Rahman and Ng (2011) combine different coreference algorithms in an ensemble-based approach. For each mention in the document, they run several models (mention-pair, mentionranking, entity-ranking) and heuristically merge their outputs. All these approaches, however, assume a very tight integration of individual components into the ensemble. Thus, they all assume the same set of mentions to be classified.2 Moreover, most algorithms can only make ensembles of rather similar components, for example, varying feature sets or parameters within the same model of corefe"
K17-1007,J01-4004,0,0.248186,"tes dataset. Section 5 summarizes our contributions and highlights directions for future research. 2 Related Work Only very few studies have so far investigated possibilities of using multiple resolvers for coreference. The first group of approaches aim at parameter optimization for choosing the best overall partition from the components’ outputs. This line of research is motivated by the fact that in most approaches to coreference, the underlying classifier does not take into account the task metric, such as, for example, MUC or MELA scores. For instance, in the classical mention-pair model (Soon et al., 2001), the classifier is trained to distinguish between coreferent and non-coreferent pairs. The output of this classifier is then processed heuristically to create coreference partitions. There is therefore no guarantee that the classifier optimized on pairs would lead to the best-scoring partition. One way to overcome this issue involves training a 2 The CoNLL systems differ considerably with respect to their underlying mentions, thus, the mention detection Fscore between two systems varies from 50.11 (xinxin vs. li) to 99.07 (chunyang vs. shou). 48 Algorithm 1 Collaborative Partitioning allows u"
K17-1007,N16-1114,0,0.0297212,"Missing"
K17-1007,N09-3001,0,0.0250875,"aha et al., 2011). Another possible solution is to learn a ranker that would pick the best model on a per-document basis, using partition-specific features (Ng, 2005). While these approaches can integrate arbitrary systems, they only allow to pick the best output partition, thus, only considering a single solution at a given time. Our algorithm, on the contrary, builds a new partition in a collaborative way, manipulating entities produced by individual components. The second research line involves training ensembles of classifiers within the same model, using bagging, boosting or co-training (Vemulapalli et al., 2009; Ng and Cardie, 2003b; Ng and Cardie, 2003a; Kouchnir, 2004). Building upon these studies, Rahman and Ng (2011) combine different coreference algorithms in an ensemble-based approach. For each mention in the document, they run several models (mention-pair, mentionranking, entity-ranking) and heuristically merge their outputs. All these approaches, however, assume a very tight integration of individual components into the ensemble. Thus, they all assume the same set of mentions to be classified.2 Moreover, most algorithms can only make ensembles of rather similar components, for example, varyi"
K17-1027,D11-1096,1,0.33943,"34110, Doha, Qatar {m.nicosia,amoschitti}@gmail.com Abstract from distributed representations of words, by applying a sequence of linear and non linear functions to the input. Word representations are learned from large corpora, or directly from the training data of the task at hand. Clearly, joining the two approaches above would have the advantage of easily integrating structures with kernels, and lexical representations with embeddings into learning algorithms. In this respect, the Smoothed Partial Tree Kernel (SPTK) is a noticeable approach for using lexical similarity in tree structures (Croce et al., 2011). SPTK can match different tree fragments, provided that they only differ in lexical nodes. Although the results were excellent, the used similarity did not consider the fact that words in context assume different meanings or weights for the final task, i.e., it does not consider the context. In contrast, SPTK would benefit to use specific word similarity when matching subtrees corresponding to different constituency. For example, the two questions: Tree kernels (TKs) and neural networks are two effective approaches for automatic feature engineering. In this paper, we combine them by modeling"
K17-1027,P12-1028,1,0.901196,"Missing"
K17-1027,S15-2086,0,0.0133037,"ute percent points over using word vectors alone, comes from modeling the words in context with the BiGRU encoder, confirming it as an effective strategy to improve the modeling capabilities of NSPTK. Interestingly, our model with a single kernel function and without complex text normalization techniques outperforms a multikernel system (Castellucci et al., 2013), when the word-incontext embeddings are incorporated. The multikernel system is applied on preprocessed text and includes a Bag-Of-Words Kernel, a Lexical Semantic Kernel, and a Smoothed Partial Tree Kernel. State-of-the-art systems (Dong et al., 2015; Severyn and Moschitti, 2015b) include many lexical and clustering features, sentiment lexicons, and distant supervision techniques. Our approach does not include any of the former. Results of our Bidirectional GRU for Word Similarity Table 5 shows the results of encoding the words in context using a more sophisticated approach: mapping the word to a representation learned with the Siamese Network that we optimize on the derived classification task presented in Section 6.1. The NSPTK operating on word vectors (best vectors from Table 3) concatenated with the wordin-context vectors produced by"
K17-1027,S16-1172,1,0.890293,"Missing"
K17-1027,P16-1231,0,0.0194857,"etween different methods, i.e., word2vec, using CBOW and SkipGram, and Structural Kernels (Moschitti, 2006) can automatically represent syntactic and semantic structures in terms of substructures, showing high accuracy in several tasks, e.g., relation extraction (Nguyen et al., 2009; Nguyen and Moschitti, 2011; Plank and Moschitti, 2013; Nguyen et al., 2015) and sentiment analysis (Nguyen and Shirai, 2015). At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA) (Severyn and Moschitti, 2015a; Rao et al., 2016), and parsing (Andor et al., 2016), to name a few. Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals 260 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics syntax are presented. Considering embeddings only, Levy and Goldberg (2014) proposed to learn word representations that incorporate syntax from dependency-based contexts. In contrast, we inject syntactic information by means of TKs, which establish a hard match between tree"
K17-1027,P15-1097,1,0.841919,"tested on two tasks, question and sentiment classification, shows that modeling the context further improves the semantic kernel accuracy compared to only using standard word embeddings. 2 Related Work Distributed word representations are an effective and compact way to represent text and are widely used in neural network models for NLP. The research community has also studied them in the context of many other machine learning models, where they are typically used as features. SPTK is an interesting kernel algorithm that can compute word to word similarity with embeddings (Croce et al., 2011; Filice et al., 2015, 2016). In our work, we go beyond simple word similarity and improve the modeling power of SPTK using contextual information in word representations. Our approach mixes the syntactic and semantic features automatically extracted by the TK, with representations learned with deep learning models (DLMs). Early attempts to incorporate syntactic information in DLMs use grammatical relations to guide the composition of word embeddings, and recursively compose the resulting substructural embeddings with parametrized functions. In Socher et al. (2012) and Socher et al. (2013), a parse tree is used to"
K17-1027,P16-1085,0,0.0245043,"in word embeddings As in (Croce et al., 2011), we observed that embeddings learned from raw words are not the most effective in the TK computation. Thus, similarly to Trask et al. (2015), we attach a special :: suffix plus the first letter of the part-of-speech (POS) to the word lemmas. This way, we differentiate words by their tags, and learn specific embedding vectors for each of them. This approach increases the performance of our models. 4.2 Modeling the word context Although a word vector encodes some information about word co-occurrences, the context around a word, as also suggested in Iacobacci et al. (2016), can explicitly contribute to the word similarity, especially when the target words are infrequent. For this reason, we also represent each word as the concatenation of its embedding with a second vector, which is supposed to model the context around the word. We build this vector as (i) a simple average of the embeddings of the other words in the sentence, and (ii) with a method specifically designed to embed longer units of text, namely paragraph2vec (Le and Mikolov, 2014). This is similar to word2vec: a network is trained to predict a word given its context, but it can access to an additio"
K17-1027,P04-1043,1,0.532754,"ntences from different categories. Batches of 64 sentences are fed to the network. The number of words sampled from each sentence is fixed to 4, and for this reason the final loss is computed over 256 pairs of words in context, for each mini-batch. The network is then trained for 5 epochs, storing the parameters corresponding to the best registered accuracy on the validation set. Those weights are later loaded and used to encode the words in a sentence by taking their corresponding output states from the last BiGRU unit. Structural models. We trained the tree kernel models using SVM-Light-TK (Moschitti, 2004), an SVM-Light extension (Joachims, 1999) with tree kernel support. We modified the software to lookup specific vectors for each word in a sentence. We preprocessed each sentence with the LTH parser4 and used its output to construct the LCT. We used the parameters for the QC classifiers from Croce et al. (2011), while we selected them on the Twitter’13 dev. set for the SC task. 7.2 Context Embedding Results Table 2 shows the QC accuracy of NSPTK with CBOW, SkipGram and GloVe. The results are reported for vector dimensions (dim) ranging from 50 to 1000, with a fixed window size of 5. The perfor"
K17-1027,P14-1062,0,0.00976754,"ral Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics syntax are presented. Considering embeddings only, Levy and Goldberg (2014) proposed to learn word representations that incorporate syntax from dependency-based contexts. In contrast, we inject syntactic information by means of TKs, which establish a hard match between tree fragments, while the soft match is enabled by the similarities of distributed representations. DLMs have been applied to the QC task. Convolutional neural neworks are explored in Kalchbrenner et al. (2014) and Kim (2014). In Ma et al. (2015), convolutions are guided by dependencies linking question words, but it is not clear how the word vectors are initialized. In our case, we only use pre-trained word vectors and the output of a parser, avoiding intensive manual feature engineering, as in Silva et al. (2010). The accuracy of these models are reported in Tab. 1 and can be compared to our QC results (Table 4) on the commonly used test set. In addition, we report our results in a cross-validation setting to better assess the generalization capabilities of the models. To encode words in context,"
K17-1027,S13-2052,0,0.0356074,"he parameter weights. hs ns hs ns - 89.8 93.0 94.2 94.6 94.4 94.2 95.2 94.8 93.4 89.8 93.6 94.0 93.6 94.4 94.0 95.0 94.6 95.2 91.0 94.2 94.2 93.2 94.2 94.4 94.8 95.0 95.2 91.6 92.8 93.8 94.2 94.2 94.0 93.8 94.4 94.6 89.8 91.6 92.4 93.2 93.6 93.8 94.4 94.2 94.0 answer type. The coarse layer maps each question into one of 6 classes: Abbreviation, Description, Entity, Human, Location and Number. Our experimental setting mirrors the setting of the original study: we train on 5,452 questions and test on 500. The SC dataset is the one of SemEval Twitter’13 for message-level polarity classification (Nakov et al., 2013). The dataset is organized in a training, development and test sets containing respectively 9,728, 1,654 and 3,813 tweets. Each tweet is labeled as positive, neutral or negative. The only preprocessing step we perform on tweets is to replace user mentions and url with a <USER&gt; and <URL&gt; token, respectively. In the cross-validation experiments, we use the training data to produce the training and test folds, whereas we use the original test set as our validation set for tuning the parameters of the network. Word embeddings. Learning high quality word embeddings requires large textual corpora. W"
K17-1027,D14-1181,0,0.0130658,"17), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics syntax are presented. Considering embeddings only, Levy and Goldberg (2014) proposed to learn word representations that incorporate syntax from dependency-based contexts. In contrast, we inject syntactic information by means of TKs, which establish a hard match between tree fragments, while the soft match is enabled by the similarities of distributed representations. DLMs have been applied to the QC task. Convolutional neural neworks are explored in Kalchbrenner et al. (2014) and Kim (2014). In Ma et al. (2015), convolutions are guided by dependencies linking question words, but it is not clear how the word vectors are initialized. In our case, we only use pre-trained word vectors and the output of a parser, avoiding intensive manual feature engineering, as in Silva et al. (2010). The accuracy of these models are reported in Tab. 1 and can be compared to our QC results (Table 4) on the commonly used test set. In addition, we report our results in a cross-validation setting to better assess the generalization capabilities of the models. To encode words in context, we employ a Sia"
K17-1027,W16-1617,0,0.0259587,"our QC results (Table 4) on the commonly used test set. In addition, we report our results in a cross-validation setting to better assess the generalization capabilities of the models. To encode words in context, we employ a Siamese Network, a DLM that has been widely used to model sentence similarity. In a Siamese setting, the same network is used to encode two sentences, and during learning, the distance between the representations of similar sentences is minimized. In Mueller and Thyagarajan (2016), an LSTM is used to encode similar sentences, and their Manhattan distance is minimized. In Neculoiu et al. (2016), a character level bidirectional LSTM is used to determine the similarity between job titles. In Tan et al. (2016), the problem of question/answer matching is treated as a similarity task, and convolutions and pooling on top of LSTM states are used to extract the sentence representations. The paper reports also experiments that include neural attention. Those mechanisms are excluded in our work, since we do not want to break the symmetry of the encoding model. In Siamese Networks, the similarity is typically computed between pair of sentences. In our work, we compute the similarity of word re"
K17-1027,P14-2050,0,0.0431004,"Shirai, 2015). At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA) (Severyn and Moschitti, 2015a; Rao et al., 2016), and parsing (Andor et al., 2016), to name a few. Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals 260 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics syntax are presented. Considering embeddings only, Levy and Goldberg (2014) proposed to learn word representations that incorporate syntax from dependency-based contexts. In contrast, we inject syntactic information by means of TKs, which establish a hard match between tree fragments, while the soft match is enabled by the similarities of distributed representations. DLMs have been applied to the QC task. Convolutional neural neworks are explored in Kalchbrenner et al. (2014) and Kim (2014). In Ma et al. (2015), convolutions are guided by dependencies linking question words, but it is not clear how the word vectors are initialized. In our case, we only use pre-traine"
K17-1027,P15-1062,0,0.0203337,"or the word model in the two contexts, i.e., those related to person and science, respectively. In this paper, we use distributed representations generated by neural approaches for computing the lexical similarity in TKs. We carry out an extensive comparison between different methods, i.e., word2vec, using CBOW and SkipGram, and Structural Kernels (Moschitti, 2006) can automatically represent syntactic and semantic structures in terms of substructures, showing high accuracy in several tasks, e.g., relation extraction (Nguyen et al., 2009; Nguyen and Moschitti, 2011; Plank and Moschitti, 2013; Nguyen et al., 2015) and sentiment analysis (Nguyen and Shirai, 2015). At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA) (Severyn and Moschitti, 2015a; Rao et al., 2016), and parsing (Andor et al., 2016), to name a few. Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals 260 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics syntax are pres"
K17-1027,P11-2048,1,0.893366,"Missing"
K17-1027,D12-1110,0,0.150734,"Missing"
K17-1027,D09-1143,1,0.784282,"that such questions are not similar, SPTK would need different embeddings for the word model in the two contexts, i.e., those related to person and science, respectively. In this paper, we use distributed representations generated by neural approaches for computing the lexical similarity in TKs. We carry out an extensive comparison between different methods, i.e., word2vec, using CBOW and SkipGram, and Structural Kernels (Moschitti, 2006) can automatically represent syntactic and semantic structures in terms of substructures, showing high accuracy in several tasks, e.g., relation extraction (Nguyen et al., 2009; Nguyen and Moschitti, 2011; Plank and Moschitti, 2013; Nguyen et al., 2015) and sentiment analysis (Nguyen and Shirai, 2015). At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA) (Severyn and Moschitti, 2015a; Rao et al., 2016), and parsing (Andor et al., 2016), to name a few. Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals 260 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - A"
K17-1027,D13-1170,0,0.00338793,"ings (Croce et al., 2011; Filice et al., 2015, 2016). In our work, we go beyond simple word similarity and improve the modeling power of SPTK using contextual information in word representations. Our approach mixes the syntactic and semantic features automatically extracted by the TK, with representations learned with deep learning models (DLMs). Early attempts to incorporate syntactic information in DLMs use grammatical relations to guide the composition of word embeddings, and recursively compose the resulting substructural embeddings with parametrized functions. In Socher et al. (2012) and Socher et al. (2013), a parse tree is used to guide the composition of word embeddings, focusing on a single parametrized function for composing all words according to different grammatical relations. In Tai et al. (2015), several LSTM architectures that follow an order determined by 3 Tree Kernels-based Lexical Similarity TKs are powerful methods for computing the similarity between tree structures. They can effec261 Model Features Accuracy SVM Unigram, syntactic information, parser output, WordNet features, hand-coded features 95.0 DCNN Unsupervised vectors 93.0 CNNns CBOW fine-tuned vectors 93.6 DepCNN Depence"
K17-1027,D14-1162,0,0.0949543,"rmance and less parameters, thus faster to train. Since we use this recurrent unit in our model, we briefly review it. Let xt and st be the input vector and state at timestep t, given a sequence of input vectors (x1 , ..., xT ), the GRU computes a sequence of states (s1 , ..., sT ) according to the following equations: Context Word Embeddings for SPTK We propose to compute the similarity function σ in SPTK as the cosine similarity of word embeddings obtained with neural networks. We experimented with the popular Continuous BagOf-Words (CBOW), SkipGram models (Mikolov et al., 2013), and GloVe (Pennington et al., 2014). 4.1 Part-of-speech tags in word embeddings As in (Croce et al., 2011), we observed that embeddings learned from raw words are not the most effective in the TK computation. Thus, similarly to Trask et al. (2015), we attach a special :: suffix plus the first letter of the part-of-speech (POS) to the word lemmas. This way, we differentiate words by their tags, and learn specific embedding vectors for each of them. This approach increases the performance of our models. 4.2 Modeling the word context Although a word vector encodes some information about word co-occurrences, the context around a wo"
K17-1027,P13-1147,1,0.821064,"need different embeddings for the word model in the two contexts, i.e., those related to person and science, respectively. In this paper, we use distributed representations generated by neural approaches for computing the lexical similarity in TKs. We carry out an extensive comparison between different methods, i.e., word2vec, using CBOW and SkipGram, and Structural Kernels (Moschitti, 2006) can automatically represent syntactic and semantic structures in terms of substructures, showing high accuracy in several tasks, e.g., relation extraction (Nguyen et al., 2009; Nguyen and Moschitti, 2011; Plank and Moschitti, 2013; Nguyen et al., 2015) and sentiment analysis (Nguyen and Shirai, 2015). At the same time, deep learning has demonstrated its effectiveness on a plethora of NLP tasks such as Question Answering (QA) (Severyn and Moschitti, 2015a; Rao et al., 2016), and parsing (Andor et al., 2016), to name a few. Deep learning models (DLMs) usually do not include traditional features; they extract relevant signals 260 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 260–270, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Lingu"
K17-1027,P15-1150,0,0.0808978,"Missing"
K17-1027,P16-1044,0,0.0286634,"g to better assess the generalization capabilities of the models. To encode words in context, we employ a Siamese Network, a DLM that has been widely used to model sentence similarity. In a Siamese setting, the same network is used to encode two sentences, and during learning, the distance between the representations of similar sentences is minimized. In Mueller and Thyagarajan (2016), an LSTM is used to encode similar sentences, and their Manhattan distance is minimized. In Neculoiu et al. (2016), a character level bidirectional LSTM is used to determine the similarity between job titles. In Tan et al. (2016), the problem of question/answer matching is treated as a similarity task, and convolutions and pooling on top of LSTM states are used to extract the sentence representations. The paper reports also experiments that include neural attention. Those mechanisms are excluded in our work, since we do not want to break the symmetry of the encoding model. In Siamese Networks, the similarity is typically computed between pair of sentences. In our work, we compute the similarity of word representations extracted from the states of a recurrent network. Such representations still depend on the entire sen"
K17-1027,S15-2079,1,0.895367,"Missing"
K17-1027,P15-2029,0,\N,Missing
K17-1027,P13-2125,1,\N,Missing
K17-1027,S13-2060,0,\N,Missing
moschitti-basili-2006-tree,A00-2018,0,\N,Missing
moschitti-basili-2006-tree,W05-0407,1,\N,Missing
moschitti-basili-2006-tree,C02-1150,0,\N,Missing
moschitti-basili-2006-tree,P04-1054,0,\N,Missing
moschitti-basili-2006-tree,P02-1034,0,\N,Missing
moschitti-basili-2006-tree,E06-1015,1,\N,Missing
N06-2025,W05-0620,0,0.241606,"Missing"
N06-2025,P02-1034,0,0.260859,"ng Case Alessandro Moschitti Department of Computer Science University of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high"
N06-2025,P97-1003,0,0.0309883,"Missing"
N06-2025,P04-1054,0,0.289219,"y of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios. In this paper, we study"
N06-2025,kingsbury-palmer-2002-treebank,0,0.0312672,"be too high and prevent their application in real scenarios. In this paper, we study the impact of the subtree (ST) (Vishwanathan and Smola, 2002), subset tree (SST) (Collins and Duffy, 2002) and partial tree (PT) kernels on Semantic Role Labeling (SRL). The PT kernel is a new function that we have designed to generate larger substructure spaces. Moreover, to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time. We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982) data sets. The results show that: (1) the kernel approach provides the same accuracy of the manually designed features. (2) The overfitting problem does not occur although the richer space of PTs does not provide better accuracy than the one based on SST. (3) The average running time of our tree kernel computation is linear. In the remainder of this paper, Section 2 introduces the different tree kernel spaces. Section 3 describes the kernel functions and our fast algorithms for their evaluation. Section 4 shows the comparative performance in terms of execution ti"
N06-2025,P03-1004,0,0.0623871,"Missing"
N06-2025,J93-2004,0,0.026916,"m in alphanumeric order and (iii) scan them to find Np . Step (iii) may require only O(|NT1 |+ |NT2 |) time, but, if label(n1 ) appears r1 times in T1 and label(n2 ) is repeated r2 times in T2 , we need to consider r1 × r2 pairs. The formal can be found in (Moschitti, 2006). 60 40 20 0 5 10 15 20 25 30 35 Number of Tree Nodes 40 45 50 55 Figure 4: Average time in µseconds for the na¨ıve SST kernel, FTK-SST and FTK-PT evaluations. http://ai-nlp.info.uniroma2.it/moschitti/ 4.2 Experiments on SRL dataset We used two different corpora: PropBank (www.cis.upenn.edu/∼ace) along with Penn Treebank 2 (Marcus et al., 1993) and FrameNet. PropBank contains about 53,700 sentences and a fixed split between training and testing used in other researches. In this split, sections from 02 to 21 are used for training, section 23 for testing and section 22 as development set. We considered a total of 122,774 and 7,359 arguments (from Arg0 to Arg5, ArgA and ArgM) in training and testing, respectively. The tree structures were extracted from the Penn Treebank. From the FrameNet corpus (www.icsi. berkeley.edu/∼framenet) we extracted all which encodes the fast tree kernels in the SVM-light software (Joachims, 1999). The multi"
N06-2025,P04-1043,1,0.916615,", Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios. In this paper, we study the impact of the"
N06-2025,E06-1015,1,0.820056,"for each pair hn1 , n2 i∈ NT1 × NT2 (Eq. 1). When the labels associated with n1 and n2 are different, we can avoid evaluating ∆(n1 , n2 ) since it is 0. Thus, we look for a node pair set Np ={hn1 , n2 i∈ NT1 × NT2 : label(n1 ) = label(n2 )}. To efficiently build Np , we (i) extract the L1 and L2 lists of nodes from T1 and T2 , (ii) sort them in alphanumeric order and (iii) scan them to find Np . Step (iii) may require only O(|NT1 |+ |NT2 |) time, but, if label(n1 ) appears r1 times in T1 and label(n2 ) is repeated r2 times in T2 , we need to consider r1 × r2 pairs. The formal can be found in (Moschitti, 2006). 60 40 20 0 5 10 15 20 25 30 35 Number of Tree Nodes 40 45 50 55 Figure 4: Average time in µseconds for the na¨ıve SST kernel, FTK-SST and FTK-PT evaluations. http://ai-nlp.info.uniroma2.it/moschitti/ 4.2 Experiments on SRL dataset We used two different corpora: PropBank (www.cis.upenn.edu/∼ace) along with Penn Treebank 2 (Marcus et al., 1993) and FrameNet. PropBank contains about 53,700 sentences and a fixed split between training and testing used in other researches. In this split, sections from 02 to 21 are used for training, section 23 for testing and section 22 as development set. We con"
N06-2025,P04-1016,0,0.148274,"Missing"
N09-2022,P98-1013,0,0.116367,"severely hurt performance. Also, a small set of FrameNet-like manual annotations is enough for realizing accurate Semantic Role Labeling on the target domains of typical Dialog Systems. 1 2 Introduction Commercial services based on spoken dialog systems have consistently increased both in number and in application scenarios (Gorin et al., 1997). Despite its success, current Spoken Language Understanding (SLU) technology is mainly based on simple conceptual annotation, where just very simple semantic composition is attempted. In contrast, the availability of richer semantic models as FrameNet (Baker et al., 1998) is very appealing for the design of better dialog managers. The first step to enable the exploitation of frame semantics is to show that accurate automatic semantic labelers can be designed for processing conversational speech. In this paper, we face the problem of performing shallow semantic analysis of speech transcrip85 FrameNet-based Semantic Role Labeling Semantic frames represent prototypical events or situations which individually define their own set of actors, or frame participants. For example, the C OMMERCE S CENARIO frame includes participants as S ELLER, B UYER, G OODS, and M ONE"
N09-2022,W05-0620,0,0.0993304,"Missing"
N09-2022,P02-1034,0,0.0342661,"manual engineering of effective features is a complex and time consuming process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicates. 3.1.1 3 Experiments Our purpose is to show that an accurate automatic FrameNet parser can be designed with reasonable effort for Italian conversational speech. For this purpose, we designed and evaluated both a semantic parser for the English FrameNet (Section 3.1) and on"
N09-2022,W05-0407,1,0.881548,"process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicates. 3.1.1 3 Experiments Our purpose is to show that an accurate automatic FrameNet parser can be designed with reasonable effort for Italian conversational speech. For this purpose, we designed and evaluated both a semantic parser for the English FrameNet (Section 3.1) and one for a corpus of Italian spoken dialogs (Section 3.2). The accuracy of"
N09-2022,J08-2003,1,0.866587,"Proceedings of NAACL HLT 2009: Short Papers, pages 85–88, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics arguments) are detected; and (iv) Role Classification (RC) (or argument classification), which assigns semantic labels to the frame elements detected in the previous step, e.g. G OODS. Therefore, we implement the full task of FrameNet-based parsing by a combination of multiple specialized SRL-like labelers, one for each frame (Coppola et al., 2008). For the design of each single labeler, we use the state-ofthe-art strategy developed in (Pradhan et al., 2005; Moschitti et al., 2008). according to the syntactic categories of the possible target predicates, namely nouns, verbs, adjectives, adverbs and prepositions; (b) we trained 782 one-versus-all multi-role classifiers RC, one for each available frame and predicate syntactic category, for a total of 5,345 binary classifiers; and (c) we applied the above models for recognizing predicate arguments and their associated semantic labels in sentences, where the frame label and the target predicate were considered as given. 2.1 Standard versus Structural Features In machine learning tasks, the manual engineering of effective fe"
N09-2022,W04-3212,0,0.0225285,"iers; and (c) we applied the above models for recognizing predicate arguments and their associated semantic labels in sentences, where the frame label and the target predicate were considered as given. 2.1 Standard versus Structural Features In machine learning tasks, the manual engineering of effective features is a complex and time consuming process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicat"
N09-2022,C98-1013,0,\N,Missing
N09-2022,J02-3001,0,\N,Missing
N10-1146,N09-1003,0,0.0403911,"Missing"
N10-1146,D09-1110,0,0.0118203,"is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syn"
N10-1146,W05-0601,1,0.800255,"syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the"
N10-1146,H05-1079,0,0.00742673,"ee kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatica"
N10-1146,W07-1402,0,0.0153556,"wing example: T6 ⇒?H6 T6 “In 1956 JFK met Marilyn Monroe” H6 “Marilyn Monroe died in 1956” The problem is that the pairs hT2 , H2 i and hT4 , H4 i share more meaningful features than the rule ρ5 , which should make the difference with respect to the relation between the pairs hT2 , H2 i and hT6 , H6 i. Indeed, the word “kill” is more semantically related to “murdered” than to “meet”. Using this information, it is possible to derive more effective rules from training examples. There are several solutions for taking this information into account, e.g. by using FrameNet semantics (e.g., like in (Burchardt et al., 2007)), it is possible to encode a lexical-syntactic rule using the KILLING and the DEATH frames, i.e.: ρ3 = X killed Y → Y died ρ7 = along with such rules, the temporal information should be taken into consideration. Given the importance of lexical-syntactic rules in RTE, many methods have been proposed for their extraction from large corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)). Unfortunately, these unsupervised methods in general produce rules that can hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Mos"
N10-1146,A00-2018,0,0.0694397,"K, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We"
N10-1146,P02-1034,0,0.0260782,"important fragment from a semantically similar sentence, which cannot be matched by STK but it is matched by SSTK. account in the model definition. Since tree kernels have been shown to be very effective for exploiting syntactic information in natural language tasks, a promising idea is to merge together the two different approaches, i.e. tree kernels and semantic similarities. 4.1 Syntactic Tree Kernel (STK) Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. The standard definition of the STK, given in (Collins and Duffy, 2002), allows for any set of nodes linked by one or more entire production rules to be valid substructures. The formal characterization is given in (Collins and Duffy, 2002) and is reported hereafter: Let F = {f1 , f2 , . . . , f|F |} be the set of tree fragments and χi (n) be an indicator function, equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree kernel function over P T1 and T2 is defined as T K(T1 , T2 ) = P n1 ∈NT1 n2 ∈NT2 ∆(n1 , n2 ), where NT1 and NT2 are the sets of nodes in T1 and T2 , respectively and P|F | ∆(n1 , n2 ) = i=1 χi (n1 )χi (n2 ). ∆ function coun"
N10-1146,E09-1025,0,0.0237506,"describes lexical similarity approaches, which can serve the generalization purpose. Section 4 describes how to integrate lexical similarity in syntactic structures using syntactic/semantic tree kernels (SSTK) whereas Section 5 shows how to use SSTK in a kernel-based RTE system. Section 6 describes the experiments and results. Section 7 discusses the efficiency and accuracy of our system compared with other RTE systems. Finally, we draw the conclusions in Section 8. 2 Related work Lexical-syntactic rules are largely used in textual entailment recognition systems (e.g., (Bar-Haim et al., 2007; Dinu and Wang, 2009)) as they conveniently encode world knowledge into linguistic structures. For example, to decide whether the simple sentences are in the entailment relation: T2 ⇒?H2 T2 “In 1980 Chapman killed Lennon.” H2 “John Lennon died in 1980.” we need a lexical-syntactic rule such as: from examples in terms of complex relational features. This approach can easily miss some useful information and rules. For example, given the pair hT2 , H2 i, to derive the entailment value of the following case: T4 ⇒?H4 T4 “In 1963 Lee Harvey Oswald murdered JFK” H4 “JFK died in 1963” we can only rely on this relatively i"
N10-1146,W07-1401,0,0.0258766,"SSTK) Kernels. The latter according to different similarities distributional vs. Wordnet-based approaches. Second, we derive qualitative and quantitative properties, which justify the selection of one with respect to the other. For this purpose, we tested four different version of SSTK, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is als"
N10-1146,H05-1049,0,0.0076299,"w that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more ac"
N10-1146,O97-1002,0,0.0476717,"Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We built different LSA matrices from the British National Corpus (BNC) and Wikipedia (Wiki). The British National Corpus (BNC) is a balanced synchronic text corpus containing 100 million words with mor"
N10-1146,E06-1015,1,0.639162,"utomatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
N10-1146,N04-3012,0,0.553662,"n hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009), where lexical-syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan a"
N10-1146,W07-1418,0,0.0285501,"xploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain"
N10-1146,C08-1107,0,0.0297184,"oded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syntactic and semantic information in RTE, without requiring either large automatic rule acquisition or hand-coding. These models exploit lexical similarities to generalize lexical-syntactic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form o"
N10-1146,H05-1047,0,0.0318507,"s realized by means of tree kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic"
N10-1146,P94-1019,0,0.146531,"tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the following measures, that we use in this study, are based on path lengths between concepts in the Wordnet Hierarchy: Path the measure is equal to the inverse of the shortest path length (path length) between two synsets c1 and c2 in WordNet SimP ath = 1 path length(c1 , c2 ) (1) WUP the Wu and Palmer (Wu and Palmer, 1994) similarity metric is based on the depth of two given synsets c1 and c2 in the WordNet taxonomy, and the depth of their least common subsumer (lcs). These are combined into a similarity score: SimW U P = 2 × depth(lcs) depth(c1 ) + depth(c2 ) (2) Wordnet similarity measures on synsets can be extended to similarity measures between words as follows: κS (w1 , w2 ) = max(c1 ,c2)∈C1 ×C2 SimS (c1 , c2 ) (3) where S is Path or WUP and Ci is the set of the synsets related to the word wi . 3.2 Distributional Semantic Similarity Latent Semantic Analysis (LSA) is one of the corpus-based measure of distr"
N10-1146,C00-2137,0,0.0288195,"m with the standard accuracy and then we determine the statistical significance by using the model RTE2 +WOK RTE3 +WOK RTE5 +WOK STK 61.5 52.62 66.38 53.25 62.0 54.33 SSTK 61.12 52.75 66.5 54.5 62.0 57.33 maxSTK 63.88 61.25 66.5 62.25 64.83 63.33 maxSSTK 64.12 59.38 67.0 64.38 64.83 62.67 STK+maxSTK 63.12 61.25 66.88 63.12 65.5 61.83 SSTK+maxSSTK 63.50 58.75 67.25 63.62 66.5 62.67 ∅ 60.62 66.75 60.67 - Table 2: Comparing different lexico-syntactic kernels with Wiki-based semantic kernels of the j parameters, i.e. j = 1, which was not selected by our limited parameter validation. described in (Yeh, 2000) and implemented in (Pad´o, 2006). 6.2 Distributional vs. WordNet-based Semantics The first experiment compares the basic kernel, i.e. WOK+STK+maxSTK, with the new semantic kernel, i.e. WOK+SSTK+maxSSTK, where SSTK and maxSSTK encode four different kinds of similarities, BNC, WIKI, WUP and Path. The aim is twofold: understanding if semantic similarities can be effectively used to derive generalized lexicosyntactic rules and to determine the best similarity model. Table 1 shows the results according to No Semantics, Wiki, BNC, Path and WUP. The three pairs of rows represent the results over the"
N10-1146,P06-1051,1,0.887883,"actic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
N15-1121,C10-3009,0,0.640623,"Missing"
N15-1121,W09-1207,0,0.306018,"function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated with the same predicat"
N15-1121,D09-1003,0,0.317743,"Missing"
N15-1121,W09-1205,0,0.0722058,"main test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full model utilizes 4-way tensor"
N15-1121,J02-3001,0,0.656237,"Zhao et al., 2009a). On three out of five languages, the tensor-based model outperforms this system. These results are particularly notable because the system of Zhao et al. (2009a) employs a rich set of language-specific features carefully engineered for this task. Finally, we demonstrate that using four-way tensor yields better performance than its three-way counterpart, highlighting the importance of modeling the relation between role labels and properties of the path. 2 Related Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among othe"
N15-1121,D09-1059,0,0.335342,"uild our scoring function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated wit"
N15-1121,P14-1130,1,0.649857,"re vectors that capture distinct facets of semantic dependence: predicate, argument, syntactic path and role label. By compressing this sparse representation into lower dimensions, we obtain dense representations for words (predicate, argument) and their connecting paths, uncovering meaningful interactions. The associated parameters are maintained as a four-way low-rank tensor, and optimized for SRL performance. Tensor modularity enables us to employ standard online algorithms for training. Our approach to SRL is inspired by recent success of our tensor-based approaches in dependency parsing (Lei et al., 2014). Applying analogous techniques to SRL brings about new challenges, however. The scoring function needs to reflect the highorder interactions between the predicate, argument, 1150 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1150–1160, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics their syntactic path and the corresponding role label. Therefore, we parametrize the scoring function as a four-way tensor. Generalization to high-order tensors also requires new initialization and update procedures"
N15-1121,Q13-1018,1,0.900111,"Missing"
N15-1121,S14-2082,0,0.0478503,"s, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computation"
N15-1121,J08-2003,1,0.868522,"dhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In technical terms, our work builds on our recent tensor-based approach for dependency parsin"
N15-1121,D14-1045,0,0.684785,"tic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In techn"
N15-1121,P03-1002,0,0.0928285,"Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature en"
N15-1121,W08-2121,1,0.852627,"Missing"
N15-1121,J08-2002,0,0.148737,"eady identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach c"
N15-1121,W04-3212,0,0.296176,"L research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in"
N15-1121,D14-1041,0,0.0571193,"re widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to p"
N15-1121,C00-2137,0,0.0458716,"Missing"
N15-1121,D14-1109,1,0.838822,"is a very simple iterative algorithm and is used to find the largest eigenvalues and eigenvectors (or singular values and vectors in SVD case) of a matrix. Its generalization directly applies to our high-order tensor case. 5 Implementation Details Decoding Following Llu´ıs et al. (2013), the decoding of SRL is formulated as a bipartite maximum assignment problem, where we assign arguments to semantic roles for each predicate. We use the maximum weighted assignment algorithm (Kuhn, 1955). For syntactic dependency parsing, we employ the randomized hill-climbing algorithm from our previous work (Zhang et al., 2014). 1155 Input: sparse tensor T , rank number i and fixed rank-1 components P (j), Q(j), R(j) and S(j) for j = 1..(i − 1) Output: new component P (i), Q(i), R(i) and S(i). 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: Randomly initialize four unit vectors p, q, r and s P T 0 = T − j P (j) ⊗ Q(j) ⊗ R(j) ⊗ S(j) repeat p = hT 0 , −, q, r, si and normalize it q = hT 0 , p, −, r, si and normalize it r = hT 0 , p, q, −, si and normalize it s = hT 0 , p, q, r, −i norm = ksk22 until norm converges P (i) = p and Q(i) = q R(i) = r and S(i) = s Figure 2: The iterative power method for highorder tensor initialization."
N15-1121,W09-1209,0,0.434698,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,W09-1208,0,0.304639,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,J13-3006,1,\N,Missing
N15-1121,W09-1201,1,\N,Missing
N15-1159,P11-2008,0,0.00844821,"Missing"
N15-1159,P14-1062,0,0.0617169,"Missing"
N15-1159,D14-1181,0,0.0909304,"Missing"
N15-1159,E12-1062,0,0.0186081,"Missing"
N15-1159,S13-2053,0,0.594167,"n a polarity label are the word lookups and averaging. However, the information about word polarities in a document are best exploited when using machine learning models to train a sentiment classifier. In fact, most successful sentiment classification systems rely on supervised learning. Interestingly, a simple bag of words model using just unigrams and bigrams with an SVM has shown excellent results (Wang and Manning, 2012) performing on par or beating more complicated models, e.g., using neural networks (Socher et al., 2011). Regarding Twitter sentiment analysis, the top performing system (Mohammad et al., 2013) from Semeval-2013 Twittter Sentiment Analysis task (Nakov et al., 2013) follows this recipe by training an SVM on various surface form, sentiment and semantic features. Perhaps, the most valuable finding is that sentiment lexicons appear to be the most useful source of features accounting for over 8 point gains in the F-measure on top of the standard feature sets. Sentiment lexicons are mappings from words to scores capturing the degree of the sentiment expressed by a given word. While several manually constructed lexicons are made available, e.g., the MPQA (Wilson et al., 2005), the Bing and"
N15-1159,P04-1035,0,0.0127169,"struct sentiment lexicons using distant supervision. We induce the sentiment association scores for the lexicon items from a model trained on a weakly supervised corpora. Our empirical findings show that features extracted from such a machine-learned lexicon outperform models using manual or other automatically constructed sentiment lexicons. Finally, our system achieves the state-of-the-art in Twitter Sentiment Analysis tasks from Semeval-2013 and ranks 2nd best in Semeval-2014 according to the average rank. 1 Introduction One of the early and rather successful models for sentiment analysis (Pang and Lee, 2004; Pang and Lee, 2008) relied on manually constructed lexicons that map words to their sentiment, e.g., positive, negative or neutral. The document-level polarity is then assigned by performing some form of averaging, e.g., majority voting, of individual word polarities found in the document. These systems show an acceptable level of accuracy, they are easy to build and are highly computationally efficient as the only operation required to assign a polarity label are the word lookups and averaging. However, the information about word polarities in a document are best exploited when using machin"
N15-1159,D11-1014,0,0.0509376,"build and are highly computationally efficient as the only operation required to assign a polarity label are the word lookups and averaging. However, the information about word polarities in a document are best exploited when using machine learning models to train a sentiment classifier. In fact, most successful sentiment classification systems rely on supervised learning. Interestingly, a simple bag of words model using just unigrams and bigrams with an SVM has shown excellent results (Wang and Manning, 2012) performing on par or beating more complicated models, e.g., using neural networks (Socher et al., 2011). Regarding Twitter sentiment analysis, the top performing system (Mohammad et al., 2013) from Semeval-2013 Twittter Sentiment Analysis task (Nakov et al., 2013) follows this recipe by training an SVM on various surface form, sentiment and semantic features. Perhaps, the most valuable finding is that sentiment lexicons appear to be the most useful source of features accounting for over 8 point gains in the F-measure on top of the standard feature sets. Sentiment lexicons are mappings from words to scores capturing the degree of the sentiment expressed by a given word. While several manually co"
N15-1159,P14-1146,0,0.0622341,"Missing"
N15-1159,P12-2018,0,0.0181514,"word polarities found in the document. These systems show an acceptable level of accuracy, they are easy to build and are highly computationally efficient as the only operation required to assign a polarity label are the word lookups and averaging. However, the information about word polarities in a document are best exploited when using machine learning models to train a sentiment classifier. In fact, most successful sentiment classification systems rely on supervised learning. Interestingly, a simple bag of words model using just unigrams and bigrams with an SVM has shown excellent results (Wang and Manning, 2012) performing on par or beating more complicated models, e.g., using neural networks (Socher et al., 2011). Regarding Twitter sentiment analysis, the top performing system (Mohammad et al., 2013) from Semeval-2013 Twittter Sentiment Analysis task (Nakov et al., 2013) follows this recipe by training an SVM on various surface form, sentiment and semantic features. Perhaps, the most valuable finding is that sentiment lexicons appear to be the most useful source of features accounting for over 8 point gains in the F-measure on top of the standard feature sets. Sentiment lexicons are mappings from wo"
N15-1159,H05-1044,0,0.229102,"ing system (Mohammad et al., 2013) from Semeval-2013 Twittter Sentiment Analysis task (Nakov et al., 2013) follows this recipe by training an SVM on various surface form, sentiment and semantic features. Perhaps, the most valuable finding is that sentiment lexicons appear to be the most useful source of features accounting for over 8 point gains in the F-measure on top of the standard feature sets. Sentiment lexicons are mappings from words to scores capturing the degree of the sentiment expressed by a given word. While several manually constructed lexicons are made available, e.g., the MPQA (Wilson et al., 2005), the Bing and Liu (Hu and Liu, 2004) and NRC Emoticon (Mohammad and Turney, 2013) lexicons, providing high quality word-sentiment associations compiled by humans, still their main drawback is low recall. For example, the largest NRC Emoticon lexicon contains only 14k items, whereas tweets with extremely sparse surface forms are known to form very large vocabularies. Hence, using larger lexicons with better recall has the potential of learning more accurate models. Extracting such lexicons automatically is a challenging and interesting problem (Lau et al., 2011; Bro and Ehrig, 2013; Liu et al."
N15-1159,S13-2052,0,\N,Missing
N16-1129,D11-1096,1,0.865306,"Missing"
N16-1129,C04-1051,0,0.192853,"Missing"
N16-1129,P15-1097,1,0.824557,"Missing"
N16-1129,N12-1019,0,0.0177255,"he same meaning. For example, the following two sentences from the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004): S1a : Although it’s unclear whether Sobig was to blame, The New York Times also asked employees at its headquarters yesterday to shut down their computers because of ”system difficulties.” S1b : The New York Times asked employees at its headquarters to shut down their computers yesterday because of ”computing system difficulties.” are paraphrases, while these other two are not: ∗ Professor at DISI, University of Trento. Most previous work on automatic PI, e.g., (Madnani et al., 2012; Socher et al., 2011), is based on a direct comparison between the two texts, exploiting different similarity scores into a machine learning framework. However, these methods consider sentences as monolithic units and can thus be misled by ancillary information that does not modify the main meaning expressed in the text. For example, the additional text fragment (ATF), “Although it’s unclear whether Sobig was to blame”, from S1a expresses ancillary information, which does not add much to the message of S1b , thus the sentences are considered paraphrases. In contrast, S2b contains the ATF, “We"
N16-1129,W13-3509,1,0.892118,"Missing"
N16-1129,E14-1070,1,0.906211,"Missing"
N16-1152,D14-1067,0,0.0849155,"o et al., 2014; Tymoshenko and Moschitti, 2015). The main difference with our previous approaches is usage of better-preprocessing algorithms and new structural representations, which highly outperform them. Recently, deep learning approaches have been successfully applied to various sentence classification tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014), and for automatically modeling text pairs, e.g., (Lu and Li, 2013; Hu et al., 2014). Additionally, a number of deep learning models have been recently applied to question answering, e.g., Yih et al. (2014) applied CNNs to open-domain QA; Bordes et al. (2014b) propose a neural embedding model Figure 1: Shallow chunk-based tree for the Q/AP pair in the running example. combined with the knowledge base for open-domain QA; Iyyer et al. (2014) applied recursive neural networks to factoid QA over paragraphs. (Miao et al., 2015) proposed a neural variational inference model and a Long-short Term Memory network for the same task. Recently (Yin et al., 2015) proposed a siamese convolutional network for matching sentences that employ an attentive average pooling mechanism, obtaining state-of-the-art results in various tasks and datasets. The work closest"
N16-1152,D11-1096,1,0.394376,"Missing"
N16-1152,P15-1097,1,0.183083,"our CNNs perform almost on par with tree kernels, i.e., an MRR of 71.07 vs. 72.51 of CTK, which again is the current state of the art on such data. The combination between CTK and CNNs produces a further boost, achieving an MRR of 75.52 and an MAP of 73.99, confirming that the research line of combining these two interesting machine learning methods is very promising. 2 Related Work Relational learning from entire pieces of text concerns several natural language processing tasks, e.g., 1269 QA (Moschitti, 2008), Textual Entailment (Zanzotto and Moschitti, 2006) and Paraphrase Identification (Filice et al., 2015). Regarding QA, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. Wang et al. (2007) used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) applied Tree Edit Distance (TED) for learning tree transformations in a Q/AP pair. (Wang and Manning, 2010) designed a probabilistic model to learn tree-edit operations on dependency parse trees. (Yao et al., 2013) applied linear chain CRF"
N16-1152,N10-1145,0,0.288655,"very promising. 2 Related Work Relational learning from entire pieces of text concerns several natural language processing tasks, e.g., 1269 QA (Moschitti, 2008), Textual Entailment (Zanzotto and Moschitti, 2006) and Paraphrase Identification (Filice et al., 2015). Regarding QA, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. Wang et al. (2007) used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) applied Tree Edit Distance (TED) for learning tree transformations in a Q/AP pair. (Wang and Manning, 2010) designed a probabilistic model to learn tree-edit operations on dependency parse trees. (Yao et al., 2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. Yih et al. (2013a) applied enhanced lexical semantics to build a word-alignment model, exploiting a number of large-scale external semantic resources. Although the above approaches are very valuable, they required considerable effort to study, define"
N16-1152,D14-1070,0,0.0377142,"Missing"
N16-1152,P14-1062,0,0.536253,"systems, the question text needs to be put in relation with the text passages retrieved from a document collection to enable an accurate extraction of the correct answers from passages. From a machine learning perspective, encoding the information above consists in manually defining expressive rules and features based on syntactic and semantic patterns. Therefore, methods for automatizing feature engineering are remarkably important also in the light of fast prototyping of commercial applications. To Although based on different principles, also CNNs can generate powerful features, e.g., see (Kalchbrenner et al., 2014; Kim, 2014). CNNs can effectively capture the compositional process of mapping the meaning of individual words in a sentence to a continuous representation of the sentence. This way CNNs can efficiently learn to embed input sentences into low-dimensional vector space, preserving important syntactic and semantic aspects of the input sentence. However, engineering features spanning two pieces of text such as in QA is a more complex task than classifying single sentences. Indeed, only very recently, CNNs were proposed for QA by Yu et al. (2014). Although, such network achieved high accuracy, its"
N16-1152,D14-1181,0,0.212272,"needs to be put in relation with the text passages retrieved from a document collection to enable an accurate extraction of the correct answers from passages. From a machine learning perspective, encoding the information above consists in manually defining expressive rules and features based on syntactic and semantic patterns. Therefore, methods for automatizing feature engineering are remarkably important also in the light of fast prototyping of commercial applications. To Although based on different principles, also CNNs can generate powerful features, e.g., see (Kalchbrenner et al., 2014; Kim, 2014). CNNs can effectively capture the compositional process of mapping the meaning of individual words in a sentence to a continuous representation of the sentence. This way CNNs can efficiently learn to embed input sentences into low-dimensional vector space, preserving important syntactic and semantic aspects of the input sentence. However, engineering features spanning two pieces of text such as in QA is a more complex task than classifying single sentences. Indeed, only very recently, CNNs were proposed for QA by Yu et al. (2014). Although, such network achieved high accuracy, its design is s"
N16-1152,C02-1150,0,0.266176,"answer pairs, CH, described in (Severyn et al., 2013; Tymoshenko and Moschitti, 2015). We represent a pair of short texts as two trees with lemmas at leaf level and their part-of-speech (POS) tags at the preterminal level. Preterminal POS-tags are grouped into chunk nodes and the chunks are further grouped into sentences. Figure 1 provides an example of this structure. We enrich the above representation with the in1270 formation about question class and question focus. Questions are classified in terms of their expected answer type. (Severyn et al., 2013) employed coarse-grained classes from (Li and Roth, 2002), namely HUM (person), ENTY (an entity), DESC (description), LOC (location), and NUM (number). In this work, we split the NUM class into three subcategories, DATE, QUANTITY, CURRENCY and train question classifiers as described in (Severyn et al., 2013). Differently from before, we add the question class node as the rightmost child of the root node both to the question and the answer structures. We detect question focus using a focus classifier, FCLASS, trained as in (Severyn et al., 2013). However, in our previous model, we classified all words over the chunks in the question and picked the on"
N16-1152,P14-5010,0,0.00271598,"e sampled from the Bing query logs and candidate answers were extracted from the summary paragraphs of the associated Wikipedia pages. The train, test, and development sets contain 2,118, 633 and 296 questions, respectively. There is no correct answer sentence for 1,245 training, 170 development and 390 test questions. Consistently with (Yin et al., 2015), we remove the questions without answers for our evaluations. Preprocessing. We used the Illinois chunker (Punyakanok and Roth, 2001), question class and focus classifiers trained as in (Severyn and Moschitti, 2013) and the Stanford CoreNLP (Manning et al., 2014) toolkit for the needed preprocessing. CTKs. We used SVM-light-TK2 to train our models. The toolkit enables the use of structural kernels (Moschitti, 2006) in SVM-light (Joachims, 2002). We applied (i) the partial tree kernel (PTK) with its default parameters to all our structures and (ii) the polynomial kernel of degree 3 on all feature vectors we generate. Metaclassifier. We used the scikit3 logistic regression classifier implementation to train the metaclassifier on the outputs of CTKs and CNNs. CNNs. We pre-initialize the word embeddings by running the word2vec tool (Mikolov et al., 2013)"
N16-1152,P07-1098,1,0.936949,"Convolutional Neural Networks (CNNs) for learning to rank answer sentences in a Question Answering (QA) setting. When dealing with QA, the key aspect is to encode relational information between the constituents of question and answer in learning algorithms. For this purpose, we propose novel CNNs using relational information and combined them with relational CTKs. The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results. 1 Regarding the former, in (Moschitti et al., 2007), we firstly used CTKs in Support Vector Machines (SVMs) to generate features from a question (Q) and their candidate answer passages (AP). CTKs enable SVMs to learn in the space of convolutional subtrees of syntactic and semantic trees used for representing Q and AP. This automatically engineers syntactic/semantic features. One important characteristic we added in (Severyn and Moschitti, 2012) is the use of relational links between Q and AP, which basically merged the two syntactic trees in a relational graph (containing relational features). Introduction The increasing use of machine learnin"
N16-1152,D13-1044,1,0.827329,"le, they required considerable effort to study, define and implement features that could capture relational representations. In contrast, we are interested in techniques that try to automatize the feature engineering step. In this respect, our work (Moschitti et al., 2007) is the first using CTKs applied to syntactic and semantic structural representations of the Q/AP pairs in a learning to rank algorithm based on SVMs. After this, we proposed several important improvement exploiting different type of relational links between Q and AP, i.e., (Severyn and Moschitti, 2012; Severyn et al., 2013; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). The main difference with our previous approaches is usage of better-preprocessing algorithms and new structural representations, which highly outperform them. Recently, deep learning approaches have been successfully applied to various sentence classification tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014), and for automatically modeling text pairs, e.g., (Lu and Li, 2013; Hu et al., 2014). Additionally, a number of deep learning models have been recently applied to question answering, e.g., Yih et al. (2014) applied CNNs to open-"
N16-1152,W13-3509,1,0.907169,"oaches are very valuable, they required considerable effort to study, define and implement features that could capture relational representations. In contrast, we are interested in techniques that try to automatize the feature engineering step. In this respect, our work (Moschitti et al., 2007) is the first using CTKs applied to syntactic and semantic structural representations of the Q/AP pairs in a learning to rank algorithm based on SVMs. After this, we proposed several important improvement exploiting different type of relational links between Q and AP, i.e., (Severyn and Moschitti, 2012; Severyn et al., 2013; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). The main difference with our previous approaches is usage of better-preprocessing algorithms and new structural representations, which highly outperform them. Recently, deep learning approaches have been successfully applied to various sentence classification tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014), and for automatically modeling text pairs, e.g., (Lu and Li, 2013; Hu et al., 2014). Additionally, a number of deep learning models have been recently applied to question answering, e.g., Yih et al."
N16-1152,E14-1070,1,0.858397,"e effort to study, define and implement features that could capture relational representations. In contrast, we are interested in techniques that try to automatize the feature engineering step. In this respect, our work (Moschitti et al., 2007) is the first using CTKs applied to syntactic and semantic structural representations of the Q/AP pairs in a learning to rank algorithm based on SVMs. After this, we proposed several important improvement exploiting different type of relational links between Q and AP, i.e., (Severyn and Moschitti, 2012; Severyn et al., 2013; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). The main difference with our previous approaches is usage of better-preprocessing algorithms and new structural representations, which highly outperform them. Recently, deep learning approaches have been successfully applied to various sentence classification tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014), and for automatically modeling text pairs, e.g., (Lu and Li, 2013; Hu et al., 2014). Additionally, a number of deep learning models have been recently applied to question answering, e.g., Yih et al. (2014) applied CNNs to open-domain QA; Bordes et al."
N16-1152,C10-1131,0,0.347411,"ge processing tasks, e.g., 1269 QA (Moschitti, 2008), Textual Entailment (Zanzotto and Moschitti, 2006) and Paraphrase Identification (Filice et al., 2015). Regarding QA, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. Wang et al. (2007) used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) applied Tree Edit Distance (TED) for learning tree transformations in a Q/AP pair. (Wang and Manning, 2010) designed a probabilistic model to learn tree-edit operations on dependency parse trees. (Yao et al., 2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. Yih et al. (2013a) applied enhanced lexical semantics to build a word-alignment model, exploiting a number of large-scale external semantic resources. Although the above approaches are very valuable, they required considerable effort to study, define and implement features that could capture relational representations. In contrast, we are interested in tec"
N16-1152,D07-1003,0,0.309953,"arity score we adopt the approach used by Yu et al. (2014). Our main novelty is the way we model relational information: we inject overlapping words directly into the word embeddings as additional dimensions. The augmented word representation is then passed through the layers of the convolutional feature extractors, which encode the relatedness between Q and AP pairs in a more structured manner. Moreover, the embedding dimensions encoding overlapping words are parameters of the network and are tuned during training. We experiment with two different QA benchmarks for sentence reranking TREC13 (Wang et al., 2007) and WikiQA (Yang et al., 2015). We compare CTKs and CNNs and then we also combine them. For this purpose, we design a new kernel that sum together CTKs and different embeddings extracted from different CNN layers. Our CTK-based models achieve the state of the art on TREC 13, obtaining an MRR of 85.53 and an MAP of 75.18 largely outperforming all the previous best results. On WikiQA, our CNNs perform almost on par with tree kernels, i.e., an MRR of 71.07 vs. 72.51 of CTK, which again is the current state of the art on such data. The combination between CTK and CNNs produces a further boost, ac"
N16-1152,D15-1237,0,0.29979,"Missing"
N16-1152,N13-1106,0,0.315325,"Missing"
N16-1152,P13-1171,0,0.0912738,"advanced QA pipeline based on deep linguistic processing and semantic resources. Wang et al. (2007) used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) applied Tree Edit Distance (TED) for learning tree transformations in a Q/AP pair. (Wang and Manning, 2010) designed a probabilistic model to learn tree-edit operations on dependency parse trees. (Yao et al., 2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. Yih et al. (2013a) applied enhanced lexical semantics to build a word-alignment model, exploiting a number of large-scale external semantic resources. Although the above approaches are very valuable, they required considerable effort to study, define and implement features that could capture relational representations. In contrast, we are interested in techniques that try to automatize the feature engineering step. In this respect, our work (Moschitti et al., 2007) is the first using CTKs applied to syntactic and semantic structural representations of the Q/AP pairs in a learning to rank algorithm based on SV"
N16-1152,P14-2105,0,0.192169,"al., 2013; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). The main difference with our previous approaches is usage of better-preprocessing algorithms and new structural representations, which highly outperform them. Recently, deep learning approaches have been successfully applied to various sentence classification tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014), and for automatically modeling text pairs, e.g., (Lu and Li, 2013; Hu et al., 2014). Additionally, a number of deep learning models have been recently applied to question answering, e.g., Yih et al. (2014) applied CNNs to open-domain QA; Bordes et al. (2014b) propose a neural embedding model Figure 1: Shallow chunk-based tree for the Q/AP pair in the running example. combined with the knowledge base for open-domain QA; Iyyer et al. (2014) applied recursive neural networks to factoid QA over paragraphs. (Miao et al., 2015) proposed a neural variational inference model and a Long-short Term Memory network for the same task. Recently (Yin et al., 2015) proposed a siamese convolutional network for matching sentences that employ an attentive average pooling mechanism, obtaining state-of-the-art resu"
N16-1152,P06-1051,1,0.667622,"rgely outperforming all the previous best results. On WikiQA, our CNNs perform almost on par with tree kernels, i.e., an MRR of 71.07 vs. 72.51 of CTK, which again is the current state of the art on such data. The combination between CTK and CNNs produces a further boost, achieving an MRR of 75.52 and an MAP of 73.99, confirming that the research line of combining these two interesting machine learning methods is very promising. 2 Related Work Relational learning from entire pieces of text concerns several natural language processing tasks, e.g., 1269 QA (Moschitti, 2008), Textual Entailment (Zanzotto and Moschitti, 2006) and Paraphrase Identification (Filice et al., 2015). Regarding QA, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. Wang et al. (2007) used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) applied Tree Edit Distance (TED) for learning tree transformations in a Q/AP pair. (Wang and Manning, 2010) designed a probabilistic model to learn tree-edit operations on dependency pars"
N16-1152,Q16-1019,0,\N,Missing
N16-1153,P13-4021,0,0.0197328,"Missing"
N16-1153,D15-1075,0,0.0326755,"summary of the noisy body, and the encoder-decoder model is trained to act as a denoising auto-encoder. Moreover, training a decoder for the title (rather than the body) is also much faster since titles tend to be short (around 10 words). The encoders pre-trained in this manner are subsequently fine-tuned according to the discriminative criterion described already in Section 3. 5 LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2015; Bowman et al., 2015; Rockt¨aschel et al., 2016). Their success can be attributed to neural gates that adaptively read or discard information to/from internal memory states. Specifically, a LSTM network successively reads the input token xt , internal state ct−1 , as well as the visible state ht−1 , and generates the new states c t , ht : it = σ(Wi xt + Ui ht−1 + bi ) ft = σ(Wf xt + Uf ht−1 + bf ) ot = σ(Wo xt + Uo ht−1 + bo ) zt = tanh(Wz xt + Uz ht−1 + bz ) ct = it zt + ft ct−1 ht = ot tanh(ct ) where i, f and o are input, forget and output gates, respectively. Given the visible state sequence {hi }li=1 , we ca"
N16-1153,W14-4012,0,0.00502985,"Missing"
N16-1153,D14-1179,0,0.0156321,"Missing"
N16-1153,P15-2114,0,0.281941,"al., 2016). Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions. They define each question as a distribution which generates each word (embedding) independently, and subsequently use a Fisher kernel to assess question similarities. Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bagof-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations. Further, we propose a training paradigm that utilizes the entire corpus of unannotated questions in a semi-supervised manner. Recent work on answer selection on community QA forums, simila"
N16-1153,P08-1019,0,0.0102075,"Missing"
N16-1153,D14-1002,0,0.00488573,"mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbuntu dataset. and associated filters map local chunks (win"
N16-1153,D13-1176,0,0.0134402,"r to focus temporal averaging in these models on key pieces of the questions. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and"
N16-1153,P14-1062,0,0.00406558,"e also train three alternative benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exch"
N16-1153,D14-1181,0,0.0115394,"ive benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbu"
N16-1153,D15-1180,1,0.218935,"Missing"
N16-1153,P11-1143,0,0.02256,"Missing"
N16-1153,S15-2047,1,0.46169,"Missing"
N16-1153,S16-1083,1,0.108575,"Missing"
N16-1153,D15-1044,0,0.015101,"s. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and hyper parameter tuning and another for testing. We evaluate our"
N16-1153,P15-2116,0,0.0315029,"Missing"
N16-1153,P15-1025,0,0.0509211,"Missing"
N18-1070,N18-2004,1,0.808385,"ores into a single snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 20"
N18-1070,D17-1317,0,0.0837272,"its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on t"
N18-1070,gencheva-etal-2017-context,1,0.834798,"aluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to so"
N18-1070,N18-5006,1,0.775642,"and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition,"
N18-1070,P16-1044,0,0.326197,"n. T imeDistributed(LST M ) (X, W, E) −−−−−−−−−−−−−−−−→ {m1 , ..., mn } (2) 768 Figure 2: The architecture of our memory network model for stance detection. Furthermore, we convert each input claim s into its representation using the corresponding LSTM and CNN networks as follows: where mj is the LSTM representation of xj , and TimeDistributed() indicates a wrapper that enables training the LSTM over all pieces of evidence by applying the same LSTM model to each time-step of a 3D input tensor, i.e., (X, W, E). While LSTM networks were designed to effectively capture and memorize their inputs (Tan et al., 2016), Convolutional Neural Networks (CNNs) emphasize the local interaction between the individual words in the input word sequence, which is important for obtaining an effective representation. Here, we use a CNN in order to encode each xj into its representation cj as shown below (see line 13 in Table 1). LST M,CN N s −−−−−−−−→ slstm , scnn (4) where slstm and scnn are the representations of s computed using LST M and CN N networks, respectively. Note that these are separate networks with different parameters from those used to encode the pieces of evidence. Lines 10–14 of Table 1 describe the ab"
N18-1070,karadzhov-etal-2017-fully,1,0.797992,"be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model"
N18-1070,N18-1074,0,0.0508096,"snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers t"
N18-1070,K15-1032,1,0.705898,"xtension of the general architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stan"
N18-1070,W14-2508,0,0.13385,"s we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have d"
N18-1070,P16-2065,1,0.613587,"architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stand-alone task. The task aims"
N18-1070,S16-1074,0,0.0172524,"ylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the"
N18-1070,S16-1003,0,0.260879,"Missing"
N18-1070,C16-1230,0,0.034862,"7; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the stance detection problem (Baird et a"
N18-1070,D14-1162,0,0.0795601,"Missing"
N18-1070,N09-2040,0,\N,Missing
N18-2004,karadzhov-etal-2017-built,1,0.798906,"st of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised"
N18-2004,N09-2040,0,0.0986266,"news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Po"
N18-2004,karadzhov-etal-2017-fully,1,0.855688,"Missing"
N18-2004,D16-1011,0,0.191672,"c.). Despite the interdependency between fact checking and stance detection, research on these two problems has not been previously supported by an integrated corpus. This is a gap we aim to bridge by retrieving documents for each claim and annotating them for stance, thus ensuring a natural distribution of the stance labels. Moreover, in order to be trusted by users, a factchecking system should be able to explain the reasoning that led to its decisions. This is best supported by showing extracts (such as sentences or phrases) from the retrieved documents that illustrate the detected stance (Lei et al., 2016). Unfortunately, existing datasets do not offer manual annotation of sentence- or phrase-level supporting evidence. While deep neural networks with attention mechanisms can infer and extract such evidence automatically in an unsupervised way (Parikh et al., 2016), potentially better results can be achieved when having the target sentence provided in advance, which enables supervised or semi-supervised training of the attention. This would allow not only more reliable evidence extraction, but also better stance prediction, and ultimately better factuality prediction. Following this idea, our co"
N18-2004,gencheva-etal-2017-context,1,0.869644,"Missing"
N18-2004,P15-2085,0,0.0248319,"unifies stance detection, stance rationale, relevant document retrieval, and fact checking. This is the first corpus to offer such a combination, not only for Arabic but in general. We further demonstrated experimentally that these unified annotations, and the gold rationales in particular, are beneficial both for stance detection and for fact checking. In future work, we plan to extend the annotations to cover other important aspects of fact checking such as source reliability, language style, and temporal information, which have been shown useful in previous research (Castillo et al., 2011; Lukasik et al., 2015; Ma et al., 2016; Mukherjee and Weikum, 2015; Popat et al., 2017). Overall, the above experiments demonstrate that having a gold rationale can enable better learning. However, the results should be considered as a kind of upper bound on the expected performance improvement, since here we used gold rationales at test time, which would not be available in a real-world scenario. Still, we believe that sizable improvements would still be possible when using the gold rationales for training only. Acknowledgment This research was carried out in collaboration between the MIT Computer Science and Art"
N18-2004,W01-0515,0,0.0674273,"as in (Karadzhov et al., 2017b), we transformed each claim into sub-queries by selecting named entities, adjectives, nouns and verbs with the highest TF.DF score, calculated on a collection of documents from the claims’ sources. Then, we used these sub-queries with the claim itself as input to the search API and retrieved the first 20 returned links, from which we excluded those directing to V ERIFY and R EUTERS, and social media websites that are mostly opinionated. Finally, we calculated two similarity measures between the links’ content (documents) and the claims: the tri-gram containment (Lyon et al., 2001) and the cosine distance between average word embeddings of both texts.6 . We only kept documents with non-zero values for both measures, yielding 3,042 documents: 1,239 for false claims and 1,803 for true claims. (2a) (original false claim) FIFA intends to investigate the game between Syria and Australia.  JË@ Ð Q ª K A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j AJË@Q @ð AK Pñ (2b) (corrected claim in V ERIFY) FIFA does not intend to investigate the game between Syria and Australia, as pro-regime pages claim.  JË@ Ð Q ª K B A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j    j® ú«Y K A"
N18-2004,K15-1032,1,0.76343,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,N18-5006,1,0.869083,"Missing"
N18-2004,R15-1058,1,0.833945,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,P16-2065,1,0.789602,"ed by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 -"
N18-2004,D16-1264,0,0.0505697,"traction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detection or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels"
N18-2004,S16-1003,0,0.170265,"Missing"
N18-2004,D17-1317,0,0.384567,"t checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detect"
N18-2004,N18-1070,1,0.860201,"Missing"
N18-2004,D16-1244,0,0.101792,"Missing"
N18-2004,N18-1074,0,0.0626421,"tion or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection betwe"
N18-2004,W14-2508,0,0.248156,"ality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in pract"
N18-2004,pasha-etal-2014-madamira,0,0.0837142,"Missing"
N18-2004,P17-2067,0,0.173053,"ext analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016).1 As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from English, and focusing on US politics.2 In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly available Arabic fact-checking dataset. Stance detection has been studied so far disjointly from fact checking. While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-"
N19-1183,S16-1138,1,0.900317,"Missing"
N19-1183,W02-1001,0,0.644144,"Missing"
N19-1183,Q18-1018,0,0.158677,") of the above model as features in SVM and the structured output models: ψ(q, d) = [φ(q), φ(d)]. Evaluation metrics We report Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Prec@1 (P@1). 5.2 0 LSP-AP Comparative analysis on WikiQA dataset We first report the results using standard similarity features in all of our models. Then, we show the outcome of our models when fed with embeddings produced by the CNN. 5.2.1 Results with textual similarity features In Tab. 1, we provide the results of our latent structural approaches optimizing MAP in comparison Reimers and Gurevych (2017); Crane (2018). Thus, we rely on the model that is easily replicable and achieves competitive results. to SVM, LSP, and SVMmap models on WikiQA dataset. LSSVM-AP and LSP-AP are better than the SVM classifier baseline by roughly 8 and 10 points, respectively, in terms of MAP metric on the test set. It should be noted that SVM uses kernels, while LSSVM and LSP are simple linear models. Moreover, for SVM, we also had to limit the number of candidates to 10 for each query to make the positive/negative example rate more balanced. The baseline LSP model (without augmented loss optimization) performs surprisingly"
N19-1183,N10-1145,0,0.0349684,"s formulation. This is done for the case when a ranking is represented by aggregating pairwise outputs between all the relevant and all the irrelevant items in the rank. Our approach is an alternative to this technique, providing an approximate max-violating inference with respect to AP for a more general case of the ranking represenation. Passage Reranking Most representative pre-neural networks work for answer selection/passage reranking is from Wang et al. (2007), who used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. Heilman and Smith (2010) and Wang and Manning (2010) applied Tree Edit Distance (TED) to learn the match between question and passage. Yao et al. (2013) applied linear chain CRFs with features derived from TED. Yih et al. (2013) used lexical semantics to build a word-alignment model. Most recently, Deep Neural Networks (DNNs) have shown to be more competitive. DNN can learn relational patterns between a question (Q) and its passage (P) in a variety of ways, e.g., (i) by using a Q-to-P transformation matrix and simple Q-to-P similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) by relying on RNN and"
N19-1183,D17-1035,0,0.0283832,"CNN embeddings) from stage (i) of the above model as features in SVM and the structured output models: ψ(q, d) = [φ(q), φ(d)]. Evaluation metrics We report Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Prec@1 (P@1). 5.2 0 LSP-AP Comparative analysis on WikiQA dataset We first report the results using standard similarity features in all of our models. Then, we show the outcome of our models when fed with embeddings produced by the CNN. 5.2.1 Results with textual similarity features In Tab. 1, we provide the results of our latent structural approaches optimizing MAP in comparison Reimers and Gurevych (2017); Crane (2018). Thus, we rely on the model that is easily replicable and achieves competitive results. to SVM, LSP, and SVMmap models on WikiQA dataset. LSSVM-AP and LSP-AP are better than the SVM classifier baseline by roughly 8 and 10 points, respectively, in terms of MAP metric on the test set. It should be noted that SVM uses kernels, while LSSVM and LSP are simple linear models. Moreover, for SVM, we also had to limit the number of candidates to 10 for each query to make the positive/negative example rate more balanced. The baseline LSP model (without augmented loss optimization) performs"
N19-1183,D17-1122,0,0.0271736,"Missing"
N19-1183,D17-1093,1,0.92027,"t al., 2007) – a structural SVM approach affording exact max-violating inference with respect to AP. Features In our study, we use two feature settings: (i) simple textual similarity features – the setting by Barr´on-Cede˜no et al. (2016), i.e., cosine similarity over the text pair, the similarity based on the PTK score, longest common substring/subsequence measure, Jaccard similarity, word containment measure, greedy string tiling, ESA similarity based on Explicit Semantic Analysis (ESA), and (ii) powerful features coming from the embeddings trained with the state-of-the-art neural networks (Tymoshenko et al., 2017). Parametrization We use the following weighting schema for the ranking structures: vj = 1j , in LSP, LSP-AP, and LSSVM-AP. LSP-AP requires specifying a loss scaling parameter C. In LSSVM and SVMmap , C is the standard trade-off between regularization and training error. In all the three models, we select C on dev. set from the values {1, 10, 100, 1000, 2000, 5000}. The max number of epochs, T , is set to 100, for both LSP and LSPAP. We apply weight averaging in the LSP models. We derive the best number, Tbest , with respect to the MAP score on dev. set. The baseline SVM is trained with polyno"
N19-1183,P16-1122,0,0.0191189,"r chain CRFs with features derived from TED. Yih et al. (2013) used lexical semantics to build a word-alignment model. Most recently, Deep Neural Networks (DNNs) have shown to be more competitive. DNN can learn relational patterns between a question (Q) and its passage (P) in a variety of ways, e.g., (i) by using a Q-to-P transformation matrix and simple Q-to-P similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) by relying on RNN and LSTM architectures (Wang and Nyberg, 2015; Shen et al., 2017), (iii) by employing attention components (Yin et al., 2016; Shen et al., 2017; Wang et al., 2016a), (iv) by decomposing input into similarity and dissimilarity matches (Wang et al., 2016b) or (v) by comparing-aggregating matching results (Wang and Jiang, 2017; Bian et al., 2017). Since our baselines, SVM and SVMmap , as well as our proposed models, LSP and LSSVM, do not apply such transformations, they may perform lower than the state of the art. Thus, we will use the embedding vectors generated by a CNN to show that they can achieve the state-of-the-art accuracy. 3 Structured prediction for ranking In this section, we provide the task formulation with an introduction of structured predi"
N19-1183,P15-2116,0,0.0408145,"Missing"
N19-1183,C10-1131,0,0.0310881,"or the case when a ranking is represented by aggregating pairwise outputs between all the relevant and all the irrelevant items in the rank. Our approach is an alternative to this technique, providing an approximate max-violating inference with respect to AP for a more general case of the ranking represenation. Passage Reranking Most representative pre-neural networks work for answer selection/passage reranking is from Wang et al. (2007), who used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. Heilman and Smith (2010) and Wang and Manning (2010) applied Tree Edit Distance (TED) to learn the match between question and passage. Yao et al. (2013) applied linear chain CRFs with features derived from TED. Yih et al. (2013) used lexical semantics to build a word-alignment model. Most recently, Deep Neural Networks (DNNs) have shown to be more competitive. DNN can learn relational patterns between a question (Q) and its passage (P) in a variety of ways, e.g., (i) by using a Q-to-P transformation matrix and simple Q-to-P similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) by relying on RNN and LSTM architectures (Wang an"
N19-1183,D07-1003,0,0.207153,"AP and propose an approximate method for inference of the max-violating constraint with respect to it. More 1847 Proceedings of NAACL-HLT 2019, pages 1847–1857 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics specifically, we provide two structured output approaches optimizing the MAP metric based on Latent Structured Perceptron (LSP) (Sun et al., 2009; Fernandes et al., 2014) and Latent Structural SVM (LSSVM) (Yu and Joachims, 2009) algorithms. We compare LSP and LSSVM using our MAP optimization strategy on WikiQA (Yang et al., 2015) and TREC13 (Wang et al., 2007) datasets against an SVM classifier and SVMmap – the structural approach of Yue et al. (2007). All the models use state-of-the-art traditional feature vectors for the task. Our experiments on WikiQA dataset show a large improvement of our structural approaches over the SVM baseline, i.e., more than 7 absolute points in MAP, MRR and Precision@1. However, we acknowledge the fact that neural models can produce better representations, which can lead to a superior performance. Thus, to collocate our results in a more general setting, we also carried out experiments using the embeddings of questions"
N19-1183,C16-1127,0,0.0135695,"r chain CRFs with features derived from TED. Yih et al. (2013) used lexical semantics to build a word-alignment model. Most recently, Deep Neural Networks (DNNs) have shown to be more competitive. DNN can learn relational patterns between a question (Q) and its passage (P) in a variety of ways, e.g., (i) by using a Q-to-P transformation matrix and simple Q-to-P similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) by relying on RNN and LSTM architectures (Wang and Nyberg, 2015; Shen et al., 2017), (iii) by employing attention components (Yin et al., 2016; Shen et al., 2017; Wang et al., 2016a), (iv) by decomposing input into similarity and dissimilarity matches (Wang et al., 2016b) or (v) by comparing-aggregating matching results (Wang and Jiang, 2017; Bian et al., 2017). Since our baselines, SVM and SVMmap , as well as our proposed models, LSP and LSSVM, do not apply such transformations, they may perform lower than the state of the art. Thus, we will use the embedding vectors generated by a CNN to show that they can achieve the state-of-the-art accuracy. 3 Structured prediction for ranking In this section, we provide the task formulation with an introduction of structured predi"
N19-1183,D15-1237,0,0.041098,"Missing"
N19-1183,N13-1106,0,0.0365666,"Missing"
N19-1183,P13-1171,0,0.0190655,"chnique, providing an approximate max-violating inference with respect to AP for a more general case of the ranking represenation. Passage Reranking Most representative pre-neural networks work for answer selection/passage reranking is from Wang et al. (2007), who used quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. Heilman and Smith (2010) and Wang and Manning (2010) applied Tree Edit Distance (TED) to learn the match between question and passage. Yao et al. (2013) applied linear chain CRFs with features derived from TED. Yih et al. (2013) used lexical semantics to build a word-alignment model. Most recently, Deep Neural Networks (DNNs) have shown to be more competitive. DNN can learn relational patterns between a question (Q) and its passage (P) in a variety of ways, e.g., (i) by using a Q-to-P transformation matrix and simple Q-to-P similarity features (Yu et al., 2014; Severyn and Moschitti, 2015), (ii) by relying on RNN and LSTM architectures (Wang and Nyberg, 2015; Shen et al., 2017), (iii) by employing attention components (Yin et al., 2016; Shen et al., 2017; Wang et al., 2016a), (iv) by decomposing input into similarity"
N19-1183,Q16-1019,0,0.0343707,"Missing"
P04-1043,P02-1034,0,0.774116,"of the predicate buckle, i.e. Fbuckle . Note that SCFs are features for predicates, (i.e. they describe predicates) whereas PAF characterizes predicate/argument pairs. Once semantic representations are defined, we need to design a kernel function to estimate the similarity between our objects. As suggested in Section 2 we can map them into vectors in <n and evaluate implicitly the scalar product among them. Predicate/Argument structure Kernel (PAK) Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in (Collins and Duffy, 2002). We divide our mapping φ in two steps: (1) from the semantic structure space F (i.e. PAF or SCF objects) to the set of all their possible sub-structures talk delivers D VP NP V talk N D N a talk VP NP a N a NP N delivers D N D VP V NP V NP N delivers delivers D N talk Figure 4: All 17 valid fragments of the semantic structure associated with Arg 1 of Figure 2. 0 0 0 F 0 = {f1 , .., f|F 0 |} and (2) from F 0 to <|F |. An example of features in F 0 is given in Figure 4 where the whole set of frag0 ments, Fdeliver,Arg1 , of the argument structure Fdeliver,Arg1 , is shown (see also Figure 2). It"
P04-1043,P97-1003,0,0.0463757,"Missing"
P04-1043,J02-3001,0,0.939782,"igure 1 shows an example of a predicate annotation in PropBank for the sentence: ""Paul gives a lecture in Rome"". A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example. FrameNet also describes predicate/argument structures but for this purpose it uses richer N lecture in Rome Arg. 1 1 IN Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation. On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features. In this paper, we select portions of syntactic trees, which include predicate/argument salient sub-structures, to define convolution kernels for the task of predicate argument classification. In particular, our kernels aim to (a) represent the re"
P04-1043,P02-1031,0,0.069221,"a predicate annotation in PropBank for the sentence: ""Paul gives a lecture in Rome"". A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example. FrameNet also describes predicate/argument structures but for this purpose it uses richer N lecture in Rome Arg. 1 1 IN Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation. On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features. In this paper, we select portions of syntactic trees, which include predicate/argument salient sub-structures, to define convolution kernels for the task of predicate argument classification. In particular, our kernels aim to (a) represent the relation between predicate"
P04-1043,kingsbury-palmer-2002-treebank,0,0.0147153,"g. (Jackendoff, 1990) claim that semantic information in natural language texts is connected to syntactic structures. Hence, to deal with natural language semantics, the learning algorithm should be able to represent and process structured data. The classical solution adopted for such tasks is to convert syntax structures into flat feature representations which are suitable for a given learning model. The main drawback is that structures may not be properly represented by flat features. In particular, these problems affect the processing of predicate argument structures annotated in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982). Figure 1 shows an example of a predicate annotation in PropBank for the sentence: ""Paul gives a lecture in Rome"". A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example. FrameNet also describes predicate/argument structures but for this purpose it uses richer N lecture in Rome Arg. 1 1 IN Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for argument identification and clas"
P04-1043,J93-2004,0,0.0318315,"Missing"
P04-1043,W04-2403,1,0.288873,"Missing"
P04-1043,N04-1030,0,0.10659,"Missing"
P04-1043,P03-1002,0,0.471819,"n PropBank for the sentence: ""Paul gives a lecture in Rome"". A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example. FrameNet also describes predicate/argument structures but for this purpose it uses richer N lecture in Rome Arg. 1 1 IN Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation. On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features. In this paper, we select portions of syntactic trees, which include predicate/argument salient sub-structures, to define convolution kernels for the task of predicate argument classification. In particular, our kernels aim to (a) represent the relation between predicate and one of its argument"
P06-1051,H05-1079,0,0.0520553,"of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends. A second class of methods can give a solution to the previous problem. These methods generally combine a similarity measure with a set of possible transformations T applied over syntactic and semantic interpretations. The entailment between T and H is detected when there is a transformation r ∈ T so that sim(r(T ), H) &gt; α. These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowed rewrite rules in (de Salvo Braz et al., 2005). The disadvantage is that such rules have to be manually designed. Moreover, they generally model better positive implications than negative ones and they do not consider errors in syntactic parsing and semantic analysis. To consider structural and lexical relation similarity, we augment syntactic trees with placeholders which identify linked words. More in detail: - We detect links between words wt in T that are equal, similar, or semantically dependent on words wh in H. We call anchors the pairs (wt , wh ) and we associat"
P06-1051,A00-2018,0,0.00616774,"Missing"
P06-1051,P02-1034,0,0.122088,"cturally similar to T 00 and H 00 , respectively and (2) the lexical relations within the pair (T 0 , H 0 ) are compatible with those in (T 00 , H 00 ). Typically, T and H show a certain degree of overlapping, thus, lexical relations (e.g., between the same words) determine word movements from T to H (or vice versa). This is important to model the syntactic/lexical similarity between example pairs. Indeed, if we encode such movements in the syntactic parse trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in (Collins and Duffy, 2002). 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is the"
P06-1051,W05-1203,0,0.322917,"se trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in (Collins and Duffy, 2002). 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is then in entailment when sim(T, H) &gt; α. These approaches can hardly determine whether the entailment holds in the examples of the previous section. From the point of view of bag-of-word methods, the pairs (T1 , H1 ) and (T1 , H2 ) have both the same intra-pair similarity since the sentences of T1 and H1 as well as those of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such"
P06-1051,C92-2082,0,0.0290809,"Missing"
P06-1051,O97-1002,0,0.0203333,"Missing"
P06-1051,E06-1015,1,0.509203,"2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw , lw0 ) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf ). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT , in SVMlight (Joachims, 1999). We used such software to implement Ks (Eq. 6), K1 , K2 (Eq. 5) and Ks + Ki kernels. The latter combines our new kernel with traditional approaches (i ∈ {1, 2}). Only the bold part of T supports the implication; the rest is useless and also misleading: if we used it to compute the similarity it would reduce the importance of the relevant part. Moreover, as we normalize the syntactic tree kernel (KT ) with respect to the size of the two trees, we need to focus only on the part relevant to the implication. The ancho"
P06-1051,N04-3012,0,0.0908262,"Missing"
P06-1051,W05-1206,0,0.0134069,"e have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art Recently, textual entailment recognition has been receiving a lot of attention. The main reason is that the understanding of the basic entailment processes will allow us to model more accurate semantic theories of natural languages (Chierchia and McConnell-Ginet, 2001) and design important applications (Dagan and Glickman, 2004), e.g., Question Answering and Information Extraction. However, previous work (e.g., (Zaenen et al., 2005)) suggests that determining whether or not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1 : “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1 : “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2 : “At the end of the year, all solid companies pay cash dividends.” Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similari"
P06-1117,kingsbury-palmer-2002-treebank,0,0.063169,"Missing"
P06-1117,W04-0803,0,0.0813086,"ecreases the amount of annotated examples needed in training (i.e. frame usage improves the generalization ability of the learning algorithm). On the other hand, the results obtained without the frame information are very poor. These results show that having broader frame coverage is very important for robust semantic parsing. Unfortunately, the 321 frames that contain at least one verb predicate cover only a small fraction of the English verb lexicon and of the possible domains. Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details). Our approach for solving such problems involves the usage of a frame-like feature, namely the Intersective Levin class (ILC). We show that the ILC can replace the frame with almost no loss in performance. At the same time, ILC provides better coverage as it can be learned also from other chines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification. Tree kernels were applied to the syntactic trees that encode the subcategorization structures of verbs. This mea"
P06-1117,J01-3003,0,0.020219,"d a Levin class is that they share the same participant roles. As FN is annotated with frame-specific semantic roles, we manually mapped these roles into the VN set of thematic roles. Given a frame, we assigned thematic roles to all frame elements that are associated with verbal predicates. For example the Speaker, Addressee, Message and Topic roles from the Telling frame were respectively mapped into the Agent, Recipient, Theme and Topic theta roles. Second, we build a frequency distribution of VN thematic roles on different syntactic positions. Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each ILC has a distinct frequency distribution of roles on different grammatical slots. As we do not have matching grammatical functions in FN and VN, we approximate that subjects and direct objects are more likely to appear on positions adjacent to the predicate, while indirect objects appear on more distant positions. The same intuition is successfully used by G&J to design the Position feature. For each thematic role θi we acquired from VN and FN data the frequencies with which θi appears on an adjacent A or distant D positions in a given frame or VN class (i.e. #hθi , clas"
P06-1117,P04-1043,1,0.852012,"t available. A solution to this problem is the automatic frame detection. Unfortunately, our preliminary experiments showed that given a FrameNet (FN) predicate-argument structure, the task of identifying the associated frame can be performed with very good results when the verb predicates have enough training examples, but becomes very challenging otherwise. The predicates belonging to new application domains (i.e. not yet included in FN) are especially problematic since there is no training data available. Therefore, we should rely on a semantic context alternative to the frame (Giuglea and Moschitti, 2004). Such context should have a wide coverage and should be easily derivable from FN data. A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000). In this paper we have investigated the above claim by designing a semi-automatic algorithm that assigns ILCs to FN verb predicates and by carrying out several semantic role labeling (SRL) experiments in which we replace the frame with the ILC information. We used support vector maThis article describes a robust semanti"
P06-1117,W05-0620,0,0.0449195,"Missing"
P06-1117,A00-2018,0,0.177522,"Missing"
P06-1117,P98-1046,0,0.0963234,"cture, the task of identifying the associated frame can be performed with very good results when the verb predicates have enough training examples, but becomes very challenging otherwise. The predicates belonging to new application domains (i.e. not yet included in FN) are especially problematic since there is no training data available. Therefore, we should rely on a semantic context alternative to the frame (Giuglea and Moschitti, 2004). Such context should have a wide coverage and should be easily derivable from FN data. A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000). In this paper we have investigated the above claim by designing a semi-automatic algorithm that assigns ILCs to FN verb predicates and by carrying out several semantic role labeling (SRL) experiments in which we replace the frame with the ILC information. We used support vector maThis article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated"
P06-1117,W04-3212,0,0.0459247,"Missing"
P06-1117,J02-3001,0,0.159398,"ciated hierarchy contains an extensive semantic analysis of verbs, nouns, adjectives and situations in which they are used, called frames. The basic assumption on which the frames are built is that each word evokes a particular situation with specific participants (Fillmore, 1968). The word that evokes a particular frame is called target word or predicate and can be an adjective, noun or verb. The participant entities are defined using semantic roles and they are called frame elements. Several models have been developed for the automatic detection of the frame elements based on the FN corpus (Gildea and Jurafsky, 2002; Thompson et al., 2003; Litkowski, 2004). While the algorithms used vary, almost all the previous studies divide the task into: 1) the identification of the verb arguments to be labeled and 2) the tagging of each argument with a role. Also, most of the models agree on the core features as be930 Levin documented 79 alternations which constitute the building blocks for the verb classes. Although alternations are chosen as the primary means for identifying the classes, additional properties related to subcategorization, morphology and extended meanings of verbs are taken into account as well. Th"
P06-1117,C98-1046,0,\N,Missing
P07-1098,P02-1034,0,0.930157,"various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages 776 from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We ar"
P07-1098,kingsbury-palmer-2002-treebank,0,0.643429,"achine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We argue that semantic structures can be used to characterize the relation between a question and a candidate answer. In this paper, we extensively study new structural representations, encoding parse trees, bag-of-words, POS tags and predicate argument structures (PASs) for question classification and answer re-ranking. We define new tree representations for both simple and nested PASs, i.e. PASs whose arguments are other predicates (Section 2). Moreover, we define new kernel functions to exploit PASs, which we automatically derive"
P07-1098,W05-0630,1,0.767863,"Missing"
P07-1098,P06-1136,0,\N,Missing
P08-1091,P98-1013,0,0.0162825,"Missing"
P08-1091,W05-0620,0,0.235943,"Missing"
P08-1091,W03-1006,0,0.0357672,"ts in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support"
P08-1091,P02-1034,0,0.023017,"us to save a lot of time in the design and implementation of features. The basic idea is (a) to design a set of basic value-attribute features and apply polynomial kernels and generate all possible combinations; or (b) to design basic tree structures expressing properties related to the target linguistic objects and use tree kernels to generate all possible tree subparts, which will constitute the feature representation vectors for the learning algorithm. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures (Collins and Duffy, 2002). For example, Figure 2, shows a small parse tree and some of its fragments. To design a function which computes the number of common substructures between two trees t1 and t2 , let us define the set of fragments F={f1 , f2 , ..} and the indicator function Ii (n), equal to 1 if the target fi is rooted at node n and 0 otherwise. A tree kernel function KT (·) over two trees is defined as: 802 VP VBD  @YK. NP NP NN  KP NP NP NNP NNP ð P ú m.' ðP úæJ Ë@ Z@P PñË@ NN JJ Figure 3: Example of the positive AST structured feature encoding the argument ARG0 in the sentence depicted in Figure 1. P P"
P08-1091,S07-1017,1,0.883154,"Missing"
P08-1091,2007.mtsummit-papers.20,1,0.75554,"st for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and argument classification. It is trained and tested using the pilot Arabic Propbank data released as part of the SemEval 2007 data. Given the lack of a reliable Arabic deep syntactic parser, we 1 2 http://nlp.cs.swarthmore.edu/semeval/ We"
P08-1091,erk-pado-2006-shalmaneser,0,0.0180766,"ea applies to passive constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on t"
P08-1091,P02-1031,0,0.0280664,"oles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised m"
P08-1091,P05-1071,0,0.031638,"ic language as expressed through morphology. Hence, we explicitly encode new SRL features that capture the richness of Arabic morphology and its role in morpho-syntactic behavior. The set of morphological attributes include: inflectional morphology such as Number, Gender, Definiteness, Mood, Case, Person; derivational morphology such as the Lemma form of the words with all the diacritics explicitly marked; vowelized and fully diacritized form of the surface form; the English gloss6 . It is worth noting that there exists highly accurate morphological taggers for Arabic such as the MADA system (Habash and Rambow, 2005; Roth et al., 2008). MADA tags 6 The gloss is not sense disambiguated, hence they include homonyms. Feature Name Definiteness Number Gender Case Mood Person Lemma Gloss Vocalized word Unvowelized word Description Applies to nominals, values are definite, indefinite or inapplicable Applies to nominals and verbs, values are singular, plural or dual or inapplicable Applies to nominals, values are feminine, masculine or inapplicable Applies to nominals, values are accusative, genitive, nominative or inapplicable Applies to verbs, values are subjunctive, indicative, jussive or inapplicable Applies"
P08-1091,W05-0623,0,0.0215325,"ns such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and argument classification."
P08-1091,maamouri-etal-2006-developing,1,0.898791,"Missing"
P08-1091,W05-0630,1,0.888893,"beyond the previously proposed basic SRL system for Arabic (Diab et al., 2007a; Diab and Moschitti, 2007). We exploit the full morphological potential of the language to verify our hypothesis that taking advantage of the interaction between morphology and syntax can improve on a basic SRL system for morphologically rich languages. Similar to the previous Arabic SRL systems, our adopted SRL models use Support Vector Machines to implement a two step classification approach, i.e. boundary detection and argument classification. Such models have already been investigated in (Pradhan et al., 2005; Moschitti et al., 2005). The two step classification description is as follows. 3.1 Predicate Argument Extraction The extraction of predicative structures is based on the sentence level. Given a sentence, its predicates, as indicated by verbs, have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of the argument words in the sentence, and (b) the classification of the argument type, e.g. Arg0 or ArgM for Propbank S NP VP VP ⇒ VBD VP NNP VBD Mary bought NP bought D N a cat Figure 2: NP VBD VP NP VBD D N D N"
P08-1091,P07-1098,1,0.866381,"same. Hence, for the sentence ‘John opened the door’ and ‘the door opened’, though ‘the door’ is the object of the first sentence and the subject of the second, it is the ‘theme’ in both sentences. Same idea applies to passive constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the"
P08-1091,P04-1043,1,0.937443,"p toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument b"
P08-1091,E06-1015,1,0.886131,"owelized Arabic in the Buckwalter transliteration scheme for deriving the basic features for the AST experimental condition. The data comprises a development set, a test set and a training set of 886, 902 and 8,402 sentences, respectively, where each set contain 1725, 1661 and 21,194 argument instances. These instances are distributed over 26 different role types. The training instances of the boundary detection task also include parse-tree nodes that do not correspond to correct boundaries (we only considered 350K examples). For the experiments, we use SVM-Light-TK toolkit8 (Moschitti, 2004; Moschitti, 2006) and its SVM-Light default parameters. The system performance, i.e. F1 on single boundary and role classifier, accuracy of the role multi-classifier and the F1 of the complete SRL systems, are computed by means of the CoNLL evaluator9 . 4.2 Results Figure 5 reports the F1 of the SVM boundary classifier using Polynomial Kernels with a degree from 1 to 6 (i.e. Polyi), the AST and the EAST kernels and their combinations. We note that as we introduce conjunctions, i.e. a degree larger than 2, the F1 increases by more than 3 percentage points. Thus, not only are the English features meaningful for"
P08-1091,W06-2909,1,0.926303,"Missing"
P08-1091,P08-2030,1,0.812311,"through morphology. Hence, we explicitly encode new SRL features that capture the richness of Arabic morphology and its role in morpho-syntactic behavior. The set of morphological attributes include: inflectional morphology such as Number, Gender, Definiteness, Mood, Case, Person; derivational morphology such as the Lemma form of the words with all the diacritics explicitly marked; vowelized and fully diacritized form of the surface form; the English gloss6 . It is worth noting that there exists highly accurate morphological taggers for Arabic such as the MADA system (Habash and Rambow, 2005; Roth et al., 2008). MADA tags 6 The gloss is not sense disambiguated, hence they include homonyms. Feature Name Definiteness Number Gender Case Mood Person Lemma Gloss Vocalized word Unvowelized word Description Applies to nominals, values are definite, indefinite or inapplicable Applies to nominals and verbs, values are singular, plural or dual or inapplicable Applies to nominals, values are feminine, masculine or inapplicable Applies to nominals, values are accusative, genitive, nominative or inapplicable Applies to verbs, values are subjunctive, indicative, jussive or inapplicable Applies to verbs and pronou"
P08-1091,N04-1032,0,0.0147664,"e constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this pape"
P08-1091,W04-3212,0,0.304138,"g important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and"
P08-1091,C98-1013,0,\N,Missing
P08-1091,J02-3001,0,\N,Missing
P08-2029,P02-1034,0,0.249951,"aracterize anxiety characterize (a) PAS A1 rel PAS A0 A1 (b) rel PAS rel rel A0 characterize characterize characterize (c) Figure 1: Compact PAS-PTK structures of s1 (a) and s2 (b) and some fragments they have in common as produced by the PTK (c). Arguments are replaced with their most important word (or semantic head) to reduce data sparseness. representations: (1) linear kernels on the bag-of-words (BOW) or bag-of-POS-tags (POS) features, (2) the String Kernel (SK) (Shawe-Taylor and Cristianini, 2004) on word sequences (WSK) and POStag sequences (POSSK), (3) the Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) on syntactic parse trees (PTs), (4) the Shallow Semantic Tree Kernel (SSTK) (Moschitti et al., 2007) and the Partial Tree Kernel (PTK) (Moschitti, 2006) on PASs. In particular, POS-tag sequences and PAS trees used with SK and PTK yield to two innovative kernels, i.e. POSSK and PAS-PTK2. In the next sections, we describe in more detail the data structures on which we applied the above kernels. similar way. To take advantage of semantic representations, we work with two types of semantic structures; first, the Word Sequence Kernel applied to both question and answer; given s0 , sample substring"
P08-2029,kingsbury-palmer-2002-treebank,0,0.0558595,"nces and PAS trees used with SK and PTK yield to two innovative kernels, i.e. POSSK and PAS-PTK2. In the next sections, we describe in more detail the data structures on which we applied the above kernels. similar way. To take advantage of semantic representations, we work with two types of semantic structures; first, the Word Sequence Kernel applied to both question and answer; given s0 , sample substrings are: What is autism, What is, What autism, is autism, etc. Then, two PAS-based trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both based on PropBank structures (Kingsbury and Palmer, 2002) are automatically generated by our SRL system (Moschitti et al., 2005). As an example, let us consider an automatically annotated sentence from our TREC-QA corpus: s1 : [A1 Autism] is [rel characterized] [A0 by a broad 2.1 Syntactic Structures The POSSK is obtained by applying the String Kernel on the sequence of POS-tags of a question or a answer. For example, given sentence s0 : What is autism?, the associated POS sequence is WP AUX NN ? and some of the substrings extracted by POSSK are WP NN or WP AUX. A more complete structure is the full parse tree (PT) of the sentence, that constitutes"
P08-2029,C02-1150,0,0.0150201,"on WEB-QA 40 38 36 34 F1-measure 3.1 Experimental Setup In our experiments, we implemented the BOW and POS kernels, WSK, POSSK, STK (on syntactic PTs derived automatically with Charniak’s parser), SSTK and PTK (on PASs derived automatically with our SRL system) as well as their combinations in SVM-light-TK3 . Since answers often contain more than one PAS (see Figure 1), we sum PTK (or SSTK) applied to all pairs P1 × P2 , P1 and P2 being the sets of PASs of the first two answers. The experimental datasets were created by submitting the 138 TREC 2001 test questions labeled as “description” in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. YourQA was run on two sources: Web documents by exploiting Google (code.google.com/ apis/) and the AQUAINT data used for TREC’07 (trec.nist.gov/data/qa) by exploiting Lucene (lucene.apache.org), yielding two different corpora: WEB-QA and TREC-QA. Each sentence of the returned paragraphs was manually evaluated based on whether it contained a correct answer to the corresponding question. To simplify our task, we isolated for each paragraph the sentence with the maximal judgment (such a"
P08-2029,W05-0630,1,0.261491,"e. POSSK and PAS-PTK2. In the next sections, we describe in more detail the data structures on which we applied the above kernels. similar way. To take advantage of semantic representations, we work with two types of semantic structures; first, the Word Sequence Kernel applied to both question and answer; given s0 , sample substrings are: What is autism, What is, What autism, is autism, etc. Then, two PAS-based trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both based on PropBank structures (Kingsbury and Palmer, 2002) are automatically generated by our SRL system (Moschitti et al., 2005). As an example, let us consider an automatically annotated sentence from our TREC-QA corpus: s1 : [A1 Autism] is [rel characterized] [A0 by a broad 2.1 Syntactic Structures The POSSK is obtained by applying the String Kernel on the sequence of POS-tags of a question or a answer. For example, given sentence s0 : What is autism?, the associated POS sequence is WP AUX NN ? and some of the substrings extracted by POSSK are WP NN or WP AUX. A more complete structure is the full parse tree (PT) of the sentence, that constitutes the input of the STK. For instance, the STK accepts the syntactic parse"
P08-2029,P07-1098,1,0.742321,"ocessing, useful information is gathered from the question and a query is created. This is submitted to an IR module, which provides a ranked list of relevant documents. From these, the QA system extracts one or more candidate answers, which can then be re-ranked following various criteria. Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g. (Shen and Lapata, 2007) has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. In (Moschitti et al., 2007), we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs1 in SVMs. 1 in PropBank format, (www.cis.upenn.edu/˜ace). In this paper, similarly to our previous approach, we design an SVM-based answer extractor, that selects the correct answers from those provided by a basic QA system by applying tree kernel technology. However, we also provide: (i) a new kernel to process PASs based on the partial tree kernel algorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very"
P08-2029,D07-1002,0,0.0268081,"n question processing and answer extraction (Chen et al., 2006; CollinsThompson et al., 2004) rather than document retrieval (a step usually carried out by off-the shelf IR engines). In question processing, useful information is gathered from the question and a query is created. This is submitted to an IR module, which provides a ranked list of relevant documents. From these, the QA system extracts one or more candidate answers, which can then be re-ranked following various criteria. Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g. (Shen and Lapata, 2007) has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. In (Moschitti et al., 2007), we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs1 in SVMs. 1 in PropBank format, (www.cis.upenn.edu/˜ace). In this paper, similarly to our previous approach, we design an SVM-based answer extractor, that selects the correct answers from those provided by a basic QA system by applying tree kernel technology. However, we also provide: (i) a new kernel to process PASs based"
P08-2029,P06-1136,0,\N,Missing
P08-4003,P05-1022,0,0.0193699,"as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables c"
P08-4003,N07-1011,0,0.0838311,"Missing"
P08-4003,P05-1045,0,0.0128475,"RT is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A s"
P08-4003,W00-0730,0,0.105306,"grated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE m"
P08-4003,P00-1023,0,0.0981402,"Missing"
P08-4003,E06-1015,1,0.765228,"pFigure 2: Example system configuration ment. The set of feature extractors that the system uses is set in an XML description file, which allows for straightforward prototyping and experimentation with different feature sets. Learning BART provides a generic abstraction layer that maps application-internal representations to a suitable format for several machine learning toolkits: One module exposes the functionality of the the WEKA machine learning toolkit (Witten and Frank, 2005), while others interface to specialized state-of-the art learners. SVMLight (Joachims, 1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. SVM Classification uses a Java Native Interface-based wrapper replacing SVMLight/TK’s svm classify program to improve the classification speed. Also included is a Maximum entropy classifier that is based upon Robert Dodier’s translation of Liu and Nocedal’s (1989) L-BFGS optimization code, with a function for programmatic feature combination.2 Training/Testing The training and testing phases slightly differ from each other. In the training phase, the pairs that are to be used as training examples have to be selected in a process of sample selection"
P08-4003,P06-1055,0,0.0140385,"unking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence class"
P08-4003,N06-1025,1,0.924334,"Missing"
P08-4003,qiu-etal-2004-public,0,0.168384,"Missing"
P08-4003,J01-4004,0,0.934827,"er NLP applications. Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. 1 A number of systems that perform coreference resolution are publicly available, such as G UITAR (Steinberger et al., 2007), which handles the full coreference task, and JAVA RAP (Qiu et al., 2004), which only resolves pronouns. However, literature on coreference resolution, if providing a baseline, usually uses the algorithm and feature set of Soon et al. (2001) for this purpose. Introduction Coreference resolution refers t"
P08-4003,N03-1033,0,0.00525991,"forming qualitative error analysis using integrated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on th"
P08-4003,uryupina-2006-coreference,0,0.0608766,"Missing"
P08-4003,wellner-vilain-2006-leveraging,0,0.0159334,"cognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence classes by the resolution process and a coreference layer is written into the document, which can be used for detailed error analysis. Feature Extraction BART’s default resolver goes through all mentions and looks for p"
P08-4003,P06-1006,1,0.890114,"Missing"
P08-4003,P04-1018,0,\N,Missing
P08-4003,I05-1063,1,\N,Missing
P11-2018,D08-1083,0,0.0176718,"ed to use simple local features. In contrast, in (Johansson and Moschitti, 2010b), we showed that global structure matters: opinions interact to a large extent, and we can learn about their interactions on the opinion level by means of their interactions on the syntactic and semantic levels. It is intuitive that this should also be valid when polarities enter the Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101–106, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics picture – this was also noted by Choi and Cardie (2008). Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities may be conjoined through a contrastive discourse connective such as but. In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by Choi and Cardie (2010). Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence. O"
P11-2018,P10-2050,0,0.571689,"led in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010"
P11-2018,W06-1651,0,0.72509,"The polarity takes the values P OSITIVE, N EUTRAL, N EGATIVE, and B OTH; for compatibility with Choi and Cardie (2010), we mapped B OTH to N EUTRAL. 3 The Baselines In order to test our hypothesis against strong baselines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign"
P11-2018,W02-1001,0,0.0123493,"tems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully dev"
P11-2018,C10-1059,1,0.852824,". Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010"
P11-2018,W10-2910,1,0.742708,"n and structured machine learning. A crucial step in the automatic analysis of opinion is to mark up the opinion expressions: the pieces of 101 text allowing us to infer that someone has a particular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lac"
P11-2018,W04-2705,0,0.0186857,"he [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame 103 Figure 1: Syntactic and shallow semantic structure. The model used the following novel features that take the polarities of the expressions into account. The examples are given with respect to the two expressions (appeasement and terrorists) in Figure 1. Base polarity classifier score. Sum of the"
P11-2018,E09-1066,1,0.886546,"Missing"
P11-2018,J05-1004,0,0.00785167,"t]− emboldened the [terrorists]− The [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame 103 Figure 1: Syntactic and shallow semantic structure. The model used the following novel features that take the polarities of the expressions into account. The examples are given with respect to the two expressions (appeasement and terrorists) in Figure 1. Base pol"
P11-2018,D09-1018,0,0.0179178,"ne by 4 points in intersection F-measure and 7 points in recall. The improvements over Choi and Cardie (2010) ranged between 10 and 15 in overlap F-measure and between 17 and 24 in recall. This is not only of practical value but also confirms our linguistic intuitions that surface phenomena such as syntax and semantic roles are used in encoding the rhetorical organization of the sentence, and that we can thus extract useful information from those structures. This would also suggest that we should leave the surface and instead process the discourse structure, and this has indeed been proposed (Somasundaran et al., 2009). However, automatic discourse structure analysis is still in its infancy while syntactic and shallow semantic parsing are relatively mature. Interesting future work should be devoted to address the use of structural kernels for the proposed reranker. This would allow to better exploit syntactic and shallow semantic structures, e.g. as in (Moschitti, 2008), also applying lexical similarity and syntactic kernels (Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Moschitti, 2009). Acknowledgements The research described in this paper has received funding fro"
P11-2018,H05-1044,0,0.0311429,"nion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguistic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around th"
P11-2018,J09-3003,0,0.0506316,"be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a s"
P11-2018,W08-2123,1,\N,Missing
P11-2048,H05-1091,0,0.15341,"o the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset Creation YAGO YAGO (Suchanek et al., 2007) is a huge semantic knowledge base derived from WordNet and Wikipedia. It comprises more than 2 million entities (like persons, organizations, cities, etc.) and 20 million facts connecting these entities. These include the taxonomic Is-A hierarchy as well as semantic relations between entities. We use the YAGO version of 2008-w40-2 with a manua"
P11-2048,P07-1073,0,0.722017,"Missing"
P11-2048,P04-1054,0,0.0480678,"use YAGO, a large knowledge base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset Creation YAGO YAGO (Suchanek"
P11-2048,doddington-etal-2004-automatic,0,0.135442,"Missing"
P11-2048,P04-1053,0,0.0404612,"rom YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset Creation YAGO YAGO (Suchanek et al., 2007) is a huge semantic knowledge base derived from WordNet and Wikipedia. It comprises more than 2 million entities (like persons, org"
P11-2048,P10-1030,0,0.252651,", they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. 277 The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specifically named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT (x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz et al., 2009) to provide reliable training data. However, only (Hoffmann et al., 2010) used DS to define extractors that are"
P11-2048,P04-3022,0,0.191368,"base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset Creation YAGO YAGO (Suchanek et al., 2007) is"
P11-2048,P09-1113,0,0.239315,"upervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. 277 The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specifically named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT (x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz et al., 2009) to provide reliable training data. However, only (Hoffmann et al., 2010) used DS to de"
P11-2048,P04-1043,1,0.794476,"Missing"
P11-2048,D09-1143,1,0.623402,"ity James Cameron. We use an efficient procedure formally described in Alg. 2.1: for each Wikipedia article in Freebase, we scan all of its NEs. Then, for each pair of entities2 seen in the sentence, we query YAGO to 2 Our algorithm is robust to the lack of knowledge about the existence of any relation between two entities. If the relation 279 Distant Supervised Learning with Kernels We model relation extraction (RE) using state-ofthe-art classifiers based on kernel methods. The main idea is that syntactic/semantic structures are used to represent relation instances. We followed the model in (Nguyen et al., 2009) that has shown significant improvement on the state-of-the-art. This combines a syntactic tree kernel and a polynomial kernel over feature extracted from the entities: CK1 = α · KP + (1 − α) · T K (1) where α is a coefficient to give more or less impact to the polynomial kernel, KP , and T K is the syntactic tree kernel (Collins and Duffy, 2001). The best model combines the advantages of the two parsing paradigms by adding the kernel above with six sequence kernels (described in (Nguyen et al., 2009)). CSK = α · KP + (1 − α) · (T K + X SKi ) (2) i=1,..,6 Such kernels cannot be applied to Wiki"
P11-2048,C10-2104,1,0.877844,"Missing"
P11-2048,W02-1010,0,0.0611479,"istant supervision. We use YAGO, a large knowledge base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset"
P11-2048,I05-1034,0,0.0238469,"onal facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1 Previous work assumes the page related to the Infobox as the only source for the training data. 278 Resources and Dataset Creation YAGO YAGO (Suchanek et al., 2007) is a huge semantic knowledge base derived from WordNet and Wikipedia. It comprises more than 2 million entities (like persons, organizations, cities, etc.) and 20 million facts connecting these entities. These include the taxonomic Is-A hierarchy as well as semantic relations between entities. We use the YAGO version of 2008-w40-2 with a manually confirmed accura"
P11-2048,P06-1104,0,0.0607276,"Missing"
P12-1028,P98-1013,0,0.214058,"t something like in: ... Michelle blabs about it to a sandwich man while ordering lunch over the phone . Introduction Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Ve"
P12-1028,W11-0110,1,0.906954,"in a specific argument position and the above verb order is a clear example. In the direct object position of the example sentence for the first sense 60.1 of order, we found 264 commission in the role PATIENT of the predicate. It clearly satisfies the +A NIMATE/+O RGANIZATION restriction on the PATIENT role. This is not true for the direct object dependency of the alternative sense 13.5.1, which usually expresses the T HEME role, with unrestricted type selection. When properly generalized, the direct object information has thus been shown highly predictive about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the a"
P12-1028,H05-1091,0,0.0169362,"Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 20"
P12-1028,A00-2018,0,0.147273,"Missing"
P12-1028,P02-1034,0,0.405859,"006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Mosch"
P12-1028,P10-1025,1,0.852968,"ity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIEN"
P12-1028,D11-1096,1,0.930454,"tov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexical nodes 265 are just"
P12-1028,W04-3233,0,0.0650439,"Missing"
P12-1028,P08-2008,1,0.873656,"e about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenste"
P12-1028,J02-3001,0,0.0553081,"verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Verb class 13.5.1. For such a case, selectional restrictions are needed. These have also been shown to be useful for semantic role classification (Zapirain et al., 2010). Note that their coding in learning algorithms is rather complex: we need to take into a"
P12-1028,P02-1031,1,0.652564,"(NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Verb class 13.5.1. For such a case, selectional restrictions are needed. These have also been shown to be useful for semantic role classification (Zapirain et al., 2010). Note that their coding in learning algorithms is rather complex: we need to take into account syntactic structures, which may require a"
P12-1028,P06-1117,1,0.924782,"Missing"
P12-1028,P05-1050,0,0.0134613,"to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structu"
P12-1028,W08-2123,0,0.0663246,"Missing"
P12-1028,P03-1004,0,0.046988,"matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classif"
P12-1028,P05-1024,0,0.023694,"ntered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 20"
P12-1028,N10-1146,1,0.878723,"Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexica"
P12-1028,N07-1071,0,0.0210915,"eometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. In supervised language learning, when few examples are available, DMs support cost-effective lexical generalizations, often outperforming knowledge based resources (such as WordNet, as in (Pantel et al., 2007)). Obviously, the choice of the context type determines the type of targeted semantic properties. Wider contexts (e.g., entire documents) are shown to suggest topical relations. Smaller contexts tend to capture more specific semantic aspects, e.g. the syntactic behavior, and better capture paradigmatic relations, such as synonymy. In particular, word space models, as described in (Sahlgren, 2006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of t"
P12-1028,W09-1106,1,0.879694,"Missing"
P12-1028,D09-1012,1,0.868753,"Missing"
P12-1028,W10-2926,1,0.864629,"Missing"
P12-1028,J98-1004,0,0.201143,"a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case o"
P12-1028,W03-1012,0,0.0171574,"the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehd"
P12-1028,W06-2902,0,0.0182858,"get word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 S"
P12-1028,W04-3222,0,0.0290889,"in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b"
P12-1028,C00-2137,0,0.0355617,"Missing"
P12-1028,N10-1058,0,0.0368592,"Missing"
P12-1028,W02-1010,0,0.0339909,"irs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels"
P12-1028,N06-1037,0,0.0976457,"Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new in"
P12-1028,C98-1013,0,\N,Missing
P12-1028,P98-2127,0,\N,Missing
P12-1028,C98-2122,0,\N,Missing
P12-1080,H05-1091,0,0.0501161,"1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al."
P12-1080,P02-1034,0,0.660364,"we need a representation from which the dependencies between the dif760 1 We used the conversion of margin into probability provided by LIBSVM. P MCAT M11 M13 i=1..l yi αi φ(oi )φ(o) M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node"
P12-1080,W04-3233,0,0.0703245,"Missing"
P12-1080,P05-1050,0,0.0333768,"Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, wh"
P12-1080,P03-1004,0,0.0377721,"um number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descende"
P12-1080,P05-1024,0,0.0204355,"ing the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is ("
P12-1080,P10-1052,0,0.0719159,"Missing"
P12-1080,E06-1015,1,0.793324,"from which the dependencies between the dif760 1 We used the conversion of margin into probability provided by LIBSVM. P MCAT M11 M13 i=1..l yi αi φ(oi )φ(o) M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node labels with “-”,"
P12-1080,W03-1012,0,0.369149,"he average running time again tends to be linear for natural language syntactic trees (Moschitti, 2006a). Given a target T , PTK can generate any subset of connected nodes of T , whose edges are in T . For example, Fig. 5 shows the tree fragments from the hypothesis of Fig. 2. Note that each fragment captures dependencies between different categories. 3.3 Preference reranker When training a reranker model, the task of the machine learning algorithm is to learn to select the best candidate from a given set of hypotheses. To use SVMs for training a reranker, we applied Preference Kernel Method (Shen et al., 2003). The reduction method from ranking tasks to binary classification is an active research area; see for instance (Balcan et al., 2008) and (Ailon and Mohri, 2010). Category C152 GPOL M11 .. C31 E41 GCAT .. E31 M14 G15 Total: 103 Train 837 723 604 .. 313 191 345 .. 11 96 5 10,000 Child-free Train1 Train2 370 467 357 366 309 205 .. .. 163 150 89 95 177 168 .. .. 4 7 49 47 4 1 5,000 5,000 TEST 438 380 311 .. 179 102 173 .. 6 58 0 5,000 Train 837 723 604 .. 531 223 3293 .. 32 1175 290 10,000 Child-full Train1 Train2 370 467 357 366 309 205 .. .. 274 257 121 102 1687 1506 .. .. 21 11 594 581 137 153"
P12-1080,W06-2902,0,0.0170586,"resentation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they"
P12-1080,W04-3222,0,0.0344568,"es of 103 nodes. When using the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly r"
P12-1080,W02-1010,0,0.0314675,"However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant wor"
P12-1080,N06-1037,0,0.0264594,"on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais a"
P12-1080,W04-3223,0,\N,Missing
P12-4002,D11-1096,1,0.862552,"Missing"
P12-4002,P04-1043,1,0.836022,"intuition and deep knowledge about the target linguistic phenomena. Kernel Methods (KM) are powerful ML tools (see e.g., (Shawe-Taylor and Cristianini, 2004)), which can alleviate the data representation problem. They substitute feature-based similarities with similarity functions, i.e., kernels, directly defined between training/test instances, e.g., syntactic trees. Hence feature vectors are not needed any longer. Additionally, kernel engineering, i.e., the composition or adaptation of several prototype kernels, facilitates the design of effective similarities required for new tasks, e.g., (Moschitti, 2004; Moschitti, 2008). Tutorial Content The tutorial aims at addressing the problems above: firstly, it will introduce essential and simplified theory of Support Vector Machines and KM with the only aim of motivating practical procedures and interpreting the results. Secondly, it will simply describe the current best practices for designing applications based on effective kernels. For this purpose, it will survey state-of-the-art kernels for diverse NLP applications, reconciling the different approaches with a uniform and global notation/theory. Such survey will benefit from practical expertise a"
P12-4002,W09-1106,1,0.896546,"Missing"
P13-1147,P07-1073,0,0.0241181,"following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identic"
P13-1147,C10-1018,0,0.286619,"Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i.d.) samples is viola"
P13-1147,P04-1054,0,0.34107,"combination of syntax and lexical generalization is very promising for domain adaptation. 1 Introduction Relation extraction is the task of extracting semantic relationships between entities in text, e.g. to detect an employment relationship between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data"
P13-1147,P07-1033,0,0.237181,"Missing"
P13-1147,D10-1057,0,0.0358331,"Missing"
P13-1147,P07-1034,0,0.901861,"r question classification (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Croce et al., 2011). These kernels have not yet been studied for either domain adaptation or RE. Brown clusters were studied previously for feature-based approaches to RE (Sun et al., 2011; Chan and Roth, 2010), but they were not yet evaluated in kernels. Thus, we present a novel application of semantic syntactic tree kernels and Brown clusters for domain adaptation of tree-kernel based relation extraction. Regarding domain adaptation, several methods have been proposed, ranging from instance weighting (Jiang and Zhai, 2007) to approaches that change the feature representation (Daum´e III, 2007) or try to exploit pivot features to find a generalized shared representation between domains (Blitzer et al., 2006). The easy-adapt approach presented in Daum´e III (2007) assumes the supervised adaptation setting and is thus not applicable here. Structural correspondence learning (Blitzer et al., 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains. These correspondences are then integrated as new features in the labeled data of the source domain."
P13-1147,P09-1114,0,0.0565774,"f independent and identically distributed (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computati"
P13-1147,P09-1113,0,0.128507,"ted (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498–1507, c Sofia, B"
P13-1147,W11-2906,0,0.033603,"ion that is minimized during the training process. Let l(x, y, θ) be some loss function. Then, as shown in Jiang and Zhai (2007), the loss function can be weighted i) by βi l(x, y, θ), such that βi = PPst (x (xi ) , where Ps and Pt are the source and target distributions, respectively. Huang et al. (2007) present an application of instance weighting to support vector machines by minimizing the following re-weighted P function: minθ,ξ 21 ||θ||2 + C m i=1 βi ξi . Finding a good weight function is non-trivial (Jiang and Zhai, 2007) and several approximations have been evaluated in the past, e.g. Søgaard and Haulrich (2011) use a bigram-based text classifier to discriminate between domains. We will use a binary classifier trained on RE instance representations. 4 Computational Structures for RE A common way to represent a constituency-based relation instance is the PET (path-enclosed-tree), the smallest subtree including the two target entities (Zhang et al., 2006). This is basically the former structure PAF2 (predicate argument feature) defined in Moschitti (2004) for the extraction of predicate argument relations. The syntactic rep2 It is the smallest subtree enclosing the predicate and one of its argument nod"
P13-1147,C12-2114,0,0.0226788,"nald, 2012). Participants were asked to build a single system that can robustly parse all domains (reviews, weblogs, answers, emails, newsgroups), rather than to build several domain-specific systems. We consider this as a shift in what was considered domain adaptation in the past (adapt from source to a specific target) and what can be considered a somewhat different recent view of DA, that became widespread since 2011/2012. The latter assumes that the target domain(s) is/are not really known in advance. In this setup, the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012), i.e. one wants to build a system that can robustly handle any kind of data. We propose to combine (i) term generalization approaches and (ii) structured kernels to improve the performance of a relation extractor on new domains. Previous studies have shown that lexical and syntactic features are both very important (Zhang et al., 2006). We combine structural features with lexical information generalized by clusters or similarity. Given the complexity of feature engineering, we exploit kernel methods (ShaweTaylor and Cristianini, 2004). We encode word clusters or similarity in tree kernels, wh"
P13-1147,P11-1053,0,0.633811,"ess announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i.d.) samples is violated. Domain adaptat"
P13-1147,I08-2119,0,0.231292,"P 70.7 71.3 72.6 Table 3: Comparison to previous work on the 7 relations of ACE 2004. K: kernel-based; F: featurebased; yes/no: models argument order explicitly. 0.0 nw_bn Type K,yes K,yes K,yes F,yes F,no Type K,yes K,yes K,yes http://disi.unitn.it/ikernels/ Alignment to Prior Work Although most prior studies performed 5-fold cross-validation on ACE 2004, it is often not clear whether the partitioning has been done on the instance or on the document level. Moreover, it is often not stated whether argument order is modeled explicitly, making it difficult to compare system performance. Citing Wang (2008), “We feel that there is a sense of increasing confusion down this line of research”. To ease comparison for future research we use the same 5-fold split on the document level as Sun et al. (2011)13 and make our system publicly available (see Section 5). Table 3 shows that our system (bottom) aligns well with the state of the art. Our best system (composite kernel with polynomial expansion) reaches an F1 of 70.1, which aligns well to the 70.4 of Sun et al. (2011) that use the same datasplit. This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitio"
P13-1147,xu-etal-2008-adaptation,0,0.0186049,"the assumption of independent and identically distributed (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association"
P13-1147,P04-1043,1,0.615409,"ξi . Finding a good weight function is non-trivial (Jiang and Zhai, 2007) and several approximations have been evaluated in the past, e.g. Søgaard and Haulrich (2011) use a bigram-based text classifier to discriminate between domains. We will use a binary classifier trained on RE instance representations. 4 Computational Structures for RE A common way to represent a constituency-based relation instance is the PET (path-enclosed-tree), the smallest subtree including the two target entities (Zhang et al., 2006). This is basically the former structure PAF2 (predicate argument feature) defined in Moschitti (2004) for the extraction of predicate argument relations. The syntactic rep2 It is the smallest subtree enclosing the predicate and one of its argument node. 1500 resentation used by Zhang et al. (2006) (we will refer to it as PET Zhang) is the PET with enriched entity information: e.g. E1-NAM-PER, including entity type (PER, GPE, LOC, ORG) and mention type (NAM, NOM, PRO, PRE: name, nominal, pronominal or premodifier). An alternative kernel that does not use syntactic information is the Bag-of-Words (BOW) kernel, where a single root node is added above the terminals. Note that in this BOW kernel w"
P13-1147,W02-1010,0,0.0290122,"shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. 1 Introduction Relation extraction is the task of extracting semantic relationships between entities in text, e.g. to detect an employment relationship between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismat"
P13-1147,I05-1034,0,0.00665656,"between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of l"
P13-1147,D09-1143,1,0.851526,"snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i"
P13-1147,P06-1104,0,0.808043,"ompany Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independ"
P13-1147,P05-1053,0,0.496205,"arry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the as"
P13-1147,J92-4003,0,\N,Missing
P13-1147,W06-1615,0,\N,Missing
P13-1147,D07-1096,0,\N,Missing
P13-2125,P08-2029,1,0.894358,"gust 4-9 2013. 2013 Association for Computational Linguistics in the text we refer to structural tree kernel models as TK and explicit feature vector representation as fvec. Having defined a way to jointly model text pairs using structural TK representations along with the similarity features fvec, we next briefly review tree kernels and our relational structures. ties of the input text pairs into tree structures and rely on tree kernels to automatically generate rich feature spaces. This work extends in several directions our earlier work in question answering, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008), in textual entailment recognition, e.g., (Moschitti and Zanzotto, 2007), and more in general in relational text categorization (Moschitti, 2008; Severyn and Moschitti, 2012). In this section we describe: (i) a kernel framework to combine structural and vector models; (ii) structural kernels to handle feature engineering; and (iii) suitable structural representations for relational learning. 2.2 Tree Kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures."
P13-2125,P07-1098,1,0.762423,"8, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics in the text we refer to structural tree kernel models as TK and explicit feature vector representation as fvec. Having defined a way to jointly model text pairs using structural TK representations along with the similarity features fvec, we next briefly review tree kernels and our relational structures. ties of the input text pairs into tree structures and rely on tree kernels to automatically generate rich feature spaces. This work extends in several directions our earlier work in question answering, e.g., (Moschitti et al., 2007; Moschitti and Quarteroni, 2008), in textual entailment recognition, e.g., (Moschitti and Zanzotto, 2007), and more in general in relational text categorization (Moschitti, 2008; Severyn and Moschitti, 2012). In this section we describe: (i) a kernel framework to combine structural and vector models; (ii) structural kernels to handle feature engineering; and (iii) suitable structural representations for relational learning. 2.2 Tree Kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than"
P13-2125,S12-1051,0,0.0121536,"he goal is to learn a scoring model that given a pair of two short texts returns a similarity score that correlates with human judgement. Hence, the key aspect of having an accurate STS framework is the design of features that can adequately represent various aspects of the similarity between texts, e.g., using lexical, syntactic and semantic similarity metrics. The majority of approaches treat input text pairs as feature vectors where each feature is a score corresponding to a certain type of similarity. This approach is conceptually easy to implement and the STS shared task at SemEval 2012 (Agirre et al., 2012) (STS-2012) has shown that the best systems were built following this idea, i.e., a number of features encoding similarity of an input text pair were combined in a single scoring model, e.g., SVR. Nevertheless, one limitation of using only similarity features to represent a text pair is that of low representation power. The novelty of our approach is that we treat the input text pairs as structural objects and rely on the power of kernel learning to extract relevant structures. To link the documents in a pair we mark the 2 Structural Relational Similarity The approach of relating pairs of inpu"
P13-2125,S12-1059,0,0.030727,"nd Kfvec is a kernel over feature vectors, e.g., linear, polynomial or RBF, etc. Further 715 Figure 1: A phrase dependency-based structural representation of a text pair (s1, s2): A woman with a knife is slicing a pepper (s1) vs. A women slicing green pepper (s2) with a high semantic similarity (human judgement score 4.0 out of 5.0). Related tree fragments are linked with a REL tag. Baseline features. (base) We adopt similarity features from two best performing systems of STS-2012, which were publicly released1 : ˇ c et al., 2012) namely, the Takelab2 system (Sari´ 3 and the UKP Lab’s system (Bar et al., 2012). Both systems represent input texts with similarity features combining multiple text similarity measures of varying complexity. UKP (U) provides metrics based on matching of character, word n-grams and common subsequences. It also includes features derived from Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and aggregation of word similarity based on lexical-semantic resources, e.g., WordNet. In total it provides 18 features. Takelab (T) includes n-gram matching of varying size, weighted word matching, length difference, WordNet similarity and vector space similarity where pair"
P13-2125,W06-1670,0,0.0139673,"Missing"
P13-2125,S12-1060,0,0.0420044,"Missing"
P13-2125,P02-1034,0,0.0201093,"and (iii) suitable structural representations for relational learning. 2.2 Tree Kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures. Hence, we rely on tree kernels to compute KTK (·, ·). Given two trees it evaluates the number of substructures (or fragments) they have in common, i.e., it is a measure of their overlap. Different TK functions are characterized by alternative fragment definitions. In particular, we focus on the Syntactic Tree kernel (STK) (Collins and Duffy, 2002) and a Partial Tree Kernel (PTK) (Moschitti, 2006). STK generates all possible substructures rooted in each node of the tree with the constraint that production rules can not be broken (i.e., any node in a tree fragment must include either all or none of its children). PTK can be more effectively applied to both constituency and dependency parse trees. It generalizes STK as the fragments it generates can contain any subset of nodes, i.e., PTK allows for breaking the production rules and generating an extremely rich feature space, which results in higher generalization ability. 2.1 Structural K"
P13-2125,C10-1131,0,0.013652,"limitation of using only similarity features to represent a text pair is that of low representation power. The novelty of our approach is that we treat the input text pairs as structural objects and rely on the power of kernel learning to extract relevant structures. To link the documents in a pair we mark the 2 Structural Relational Similarity The approach of relating pairs of input structures by learning predictable syntactic transformations has shown to deliver state-of-the-art results in question answering, recognizing textual entailment, and paraphrase detection, e.g. (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010). Previous work relied on fairly complex approaches, e.g. applying quasi-synchronous grammar formalism and variations of tree edit distance alignments, to extract syntactic patterns relating pairs of input structures. Our approach is conceptually simpler, as it regards the problem within the kernel learning framework, where we first encode salient syntactic/semantic proper714 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714–718, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics in the t"
P13-2125,D07-1003,0,0.0683325,"Nevertheless, one limitation of using only similarity features to represent a text pair is that of low representation power. The novelty of our approach is that we treat the input text pairs as structural objects and rely on the power of kernel learning to extract relevant structures. To link the documents in a pair we mark the 2 Structural Relational Similarity The approach of relating pairs of input structures by learning predictable syntactic transformations has shown to deliver state-of-the-art results in question answering, recognizing textual entailment, and paraphrase detection, e.g. (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010). Previous work relied on fairly complex approaches, e.g. applying quasi-synchronous grammar formalism and variations of tree edit distance alignments, to extract syntactic patterns relating pairs of input structures. Our approach is conceptually simpler, as it regards the problem within the kernel learning framework, where we first encode salient syntactic/semantic proper714 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714–718, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computatio"
P13-2125,D09-1159,0,0.00492823,"ures (A) which includes: a cosine similarity scores computed over (i) n-grams of part-of-speech tags (up to 4-grams), (ii) SuperSense tags (Ciaramita and pendency relations are used to link words in a way that they are always at the leaf level. This reordering of the nodes helps to avoid the situation where nodes with words tend to form long chains. This is essential for PTK to extract meaningful fragments. We also plug part-of-speech tags between the word nodes and nodes carrying their grammatical role. Phrase-dependency tree. We explore a phrasedependency tree similar to the one defined in (Wu et al., 2009). It represents an alternative structure derived from the dependency tree, where the dependency relations between words belonging to the same phrase (chunk) are collapsed in a unified node. Different from (Wu et al., 2009), the collapsed nodes are stored as a shallow subtree rooted at the unified node. This node organization is particularly suitable for PTK that effectively runs a sequence kernel on the tree fragments inside each chunk subtree. Fig 1 gives an example of our variation of a phrase dependency tree. As a final consideration, if a document contains multiple sentences they are merge"
P13-2125,N10-1145,0,0.0124863,"similarity features to represent a text pair is that of low representation power. The novelty of our approach is that we treat the input text pairs as structural objects and rely on the power of kernel learning to extract relevant structures. To link the documents in a pair we mark the 2 Structural Relational Similarity The approach of relating pairs of input structures by learning predictable syntactic transformations has shown to deliver state-of-the-art results in question answering, recognizing textual entailment, and paraphrase detection, e.g. (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010). Previous work relied on fairly complex approaches, e.g. applying quasi-synchronous grammar formalism and variations of tree edit distance alignments, to extract syntactic patterns relating pairs of input structures. Our approach is conceptually simpler, as it regards the problem within the kernel learning framework, where we first encode salient syntactic/semantic proper714 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714–718, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics in the text we refer to structural"
P13-2125,P06-1051,1,\N,Missing
P14-1118,J08-4004,0,0.0253937,"Missing"
P14-1118,I13-1041,0,0.0122599,"ouTube contain rapidly changing information generated by millions of users that can dramatically affect the reputation of a person or an organization. This raises the importance of automatic extraction of sentiments and opinions expressed in social media. YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between people sharing an interest. Hence, doing sentiment research in such an environment is highly relevant for the community. While the linguistic conventions used on Twitter and YouTube indeed show similarities (Baldwin et al., 2013), focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by The comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video"
P14-1118,P07-1056,0,0.0386524,"des (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn"
P14-1118,P11-2008,0,0.0867098,"Missing"
P14-1118,P13-1147,1,0.835714,"by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarit"
P14-1118,D11-1141,0,0.0139627,"cally targets the sentiment and comment type classification tasks. In particular, our shallow tree structure is a two-level syntactic hierarchy built from word lemmas (leaves) and part-of-speech tags that are further grouped into chunks (Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts"
P14-1118,D13-1044,1,0.825377,"l., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods"
P14-1118,P13-2125,1,0.826898,"own to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods. 3.1 Feature Set We enrich the traditional bag-of-word"
P14-1118,D11-1014,0,0.0282501,"atings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust sys"
P14-1118,C12-2114,0,0.0125563,"ently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e"
P14-1118,E06-1015,1,0.894293,"sible subtrees, thus producing generalized (back-off) features, e.g., [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]]. nary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVMlight-TK4 , which encodes structural kernels in the SVM-light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006a) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x using Pthe following prediction funcx, x i ), where αi are x) = tion: h(x i αi yi K(x the model parameters estimated from the training data, yi are target variables, xi are support vectors, and K(·, ·) is a kernel function. The latter computes the similarity between two comments. The STRUCT model treats each comment as a tuT , v i comp"
P14-1118,P11-2100,0,0.0628624,"Missing"
P14-1118,uryupina-etal-2014-sentube,1,0.829353,"ogle.com/youtube/v3/ annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO). To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in Uryupina et al. (2014). 5 Experiments This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalability to other domains. 5.1 Task description Sentiment classification. We treat each comment as expressing positive, negative or neutral sentiment. Hence, the task is a threeway classification. Type classification. One of the challeng"
P14-1118,N13-1039,0,0.0154655,"(Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts of the video, (ii) sentiment-bearing words expressing positive or negative sentiment and (iii) negation words. To automatically identify concept words of the video we use context words (tokens detected as nouns by the part-of-speech tagger"
P14-1118,pak-paroubek-2010-twitter,0,0.00951516,"si.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. A recent study focuses on sentiment analysis for Twitter (Pak and Paroubek, 2010), however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only. Siersdorfer et al. (2010) focus on exploiting user ratings (counts of ‘thumbs up/down’ as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we b"
P14-1118,P12-2018,0,0.0351227,"a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013)"
P14-1118,H05-1044,0,0.00590306,"Feature Set We enrich the traditional bag-of-word representation with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (FVEC) encodes each document using the following feature groups: - word n-grams: we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the presence/absence of a given item. - lexicon: a sentiment lexicon is a collection of words associated with a positive or negative sentiment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lexicon (Wilson et al., 2005) and the lexicon of Hu and Liu (2004). For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. - negation: the count of negation words, e.g., {don’t, never, not, etc.}, found in a comment.2 Our structural representation (defined next) enables a more involved treatment of negation. - video concept: cosine similarity between a comment and the title/description of the video. Most of the videos come with a title and a short description, which can be used to encode the topicality of 2 The list of negation words is adopted htt"
P14-1118,P07-1033,0,\N,Missing
P15-1097,S12-1059,0,0.0198979,"Missing"
P15-1097,W05-0620,0,0.0400832,"Missing"
P15-1097,W10-2802,0,0.0219821,"chine Learning platform that implements tree kernels. In both tasks, we applied the kernels described in Sec. 4, where the trees are generated with the Stanford parser4 . SP T K uses a node similarity function σ(n1 , n2 ) implemented as follows: if n1 and n2 are two identical syntactic nodes σ = 1. If n1 and n2 are two lexical nodes with the same POS tag, their similarity is evaluated computing the cosine similarity of their corresponding vectors in a wordspace. In all the other cases σ = 0. We generated two different wordspaces. The first is 3 4 a co-occurrence LSA embedding as described in (Croce and Previtali, 2010). The second space is derived by applying a skip-gram model (Mikolov et al., 2013) with the word2vec tool5 . SP T K using the LSA will be referred to as SP T KLSA , while when adopting word2vec it will be indicated with SP T KW 2V . We used default parameters both for P T K and SP T K whereas we selected h and D parameters of N SP DK that obtained the best average accuracy using a 5-fold cross validation on the training set. 5.1.2 Performance measures The two considered tasks are binary classification problems thus we used Accuracy, Precision, Recall and F1. The adopted corpora have a predefin"
P15-1097,D11-1096,1,0.494753,"X K(S1 , S2 ) = ∆(n1 , n2 ), (2) n1 ∈NS1 n2 ∈NS2 where NS1 and NS2 are the sets of the S1 ’s and S2 ’s nodes, respectively and ∆(n1 , n2 ) = |S| X χi (n1 )χi (n2 ). (3) i=1 The latter is equal to the number of common substructures rooted in the n1 and n2 nodes. In order to have a similarity score between 0 and 1, a normalization in the kernel space, i.e., K(S1 ,S2 ) √ is usually applied. From a provides clear advantages over all-vs-all words similarity, which tends to semantically diverge. Indeed, syntax provides the necessary restrictions to compute an effective semantic similarity. SP T K (Croce et al., 2011) generalizes P T K by enabling node similarity during substructure matching. More formally, SP T K is computed by Eq. 2 using the following P|S| ∆SP T K (n1 , n2 ) = i,j=1 χi (n1 )χj (n2 )Σ(si , sj ), where Σ is a similarity between structures1 . The recursive definition of ∆SP T K is the following: 1. if n1 and n2 are leaves ∆SP T K (n1 , n2 ) = µλσ(n1 , n2 );  2. else ∆SP T K (n1 , n2 ) = µσ(n1 , n2 ) × λ2 + X K(S1 ,S1 )×K(S2 ,S2 ) practical computation viewpoint, it is convenient to divide structural kernels in two classes of algorithms working either on trees or graphs. 3.1 The Partial Tr"
P15-1097,C04-1051,0,0.0286445,"Hard problem (i.e., it shows the same complexity of the graph isomorphism problem) (Gartner et al., 2003). Thus most kernels for graphs only associate specific types of substructures with features, such as paths (Borgwardt and Kriegel, 2005; Heinonen et al., 2012), walks (Kashima et al., 2003; Vishwanathan et al., 2006) and tree structures (Cilia and Moschitti, 2007; Mah´e and Vert, 2008; Shervashidze et al., 2011; Da San Martino et al., 2012). We exploit structural kernels for PI, whose task is to evaluate whether a given pair of sentences is in the paraphrase class or not, (see for example (Dolan et al., 2004)). Paraphrases can be seen as a restatement of a text in another form that preserves the original meaning. This task has a primary importance in many other NLP and IR tasks such as Machine Translation, Plagiarism Detection and QA. Several approaches have been proposed, e.g., (Socher et al., 2011) apply a recursive auto encoder with dynamic pooling, and (Madnani et al., 2012) use eight machine translation metrics to achieve the state of the art. To our knowledge no previous model based on kernel methods has been Structural kernels Kernel Machines carry out learning and classification by only re"
P15-1097,W07-1401,0,0.0120327,"art tree kernels and graph kernels and apply them to innovative computational structures. These innovative combinations use for the fist time semantic/syntactic tree kernels and graph kernels for the tackled tasks. (ii) Our kernels provide effective and efficient solutions, which solve the previous scalability problem and, at the same time, exceed the state of the art on both RTE and PI. Finally, our study suggests research directions for designing effective graph kernels for RL. applied before: with such methods, we outperform the state of the art in PI. A description of RTE can be found in (Giampiccolo et al., 2007): it is defined as a directional relation extraction between two text fragments, called text and hypothesis. The implication is supposed to be detectable only based on the text content. Its applications are in QA, Information Extraction, Summarization and Machine translation. One of the most performing approaches of RTE 3 was (Iftene and Balahur-Dobrescu, 2007), which largely relies on external resources (i.e., WordNet, Wikipedia, acronyms dictionaries) and a base of knowledge developed ad hoc for the dataset. In (Zanzotto and Moschitti, 2006), we designed an interesting but computationally ex"
P15-1097,J02-3001,0,0.0111537,"relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition. 1 Introduction Advanced NLP systems, e.g., IBM Watson system (Ferrucci et al., 2010), are the result of effective use of syntactic/semantic information along with relational learning (RL) methods. This research area is rather vast including, extraction of syntactic relations, e.g., (Nastase et al., 2013), predicate relations, e.g., Semantic Role Labeling (Carreras and M`arquez, 2005) or FrameNet parsing (Gildea and Jurafsky, 2002) and relation extraction between named entities, e.g., (Mintz et al., 2009). Although extremely interesting, the above methods target relations only between text constituents whereas the final goal of an intelligent system would be to interpret the semantics of larger pieces of text, e.g., sentences or paragraphs. This line of research relates to three Automatic learning a model for deriving the relations above is rather complex as any of the text constituents, e.g., License revenue, a key measure of demand, in the two sentences plays an important role. Therefore, a suitable approach should ex"
P15-1097,C12-2040,1,0.412866,"ch features are matched in b1 , they provide the fuzzy information: there should be a match similar to [NP [NP-REL [JJ-REL]] also between a2 and b2 . This kind of matches establishes a sort of relational pair features. It should be noted that we proposed more complex REL tagging policies for Passage Reranking, exploiting additional resources such as Linked Open Data or WordNet (Tymoshenko et al., 2014). Another interesting application of this RL framework is the Machine Translation Evaluation (Guzm´an et al., 2014). Finally, we used a similar model for translating questions to SQL queries in (Giordani and Moschitti, 2012). 4.2 Graph Representations The relational tree representation can capture relational features but the use of the same REL tag for any match between the two trees prevents to deterministically establish the correspondences between nodes. For exactly representing such matches (without incurring in non-valid kernels or sparsity problems), a graph representation is needed. If we connect matching nodes (or also nodes labelled as REL) in Fig. 1 (see dashed lines), we obtain a relational graph. Substructures of such graph clearly indicate how constituents, e.g., NPs, VPs, PPs, from one sentence map"
P15-1097,D14-1027,1,0.89161,"Missing"
P15-1097,W07-1421,0,0.011487,"state of the art on both RTE and PI. Finally, our study suggests research directions for designing effective graph kernels for RL. applied before: with such methods, we outperform the state of the art in PI. A description of RTE can be found in (Giampiccolo et al., 2007): it is defined as a directional relation extraction between two text fragments, called text and hypothesis. The implication is supposed to be detectable only based on the text content. Its applications are in QA, Information Extraction, Summarization and Machine translation. One of the most performing approaches of RTE 3 was (Iftene and Balahur-Dobrescu, 2007), which largely relies on external resources (i.e., WordNet, Wikipedia, acronyms dictionaries) and a base of knowledge developed ad hoc for the dataset. In (Zanzotto and Moschitti, 2006), we designed an interesting but computationally expensive model using simple syntactic tree kernels. In this paper, we develop models that do not use external resources but, at the same time, are efficient and approach the state of the art in RTE. 2 3 Related Work In this paper, we apply kernel methods, which enable an efficient comparison of structures in huge, possibly infinite, feature spaces. While for tre"
P15-1097,N12-1019,0,0.0219762,"and Vert, 2008; Shervashidze et al., 2011; Da San Martino et al., 2012). We exploit structural kernels for PI, whose task is to evaluate whether a given pair of sentences is in the paraphrase class or not, (see for example (Dolan et al., 2004)). Paraphrases can be seen as a restatement of a text in another form that preserves the original meaning. This task has a primary importance in many other NLP and IR tasks such as Machine Translation, Plagiarism Detection and QA. Several approaches have been proposed, e.g., (Socher et al., 2011) apply a recursive auto encoder with dynamic pooling, and (Madnani et al., 2012) use eight machine translation metrics to achieve the state of the art. To our knowledge no previous model based on kernel methods has been Structural kernels Kernel Machines carry out learning and classification by only relying on the inner product between instances. This can be efficiently and implicitly computed by kernel functions by exploiting the following P dual formulation of the model (hyperplane): i=1..l yi αi φ(oi ) · φ(o) + b = 0, where yi are the example labels, αi the support vector coefficients, oi and o are two objects, φ is a mapping from the objects to feature vectors x~i and"
P15-1097,P09-1113,0,0.0280131,"different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition. 1 Introduction Advanced NLP systems, e.g., IBM Watson system (Ferrucci et al., 2010), are the result of effective use of syntactic/semantic information along with relational learning (RL) methods. This research area is rather vast including, extraction of syntactic relations, e.g., (Nastase et al., 2013), predicate relations, e.g., Semantic Role Labeling (Carreras and M`arquez, 2005) or FrameNet parsing (Gildea and Jurafsky, 2002) and relation extraction between named entities, e.g., (Mintz et al., 2009). Although extremely interesting, the above methods target relations only between text constituents whereas the final goal of an intelligent system would be to interpret the semantics of larger pieces of text, e.g., sentences or paragraphs. This line of research relates to three Automatic learning a model for deriving the relations above is rather complex as any of the text constituents, e.g., License revenue, a key measure of demand, in the two sentences plays an important role. Therefore, a suitable approach should exploit representations that can structure the two sentences and put their co"
P15-1097,D13-1044,1,0.682306,"on::c Figure 1: Text representations for PI and RTE: (i) pair of trees, a1 (upper) and a2 (lower), (ii) combined in a graph with dashed edges, and (iii) labelled with the tag REL (in green). The nodes highlighted in yellow constitute a feature for the N SP DK kernel (h = 1, D = 3) centered at the nodes ADVB and NP-REL. assignment made our similarity function a nonvalid kernel. Thus, for this paper, we prefer to rely on a more recent solution we proposed for passage reranking in the QA domain (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b), and for Answer Selection (Severyn and Moschitti, 2013). It consists in simply labeling matching nodes with a special tag, e.g., REL, which indicates the correspondences between words. REL is attached to the father and grandfather nodes of the matching words. Fig. 1 shows several green REL tags attached to the usual POS-tag and constituent node labels of the parse trees. For example, the lemma license is matched by the two sentences, thus both its father, JJ, and its grandfather, NP, nodes are marked with REL. Thanks to such relational labeling the simple kernel, P T K(a1 , b1 ) + P T K(a2 , b2 ), can generate relational features from a1 , e.g., ["
P15-1097,W13-3509,1,0.732429,"NP NP QP-REL $-REL QP-REL NN $::$ CD-REL CD-REL demand::n 107.6::c million::c Figure 1: Text representations for PI and RTE: (i) pair of trees, a1 (upper) and a2 (lower), (ii) combined in a graph with dashed edges, and (iii) labelled with the tag REL (in green). The nodes highlighted in yellow constitute a feature for the N SP DK kernel (h = 1, D = 3) centered at the nodes ADVB and NP-REL. assignment made our similarity function a nonvalid kernel. Thus, for this paper, we prefer to rely on a more recent solution we proposed for passage reranking in the QA domain (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b), and for Answer Selection (Severyn and Moschitti, 2013). It consists in simply labeling matching nodes with a special tag, e.g., REL, which indicates the correspondences between words. REL is attached to the father and grandfather nodes of the matching words. Fig. 1 shows several green REL tags attached to the usual POS-tag and constituent node labels of the parse trees. For example, the lemma license is matched by the two sentences, thus both its father, JJ, and its grandfather, NP, nodes are marked with REL. Thanks to such relational labeling the simple kernel, P T"
P15-1097,N13-1133,0,0.0219353,"n the Semantic Textual Similarity (STS) task: – Longest common substring measure and Longest common subsequence measure, which determine the length of the longest substring shared by two text segments. – Running-Karp-Rabin Greedy String Tiling provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity based on the WordNet hierarchy. – Explicit Semantic Analysis (ESA) similarity (Gabrilovich and Markovitch, 2007) represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substitution (Szarvas et al., 2013): a supervised word sense disambiguation system is used to substitute a wide selection of highfrequency English nouns with generalizations, then Resnik and ESA features are computed on the transformed text. Combined representations (i) K + (pa , pb ) = K(a1 , b1 ) + K(a2 , b2 ), which simply sums the similarities between the first two sentences and the second two sentences whose implication has to be derived. (ii) An alternative kernel combines the two similarity scores above with the product: K × (pa , pb ) = K(a1 , b1 ) · K(a2 , b2 ). (iii) The symmetry of the PI task requires different kern"
P15-1097,E14-1070,1,0.806451,"uch relational labeling the simple kernel, P T K(a1 , b1 ) + P T K(a2 , b2 ), can generate relational features from a1 , e.g., [NP [NP-REL [JJ-REL] NN]][PP]], [NP [NN]][PP]], [NP-REL [NP [NP-REL [JJ-REL]][PP]],... If such features are matched in b1 , they provide the fuzzy information: there should be a match similar to [NP [NP-REL [JJ-REL]] also between a2 and b2 . This kind of matches establishes a sort of relational pair features. It should be noted that we proposed more complex REL tagging policies for Passage Reranking, exploiting additional resources such as Linked Open Data or WordNet (Tymoshenko et al., 2014). Another interesting application of this RL framework is the Machine Translation Evaluation (Guzm´an et al., 2014). Finally, we used a similar model for translating questions to SQL queries in (Giordani and Moschitti, 2012). 4.2 Graph Representations The relational tree representation can capture relational features but the use of the same REL tag for any match between the two trees prevents to deterministically establish the correspondences between nodes. For exactly representing such matches (without incurring in non-valid kernels or sparsity problems), a graph representation is needed. If"
P15-1097,P06-1051,1,0.796923,"ations above is rather complex as any of the text constituents, e.g., License revenue, a key measure of demand, in the two sentences plays an important role. Therefore, a suitable approach should exploit representations that can structure the two sentences and put their constituents in relation. Since the dependencies between constituents can be an exponential number and representing structures in learning algorithms is rather challenging, automatic feature engineering through kernel methods (Shawe-Taylor and Cristianini, 2004; Moschitti, 2006) can be a promising direction. In particular, in (Zanzotto and Moschitti, 2006), we represented the two evaluating sentences for the RTE task with syntactic structures and then applied tree kernels to them. The resulting system was very accurate but, unfortunately, it could not scale to large datasets as it is based on a compu1003 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1003–1013, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tationally exponential algorithm. This prevents its application to PI tasks, which"
P15-2033,P13-4021,0,0.0609935,"Missing"
P15-2033,W14-1605,1,0.848376,"e TK can be any tree kernel, e.g., the syntactic tree kernel (STK) also called SST by Moschitti (2006), and FV is the feature vector representation of the input pair, e.g., hq, ci i or hq 0 , c0j i. STK maps trees into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. It enables the exploitation of structural features, which can be effectively combined with more traditional features (described hereafter). Clue retrieval and reranking One important source of candidate answers is the DB of previously solved clues. In (Barlacchi et al., 2014a), we proposed the BM25 retrieval model to generate clue lists, which were further refined by applying our reranking models. The latter promote the most similar, which are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words"
P15-2033,P15-4014,1,0.848415,"produced by our DNN model with other rerankers to greatly improve over the previous state-of-the-art results. Finally, we collected a very large dataset composed of 2 millions clue/answer pairs that can be useful to the NLP community for developing semantic textual similarity models. Future research will be devoted to find models to effectively combine TKs and DNN. In particular, our previous model exploiting Linked Open Data in QA (Tymoshenko et al., 2014) seems very promising to find correct answer to clues. This as well as further research will be integrated in our CP system described in (Barlacchi et al., 2015). (iv) finally, the combination with TK, i.e., SVM(DNNFLD ,TK), does not significantly improve the previous results. In summary, when a dataset is relatively small DNNM fails to deliver any noticeable improvement over the SE baseline even when combined with additional similarity features. SVM and TK models generalize much better on the smaller dataset. Additionally, it is interesting to see that training an SVM on a small number of examples enriched with the features produced by a DNN trained on large data gives us the same results of DNN trained on the large dataset. Hence, it is desired to u"
P15-2033,D14-1067,0,0.0576581,"Missing"
P15-2033,D13-1044,1,0.850177,"text of the Semantic Textual Similarity (STS) challenge (B¨ar et al., 2013); and (iii) WebCrow features (WC), which are the similarity measures computed on the clue pairs by WebCrow (using the Levenshtein distance) and the SE score. 3 Distributional models for clue reranking The architecture of our distributional matching model for measuring similarity between clues is presented in Fig. 1. Its main components are: Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and tradi(i) sentence matrices sci ∈ Rd×|ci |obtained by the concatenation of the word vectors wj ∈ Rd (with d being the size of the embeddings) of the corresponding words wj from the input clues ci ; (ii) a distributional sentence model f : Rd×|ci |→ Rm that maps the sentence 200 Figure 1: Distributional sentence matching model for computing similarity b"
P15-2033,W13-3509,1,0.82448,"larity, which defines features used in the context of the Semantic Textual Similarity (STS) challenge (B¨ar et al., 2013); and (iii) WebCrow features (WC), which are the similarity measures computed on the clue pairs by WebCrow (using the Levenshtein distance) and the SE score. 3 Distributional models for clue reranking The architecture of our distributional matching model for measuring similarity between clues is presented in Fig. 1. Its main components are: Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and tradi(i) sentence matrices sci ∈ Rd×|ci |obtained by the concatenation of the word vectors wj ∈ Rd (with d being the size of the embeddings) of the corresponding words wj from the input clues ci ; (ii) a distributional sentence model f : Rd×|ci |→ Rm that maps the sentence 200 Figure 1: Distributio"
P15-2033,P14-1062,0,0.0277907,"Missing"
P15-2033,D14-1181,0,0.0038314,"Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and tradi(i) sentence matrices sci ∈ Rd×|ci |obtained by the concatenation of the word vectors wj ∈ Rd (with d being the size of the embeddings) of the corresponding words wj from the input clues ci ; (ii) a distributional sentence model f : Rd×|ci |→ Rm that maps the sentence 200 Figure 1: Distributional sentence matching model for computing similarity between clues. matrix of an input clue ci to a fixed-size vector representations xci of size m; 2014; Kim, 2014). In this paper, P we opt for a simple solution where f (sci ) = i wi /|ci |, i.e., the word vectors, are averaged to a single fixed-sized vector x ∈ Rd . Our preliminary experiments revealed that this simpler model works just as well as more complicated single or multi-layer convolutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role. Considering recent deep learning models for matching sentences, our network is most similar to the models in Hu et al. (2014) applied"
P15-2033,E14-1070,1,0.848578,"ost important finding is that our distributional neural network model is very effective in establishing similarity matching between clues. We combine the features produced by our DNN model with other rerankers to greatly improve over the previous state-of-the-art results. Finally, we collected a very large dataset composed of 2 millions clue/answer pairs that can be useful to the NLP community for developing semantic textual similarity models. Future research will be devoted to find models to effectively combine TKs and DNN. In particular, our previous model exploiting Linked Open Data in QA (Tymoshenko et al., 2014) seems very promising to find correct answer to clues. This as well as further research will be integrated in our CP system described in (Barlacchi et al., 2015). (iv) finally, the combination with TK, i.e., SVM(DNNFLD ,TK), does not significantly improve the previous results. In summary, when a dataset is relatively small DNNM fails to deliver any noticeable improvement over the SE baseline even when combined with additional similarity features. SVM and TK models generalize much better on the smaller dataset. Additionally, it is interesting to see that training an SVM on a small number of exa"
P15-2113,P07-1098,1,0.451732,"al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s"
P15-2113,N10-1145,0,0.0528465,"answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high"
P15-2113,S15-2047,1,0.84994,"Missing"
P15-2113,S15-2035,0,0.0819635,"1.45 65.57±1.54 76.23±0.45 76.43±0.92 75.05±0.70 75.61±0.63 75.71±0.71 Table 3: Precision, Recall, F1 , Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1 , F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup As in the competition, the results are macroaveraged at class level. The results of the top 3 Our local classifiers are support vector machines systems are reported for comparison: JAIST (Tran (SVM) with C = 1 (Joachims, 1999), logistic et al., 2015), HITSZ (Hou et al., 2015) and regression with a Gaussian prior with variance 10, QCRI (Nicosia et al., 2015), where the latter refers and logistic ordinal regression (McCullagh, 1980). to our old system that we used for the competition. In order to capture long-range sequential depenThe two main observations are (i) using threaddencies, we use a second-order SVMhmm (Yu level features helps significantly; and (ii) the ordiand Joachims, 2008) (with C = 500 and nal regression model, which captures the idea that epsilon = 0.01) and a second-order linear-chain potential lies between good and bad, achieves at CRF, which con"
P15-2113,S15-2036,1,0.620958,"Missing"
P15-2113,D13-1044,1,0.903656,"Missing"
P15-2113,W01-0515,0,0.0237286,"can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy. In other words, labels should be used together with answers’ content to account for stronger and more effective dependencies. 2 Basic and Thread-Level Features 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for"
P15-2113,W13-3509,1,0.800933,"g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used"
P15-2113,D07-1002,0,0.0974152,"a Abstract This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and"
P15-2113,P08-1082,0,0.0417245,"n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b"
P15-2113,S15-2038,0,0.16032,"Missing"
P15-2113,C10-1131,0,0.223512,"Missing"
P15-2113,N13-1106,0,0.0861461,"Missing"
P15-4014,P13-4021,0,0.138039,"Missing"
P15-4014,W14-1605,1,\N,Missing
P16-4027,pianta-etal-2008-textpro,0,0.0882451,"Missing"
P16-4027,agerri-etal-2014-ixa,0,0.0559234,"Missing"
P16-4027,P13-1147,1,0.764657,"ct opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) is a tree-kernel based system developed at the University of Trento (Moschitti, 2006; Plank and Moschitti, 2013). Tree kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first version of the relation extractor was trained on the ACE 2004 data. It provides the following binary relations as output: Physical, Personal/Social, Employment/Membership, PER/ORG Affiliation and GPE Affiliation. An extended version of the Relation Extractor includes an additional model trained on the CoNLL 2004"
P16-4027,W14-1605,1,0.760477,"l users that need high-performance standard tools at a zero engineering cost. The local version, on the contrary, requires some installation and configuration effort, but in return it offers a great flexibility in implementing and integrating user-specific modules. Since the beginning of the LiMoSINe project, the platform has been used for providing robust preprocessing for a variety of high-level tasks. Thus, we have recently shown how structural representations, extracted with our pipeline, improve multilingual opinion mining on YouTube (Severyn et al., 2015) or crossword puzzle resolution (Barlacchi et al., 2014). The pipeline has been adopted by other parties, most importantly by the joint QCRI and MIT project IYAS (Interactive sYstem for Answer Selection). IYAS focuses on Question Answering, showing that representations, based on linguistic preprocessing, significantly outperform more shallow methods (Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2014). As part of the LiMoSINe project, we have created the LiMoSINe Common Corpus: a large collection of documents downloaded from different web resources 4 http://www.let.rug.nl/vannoord/alp/ Alpino/ 161 in any of the four addressed languages. These"
P16-4027,W04-2401,0,0.0741955,"kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first version of the relation extractor was trained on the ACE 2004 data. It provides the following binary relations as output: Physical, Personal/Social, Employment/Membership, PER/ORG Affiliation and GPE Affiliation. An extended version of the Relation Extractor includes an additional model trained on the CoNLL 2004 data (Roth and Yih, 2004) following the setup of Giuliano et al. (2007). The model uses a composite kernel consisting of a constituency-based path-enclosed tree kernel and a linear feature vector encoding local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Mure"
P16-4027,S10-1021,1,0.842758,"ns used by the English RE models. Coreference Resolution. A coreference model for BART has been trained on the Italian portion of the SemEval-2010 Task 1 dataset (Uryupina and Moschitti, 2014). Apart from retraining the model, we have incorporated some language-specific features to account, 3 160 http://www.di.unito.it/˜tutreeb/ for example, for abbreviation and aliasing patterns in Italian. The Italian version of BART, therefore, is a high-performance language-specific system. It has shown reliable performance at the recent shared tasks for Italian, in particular, at the SemEval-2010 Task 1 (Broscheit et al., 2010) and at the EvalIta 2009 (Biggio et al., 2009). Both our English and Italian coreference modules are based on BART. Their configurations (parameter settings and features) have been optimized separately to enhance the performance level on a specific language. Since BART is a highly modular toolkit itself and its language-specific functionality can be controlled via a Language Plugin, no extra BART installation is required to run the Italian coreference resolver. 3.3 Spanish We have tested two publicly available toolkits supporting language processing in Spanish: OpenNLP and IXA (Agerri et al.,"
P16-4027,E14-1070,1,0.83486,"robust preprocessing for a variety of high-level tasks. Thus, we have recently shown how structural representations, extracted with our pipeline, improve multilingual opinion mining on YouTube (Severyn et al., 2015) or crossword puzzle resolution (Barlacchi et al., 2014). The pipeline has been adopted by other parties, most importantly by the joint QCRI and MIT project IYAS (Interactive sYstem for Answer Selection). IYAS focuses on Question Answering, showing that representations, based on linguistic preprocessing, significantly outperform more shallow methods (Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2014). As part of the LiMoSINe project, we have created the LiMoSINe Common Corpus: a large collection of documents downloaded from different web resources 4 http://www.let.rug.nl/vannoord/alp/ Alpino/ 161 in any of the four addressed languages. These data were annotated automatically. We illustrate the processing capabilities of our pipeline on the Spanish part of the corpus (EsLCC). To this end, we developed a UIMA Collection Processing Engine (CPE). Once the EsLCC was downloaded it was first tidied up with Apache Tika. The pipeline was then applied to clean text. It was capable of processing the"
P16-4027,I13-1012,1,0.859539,"), covering a wide variety of mentions, has been developed at the University of Trento as a part of BART (see below). A more recent version has been proposed for the CoNLL-2011/2012 Shared Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion"
P16-4027,W11-1908,1,0.905436,"Missing"
P16-4027,C12-2039,0,0.0196184,"and Yih, 2004) following the setup of Giuliano et al. (2007). The model uses a composite kernel consisting of a constituency-based path-enclosed tree kernel and a linear feature vector encoding local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Muresan, 2012). Coreference Resolution. Our coreference resolution Analysis Engine is a wrapper around BART—a toolkit for Coreference Resolution developed at the University of Trento (Versley et al., 2008; Uryupina et al., 2012). It is a modular anaphora resolution system that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART implements several models of anaphora resolution (mentionpair and entity-mention; best-first vs. ranking), has interfaces to different machine learners (MaxEnt, SVM, decision trees) and provides a large set of linguistically mo"
P16-4027,P11-2018,1,0.836762,"ed Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) i"
P16-4027,D08-1008,0,0.0171416,"based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) is a tree-kernel based system developed at the University of Trento (Moschitti, 2006; Plank and Moschitti, 2013). Tree kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first ve"
P16-4027,W12-4515,1,0.938727,"ver and relation extractor require information on mentions—textual units that correspond to real-world objects. Even though some studies focus on specific subtypes of mentions (for example, on pronominal coreference or on relations between named entities), we believe that a reliable pipeline should provide information on all the possible mentions. An entity mention detector (EMD), covering a wide variety of mentions, has been developed at the University of Trento as a part of BART (see below). A more recent version has been proposed for the CoNLL-2011/2012 Shared Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-graine"
P16-4027,P08-4003,1,0.782988,"local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Muresan, 2012). Coreference Resolution. Our coreference resolution Analysis Engine is a wrapper around BART—a toolkit for Coreference Resolution developed at the University of Trento (Versley et al., 2008; Uryupina et al., 2012). It is a modular anaphora resolution system that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART implements several models of anaphora resolution (mentionpair and entity-mention; best-first vs. ranking), has interfaces to different machine learners (MaxEnt, SVM, decision trees) and provides a large set of linguistically motivated features, along with the possibility to design new ones. Entity Linking. The Entity Linking Analysis Engine (“Semanticizer”) makes use of the Entity Linking Web Service developed by"
P16-4027,P14-5010,0,\N,Missing
P16-4027,P15-1062,1,\N,Missing
P16-4027,padro-stanilovsky-2012-freeling,0,\N,Missing
P17-1094,P14-1005,0,0.250024,"Missing"
P17-1094,W10-4305,0,0.0165248,"ss functions showing the optimality property may not be enough to optimize it. Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact inference. However, the computational complexity of the measure prevents to carry out an effective learning. Our solution is thus to learn MELA with a fast linear regressor, which also produces a continuos version of the measure. 4.1 Measures for CR MELA is the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (CEAF variant with entity-based similarity) (Luo, 2005; Cai and Strube, 2010) scores, having heterogeneous nature. MUC is based on the number of correctly predicted links between mentions. The number of links P required for obtaining the key entity set K is ki ∈K (|ki |− 1), where ki are key entities in K (cardinality of each entity minus one). MUC recall computes what fraction of these were P predicted, and the predicted were as many as ki ∈K (|ki |− P |p(ki )|) = ki ∈K (|ki |− 1 − (|p(ki ) |− 1)), where p(ki ) is a partition of the key entity ki formed by intersecting it with the corresponding response entities rj ∈ R, s.t., ki ∩ rj 6= ∅. This number equals to the nu"
P17-1094,W12-4513,0,0.0190975,"oting mentions; set of the incoming candidate edges, E(v), v ∈ V ; w, ground truth tree h∗ ˆ←∅ 2: h 3: score ← 0 4: repeat 5: prev score = score 6: score = 0 7: for v ∈ V do ˆ  e(v) 8: h=h 9: ˆ e = argmaxhw, ei + C × ∆(y, h∗ , h ∪ e) e∈E(v) ˆ = h∪ˆ 10: h e 11: score = score + hw, ˆ ei 12: end for ˆ 13: score = score + ∆(y, h∗ , h) 14: until score = prev score ˆ 15: return max-violating tree h in Yu and Joachims (2009). The candidate graph, by construction, does not contain cycles, and the inference by Edmonds’ algorithm does technically the same as the ”best-left-link” inference algorithm by Chang et al. (2012). This can be schematically represented in Alg. 2. When we deal with ∆ρ , Alg. 2 cannot be longer applied as our new loss function is nonfactorizable. Thus, we designed a greedy solution, Alg. 3, which still uses the spanning tree algorithm, though, it is not guaranteed to deliver the max-violating constraint. However, finding even a suboptimal solution optimizing a more accurate loss function may achieve better performance both in terms of speed and accuracy. We reformulate Step 4 of Alg. 2, where a maxviolating incoming edge ˆ e is identified for a vertex v. The new max-violating inference o"
P17-1094,P16-1061,0,0.0564534,"s. The solution proposed by Zhao and Ng (2010) consists in finding an optimal weighting (by beam search) of training instances, which would maximize the target coreference metric. Their models, optimizing MUC and B3 , deliver a significant improvement on the MUC and ACE corpora. Uryupina et al. (2011) benefited from applying genetic algorithms for the selection of features and architecture configuration by multi-objective optimization of MUC and the two CEAF variants. Our approach is different in that the evaluation measure (its approximation) is injected directly into the learning algorithm. Clark and Manning (2016) optimize B3 directly as well within a mention-ranking model. For the efficiency reasons, they omit optimization of CEAF, which we enable in this work. SVMcluster – a structured output approach by Finley and Joachims (2005) – enables optimization to any clustering loss function (including nondecomposable ones). The authors experimentally show that optimizing particular loss functions results into a better classification accuracy in terms of the same functions. However, these are in general fast to compute, which is not the MELA case. While Finley and Joachims are compelled to perform approxima"
P17-1094,D13-1203,0,0.0179814,"ely. The Arabic data includes 359, 44, and 44 documents for training, dev. and test sets, respectively. Models We implement our version of LSP, where LSPF , LSPY J , and LSPρ use the loss functions, ∆F , ∆Y J , and ∆ρ , defined in Section 3.3 and 5.2, respectively. We used cort5 – coreference toolkit by Martschat and Strube (2015) both to preprocess the English data and to extract candidate mentions and features (the basic set). For Arabic, we used mentions and features from BART6 (Uryupina et al., 2012). We extended the initial feature set for Arabic with the feature combinations proposed by Durrett and Klein (2013), those permitted by the available initial features. Parametrization All the perceptron models require tuning of a regularization parameter C. LSPF and LSPY J – also tuning of a specific loss parameter r. We select the parameters on the entire dev. set by training on 100 random documents from the training set. We pick up C ∈ {1.0, 100.0, 1000.0, 2000.0}, the r values for LSPF from the interval [0.5, 2.5] with step 0.5, and the r values for LSPY J – from {0.05, 0.1, 0.5}. Ultimately, for English, we used C = 1000.0 in all the models; r = 1.0 in LSPF and r = 0.1 in LSPY J . And wider ranges of p"
P17-1094,E17-2023,1,0.551614,"y useful, we filter out less important features, preserving the model accuracy (at least when the selection is not extremely harsh). For this purpose, we use a feature selection approach using a basic binary classifier trained to discriminate between correct and incorrect mention pairs. It is typically used in non structured CR methods and has a nice property of using the same features of LSP (we do not use global features in our study). We carried out a selection using the absolute values of the model weights of the classifier for ranking features and then selecting those having higher rank (Haponchyk and Moschitti, 2017). The MELA produced by our models using all the training data is presented in Figure 3. The first 7 plots show learning curves in terms of LSP epochs for different feature sets with increasing size N , evaluated on the dev. set. We note that: firstly, the fewer features are available, the better LSPρ curves are than those of LSPF and LSPY J in terms of accuracy and convergence speed. The intuition is that finding a separation of the training set (generalizing well) becomes more challenging (e.g., with 10k features, the data is not linearly sep1025 arable) thus a loss function which is closer t"
P17-1094,H05-1004,0,0.334578,"). Thus, loss functions showing the optimality property may not be enough to optimize it. Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact inference. However, the computational complexity of the measure prevents to carry out an effective learning. Our solution is thus to learn MELA with a fast linear regressor, which also produces a continuos version of the measure. 4.1 Measures for CR MELA is the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (CEAF variant with entity-based similarity) (Luo, 2005; Cai and Strube, 2010) scores, having heterogeneous nature. MUC is based on the number of correctly predicted links between mentions. The number of links P required for obtaining the key entity set K is ki ∈K (|ki |− 1), where ki are key entities in K (cardinality of each entity minus one). MUC recall computes what fraction of these were P predicted, and the predicted were as many as ki ∈K (|ki |− P |p(ki )|) = ki ∈K (|ki |− 1 − (|p(ki ) |− 1)), where p(ki ) is a partition of the key entity ki formed by intersecting it with the corresponding response entities rj ∈ R, s.t., ki ∩ rj 6= ∅. This"
P17-1094,Q15-1029,0,0.0909098,"ing the use of more complex loss functions for coreference resolution (CR). Most noteworthily, we show that such functions can be (i) automatically learned also from controversial but commonly accepted CR measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL–2012 setting shows the benefit of more expressive loss for Arabic and English data. 1 Introduction In recent years, interesting structured prediction methods have been developed for coreference resolution (CR), e.g., (Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). These models are supposed to output clusters but, to better control the exponential nature of the problem, the clusters are converted into tree structures. Although this simplifies the problem, optimal solutions are associated with an exponential set of trees, requiring to maximize over such a set. This originated latent models (Yu and Joachims, 2009) optimizing the so-called lossaugmented objective functions. In this setting, loss functions need to be factorizable together with the feature representations for finding the max-violating constraints. The consequence is that only simple loss fu"
P17-1094,P16-1060,0,0.389552,"optimize the maximum of a measure, e.g., in case of Precision and Recall, or Accuracy, therefore, loss functions able to at least achieve such an optimum are preferable. 4 Automatically learning a loss function How to measure a complex task such as CR has generated a long and controversial discussion in the research community. While such a debate is progressing, the most accepted and used measure is the so-called Mention, Entity, and Link Average (MELA) score. As it will be clear from the description below, MELA is not easily interpretable and not robust to the mention identification effect (Moosavi and Strube, 2016). Thus, loss functions showing the optimality property may not be enough to optimize it. Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact inference. However, the computational complexity of the measure prevents to carry out an effective learning. Our solution is thus to learn MELA with a fast linear regressor, which also produces a continuos version of the measure. 4.1 Measures for CR MELA is the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (CEAF variant with entity-based similarity) (L"
P17-1094,W12-4501,1,0.952863,"e together with the feature representations for finding the max-violating constraints. The consequence is that only simple loss functions, basically just counting incorrect edges, were applied in previous work, giving up expressivity for simplicity. This is a critical limitation as domain experts consider more information than just counting edges. In this paper, we study the use of more expressive loss functions in the structured prediction framework for CR, although some findings are clearly applicable to more general settings. We attempted to optimize the complicated official MELA measure1 (Pradhan et al., 2012) of CR within the learning algorithm. Unfortunately, MELA is the average of measures, among which CEAFe has an excessive computational complexity preventing its direct use. To solve this problem, we defined a model for learning MELA from data using a fast linear regressor, which can be then effectively used in structured prediction algorithms. We defined features to learn such a loss function, e.g., different link counts or aggregations such as Precision and Recall. Moreover, we designed methods for generating training data from which our regression loss algorithm (RL) can generalize well and"
P17-1094,W12-4515,1,0.811809,"lts of our and previous work models evaluated Figure 2: Regressor Learning curves. dev. and test parts, respectively. The Arabic data includes 359, 44, and 44 documents for training, dev. and test sets, respectively. Models We implement our version of LSP, where LSPF , LSPY J , and LSPρ use the loss functions, ∆F , ∆Y J , and ∆ρ , defined in Section 3.3 and 5.2, respectively. We used cort5 – coreference toolkit by Martschat and Strube (2015) both to preprocess the English data and to extract candidate mentions and features (the basic set). For Arabic, we used mentions and features from BART6 (Uryupina et al., 2012). We extended the initial feature set for Arabic with the feature combinations proposed by Durrett and Klein (2013), those permitted by the available initial features. Parametrization All the perceptron models require tuning of a regularization parameter C. LSPF and LSPY J – also tuning of a specific loss parameter r. We select the parameters on the entire dev. set by training on 100 random documents from the training set. We pick up C ∈ {1.0, 100.0, 1000.0, 2000.0}, the r values for LSPF from the interval [0.5, 2.5] with step 0.5, and the r values for LSPY J – from {0.05, 0.1, 0.5}. Ultimatel"
P17-1094,W11-1908,0,0.0562174,"creases by just 0.3 points, still improving the state of the art in structured prediction. Accordingly, in the Arabic setting, where the available features are less discriminative, our approach highly improves the standard LSP. 2 Related Work There is a number of works attempting to directly optimize coreference metrics. The solution proposed by Zhao and Ng (2010) consists in finding an optimal weighting (by beam search) of training instances, which would maximize the target coreference metric. Their models, optimizing MUC and B3 , deliver a significant improvement on the MUC and ACE corpora. Uryupina et al. (2011) benefited from applying genetic algorithms for the selection of features and architecture configuration by multi-objective optimization of MUC and the two CEAF variants. Our approach is different in that the evaluation measure (its approximation) is injected directly into the learning algorithm. Clark and Manning (2016) optimize B3 directly as well within a mention-ranking model. For the efficiency reasons, they omit optimization of CEAF, which we enable in this work. SVMcluster – a structured output approach by Finley and Joachims (2005) – enables optimization to any clustering loss function"
P17-1094,M95-1005,0,0.525236,"is not easily interpretable and not robust to the mention identification effect (Moosavi and Strube, 2016). Thus, loss functions showing the optimality property may not be enough to optimize it. Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact inference. However, the computational complexity of the measure prevents to carry out an effective learning. Our solution is thus to learn MELA with a fast linear regressor, which also produces a continuos version of the measure. 4.1 Measures for CR MELA is the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (CEAF variant with entity-based similarity) (Luo, 2005; Cai and Strube, 2010) scores, having heterogeneous nature. MUC is based on the number of correctly predicted links between mentions. The number of links P required for obtaining the key entity set K is ki ∈K (|ki |− 1), where ki are key entities in K (cardinality of each entity minus one). MUC recall computes what fraction of these were P predicted, and the predicted were as many as ki ∈K (|ki |− P |p(ki )|) = ki ∈K (|ki |− 1 − (|p(ki ) |− 1)), where p(ki ) is a partition of the key entity ki forme"
P17-1094,N16-1114,0,0.1042,"Missing"
P17-1094,C10-1147,0,0.174815,"d different feature sets of a smaller size and found out that in such conditions, RL requires less epochs for converging and produces better results than the other simpler loss functions. The accuracy of RL-based model, using 16 times less features, decreases by just 0.3 points, still improving the state of the art in structured prediction. Accordingly, in the Arabic setting, where the available features are less discriminative, our approach highly improves the standard LSP. 2 Related Work There is a number of works attempting to directly optimize coreference metrics. The solution proposed by Zhao and Ng (2010) consists in finding an optimal weighting (by beam search) of training instances, which would maximize the target coreference metric. Their models, optimizing MUC and B3 , deliver a significant improvement on the MUC and ACE corpora. Uryupina et al. (2011) benefited from applying genetic algorithms for the selection of features and architecture configuration by multi-objective optimization of MUC and the two CEAF variants. Our approach is different in that the evaluation measure (its approximation) is injected directly into the learning algorithm. Clark and Manning (2016) optimize B3 directly"
P17-2082,D14-1164,0,0.0735223,"tem trained on it. Hoffmann et al. (2011) showed that by simply adding a small set of high quality labeled instances (i.e., human-annotated training data) to a larger set of instances annotated by DS, makes the overall precision of the system significantly increases. Such level of quality of the labels usually can be obtained at low cost via crowdsourcing. However, this finding does not hold for more complex tasks, where the annotators1 need to have some expertise on them. For instance in RE, several works have shown that only a marginal improvement can be achieved via crowdsourcing the data (Angeli et al., 2014; Zhang et al., 2012; Pershina et al., 2014). In such papers, the wellknown Gold Standard quality control mechanism was used without annotators being trained. Very recently, despite the previous results, Liu et al. (2016) showed a larger improvement for the RE task when training crowd workers in an interactive tutorial procedure called “Gated Instruction”. This approach, however, requires a set of high-quality labeled data (i.e., the Gold Standard) for providing the instruction and feedback to the crowd workers. However, acquiring such data requires a considerable amount of human effort. In th"
P17-2082,P07-1073,0,0.0561519,"sed computational approaches, which showed that the crowd intelligence can effectively alleviate the drifting problem in auto-annotation systems (Sun et al., 2014; Russakovsky et al., 2015). Our study shows that even without using any gold standard, we can still train workers and their annotations can achieve results comparable with the more costly state-of-the-art methods. In summary our contributions are the following: There is a large body of work on DS for RE, but we only discuss the most related to our work and refer the reader to other recent work (Wu and Weld, 2007; Mintz et al., 2009; Bunescu, 2007; Hoffmann et al., 2010; Riedel et al., 2010; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). Many researchers have exploited the techniques of combining the DS data with small human annotated data collected via crowdsourcing, to improve the relation extractor accuracy (Liu et al., 2016; Angeli et al., 2014; Zhang et al., 2012). Angeli et al. (2014) reported a minor improvement using active learning methods to select the best instances to be crowdsourced. In the same direction, Zhang et al. (2012) studied the effect of providing human feedback in crowdsourcing tasks and observed a minor im"
P17-2082,P11-1055,0,0.0304764,"s Silver Standard. More in detail, our approach is based on a noisy-label dataset, DS, whose labels are extracted in a distant supervision fashion and CS a dataset to be labeled by the crowd. The first step is to divide CS into three parts: CSI , which is used to create the instructions for the crowd workers; CSQ , which is used for asking questions about sentence annotations; and CSA , which is used to collect the labels from annotators, after they have been trained. To select CSI , we train a classifier C on DS, and then used it to label CS examples. In particular, we used MultiR framework (Hoffmann et al., 2011) to train C, as it is a widely used framework for RE. Then, we sort CS in a descending order according to the classifier prediction scores and select the first Ni elements, obtaining CSI . Next, we select the Nq examples of CS  CSI with the highest score to create the set CSQ . Note that the latter contains highly-reliable classifier annotations but since the scores are lower than for 2 520 www.crowdflower.com 0.53 0.5 CSI 0.47 CSQ Model DS-only Our Method Gated Instruction CSA 0.48 0.44 0.41 0.4 0.38 0.4 0.35 Recall F1 4.2 worker training, we perform a selection of the sentences in CSQ to be"
P17-2082,P10-1030,0,0.0747261,"Missing"
P17-2082,N16-1104,0,0.313844,"ision of the system significantly increases. Such level of quality of the labels usually can be obtained at low cost via crowdsourcing. However, this finding does not hold for more complex tasks, where the annotators1 need to have some expertise on them. For instance in RE, several works have shown that only a marginal improvement can be achieved via crowdsourcing the data (Angeli et al., 2014; Zhang et al., 2012; Pershina et al., 2014). In such papers, the wellknown Gold Standard quality control mechanism was used without annotators being trained. Very recently, despite the previous results, Liu et al. (2016) showed a larger improvement for the RE task when training crowd workers in an interactive tutorial procedure called “Gated Instruction”. This approach, however, requires a set of high-quality labeled data (i.e., the Gold Standard) for providing the instruction and feedback to the crowd workers. However, acquiring such data requires a considerable amount of human effort. In this paper, we propose to alternatively use Silver Standard, i.e., a high-quality automatic annotated data, to train the crowd workers. Specifically, we introduce a self-training strategy for crowd-sourcing, where the worke"
P17-2082,P09-1113,0,0.363147,"studies in human-based computational approaches, which showed that the crowd intelligence can effectively alleviate the drifting problem in auto-annotation systems (Sun et al., 2014; Russakovsky et al., 2015). Our study shows that even without using any gold standard, we can still train workers and their annotations can achieve results comparable with the more costly state-of-the-art methods. In summary our contributions are the following: There is a large body of work on DS for RE, but we only discuss the most related to our work and refer the reader to other recent work (Wu and Weld, 2007; Mintz et al., 2009; Bunescu, 2007; Hoffmann et al., 2010; Riedel et al., 2010; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). Many researchers have exploited the techniques of combining the DS data with small human annotated data collected via crowdsourcing, to improve the relation extractor accuracy (Liu et al., 2016; Angeli et al., 2014; Zhang et al., 2012). Angeli et al. (2014) reported a minor improvement using active learning methods to select the best instances to be crowdsourced. In the same direction, Zhang et al. (2012) studied the effect of providing human feedback in crowdsourcing tasks and obse"
P17-2082,P11-2048,1,0.921814,"bio-medical text mining. The aim of this task is to identify the type of relation between two entities in a given text. Most work on RE has mainly regarded the application of supervised methods, which require costly annotation, especially for large-scale datasets. To overcome the annotation problem, Craven et al. (1999) firstly proposed to collect automatic annotation through Distant Supervision (DS). In the DS setting, the training data for RE is often automatically annotated utilizing an external Knowledge-Base (KB) such as Wikipedia or Freebase (Hoffmann et al., 2010; Riedel et al., 2010; Nguyen and Moschitti, 2011). Although DS has 1 From now, the both entities annotators and crowd workers refer to the same concept. 518 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 518–523 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2082 Figure 1: User Interface of crowd worker training: instruction phase 2 Background Work ing, starting from the simplest concepts. Moreover, we propose an iterative humanmachine co-training framework for the task of RE. The main idea is (i) to a"
P17-2082,P14-2119,0,0.0434255,"Missing"
P17-2082,D12-1042,0,0.032664,"ntelligence can effectively alleviate the drifting problem in auto-annotation systems (Sun et al., 2014; Russakovsky et al., 2015). Our study shows that even without using any gold standard, we can still train workers and their annotations can achieve results comparable with the more costly state-of-the-art methods. In summary our contributions are the following: There is a large body of work on DS for RE, but we only discuss the most related to our work and refer the reader to other recent work (Wu and Weld, 2007; Mintz et al., 2009; Bunescu, 2007; Hoffmann et al., 2010; Riedel et al., 2010; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). Many researchers have exploited the techniques of combining the DS data with small human annotated data collected via crowdsourcing, to improve the relation extractor accuracy (Liu et al., 2016; Angeli et al., 2014; Zhang et al., 2012). Angeli et al. (2014) reported a minor improvement using active learning methods to select the best instances to be crowdsourced. In the same direction, Zhang et al. (2012) studied the effect of providing human feedback in crowdsourcing tasks and observed a minor improvement in terms of F1. At high level, our work may be viewed as"
P17-2082,P12-1087,0,0.354769,"Missing"
P17-4014,S16-1138,1,0.894236,"Missing"
P17-4014,D11-1096,1,0.82425,"nformation while CH (Fig. 1a) excludes punctuation marks and words outside of any chunk. Constituency tree representation (CONST). Standard constituency tree representation. Dependency tree representations. We provide three dependency-based relational structures: DT1, DT2 and LCT. DT1 (Fig. 1b) is a dependency tree in which grammatical relations become nodes and lemmas are located at the leaf level. DT2 (Fig. 1c) is DT1 modified to include the chunking information, and lemmas in the same chunk are grouped under the same chunk node. Finally, LCT (Fig. 1d) is a lexical-centered dependency tree (Croce et al., 2011) with the grammatical relation REL(head, child) represented as (head (child GR-REL POS-pos(head)). Here REL 2.2 RelTextRank relational links Experimental results on multiple datasets (Severyn et al., 2013; Severyn and Moschitti, 2012) show that encoding information about the relations between T1 and T2 is crucial for obtaining the state of the art in text pair ranking and classification. RelTextRank provides two kinds of links: hard string match, REL, and semantic match, FREL. REL. If some lemma occurs both in T1 and T2 , we mark the respective POS-tag nodes and their parents in the structural"
P17-4014,W14-5201,0,0.0226868,"Missing"
P17-4014,C02-1150,0,0.212719,"tructures, except for LCT. In LCT, we mark with REL- the POS (POS-) and grammar relation (GR-) nodes. FREL. When working in the question answering setting, i.e., when T1 is a question and T2 is a candidate answer passage, we encode the question focus and question class information into the structural representations. We use FREL-&lt;QC> tag to mark question focus in T1 . Then, in T2 , we mark all the named entities T2 of type compatible with the question class3 . Here, &lt;QC> is substituted 3 We use the following mappings to check for compatibility(Stanford named entity type → UIUC question class (Li and Roth, 2002)): Person, Organization → HUM ,ENTY; Misc → ENTY; Location →LOC; Date, Time, Money, Percentage, Set, Duration →NUM 80 Input Ti1 Ti …. Tin it 1. System Su bm 2. G )j ,T i AS (T i ,C Si A (C et 4. Submit (CASi, CASij) 3. Submit (CASi, CASij) 5. Return (R(Ti,Tij), R(Tij,Ti)) Experiment 8. Return (R(Ti,Tij), R(Tij,Ti), FVi,ij) 9. Submit (R(Ti,Tij), R(Tij,Ti), FVi,ij) Output Projector UIMA pipeline ) ij MLExamplei,i1 … OutputWriter MLExamplei,in 6. Submit (CASi, CASij, R(Ti,Tij), R(Tij,Ti)) 7. Similarity feature vector FVi,ij VectorFeatureExtractor Figure 2: Overall framework Step 1. Linguistic ann"
P17-4014,P14-5010,0,0.0160853,"and Massimo Nicosia† and Aliaksei Severyn† † DISI, University of Trento, 38123 Povo (TN), Italy Qatar Computing Research Institute, HBKU, 34110, Doha, Qatar {kateryna.tymoshenko,massimo.nicosia}@unitn.it {amoschitti,aseveryn}@gmail.com Abstract of highly modular applications and analysis of large volumes of unstructured information. RelTextRank is an open-source tool available at https://github.com/iKernels/ RelTextRank. It contains a number of generators for shallow and dependency-based structural representations, UIMA wrappers for multi-purpose linguistic annotators, e.g., Stanford CoreNLP (Manning et al., 2014), question classification and question focus detection modules, and a number of similarity feature vector extractors. It allows for: (i) setting experiments with the new structures, also introducing new types of relational links; (ii) generating training and test data both for kernel-based classification and reranking, also in a cross-validation setting; and (iii) generating predictions using a pre-trained classifier. In the remainder of the paper, we describe the structures that can be generated by the system (Sec 2), the overall RelTextRank architecture (Sec 3) and the specific implementatio"
P17-4014,W13-3509,1,0.768017,"f text. For example, the proposed pipeline can represent an input question and answer sentence pairs as syntacticsemantic structures, enriching them with relational information, e.g., links between question class, focus and named entities, and serializes them as training and test files for the tree kernel-based reranking framework. The pipeline generates a number of dependency and shallow chunkbased representations shown to achieve competitive results in previous work. It also enables easy evaluation of the models thanks to cross-validation facilities. 1 Introduction A number of recent works (Severyn et al., 2013; Tymoshenko et al., 2016b,a; Tymoshenko and Moschitti, 2015) show that tree kernel methods produce state-of-the-art results in many different relational tasks, e.g., Textual Entailment Recognition, Paraphrasing, question, answer and comment ranking, when applied to syntacticosemantic representations of the text pairs. In this paper, we describe RelTextRank, a flexible Java pipeline for converting pairs of raw texts into structured representations and enriching them with semantic information about the relations between the two pieces of text (e.g., lexical exact match). The pipeline is based o"
P17-4014,W03-1012,0,0.054559,"tagen package, we provide the OutputWriters, which output the data in the SVMLight-TK format in the classification (ClassifierDataGen) and the reranking (RerankingDataGenTrain and RerankingDataGenTest) modes. Currently, the type of the OutputWriter can only be specified in the code of the System module. It is possible to create a new System module starting from the existing one and code a different OutputWriter. In the classification mode, one OutputWriter generates one example for each text pair (Ti , Tij ). Another OutputWriter implementation generates input data for kernel-based reranking (Shen et al., 2003) using the strategy described in Alg. 1. VectorFeatureExtractors. RelTextRank contains feature extractors to compute: (i) cosine similarity over the text pair: simCOS (T1 , T2 ), where the input vectors are composed of word lemmas, bi-, three- an four-grams, POS-tags; similarity based on the PTK score computed for the structural repto be used. In this section, we describe the implementations of the architectural modules currently available within RelTextRank. System modules. These are the entry point to the pipeline, they initialize the specific structure and feature vector generation strategi"
P17-4014,N16-1152,1,0.889137,"he proposed pipeline can represent an input question and answer sentence pairs as syntacticsemantic structures, enriching them with relational information, e.g., links between question class, focus and named entities, and serializes them as training and test files for the tree kernel-based reranking framework. The pipeline generates a number of dependency and shallow chunkbased representations shown to achieve competitive results in previous work. It also enables easy evaluation of the models thanks to cross-validation facilities. 1 Introduction A number of recent works (Severyn et al., 2013; Tymoshenko et al., 2016b,a; Tymoshenko and Moschitti, 2015) show that tree kernel methods produce state-of-the-art results in many different relational tasks, e.g., Textual Entailment Recognition, Paraphrasing, question, answer and comment ranking, when applied to syntacticosemantic representations of the text pairs. In this paper, we describe RelTextRank, a flexible Java pipeline for converting pairs of raw texts into structured representations and enriching them with semantic information about the relations between the two pieces of text (e.g., lexical exact match). The pipeline is based on the Apache UIMA technol"
P17-4014,E14-1070,1,0.911229,"Missing"
P17-4014,D07-1003,0,0.234811,"Eval-2016, 3.D, Arabic Reference paper Struct. Feat. (Tymoshenko et al., 2016a)∗ (Tymoshenko et al., 2016a) (Tymoshenko et al., 2016b) (Barr´on-Cede˜no et al., 2016) CHp CH CONST CONST VCN N VCN N Vt Vb MRR 67.49 79.32 82.98 43.75 V MAP 66.41 73.37 73.50 38.33 + Rel.Structures MRR MAP 73.88 71.99 85.53* 75.18* 86.26 78.78 52.55 45.50 Table 3: Previous uses of RelTextRank mention which of the structures described in Sec. 2 they employ. (Tymoshenko et al., 2016a)∗ is a new structure and embedding combination approach. We show the results on two AS corpora, WikiQA (Yang et al., 2015) and TREC13 (Wang et al., 2007). Then, we report the results obtained when using RelTextRank in a cQA system for English and Arabic comment selection tasks in the SemEval-2016 competition, Tasks 3.A and 3.D (Nakov et al., 2016). V column reports the performance of the systems that employ feature vectors only, while +Rel.Structures corresponds to the systems using a combination of relational structures generated by the earlier versions of RelTextRank and feature vectors. The numbers marked by * were obtained using relational structures only, since combining features and trees decreased the overall performance in that specifi"
P17-4014,D15-1237,0,0.0498558,"SemEval-2016, 3.A, English SemEval-2016, 3.D, Arabic Reference paper Struct. Feat. (Tymoshenko et al., 2016a)∗ (Tymoshenko et al., 2016a) (Tymoshenko et al., 2016b) (Barr´on-Cede˜no et al., 2016) CHp CH CONST CONST VCN N VCN N Vt Vb MRR 67.49 79.32 82.98 43.75 V MAP 66.41 73.37 73.50 38.33 + Rel.Structures MRR MAP 73.88 71.99 85.53* 75.18* 86.26 78.78 52.55 45.50 Table 3: Previous uses of RelTextRank mention which of the structures described in Sec. 2 they employ. (Tymoshenko et al., 2016a)∗ is a new structure and embedding combination approach. We show the results on two AS corpora, WikiQA (Yang et al., 2015) and TREC13 (Wang et al., 2007). Then, we report the results obtained when using RelTextRank in a cQA system for English and Arabic comment selection tasks in the SemEval-2016 competition, Tasks 3.A and 3.D (Nakov et al., 2016). V column reports the performance of the systems that employ feature vectors only, while +Rel.Structures corresponds to the systems using a combination of relational structures generated by the earlier versions of RelTextRank and feature vectors. The numbers marked by * were obtained using relational structures only, since combining features and trees decreased the over"
P17-4014,P13-4021,0,0.328951,"Missing"
P18-2046,P16-1228,0,0.051561,"Missing"
P18-2046,P17-1032,0,0.0347,"Missing"
P18-2046,P16-1036,0,0.040984,"Missing"
P18-2046,D15-1278,0,0.0203702,"dels struggled to get good accuracy as (i) large training sets are typically not available 2 , and (ii) effectively exploiting full-syntactic parse information in NNs is still an open issue. Indeed, despite Das et al. (2016) showed that NNs are very effective to manage lexical variability, no neural model encoding syntactic information has shown a clear improvement. Indeed, also NNs directly exploiting syntactic information, such as the Recursive Neural Networks by Socher et al. (2013) or the Tree-LSTM by Tai et al. (2015), have been shown to be outperformed by well-trained sequential models (Li et al., 2015). Finally, such tree-based approaches depend on sentence structure, thus are difficult to optimize and parallelize. This is a shame as NNs are very flexible in general and enable an easy system deployment in real applications, while TK models require syntactic parsing and longer testing time. In this paper, we propose an approach that aims at injecting syntactic information in NNs, still keeping them simple. It consists of the following steps: (i) train a TK-based model on a few thousands training examples; (ii) apply such classifier to a much larger set of unlabeled training examples to gener"
P18-2046,W01-0515,0,0.152302,"oposed: (i) vectors of similarity features derived between two questions; (ii) a TK function applied to the syntactic structure of question pairs; or (iii) a combination of both. Feature Vectors (FV) are built for question pairs, (q1 , q2 ), using a set of text similarity features that capture the relations between two questions. More specifically, we compute 20 similarities sim(q1 , q2 ) using word n-grams (n = [1, . . . , 4]), after stopword removal, greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. Tasks and Baseline Models We introduce our question similarity tasks along with two of the most competitive models for their solutions. 2.1 Support Vector machines Question Matching and Ranking Question similarity in forums can be set in different ways, e.g., detecting if two questions are semantically similar or ranking a set of retrieved questions in terms of their similarity with the original question. We describe the two methods below: The Quora task regards detecting if two questions are duplicate or not, or, in other words, if they have the same intent. The assoc"
P18-2046,P08-1019,0,0.0477296,"-supervision is an important hyper-parameter that needs to be carefully chosen. 5 Automatic Data Related Work Determining question similarity is one of the main challenges in building systems that answer real user questions (Agichtein et al., 2015, 2016) in community QA, thus different approaches have been proposed. Jeon et al. (2005) used a language model based on word translation table to compute the probability of generating a query question, given a target/related question. Zhou et al. (2011) showed the effectiveness of phrase-based translation models on Yahoo! Answers. Cao et al. (2009); Duan et al. (2008) proposed a similarity between two questions based on a language model that exploits the category structure of Yahoo! Answers. Wang et al. (2009) proposed a model to find semantically related questions by computing similarity between syntactic trees representing questions. Ji et al. (2012) and Zhang et al. (2014) used latent semantic topics that generate question/answer pairs. Regarding the use of automatically labelled data, Blum and Mitchell (1998) applied semisupervised approaches, such as self-training and co-training to non-neural models. The main point 6 Conclusion In this work, we have"
P18-2046,S16-1172,1,0.92313,"ity of reusing previously asked questions makes forums much more useful. Thus, many tasks have been proposed to build automatic systems for detecting duplicate questions. These were both organized in academia, e.g., SemEval (Nakov et al., 2016, 2017), or companies, e.g., Quora 1 . An interesting outcome of the SemEval challenge was that syntactic information is essential to achieve high accuracy in question reranking tasks. Indeed, the top-systems were built using Support Vector Machines (SVMs) trained with Tree Kernels (TKs), which were applied to a syntactic representation of question text (Filice et al., 2016, 2017; Barr´onCede˜no et al., 2016). 1 2 SQuAD by Rajpurkar et al. (2016) is an exception, also possible because dealing with a simpler factoid QA task https://www.kaggle.com/c/quora-question-pairs 285 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 285–291 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2016) users were provided with a new (original) question qo and a set of related questions (q1 , q2 , ...qn ) from the QL forum3 retrieved by a search engine, i.e., Google. The goal is to"
P18-2046,S17-2003,1,0.790465,"Missing"
P18-2046,S17-2053,1,0.893845,"Missing"
P18-2046,D16-1264,0,0.0494031,"l. Thus, many tasks have been proposed to build automatic systems for detecting duplicate questions. These were both organized in academia, e.g., SemEval (Nakov et al., 2016, 2017), or companies, e.g., Quora 1 . An interesting outcome of the SemEval challenge was that syntactic information is essential to achieve high accuracy in question reranking tasks. Indeed, the top-systems were built using Support Vector Machines (SVMs) trained with Tree Kernels (TKs), which were applied to a syntactic representation of question text (Filice et al., 2016, 2017; Barr´onCede˜no et al., 2016). 1 2 SQuAD by Rajpurkar et al. (2016) is an exception, also possible because dealing with a simpler factoid QA task https://www.kaggle.com/c/quora-question-pairs 285 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 285–291 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2016) users were provided with a new (original) question qo and a set of related questions (q1 , q2 , ...qn ) from the QL forum3 retrieved by a search engine, i.e., Google. The goal is to rank question candidates, qi , by their similarity with respect to qo . qi"
P18-2046,D13-1170,0,0.00300963,"ly Amazon, Manhattan Beach, CA, USA, 90266 {antonio.uva,d.bonadiman}@unitn.it amosch@amazon.com Abstract In contrast, NNs-based models struggled to get good accuracy as (i) large training sets are typically not available 2 , and (ii) effectively exploiting full-syntactic parse information in NNs is still an open issue. Indeed, despite Das et al. (2016) showed that NNs are very effective to manage lexical variability, no neural model encoding syntactic information has shown a clear improvement. Indeed, also NNs directly exploiting syntactic information, such as the Recursive Neural Networks by Socher et al. (2013) or the Tree-LSTM by Tai et al. (2015), have been shown to be outperformed by well-trained sequential models (Li et al., 2015). Finally, such tree-based approaches depend on sentence structure, thus are difficult to optimize and parallelize. This is a shame as NNs are very flexible in general and enable an easy system deployment in real applications, while TK models require syntactic parsing and longer testing time. In this paper, we propose an approach that aims at injecting syntactic information in NNs, still keeping them simple. It consists of the following steps: (i) train a TK-based model"
P18-2046,P15-1150,0,0.0523582,"6 {antonio.uva,d.bonadiman}@unitn.it amosch@amazon.com Abstract In contrast, NNs-based models struggled to get good accuracy as (i) large training sets are typically not available 2 , and (ii) effectively exploiting full-syntactic parse information in NNs is still an open issue. Indeed, despite Das et al. (2016) showed that NNs are very effective to manage lexical variability, no neural model encoding syntactic information has shown a clear improvement. Indeed, also NNs directly exploiting syntactic information, such as the Recursive Neural Networks by Socher et al. (2013) or the Tree-LSTM by Tai et al. (2015), have been shown to be outperformed by well-trained sequential models (Li et al., 2015). Finally, such tree-based approaches depend on sentence structure, thus are difficult to optimize and parallelize. This is a shame as NNs are very flexible in general and enable an easy system deployment in real applications, while TK models require syntactic parsing and longer testing time. In this paper, we propose an approach that aims at injecting syntactic information in NNs, still keeping them simple. It consists of the following steps: (i) train a TK-based model on a few thousands training examples;"
P18-2046,P11-1066,0,0.39236,"lity of information contained in the gold labeled data deteriorates. In other words, using the right amount of weekly-supervision is an important hyper-parameter that needs to be carefully chosen. 5 Automatic Data Related Work Determining question similarity is one of the main challenges in building systems that answer real user questions (Agichtein et al., 2015, 2016) in community QA, thus different approaches have been proposed. Jeon et al. (2005) used a language model based on word translation table to compute the probability of generating a query question, given a target/related question. Zhou et al. (2011) showed the effectiveness of phrase-based translation models on Yahoo! Answers. Cao et al. (2009); Duan et al. (2008) proposed a similarity between two questions based on a language model that exploits the category structure of Yahoo! Answers. Wang et al. (2009) proposed a model to find semantically related questions by computing similarity between syntactic trees representing questions. Ji et al. (2012) and Zhang et al. (2014) used latent semantic topics that generate question/answer pairs. Regarding the use of automatically labelled data, Blum and Mitchell (1998) applied semisupervised appro"
P18-4023,S16-1138,1,0.902898,"Missing"
P18-4023,S17-2003,1,0.899267,"Missing"
P18-4023,P15-2114,0,0.0320864,"onships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Ba"
P18-4023,P08-1019,0,0.0273251,"ly available to the research and industrial community by also providing our toolkit with tutorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task"
P18-4023,W14-5201,0,0.0578265,"Missing"
P18-4023,S16-1172,1,0.905097,"Missing"
P18-4023,S15-2036,1,0.897104,"Missing"
P18-4023,P15-1097,1,0.832041,"A) or a set of questions (Task 3-B). For example, r can be a linear function, r(x, x0 ) = w ~ · φ(x, x0 ), where w ~ is the model and φ() provides a feature vector representation of the pair, (x, x0 ). The vectors φ used by Barr´on-Cede˜no et al. (2016); Filice et al. (2016) are a combination of tree kernel similarity functions and features derived from similarity measures between the two comments/questions constituting one learning example, as well as features extracting information from the forum threads the comments/questions belong to. 3.1 Tree Kernel We use the kernel function defined by Filice et al. (2015):  K((x1 , x01 ), (x2 , x02 )) = TK tx01 (x1 ), tx02 (x2 )  + TK tx1 (x01 ), tx2 (x02 ) where TK is the Partial Tree Kernel by Moschitti (2006) and ty (x) is a function which enriches the tree x with information derived from its structural similarity with the tree y (see (Severyn and Moschitti, 2012; Filice et al., 2016) for details). 135 3.2 Feature Vectors module is designed to be replicated in multiple instances to achieve scalability. Each of these modules is a pipeline deployed as UIMA-AS service that listens to a queue of processing requests (registered on the broker). Each module can"
P18-4023,P17-4004,0,0.0645536,"Missing"
P18-4023,S17-2053,1,0.895072,"Missing"
P18-4023,S15-2038,0,0.023764,"idering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse t"
P18-4023,P16-4027,1,0.845066,"f the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 134–139 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics that SVM systems based on a combination of vectorial features and tree kernels perform consistently well on the different editions of the competition (Barr´on-Cede˜no et al., 2016; Filice et al., 2016, 2017): the systems described in those papers won Task 3-A both years, placed second and first on Task 3-B in years 2016 and 2017, respectively. The most related demonstration papers to ours are (Uryupina et al., 2016; R¨uckl´e and Gurevych, 2017). As ours, the system of Uryupina et al. (2016) is a UIMA-based pipeline. Yet in their case the input is a single text and the output is the result of different levels of textual annotation (e.g., tokens, syntactic information, or wikification). R¨uckl´e and Gurevych (2017) developed an architecture to perform question and answer reranking in cQA based on deep learning. Their main focus is the analysis of attention models in these tasks. and Arabic (Barr´on-Cede˜no et al., 2016) by simply adding basic linguistic modules, e.g., the syntactic parsers, for both langu"
P18-4023,S16-1126,0,0.0200807,"(2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statisticallysignificant differences were observed in the performance of these three systems. In summary, the results for both tasks show 3 Structural Linguistic Models for cQA In this section, we describe the components of the two learning systems. The ranking function for both task"
P18-4023,S15-2035,0,0.0312867,"on model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statisticallysignificant differences were observed in the performance of these thr"
P18-4023,P11-1066,0,0.0188308,"utorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation,"
P18-4023,D15-1068,1,0.908478,"Missing"
P18-4023,P15-1025,0,0.0144318,"s, for both languages. We make our software framework, based on UIMA technology, freely available to the research and industrial community by also providing our toolkit with tutorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approach"
quarteroni-moschitti-2010-comprehensive,D07-1002,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,A00-2018,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,C94-1042,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,W05-0630,1,\N,Missing
quarteroni-moschitti-2010-comprehensive,C02-1150,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,J03-4003,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,P08-2029,1,\N,Missing
quarteroni-moschitti-2010-comprehensive,P07-1098,1,\N,Missing
quarteroni-moschitti-2010-comprehensive,P02-1034,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,J05-1004,0,\N,Missing
quarteroni-moschitti-2010-comprehensive,korhonen-etal-2006-large,0,\N,Missing
S07-1026,W05-0620,0,0.120609,"Missing"
S07-1026,J02-3001,0,0.666067,"t applications such as document retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify"
S07-1026,W05-0630,1,0.808058,"ic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify if it is suitable for Arabic. Our adopted SRL models use Support Vector Machines (SVM) to implement a two steps classification approach, i.e. boundary detection and argument classification. Such models have already been investigated in (Pradhan et al., 2003; Moschitti et al., 2005) and their description is hereafter reported. 2.1 Predicate Argument Extraction The extraction of predicative structures is carried out at the sentence level. Given a predicate within a natural language sentence, its arguments have to be properly labeled. This problem is usually divided in two subtasks: (a) the detection of the boundaries, i.e. the word spans of the arguments, and (b) the classification of their type, e.g. Arg0 and ArgM in 133 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 133–136, c Prague, June 2007. 2007 Association for Computati"
S07-1026,P04-1043,1,0.911905,"75.00 86.83 In this paper, we presented a first system for Arabic SRL system. The system yields results that are very promising, 94.06 for argument boundary detection and 81.43 on argument classification. For future work, we would like to experiment with explicit morphological features and different POS tag sets that are tailored to Arabic. The results presented here are based on gold parses. We would like to experiment with automatic parses and shallower representations such as chunked data. Finally, we would like to experiment with more sophisticated kernels, the tree kernels described in (Moschitti, 2004), i.e. models that have shown a lot of promise for the English SRL process. Acknowledgements The first author is funded by DARPA Contract No. HR001106-C-0023. References Table 3: Argument classification results on the test set. more difficult classify in Arabic than it is in English. In our current experiments, the F1 for ARG1 is only 89.83 (compared to 95.42 for ARG0). This may be attributed to two main factors. Arabic allows for different types of syntactic configurations, subject-verb-object, object-verb-subject, verb-subject-object, hence the logical object of a predicate is highly confusa"
S07-1026,N04-1032,0,0.0873263,"guments, semantic role labeling (SRL), in a sentence has a lot of potential for and is a significant step towards the improvement of important applications such as document retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in S"
S07-1026,W04-3212,0,0.079245,"ment retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify if it is suitable for"
S07-1062,P02-1034,0,0.0211455,"b basis. In order to build a development set (Dev), we sampled about one tenth, i. e. 1,606 annotations, of the original training set. For the final evaluation on the test set (Test), consisting of 3,094 annotations, we trained our classifiers on the whole training data. Statistics on the dataset composition are shown in Table 1. The evaluations were carried out with the SVMLight-TK2 software (Moschitti, 2004) which extends the SVM-Light package (Joachims, 1999) with tree kernel functions. We used the default polynomial kernel (degree=3) for the linear features and a SubSet Tree (SST) kernel (Collins and Duffy, 2002) for the comparison of ASTm 1 structured features. The kernels are normalized and summed by assigning a weight of 0.3 to the TK contribution. Training all the 50 boundary classifiers and the 619 role classifiers on the whole dataset took about 4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3 . 4.2 Evaluation All the evaluations were carried out using the CoNLL2005 evaluator tool available at http://www.lsi.upc.es/∼srlconll/soft.html. Table 2 shows the aggregate results on boundary detection (BD) and the complete SRL task (BD+RC) on the development set using the polynomial kernel alone (poly) or"
S07-1062,W06-2909,1,0.925753,"ed by each individual role classifier. The role label associated with the maximum among the scores provided by the individual classifiers is eventually selected. To make the annotations consistent with the underlying linguistic model, we employ a few simple heuristics to resolve the overlap situations that may occur, e. g. both “charter” and “the charter” in Figure 1 may be assigned a role: • if more than two nodes are involved, i. e. a node d and two or more of its descendants ni are classified as arguments, then assume that d is not an argument. This choice is justified by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); 289 a) S NP VP DT NN"
S07-1062,W06-2607,1,0.875194,"ed by each individual role classifier. The role label associated with the maximum among the scores provided by the individual classifiers is eventually selected. To make the annotations consistent with the underlying linguistic model, we employ a few simple heuristics to resolve the overlap situations that may occur, e. g. both “charter” and “the charter” in Figure 1 may be assigned a role: • if more than two nodes are involved, i. e. a node d and two or more of its descendants ni are classified as arguments, then assume that d is not an argument. This choice is justified by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); 289 a) S NP VP DT NN"
S07-1062,W04-3212,0,0.0688974,"Missing"
S07-1062,P04-1043,1,\N,Missing
S07-1062,J02-3001,0,\N,Missing
S13-1006,S13-1004,0,0.110348,"Missing"
S13-1006,S12-1059,0,0.190036,"Missing"
S13-1006,W06-1670,0,0.0172174,"Missing"
S13-1006,P02-1034,0,0.186011,"Missing"
S13-1006,S12-1088,0,0.0385256,"Missing"
S13-1006,P10-1040,0,0.032791,"Missing"
S13-1006,S12-1060,0,0.101532,"Missing"
S13-1006,S12-1051,0,\N,Missing
S15-1034,D08-1031,0,0.196066,"Pair model (EFMP): (i) EFMP does not evaluate links between entities or clusters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempted exhaustive analysis of features and their impact on the overall performance (Recasens and Hovy, 2009; Uryupina, 2006; Bengtson and Roth, 2008; Durrett and Klein, 2013). We refer the reader to (Ng, 2010) for an overview of different features. Kobdani et al. (2010) create a framework that facilitates the engineering process for complex features. This approach, however, still relies on the human expertise for creating meaningful combinations. Versley et al. (2008) use kernel-based similarity as an implicit feature induction technique. The only study we are aware of that investigates an explicit feature combination technique has been conducted by Fernandes et al. (2012). Their algorithm for Entropy-based feature induction (EFI), shows"
S15-1034,W12-4503,0,0.433022,"Missing"
S15-1034,P14-1005,0,0.0250333,"Missing"
S15-1034,D13-1203,0,0.0143342,"MP does not evaluate links between entities or clusters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempted exhaustive analysis of features and their impact on the overall performance (Recasens and Hovy, 2009; Uryupina, 2006; Bengtson and Roth, 2008; Durrett and Klein, 2013). We refer the reader to (Ng, 2010) for an overview of different features. Kobdani et al. (2010) create a framework that facilitates the engineering process for complex features. This approach, however, still relies on the human expertise for creating meaningful combinations. Versley et al. (2008) use kernel-based similarity as an implicit feature induction technique. The only study we are aware of that investigates an explicit feature combination technique has been conducted by Fernandes et al. (2012). Their algorithm for Entropy-based feature induction (EFI), shows substantial improvement on"
S15-1034,P08-2012,0,0.147854,"nks. We Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 289–298, Denver, Colorado, June 4–5, 2015. thus propose a procedure for propagating positive and negative links to create the final coreference partition: we start from the most confident among all the classifier’s decisions and iteratively construct coreference partitions by merging coreference chains (positive links) or blacklisting future merges (negative links). This decoding strategy is slower than the commonly used best-link model, but considerably faster than ILP-based decoding (Finkel and Manning, 2008; Denis and Baldridge, 2009). Second, we show that our approach, being very fast and easy to implement, can be used for a variety of low-level experiments on coreference resolution, in particular, for studies on feature engineering or selection. Thus, we augment our system with two feature combination techniques, Jaccard Item Mining (Segond and Borgelt, 2011) and Entropy Guided Feature Inductions (Fernandes et al., 2012). While the latter has been used for coreference resolution before, Jaccard Item Mining (JIM), to our knowledge, has never been applied to any NLP task. The JIM algorithm has b"
S15-1034,W11-1902,0,0.0637728,"Missing"
S15-1034,P02-1014,0,0.181167,"per, we advocate a new easy-first mention-pair algorithm (EFMP): while it is based solely on pairs of mentions and does not attempt any global inference, it benefits from the decision propagation strategy to create a coreference partition. Augmented with the sieve-style prefiltering, the system achieves a performance level comparable to the state of the art. The contribution of this paper is two-fold. First, we propose a novel decoding approach that combines predictions of the mention-pair classifier based on its confidence score, taking into account—in contrast to the previous studies, e.g. (Ng and Cardie, 2002; Stoyanov and Eisner, 2012; Bj¨orkelund and Kuhn, 2014)—both positive and negative links. We Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 289–298, Denver, Colorado, June 4–5, 2015. thus propose a procedure for propagating positive and negative links to create the final coreference partition: we start from the most confident among all the classifier’s decisions and iteratively construct coreference partitions by merging coreference chains (positive links) or blacklisting future merges (negative links). This decoding strategy is slower tha"
S15-1034,P10-1142,0,0.0175692,"sters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempted exhaustive analysis of features and their impact on the overall performance (Recasens and Hovy, 2009; Uryupina, 2006; Bengtson and Roth, 2008; Durrett and Klein, 2013). We refer the reader to (Ng, 2010) for an overview of different features. Kobdani et al. (2010) create a framework that facilitates the engineering process for complex features. This approach, however, still relies on the human expertise for creating meaningful combinations. Versley et al. (2008) use kernel-based similarity as an implicit feature induction technique. The only study we are aware of that investigates an explicit feature combination technique has been conducted by Fernandes et al. (2012). Their algorithm for Entropy-based feature induction (EFI), shows substantial improvement on the OntoNotes dataset. In the pres"
S15-1034,W06-1633,0,0.0242954,"sions (cf. Table 4). 2 Related work An improvement over the original mention-pair model (Soon et al., 2001) has been proposed by Ng and Cardie (2002). Their “best-link” algorithm picks the most confident antecedent for each anaphor. Unlike Ng and Cardie (2002), we do not process the input text from left to right incrementally, instead, we assess the confidence of all the proposed links at the same time (“easy-first”) and keep track of negative assignments. 290 Our work has been motivated by more complex algorithms using the easy-first strategy, most importantly, by Stoyanov and Eisner (2012), Nicolae and Nicolae (2006) and Bj¨orkelund and Farkas (2012). There are two important differences between these studies and the Easy-First Mention-Pair model (EFMP): (i) EFMP does not evaluate links between entities or clusters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempt"
S15-1034,W12-4501,1,0.82728,"setting, with no feature combination techniques. We compare against the CoNLL submission of the BART group to make sure that our (Soon et al., 2001)-style mention-pair baseline shows an acceptable performance. We then evaluate the EFMP approach to confirm that it provides much higher performance figures and is on par with the state of the art. In our second experiment, we use EFMP to assess the impact of the feature combina294 tion techniques on the performance of a coreference resolution system. 5.1 Experimental Setup We evaluate our approach on the English portion of the CoNLL-2012 dataset (Pradhan et al., 2012). To asses the system’s performance, we use the official scorer, provided by the CoNLL organizers. However, the version used at the competition time (v4) was later found to contain errors and replaced with another implementation (v7). This procedure resulted in a performance drop for all the systems, but didn’t affect their ranking. To facilitate comparison against previous and future studies, we report both v4 and v7 MELA scores. All the experiments are performed on automatically extracted mentions and use no gold information. For our study, we use the publicly available BART toolkit (Uryupin"
S15-1034,J01-4004,0,0.81084,"ta. At the test step, they are considered to be very confidently positive/negative instances, outscoring any test pairs, originating from the classifier output. Such pairs do not contribute to speeding up the EFMP part, however, they help improve the quality of our pairwise classifier, decreasing the bias towards negative instances in the data. 4 Techniques for generating feature combinations Most state-of-the-art coreference resolution systems combine complex modeling with rich feature sets. While early data-driven approaches were essentially knowledge-poor (for example, the famous system of Soon et al. (2001) is based on 12 shallow features), modern algorithms rely on dozens of carefully engineered features, encoding various clues relevant for the task: from different measures of surface similarity, to morphological, syntactic, semantic and discourse properties, and world knowledge. This study focuses on the automatic feature engineering task. We start from atomic features that are already encoded in a state-of-the-art toolkit (BART) and use a data mining technique, Jaccard item mining, to boost the system performance through automatic induction of complex features. The features used by most coref"
S15-1034,C12-1154,0,0.0153318,"d above all the other submissions (cf. Table 4). 2 Related work An improvement over the original mention-pair model (Soon et al., 2001) has been proposed by Ng and Cardie (2002). Their “best-link” algorithm picks the most confident antecedent for each anaphor. Unlike Ng and Cardie (2002), we do not process the input text from left to right incrementally, instead, we assess the confidence of all the proposed links at the same time (“easy-first”) and keep track of negative assignments. 290 Our work has been motivated by more complex algorithms using the easy-first strategy, most importantly, by Stoyanov and Eisner (2012), Nicolae and Nicolae (2006) and Bj¨orkelund and Farkas (2012). There are two important differences between these studies and the Easy-First Mention-Pair model (EFMP): (i) EFMP does not evaluate links between entities or clusters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several"
S15-1034,W12-4515,1,0.852051,", 2012). To asses the system’s performance, we use the official scorer, provided by the CoNLL organizers. However, the version used at the competition time (v4) was later found to contain errors and replaced with another implementation (v7). This procedure resulted in a performance drop for all the systems, but didn’t affect their ranking. To facilitate comparison against previous and future studies, we report both v4 and v7 MELA scores. All the experiments are performed on automatically extracted mentions and use no gold information. For our study, we use the publicly available BART toolkit (Uryupina et al., 2012). We have made several adjustments, starting from the configuration, suggested in the BART distribution for the OntoNotes/CoNLL data. Thus, we have modified the mention detection module, improving the treatment of coordinations and eliminating numeric named entities (PERCENT, MONEY etc). We have replaced the original split architecture with a single-classifier approach to be able to estimate the impact of our feature combination techniques in a more principled way. We have also replaced Decision Trees (Weka J48) with the LibLinear SVM package, to get a classifier outputting reliable confidence"
S15-1034,uryupina-2006-coreference,1,0.77933,"y-First Mention-Pair model (EFMP): (i) EFMP does not evaluate links between entities or clusters, always operating on mention pairs instead; (ii) EFMP integrates both positive and negative assignments in its hierarchy of easy-tohard decisions. Being conceptually very simple, our algorithm allows for a straightforward integration of other techniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempted exhaustive analysis of features and their impact on the overall performance (Recasens and Hovy, 2009; Uryupina, 2006; Bengtson and Roth, 2008; Durrett and Klein, 2013). We refer the reader to (Ng, 2010) for an overview of different features. Kobdani et al. (2010) create a framework that facilitates the engineering process for complex features. This approach, however, still relies on the human expertise for creating meaningful combinations. Versley et al. (2008) use kernel-based similarity as an implicit feature induction technique. The only study we are aware of that investigates an explicit feature combination technique has been conducted by Fernandes et al. (2012). Their algorithm for Entropy-based featur"
S15-1034,C08-1121,1,0.789141,"chniques proposed in the literature, in particular, sievestyle prefiltering (Lee et al., 2011) and feature induction. Several recent studies have attempted exhaustive analysis of features and their impact on the overall performance (Recasens and Hovy, 2009; Uryupina, 2006; Bengtson and Roth, 2008; Durrett and Klein, 2013). We refer the reader to (Ng, 2010) for an overview of different features. Kobdani et al. (2010) create a framework that facilitates the engineering process for complex features. This approach, however, still relies on the human expertise for creating meaningful combinations. Versley et al. (2008) use kernel-based similarity as an implicit feature induction technique. The only study we are aware of that investigates an explicit feature combination technique has been conducted by Fernandes et al. (2012). Their algorithm for Entropy-based feature induction (EFI), shows substantial improvement on the OntoNotes dataset. In the present work, we propose an alternative to EFI, based on the recent advances in Data Mining. We believe that Fernandes et al. (2012) have opened a very important research direction with their feature induction approach. We want therefore to evaluate EFI in a simpler"
S15-1034,W12-4502,0,\N,Missing
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S15-2047,S15-2048,0,0.127647,"Missing"
S15-2047,N10-1145,0,0.0124059,"A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment,"
S15-2047,S15-2039,0,0.0505919,"Missing"
S15-2047,S15-2035,0,0.132649,"Missing"
S15-2047,S15-2040,0,0.0642528,"Missing"
S15-2047,P07-1098,1,0.123047,"are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with featu"
S15-2047,S15-2036,1,0.483036,"Missing"
S15-2047,D14-1162,0,0.0890449,"tures above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems. Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing graph traversal, using word alignments between the question and the comment, time modeling, and sentiment analysis. Finally, for Arabic, some participants translated the Arabic data to English, and then extracted features from bo"
S15-2047,S15-2044,0,0.0450275,"Missing"
S15-2047,D13-1044,1,0.0589439,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA portal, the Fatwa website,3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic. Overall for the task, we needed manual annotations in two different languages an"
S15-2047,D07-1002,0,0.0113579,"focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied line"
S15-2047,P08-1082,0,0.294043,"Eval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to"
S15-2047,S15-2038,0,0.257423,"Missing"
S15-2047,S15-2041,0,0.062348,"Missing"
S15-2047,C10-1131,0,0.028049,"Missing"
S15-2047,D07-1003,0,0.0108685,"hop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; t"
S15-2047,N13-1106,0,0.0203816,"Missing"
S15-2047,S15-2042,0,0.0485548,"Missing"
S15-2047,S15-2043,0,0.0966631,"Missing"
S15-2047,S15-2037,0,0.0693424,"Missing"
S15-2079,P14-1062,0,0.0830087,"08; Mikolov et al., 2013) which is trained on a large unsupervised collection of tweets; (ii) we use our convolutional neural network to further refine the embeddings on a large distant supervised corpus (Go et al., 2009); (iii) the word embeddings and other parameters of the network obtained at the previous stage are used to initialize the network that is then trained on a supervised corpus from Semeval-2015. Introduction In this work we describe our deep convolutional neural network for sentiment analysis of tweets. Its architecture is most similar to the deep learning systems presented in (Kalchbrenner et al., 2014; Kim, 2014) that have recently established new stateof-the-art results on various NLP sentence classification tasks also including sentiment analysis. While already demonstrating excellent results, training a convolutional neural network that would beat hand-engineered approaches that also rely on multiple manual and automatically constructed lexicons, We apply our deep learning model on two subtasks of Semeval-2015 Twitter Sentiment Analysis (Task 10) challenge (Rosenthal et al., 2015): phraselevel (subtask A) and message-level (subtask B). Our system ranks 1st on the official test set of th"
S15-2079,D14-1181,0,0.0379152,"hich is trained on a large unsupervised collection of tweets; (ii) we use our convolutional neural network to further refine the embeddings on a large distant supervised corpus (Go et al., 2009); (iii) the word embeddings and other parameters of the network obtained at the previous stage are used to initialize the network that is then trained on a supervised corpus from Semeval-2015. Introduction In this work we describe our deep convolutional neural network for sentiment analysis of tweets. Its architecture is most similar to the deep learning systems presented in (Kalchbrenner et al., 2014; Kim, 2014) that have recently established new stateof-the-art results on various NLP sentence classification tasks also including sentiment analysis. While already demonstrating excellent results, training a convolutional neural network that would beat hand-engineered approaches that also rely on multiple manual and automatically constructed lexicons, We apply our deep learning model on two subtasks of Semeval-2015 Twitter Sentiment Analysis (Task 10) challenge (Rosenthal et al., 2015): phraselevel (subtask A) and message-level (subtask B). Our system ranks 1st on the official test set of the phrase-lev"
S15-2079,S13-2053,0,0.0872884,"Missing"
S15-2079,S15-2078,0,0.064768,"sentiment analysis of tweets. Its architecture is most similar to the deep learning systems presented in (Kalchbrenner et al., 2014; Kim, 2014) that have recently established new stateof-the-art results on various NLP sentence classification tasks also including sentiment analysis. While already demonstrating excellent results, training a convolutional neural network that would beat hand-engineered approaches that also rely on multiple manual and automatically constructed lexicons, We apply our deep learning model on two subtasks of Semeval-2015 Twitter Sentiment Analysis (Task 10) challenge (Rosenthal et al., 2015): phraselevel (subtask A) and message-level (subtask B). Our system ranks 1st on the official test set of the phrase-level and 2nd on the message-level subtask. In addition to the test set used to establish the final ranking in Semeval-2015, all systems were also evaluated on the progress test set which consists of five test sets, where our system also shows strong results. In particular, we rank all systems according to their performance on each test set and compute their average ranks. Interestingly, our model appears to be the most robust across all six test sets ranking 1st 464 Proceedings"
S15-2079,N15-1159,1,0.872695,"Missing"
S15-2079,S14-2077,0,\N,Missing
S16-1083,S16-1128,1,0.911614,"2 MAP points over the IR baseline). They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet. Their contrastive2 run is even better, with MAP of 77.33. The second best system is that of ConvKN (Barr´on-Cede˜no et al., 2016) with MAP of 76.02; they are also first on MRR, second on AvgRec and F1 , and third on Accuracy. The third best system is KeLP (Filice et al., 2016) with MAP of 75.83; they are also first on AvgRec, F1 , and Accuracy. They have a contrastive run with MAP of 76.28, which would have ranked second. The fourth best, SLS (Mohtarami et al., 2016) is very close, with MAP of 75.55; it is also first on MRR and Accuracy, and third on AvgRec. It uses a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers. 6.3 Subtask C, English (Question-External Comment Similarity) The results for subtask C, English are shown in Table 5. This subtask attracted 10 teams, and 28 runs: 10 primary and 18 contrastive. Here the teams performed much better than they did for subtask B. The first three baselines were al"
S16-1083,P07-1098,1,0.303795,"cess of their creation. Section 5 explains the evaluation measures. Section 6 presents the results for all subtasks and for all participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree"
S16-1083,S15-2047,1,0.907588,"Missing"
S16-1083,S15-2036,1,0.813812,"Missing"
S16-1083,D13-1044,1,0.636695,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. Using information about the thread is another important direction. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, whether the answer is first, whether the answer is last (Hou et al., 2015). Similarly, the third-best team, QCRI,"
S16-1083,P08-1082,0,0.0152363,"participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to a"
S16-1083,D07-1002,0,\N,Missing
S16-1083,N10-1145,0,\N,Missing
S16-1083,S15-2035,0,\N,Missing
S16-1083,C10-1131,0,\N,Missing
S16-1083,N13-1106,0,\N,Missing
S16-1083,P15-1078,1,\N,Missing
S16-1083,P15-2113,1,\N,Missing
S16-1083,R15-1058,1,\N,Missing
S16-1083,S16-1130,1,\N,Missing
S16-1083,S16-1138,1,\N,Missing
S16-1083,S16-1126,0,\N,Missing
S16-1083,S16-1132,0,\N,Missing
S16-1083,S16-1134,0,\N,Missing
S16-1083,S16-1136,1,\N,Missing
S16-1083,S16-1133,0,\N,Missing
S16-1083,S16-1172,1,\N,Missing
S16-1083,S16-1137,1,\N,Missing
S16-1083,S16-1131,0,\N,Missing
S16-1083,N16-1084,1,\N,Missing
S16-1083,S16-1135,0,\N,Missing
S16-1083,N16-1152,1,\N,Missing
S16-1083,P16-2075,1,\N,Missing
S16-1083,P16-2065,1,\N,Missing
S16-1083,S15-2037,0,\N,Missing
S16-1083,D15-1068,1,\N,Missing
S16-1083,K15-1032,1,\N,Missing
S16-1138,P15-2113,1,0.112654,"Missing"
S16-1138,S15-2048,0,0.119863,"the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and com"
S16-1138,P15-4004,0,0.00840536,"the pairs as: K((t1 , t2 ), (u1 , u2 )) = T K(t1 , u1 ) + T K(t2 , u2 ), (1) where t and u are parse trees extracted from the text pair, i.e., either question and comment for task A or question and question for tasks B and D. 4 Submissions and Results We describe our primary submissions for the four tasks in Section 4.1. The contrastive submissions are discussed in Section 4.2. Table 1 shows our official competition results for both primary and contrastive submissions. In all submissions we employed Support Vector Machines (SVM) (Joachims, 1999) using either SVM-Light (Joachims, 1999), KeLP9 (Filice et al., 2015), or SVM-light-TK10 (Moschitti, 2006) (only the last two can handle tree kernels). 4.1 Task A. The submission consists in an SVM operating on two kernels: (i) the tree kernel described in Section 3.3, applied to the structures described by Tymoshenko and Moschitti (2015) without question and focus classification; (ii) a polynomial kernel of degree 3 applied to the feature vector that is a concatenation of the feature vector described in Section 3.1, and question and answer embeddings learned on the training set by the Convolutional Neural Network (CNN) described in (Severyn and Moschitti, 2015"
S16-1138,D15-1068,1,0.19243,"Missing"
S16-1138,N16-1084,1,0.885885,"Missing"
S16-1138,P14-5010,0,0.0035797,"found in (Nakov et al., 2016). 3 Approach In order to re-rank the comments according to their relevance, either against the forum questions or against the new questions, we train a binary SVM classifier and use its score as a measure of relevance. The classifier uses partial tree kernels (Moschitti, 2006) defined over shallow syntactic trees, along with other numeric features. We used the DKPro Core toolkit (Eckart de Castilho and Gurevych, 2014)5 for pre-processing the texts in English. More precisely, we used OpenNLP’s tokenizer, POS-tagger and chunk annotator6 , and Stanford’s lemmatizer (Manning et al., 2014), all accessible through DKPro Core. We used the MADAMIRA toolkit (Pasha et al., 2014) for segmenting Arabic texts. In order to split the texts into sentences, we used the Stanford splitter.7 For parsing Arabic texts into syntactic trees, we 3 http://www.qatarliving.com/forum https://www.webteb.com/, http://www. altibbi.com/, and http://consult.islamweb. net. 5 https://dkpro.github.io/dkpro-core/ 6 https://opennlp.apache.org/ 7 http://stanfordnlp.github.io/CoreNLP 4 used the Berkeley parser (Petrov and Klein, 2007). Following, we briefly describe the numeric features used in different tasks. 3"
S16-1138,N13-1090,0,0.00760434,"situations including the identification of potential dialogues, which usually represent a bunch of bad comments, or the position of the comment in the thread. We also considered the categories of the questions in the forum (as some of them tend to include more open-ended questions and even invite for discussion on ambiguous topics), as well as the occurrence of specific strings or the length of a comment. In-depth descriptions of these features are available in (Nicosia et al., 2015). For Arabic texts, we utilize the embedding vectors as obtained by Belinkov et al. (2015): employing word2vec (Mikolov et al., 2013) on the Arabic Gigaword corpus (Parker et al., 2011). More specifically, we concatenate the vectors representing a new question and an existing question in the question– answer pair, which is then fed to the SVM classifier. 3.2 Rank Feature The meta-information in the English corpus includes the position of the forum threads in the rank generated by the Google search engine for a given new question. We exploit this information in tasks B and C. We employ the inverse of such position as a feature and refer to it as the rank feature. 3.3 structed a syntactic tree for each comment or question. Ea"
S16-1138,P07-1098,1,0.0409837,"proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to"
S16-1138,S15-2047,1,0.299017,"; (B) to re-rank the set of questions Q according to their relevance against the new question q 0 ; and finally (C) to predict the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko e"
S16-1138,S15-2036,1,0.501211,"Missing"
S16-1138,P02-1040,0,0.114288,"ll training and development sets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model,"
S16-1138,pasha-etal-2014-madamira,0,0.0108564,"Missing"
S16-1138,D13-1044,1,0.0339644,"ms for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightl"
S16-1138,W13-3509,1,0.135312,"feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh"
S16-1138,2006.amta-papers.25,0,0.0422727,"ets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model, thus we preferred not to us"
S16-1138,E14-1070,1,0.868985,"t al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called"
S16-1138,N16-1152,1,0.718101,"eature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called “original” and “related”, respectively. 896 Proceedings of SemEval-2016, pages 896–903, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics position. In contrast, for Task C, we did n"
S16-1138,N07-1051,0,\N,Missing
S16-1138,W14-5201,0,\N,Missing
S16-1172,P15-2113,1,0.5406,"Missing"
S16-1172,D11-1096,1,0.278223,"agated to their upper constituents). This method discriminates aligned sub-fragments from non-aligned ones, allowing the learning algorithm to capture relational patterns, e.g., the REL-best beach and the RELbest option. Thus, given two pairs of sentences pa = ha1 , a2 i and pb = hb1 , b2 i, some tree kernel combinations can be defined: TK+ (pa , pb ) = × (pa , pb ) AllTK = TK(a1 , b1 ) + TK(a2 , b2 ) TK(a1 , b1 ) × TK(a2 , b2 ) + TK(a1 , b2 ) × TK(a2 , b1 ), where TK is a generic tree kernel, such as the Partial Tree Kernel (PTK) (Moschitti, 2006), or the Smoothed Partial Tree Kernel (SPTK) (Croce et al., 2011). Tree kernels, computing the shared substructures between parse trees, are effective in evaluating the syntactic similarity between two texts. The proposed tree kernel combinations extend such reasoning to text pairs, and can capture emerging pairwise patterns. Therefore this method can be effective in recognizing valid question/answer pairs, or similar questions, even in those cases in which the two texts have few words in common that would cause the failure of any intra-pair approach. 4 Task Specific Features In this section, we describe features specifically developed for cQA. A single fea"
S16-1172,P15-4004,1,0.505629,"ns, q1 , . . . , q10 , (retrieved by a search engine), each one associated with its first 10 comments, cq1 , . . . , cq10 , appearing in its thread, re-rank the 100 comments according to their relevance with respect to o, i.e., the good comments are to be ranked above potential or bad comments. All the above subtasks have been modeled as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking. All classifiers and kernels have been implemented within the Kernel-based Learning Platform2 (KeLP) (Filice et al., 2015b), thus determining the team’s name. The proposed solution provides three main contributions: (i) we employ the approach proposed in (Severyn and Moschitti, 2012), which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees. We further improve the methods using the kernels proposed in (Filice et al., 2015c). (ii) we extended the features developed in (Barr´on-Cede˜no et al., 2015), by adopting several features (also derived from Word Embeddings (Mikolov et al., 2013)). (iii) we propose 2 http://www.qatarliving.com/forum https://github.com/SAG-Ke"
S16-1172,P15-1097,1,0.834522,"ns, q1 , . . . , q10 , (retrieved by a search engine), each one associated with its first 10 comments, cq1 , . . . , cq10 , appearing in its thread, re-rank the 100 comments according to their relevance with respect to o, i.e., the good comments are to be ranked above potential or bad comments. All the above subtasks have been modeled as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking. All classifiers and kernels have been implemented within the Kernel-based Learning Platform2 (KeLP) (Filice et al., 2015b), thus determining the team’s name. The proposed solution provides three main contributions: (i) we employ the approach proposed in (Severyn and Moschitti, 2012), which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees. We further improve the methods using the kernels proposed in (Filice et al., 2015c). (ii) we extended the features developed in (Barr´on-Cede˜no et al., 2015), by adopting several features (also derived from Word Embeddings (Mikolov et al., 2013)). (iii) we propose 2 http://www.qatarliving.com/forum https://github.com/SAG-Ke"
S16-1172,P07-1098,1,0.29818,"a score corresponding to a certain type of shared information or similarity between the elements within a pair. These intra-pair similarity approaches cannot capture complex relational pattern between the elements in the pair, such as a rewriting rule characterizing a valid paraphrase, or a questionanswer pattern. Such information might be manually encoded into specific features, but it would require a complex feature engineering and a deep knowledge of the linguistic involved phenomena. To automatize relational learning between pairs of texts, e.g., in case of QA, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015c), we propose new inter-pair methods to directly employ text pairs into a kernel-based learning framework. In the proposed approach, we integrate the information derived from simple intra-pair similarity functions (Section 3.1) and from the structural analogies (Sectio"
S16-1172,D13-1044,1,0.0518673,"ational pattern between the elements in the pair, such as a rewriting rule characterizing a valid paraphrase, or a questionanswer pattern. Such information might be manually encoded into specific features, but it would require a complex feature engineering and a deep knowledge of the linguistic involved phenomena. To automatize relational learning between pairs of texts, e.g., in case of QA, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015c), we propose new inter-pair methods to directly employ text pairs into a kernel-based learning framework. In the proposed approach, we integrate the information derived from simple intra-pair similarity functions (Section 3.1) and from the structural analogies (Section 3.2). 3.1 Intra-pair similarities In subtasks A and C, a good comment is likely to share similar terms with the question. In subtask B a question that is relevant to another probabl"
S16-1172,W13-3509,1,0.824988,"milarity approaches cannot capture complex relational pattern between the elements in the pair, such as a rewriting rule characterizing a valid paraphrase, or a questionanswer pattern. Such information might be manually encoded into specific features, but it would require a complex feature engineering and a deep knowledge of the linguistic involved phenomena. To automatize relational learning between pairs of texts, e.g., in case of QA, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015c), we propose new inter-pair methods to directly employ text pairs into a kernel-based learning framework. In the proposed approach, we integrate the information derived from simple intra-pair similarity functions (Section 3.1) and from the structural analogies (Section 3.2). 3.1 Intra-pair similarities In subtasks A and C, a good comment is likely to share similar terms with the question. In sub"
S16-1172,E14-1070,1,0.784325,"lements in the pair, such as a rewriting rule characterizing a valid paraphrase, or a questionanswer pattern. Such information might be manually encoded into specific features, but it would require a complex feature engineering and a deep knowledge of the linguistic involved phenomena. To automatize relational learning between pairs of texts, e.g., in case of QA, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015c), we propose new inter-pair methods to directly employ text pairs into a kernel-based learning framework. In the proposed approach, we integrate the information derived from simple intra-pair similarity functions (Section 3.1) and from the structural analogies (Section 3.2). 3.1 Intra-pair similarities In subtasks A and C, a good comment is likely to share similar terms with the question. In subtask B a question that is relevant to another probably shows common words. Fol"
S16-2018,E14-1078,0,0.0698366,"standard datasets for word-sense disambiguation. The work in (Voorhees, 2000; Volkmer et al., 2007; Alonso and Mizzaro, 2012) considers relevance judgments for building IR systems. Works closer to this paper proposed by Donmez et al. (2009), Qing et al. (2014), Raykar et al. (2010), Whitehill et al. (2009) and Sheng et al. (2008), targeted the quality of crowdsourced annotation and how to deal with noisy labels via probabilistic models. Our approach is different as we do not improve the crowd annotation, but design new weighing methods that can help the learning algorithms to deal with noise. Plank et al. (2014) also propose methods for taking noise into account when training a classifier. However, they modify the loss function of a perceptron algorithms while we assign different weights to the training instances. Regarding QA and in particular answer sentence/passage reranking there has been a large 4 Weighting models for learning methods We define weighing schema for each passage of the training questions. More in detail, each question q is associated with a sorted list of answer passages. In turn, each passage p is associated with a set of annotators {a1p , a2p , ..., akp }, where ahp is the annot"
S16-2018,C14-1145,0,0.0405113,"Missing"
S16-2018,D08-1027,0,0.196732,"Missing"
S16-2018,J96-2004,0,0.373358,", which indicates the judgment value the annotators, h, have provided for the passage p; and (ii) W (u), which aims at capturing the reliability of the crowd worker u, using the product of three factors: Crowdsourcing Pilot Experiments. Before running the main crowdsourcing task, we evaluated the effect of the initial configurations of the platform on the quality of the collected annotation. We conducted two pilot crowdsourcing experiments, which show that without quota limitation, the collected sets of annotations have both high level of agreement (0.769) calculated with the Kappa statistic (Carletta, 1996). (2) where Prior Confidence, P (u), indicates the prior trust confidence score of the crowd worker, u, provided by the crowdsourcing platform based on the quality of the annotations (s)he has done in the previous tasks. Task Confidence, T (u), indicates the total number of annotations performed by the crowd worker u in this task. The score is re-scaled and normalized between (0,1) by considering the maximum and minimum number of annotations the workers have done in this task. Consistency Confidence, C(u), indicates the total number of annotation agreements between the annotator u and the majo"
S16-2018,P08-1082,0,0.0549478,"Missing"
S16-2018,N10-1086,0,0.0864002,"Missing"
S16-2018,P07-1098,1,0.832465,"Missing"
S16-2018,C10-1131,0,0.048614,"Missing"
S16-2018,D07-1003,0,0.0232909,"Missing"
S16-2018,N13-1106,0,0.0280444,"Missing"
S16-2018,D07-1002,0,\N,Missing
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
S17-2053,S16-1172,1,0.762058,"lice.simone@gmail.com {gmartino,amoschitti}@hbku.edu.qa Abstract to o, i.e., the perfect match and relevant questions should be ranked above the irrelevant ones. Subtask C: Given a new question o, and the set of the first 10 related questions, q1 , . . . , q10 , (retrieved by a search engine), each one associated with its first 10 comments, cq1 , . . . , cq10 , appearing in its thread, re-rank the 100 comments according to their relevance with respect to o, i.e., the good comments are to be ranked above potential or bad comments. We participated to the previous year edition, where our system (Filice et al., 2016) achieved very good results, i.e., first in subtask A, third in B and second in C. For the new year challenge, we therefore decided to reuse the same system applied to a new method for selecting tree structures, (Barr´on-Cede˜no et al., 2016; Romeo et al., 2016) summarized in Sec. 3. We modeled the three subtasks as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking. We implemented models within the Kernel-based Learning Platform2 (KeLP) (Filice et al., 2015a), which determined the team’s"
S17-2053,S16-1083,1,0.848826,"Missing"
S17-2053,P15-1097,1,0.892157,"ar edition, where our system (Filice et al., 2016) achieved very good results, i.e., first in subtask A, third in B and second in C. For the new year challenge, we therefore decided to reuse the same system applied to a new method for selecting tree structures, (Barr´on-Cede˜no et al., 2016; Romeo et al., 2016) summarized in Sec. 3. We modeled the three subtasks as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking. We implemented models within the Kernel-based Learning Platform2 (KeLP) (Filice et al., 2015a), which determined the team’s name. Our tests provide two main contributions: (i) we asses the results obtained in (Filice et al., 2016), demonstrating that our kernel-based models for relational learning tasks between two texts (Filice et al., 2015b) are effective for community Question Answering. (ii) We studied the impact of text selection described in (Barr´on-Cede˜no et al., 2016). Our primary submission ranked first in subtask A, and third in subtasks B and C, demonstrating that the proposed method is very accurate and adaptable to different learning problems. At the moment, we could n"
S17-2053,C16-1163,1,0.743959,"Missing"
S17-2053,D13-1044,1,0.848014,"e an expected answering form. Similarly, in Task B, related questions may be characterized by the application of some latent paraphrasing rules. Such pairwise patterns cannot be captured by any intrapair similarity feature, and require an alternative approach. Specific features may be manually defined, but this would require a complex feature engineering. To automatize relational learning between pairs of texts, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015b), we defined new interpair methods to directly employ text pairs into a kernel-based learning framework. The kernels we proposed can be directly applied to subtask B and to subtasks A and C for learnIntra-pair similarities In subtasks A and C, a good comment is likely to share similar terms with the question. In subtask B a question that is relevant to another probably shows common words. Following this intuition, given a text pair (either questio"
S17-2053,W13-3509,1,0.852492,"me question types may have an expected answering form. Similarly, in Task B, related questions may be characterized by the application of some latent paraphrasing rules. Such pairwise patterns cannot be captured by any intrapair similarity feature, and require an alternative approach. Specific features may be manually defined, but this would require a complex feature engineering. To automatize relational learning between pairs of texts, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015b), we defined new interpair methods to directly employ text pairs into a kernel-based learning framework. The kernels we proposed can be directly applied to subtask B and to subtasks A and C for learnIntra-pair similarities In subtasks A and C, a good comment is likely to share similar terms with the question. In subtask B a question that is relevant to another probably shows common words. Following this intuition, g"
S17-2053,P07-1098,1,0.290604,"q with its answers. Similarities between q and o are also employed in subtask C. 2.2 Inter-pair kernel methods In tasks A and C, some question types may have an expected answering form. Similarly, in Task B, related questions may be characterized by the application of some latent paraphrasing rules. Such pairwise patterns cannot be captured by any intrapair similarity feature, and require an alternative approach. Specific features may be manually defined, but this would require a complex feature engineering. To automatize relational learning between pairs of texts, one of the early works is (Moschitti et al., 2007; Moschitti, 2008). This approach was improved in several subsequent researches (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Severyn and Moschitti, 2013; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015), exploiting relational tags and linked open data. In particular, in (Filice et al., 2015b), we defined new interpair methods to directly employ text pairs into a kernel-based learning framework. The kernels we proposed can be directly applied to subtask B and to subtasks A and C for learnIntra-pair similarities In subtasks A and C, a good comment is likely to share similar ter"
S17-2053,S17-2003,1,0.26022,"Missing"
S17-2053,D11-1096,1,\N,Missing
S17-2053,C16-1237,1,\N,Missing
uryupina-etal-2014-sentube,J08-4004,0,\N,Missing
uryupina-etal-2014-sentube,P07-1056,0,\N,Missing
uryupina-etal-2014-sentube,pak-paroubek-2010-twitter,0,\N,Missing
versley-etal-2008-bart-modular,N07-1011,0,\N,Missing
versley-etal-2008-bart-modular,qiu-etal-2004-public,0,\N,Missing
versley-etal-2008-bart-modular,N03-1033,0,\N,Missing
versley-etal-2008-bart-modular,P00-1023,0,\N,Missing
versley-etal-2008-bart-modular,P06-1006,1,\N,Missing
versley-etal-2008-bart-modular,P05-1022,0,\N,Missing
versley-etal-2008-bart-modular,P04-1018,0,\N,Missing
versley-etal-2008-bart-modular,I05-1063,1,\N,Missing
versley-etal-2008-bart-modular,P06-1055,0,\N,Missing
versley-etal-2008-bart-modular,N06-1025,1,\N,Missing
versley-etal-2008-bart-modular,J01-4004,0,\N,Missing
versley-etal-2008-bart-modular,E06-1015,1,\N,Missing
versley-etal-2008-bart-modular,wellner-vilain-2006-leveraging,0,\N,Missing
versley-etal-2008-bart-modular,uryupina-2006-coreference,0,\N,Missing
versley-etal-2008-bart-modular,W00-0730,0,\N,Missing
versley-etal-2008-bart-modular,P05-1045,0,\N,Missing
W04-0819,P98-1013,0,\N,Missing
W04-0819,C98-1013,0,\N,Missing
W04-0819,P03-1002,1,\N,Missing
W04-0819,J02-3001,0,\N,Missing
W04-2403,W03-1006,0,0.085887,", Position and Voice cannot be expressed by SK. This suggests that a combination of the flat features (especially the named entity class (Surdeanu et al., 2003)) with SK could furthermore improve the predicate argument representation. 4 The Experiments For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank5 2 (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurasky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing data to affect the global performance. In Table 2 some characteristics of the corpus used in our experiments are reported. The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/"
W04-2403,P02-1034,0,0.47551,"g. (Surdeanu et al., 2003), to improve the flat feature space. Convolution kernels are a viable alternative to flat feature representation that aims to capture the structural information in term of sub-structures. The kernel functions can be used to measure similarities between two objects without explicitly evaluating the object features. That is, we do not need to understand which syntactic feature may be suited for representing semantic data. We need only to define the similarity function between two semantic structures. An example of convolution kernel on the parse-tree space is given in (Collins and Duffy, 2002). The aim was to design a novel syntactic parser by looking at the similarity between the testing parse-trees and the correct parse-trees available for training. In this paper, we define a kernel in a semantic structure space to learn the classification function of predicate arguments. The main idea is to select portions of syntactic/semantic trees that include the target &lt;predicate, argument> pair and to define a kernel function between these objects. If our similarity function is well defined the learning model will converge and provide an effective argument classification. Experiments on Pr"
W04-2403,W03-1008,0,0.0478224,"nnot be expressed by SK. This suggests that a combination of the flat features (especially the named entity class (Surdeanu et al., 2003)) with SK could furthermore improve the predicate argument representation. 4 The Experiments For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank5 2 (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurasky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing data to affect the global performance. In Table 2 some characteristics of the corpus used in our experiments are reported. The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with the default linear kernel"
W04-2403,J02-3001,0,0.808628,"rn the classification function of predicate arguments. The main idea is to select portions of syntactic/semantic trees that include the target &lt;predicate, argument> pair and to define a kernel function between these objects. If our similarity function is well defined the learning model will converge and provide an effective argument classification. Experiments on PropBank data show not only that Support Vector Machines (SVMs) trained with the proposed semantic kernel converge but also that they have a higher accuracy than SVMs trained with a linear kernel on the standard features proposed in (Gildea and Jurasky, 2002). This provides a piece of evidence that convolution kernel can be used to learn semantic linguistic structures. Moreover, interesting research lines on the use of kernel for NLP are enabled, e.g. question classification in Question/Answering or automatic template designing in Information Extraction. The remaining of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it. In Section 3 we present our approach based on the parse-tree kernel whereas in Section 4 we show our comparative results between SVMs using standa"
W04-2403,P02-1031,0,0.269803,"suggests that a combination of the flat features (especially the named entity class (Surdeanu et al., 2003)) with SK could furthermore improve the predicate argument representation. 4 The Experiments For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank5 2 (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurasky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing data to affect the global performance. In Table 2 some characteristics of the corpus used in our experiments are reported. The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with the default linear kernel for the standard feature"
W04-2403,kingsbury-palmer-2002-treebank,0,0.0462809,"uments defined in PropBank. Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel. fined on syntactic parse trees. For example, Figure 1 shows the parse tree of the sentence: ""Paul gives a lecture in Rome"" along with the annotation of predicate arguments. A predicate may be a verb or a noun or an adjective whereas generally Arg 0 stands for agent, Arg 1 for direct object or theme or patient and ArgM may indicate locations, as in our example. A standard for predicate argument annotation is provided in the PropBank project (Kingsbury and Palmer, 2002). It has produced one million word corpus annotated with predicate-argument structures on top of the Penn Treebank 2 Wall Street Journal texts. In this way, for a large number of the Penn TreeBank parse-trees, there are available predicate annotations in a style similar to that shown in Figure 1. S Paul Arg. 0 1 Introduction Several linguistic theories, e.g. (Jackendoff, 1990), claim that semantic information in natural language texts is connected to syntactic structures. Hence, to deal with natural language semantics, the learning algorithm should be able to represent and process structured d"
W04-2403,J93-2004,0,0.0265106,"[DT a] [N talk]] which explicitly encode the Phrase Type feature NP for Arg 1 in Figure 2.b. The Predicate Word is represented by the fragment [V delivers] and the Head Word is present as [N talk]. Finally, Governing Category, Position and Voice cannot be expressed by SK. This suggests that a combination of the flat features (especially the named entity class (Surdeanu et al., 2003)) with SK could furthermore improve the predicate argument representation. 4 The Experiments For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank5 2 (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurasky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing data to affect the global per"
W04-2403,P03-1002,0,0.44157,"s. Second, some information contained in the standard features is embedded in SK: Phrase Type, Predicate Word and Head Word explicitly appear as structure fragments. For example, in Figure 3 are shown fragments like [NP [DT] [N]] or [NP [DT a] [N talk]] which explicitly encode the Phrase Type feature NP for Arg 1 in Figure 2.b. The Predicate Word is represented by the fragment [V delivers] and the Head Word is present as [N talk]. Finally, Governing Category, Position and Voice cannot be expressed by SK. This suggests that a combination of the flat features (especially the named entity class (Surdeanu et al., 2003)) with SK could furthermore improve the predicate argument representation. 4 The Experiments For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank5 2 (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurasky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training"
W04-2403,N04-1030,0,\N,Missing
W04-2505,C02-1042,0,0.0681666,"Missing"
W04-2505,W03-1008,0,0.0125115,"basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers u"
W04-2505,J02-3001,0,0.0834271,"tructures are used for indexing/retrieving candidate passages. The Answer Processing function involves the recognition of the answer structure and intentional structure. Often this requires reference resolution. The implied information coerced from both the question and the candidate answer is also validated before deciding on the answer correctness. 4 Predicate-Argument Structures To identify predicate-argument structures in questions and passages, we have: (1) used the Proposition Bank or PropBank as training data; and (2) a mode for predicting argument roles similar to the one employed by (Gildea and Jurafsky, 2002). PropBank is a one million word corpus annotated with predicate-argument structures on top of the Penn Treebank 2 Wall Street Journal texts. For any given predicate, the expected arguments are labeled sequentially from Arg 0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for direct object or theme or patient, Arg 2 for indirect object or benefactive or instrument or attribute or end state, Arg 3 for start point or benefactive or attribute and Arg4 for end point. In addition to these core arguments, adjunctative arguments are marked up. They include functional tags from Treebank, e.g. ArgM"
W04-2505,P02-1031,0,0.0154399,"adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usually are not able to pr"
W04-2505,W03-1006,0,0.0127029,"common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases"
W04-2505,C00-1043,1,0.873305,"Missing"
W04-2505,P01-1037,1,0.841954,"ing and (3) Answer Extraction. In the case of factoid questions , question processing involves the classification of questions with the purpose of predicting what semantic class the answer should belong to. Thus we may have questions asking about P EOPLE, O RGANIZATIONS, T IME or L OCATIONS. Since opendomain Q/A systems process questions regardless of the domain of interest, question processing must be based on an extended ontology of answer types. The identification of the expected answer type is based either on binary semantic dependencies extracted from the syntactic parse of the question (Harabagiu et al., 2001) or on the predicateargument structure of the question. In both cases, the relation to the question stem (i.e. what, who, when) enables the classification. Figure 2 illustrates a factoid question generated as an intended question and the derivation of its expected answer type. However, many times the expected answer type needs to be identified from an ontology that has high lexicosemantic coverage. Many Q/A systems use the WordNet database for this purpose. In contrast, definition questions do not require the identification of the expected anDefinition Question: Question Parse: What is ETA in"
W04-2505,P03-1002,1,0.905027,"tion of the question context. In this paper, by considering the intentional information and the implied information that can be derived when processing questions, we introduce a novel model of Q/A, which has access to rich semantic structures and enables the retrieval of more accurate answers as well as inference processes that explain the validity and contextual coverage of answers. Figure 5 shows the structure of the novel model of Q/A we propose. Both Question Processing and Document Processing have the recognition of predicate-argument structures as a crux of their models. As reported in (Surdeanu et al., 2003), the recognition of predicate-argument structures depends on features made available by full syntactic parses and by Named Entity Recognizers. As we shall show in this paper, the predicate-argument structures enable the recognition of question pattern, the question focus and the intentional structure associated with Question Processing Syntactic Parse Document Processing Named Entity Recognition Answer Processing Named Entity Recognition Syntactic Parse Recognition of Answer Structure Identification of Predicate−Argument Structures Recognition of Identification of Question Pattern Predicate−A"
W04-2505,J03-2004,0,\N,Missing
W04-2505,P03-1003,0,\N,Missing
W04-2505,N04-1030,0,\N,Missing
W04-2506,A00-2018,0,0.0235811,"lues. However, most of the time the question stems are either ambiguous or they simply do not exist. For example, questions having what as their stem may ask about anything. In this case another word from the question needs to be used to determine the semantic class of the expected answer. In particular, the additional word is semantically classified against an ontology of semantic classes. To determine which word indicates the semantic class of the expected answer, the syntactic dependencies1 between the question words may be employed (Harabagiu 1 Syntactic parsers publicly available, e.g., (Charniak, 2000; et al., 2000; Pasca and Harabagiu, 2001; Harabagiu et al., 2001). Sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers. The use of text classification aims to filter out the incorrect set of answers that Q/A systems provide. 2.2 Paragraph Retrieval Once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in a list. To find the morphological variations of th"
W04-2506,P97-1003,0,0.160529,"anking: We compute the sentence ranks as a by product of sorting the selected sentences. To sort the sentences, we may use any sorting algorithm, e.g., the quicksort, given that we provide a comparison function between each pair of sentences. To learn the comparison function we use a simple neural network, namely, the perceptron, to compute a relative comparison between any two sentences. This score is computed by considering four different features for each sentence as explained in (Pasca and Harabagiu, 2001). Step 3) Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997), can be used to capture the binary dependencies between the head of each phrase. answers. If we lead fewer than 5 sentences to select from, we return all of them. Once the answers are extracted we can apply an additional filter based on text categories. The idea is to match the categories of the answers against those of the questions. Next section addresses the problem of question and answer categorization. 3 Text and Question Categorization To exploit category information for Q/A we categorize both answers and questions. For the former, we define as categories of an answer a the categories o"
W04-2506,C00-1043,1,0.874019,"Missing"
W04-2506,P01-1037,1,0.850815,"ther ambiguous or they simply do not exist. For example, questions having what as their stem may ask about anything. In this case another word from the question needs to be used to determine the semantic class of the expected answer. In particular, the additional word is semantically classified against an ontology of semantic classes. To determine which word indicates the semantic class of the expected answer, the syntactic dependencies1 between the question words may be employed (Harabagiu 1 Syntactic parsers publicly available, e.g., (Charniak, 2000; et al., 2000; Pasca and Harabagiu, 2001; Harabagiu et al., 2001). Sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers. The use of text classification aims to filter out the incorrect set of answers that Q/A systems provide. 2.2 Paragraph Retrieval Once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in a list. To find the morphological variations of the nouns, we used the CELEX (Baayen et al., 1995) database. The lis"
W05-0407,P02-1034,0,0.332912,"e selection techniques (Kohavi and Sommerfield, 1995) are not so useful since the critical problem relates to feature generation rather than selection. For example, the design of features for a natural language syntactic parse-tree re-ranking problem (Collins, 2000) cannot be carried out without a deep knowledge about automatic syntactic parsing. The modeling of syntactic/semantic based features should take into account linguistic aspects to detect the interesting context, e.g. the ancestor nodes or the semantic dependencies (Toutanova et al., 2004). A viable alternative has been proposed in (Collins and Duffy, 2002), where convolution kernels were used to implicitly define a tree substructure space. The selection of the relevant structural features was left to the voted perceptron learning algorithm. Another interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al., 2004). The good results show that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited. Along the same line, automatic learning tasks that rely on syntactic information may take advantage of a tree kernel approach. One of s"
W05-0407,P04-1054,0,0.0852124,"feature descriptors to generate different feature type. The descriptors, which are quantified logical prepositions, are instantiated by means of a concept graph which encodes the structural data. In the case of relation extraction the concept graph is associated with a syntactic shallow parse and the extracted propositional features express fragments of a such syntactic structure. The experiments over the named entity class categorization show that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy. In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. The major novelty was the combination of the contiguous and sparse kernels with the word kernel. The results show that the contiguous outperforms the sparse kernel and the bag-of-words. 7 Conclusions The feature design for new natural language learning tasks is difficult. We can take advantage from the kernel methods to model our intuitive knowledge about the target linguistic phenomenon. In this paper we have shown that we can exploit the properties of tree kernels to engineer syntactic features fo"
W05-0407,W03-1008,0,0.0144628,"ally from Arg0 to Arg9, ArgA and ArgM. Figure 1 shows an example of the PB predicate annotation of the sentence: John Predicates in PB are only embodied by verbs whereas most of the times Arg0 is the subject, Arg1 is the direct object and ArgM indicates locations, as in our example. S VP N John Arg. 0 V NP rented D Predicate a PP N IN N room in Boston Arg. 1 Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for automatic predicate argument extraction have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003; Pradhan et al., 2004). Their common characteristic is the adoption of feature spaces that model predicateargument structures in a flat feature representation. In the next section, we present the common parse tree-based approach to this problem. 2.1 Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of its words in the sentence, and (b) the classification of the argume"
W05-0407,J02-3001,0,0.417786,"edicate, the expected arguments are labeled sequentially from Arg0 to Arg9, ArgA and ArgM. Figure 1 shows an example of the PB predicate annotation of the sentence: John Predicates in PB are only embodied by verbs whereas most of the times Arg0 is the subject, Arg1 is the direct object and ArgM indicates locations, as in our example. S VP N John Arg. 0 V NP rented D Predicate a PP N IN N room in Boston Arg. 1 Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for automatic predicate argument extraction have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003; Pradhan et al., 2004). Their common characteristic is the adoption of feature spaces that model predicateargument structures in a flat feature representation. In the next section, we present the common parse tree-based approach to this problem. 2.1 Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of its words in"
W05-0407,P02-1031,0,0.0841262,"ents are labeled sequentially from Arg0 to Arg9, ArgA and ArgM. Figure 1 shows an example of the PB predicate annotation of the sentence: John Predicates in PB are only embodied by verbs whereas most of the times Arg0 is the subject, Arg1 is the direct object and ArgM indicates locations, as in our example. S VP N John Arg. 0 V NP rented D Predicate a PP N IN N room in Boston Arg. 1 Arg. M Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for automatic predicate argument extraction have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003; Pradhan et al., 2004). Their common characteristic is the adoption of feature spaces that model predicateargument structures in a flat feature representation. In the next section, we present the common parse tree-based approach to this problem. 2.1 Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of its words in the sentence, and (b) th"
W05-0407,kingsbury-palmer-2002-treebank,0,0.0612761,"the relevant structural features was left to the voted perceptron learning algorithm. Another interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al., 2004). The good results show that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited. Along the same line, automatic learning tasks that rely on syntactic information may take advantage of a tree kernel approach. One of such tasks is the automatic boundary detection of predicate arguments of the kind defined in PropBank (Kingsbury and Palmer, 2002). For this purpose, given a predicate p in a sentence s, we can define the notion of predicate argument spanning trees (P AST s) as those syntactic subtrees of s which exactly cover all and only the p’s arguments (see Section 4.1). The set of nonspanning trees can be then associated with all the remaining subtrees of s. An automatic classifier which recognizes the spanning trees can potentially be used to detect the predicate argument boundaries. Unfortunately, the application of such classifier to all possible sentence subtrees would require an exponential execution time. As a consequence, we"
W05-0407,J93-2004,0,0.0282553,"Missing"
W05-0407,P04-1043,1,0.872609,"f the argument spanning trees and (2) how to deal with the exponential number of N ST s. For the first problem, the use of tree kernels over the P AST s can be an alternative to the manual features design as the learning machine, (e.g. SVMs) can select the most relevant features from a high dimensional feature space. In other words, we can use Eq. 1 to estimate the similarity between two P AST s avoiding to define explicit features. The same idea has been successfully applied to the parse-tree reranking task (Taskar et al., 2004; Collins and Duffy, 2002) and predicate argument classification (Moschitti, 2004). For the second problem, i.e. the high computational complexity, we can cut the search space by us52 ing a traditional boundary classifier (tbc), e.g. (Pradhan et al., 2004), which provides a small set of potential argument nodes. Let PA be the set of nodes located by tbc as arguments. We may consider the set P of the N ST s associated with any subset of PA, i.e. P = {ps : s ⊆ PA}. However, also the classification of P may be computationally problematic since theoretically there are |P |= 2|PA| members. In order to have a very efficient procedure, we applied pastc to only the PA sets associat"
W05-0407,W04-3201,0,0.194639,"eep knowledge about automatic syntactic parsing. The modeling of syntactic/semantic based features should take into account linguistic aspects to detect the interesting context, e.g. the ancestor nodes or the semantic dependencies (Toutanova et al., 2004). A viable alternative has been proposed in (Collins and Duffy, 2002), where convolution kernels were used to implicitly define a tree substructure space. The selection of the relevant structural features was left to the voted perceptron learning algorithm. Another interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al., 2004). The good results show that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited. Along the same line, automatic learning tasks that rely on syntactic information may take advantage of a tree kernel approach. One of such tasks is the automatic boundary detection of predicate arguments of the kind defined in PropBank (Kingsbury and Palmer, 2002). For this purpose, given a predicate p in a sentence s, we can define the notion of predicate argument spanning trees (P AST s) as those syntactic subtrees of s which"
W05-0407,W04-3222,0,0.0995259,"Missing"
W05-0601,C96-1005,0,0.0243895,"ss pieces and this impacts on the similarity score that should be used in IR applications. Methods to solve the above problems attempt to map a priori the terms to specific generalizations levels, i.e. to cuts in the hierarchy (e.g. (Li and Abe, 1998; Resnik, 1997)), and use corpus statistics for weighting the resulting mappings. For several tasks (e.g. in TC) this is unsatisfactory: different contexts of the same corpus (e.g. documents) may require different generalizations of the same word as they independently impact on the document similarity. On the contrary, the Conceptual Density (CD) (Agirre and Rigau, 1996) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. The CD defines a metrics according to the topological structure of WordNet and can be seemingly applied to two or more words. The measure formalized hereafter adapt to word pairs a more general definition given in (Basili et al., 2004). We denote by s¯ the set of nodes of the hierarchy rooted in the synset s, i.e. {c ∈ S|c isa s}, where S is the set of WN synsets. By definition ∀s ∈ S, s ∈ s¯. CD makes a guess about the proximity of the senses, s1 and s2 , o"
W05-0601,basili-etal-2004-similarity,1,0.81781,"ideal tree must contain 4 nodes, i.e. the grandfather which has a bf of 1 and the father which has bf of 2 for an average of 1.5. When bf is 1 the Eq. 1 degenerates to the inverse of the number of nodes in the path between s1 and s2 , i.e. the simple proximity measure used in (Siolas and d’Alch Buc, 2000). It is worth noting that for each pair CD(u1 , u2 ) determines the similarity according to the closest lexical senses, s1 , s2 ∈ s¯: the remaining senses of u1 and u2 are irrelevant, with a resulting semantic disambiguation side effect. CD has been successfully applied to semantic tagging ((Basili et al., 2004)). As the WN hierarchies for other POS classes (i.e. verb and adjectives) have topological properties different from the noun hyponimy network, their semantics is not suitably captured by Eq. 1. In this paper, Eq. 1 has thus been only applied to noun pairs. As the high number of such pairs increases the computational complexity of the target learning algorithm, efficient approaches are needed. The next section describes how kernel methods can make practical the use of the Conceptual Density in Text Categorization. 3 A WordNet Kernel for document similarity Term similarities are used to design"
W05-0601,J02-2003,0,0.0233708,"lly Section 6 derives the conclusions. 2 Term similarity based on general knowledge In IR, any similarity metric in the vector space models is driven by lexical matching. When small training material is available, few words can be effectively used and the resulting document similarity metrics may be inaccurate. Semantic generalizations overcome data sparseness problems as contributions from different but semantically similar words are made available. Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002)). The resource employed in most works is WordNet (Fellbaum, 1998) which contains three subhierarchies: for nouns, verbs and adjectives. Each hierarchy represents lexicalized concepts (or senses) organized according to an ”isa-kind-of ” relation. A concept s is described by a set of words syn(s) called synset. The words w ∈ syn(s) are synonyms according to the sense s. For example, the words line, argumentation, logical argument and line of reasoning describe a synset which expresses the methodical process of logical reasoning (e.g. ”I can’t follow your line of reasoning”). Each word/term may"
W05-0601,J98-2002,0,0.0374322,"stance is unclear. The pervasive lexical ambiguity is also problematic as it impacts on the measure of conceptual distances between word pairs. Second, the approximation of a set of concepts by means of their generalization in the hierarchy implies a conceptual loss that affects the target IR (or NLP) tasks. For example, black and white are colors but are also chess pieces and this impacts on the similarity score that should be used in IR applications. Methods to solve the above problems attempt to map a priori the terms to specific generalizations levels, i.e. to cuts in the hierarchy (e.g. (Li and Abe, 1998; Resnik, 1997)), and use corpus statistics for weighting the resulting mappings. For several tasks (e.g. in TC) this is unsatisfactory: different contexts of the same corpus (e.g. documents) may require different generalizations of the same word as they independently impact on the document similarity. On the contrary, the Conceptual Density (CD) (Agirre and Rigau, 1996) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. The CD defines a metrics according to the topological structure of WordNet and can be see"
W05-0601,W97-0209,0,0.0322557,"The pervasive lexical ambiguity is also problematic as it impacts on the measure of conceptual distances between word pairs. Second, the approximation of a set of concepts by means of their generalization in the hierarchy implies a conceptual loss that affects the target IR (or NLP) tasks. For example, black and white are colors but are also chess pieces and this impacts on the similarity score that should be used in IR applications. Methods to solve the above problems attempt to map a priori the terms to specific generalizations levels, i.e. to cuts in the hierarchy (e.g. (Li and Abe, 1998; Resnik, 1997)), and use corpus statistics for weighting the resulting mappings. For several tasks (e.g. in TC) this is unsatisfactory: different contexts of the same corpus (e.g. documents) may require different generalizations of the same word as they independently impact on the document similarity. On the contrary, the Conceptual Density (CD) (Agirre and Rigau, 1996) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. The CD defines a metrics according to the topological structure of WordNet and can be seemingly applied"
W05-0630,W05-0620,0,0.154804,"Missing"
W05-0630,J02-3001,0,0.712927,"arquez, 2005), we capitalized on our experience on the semantic shallow parsing by extending our system, widely experimented on PropBank and FrameNet (Giuglea and Moschitti, 2004) data, with a twostep boundary detection and a hierarchical argument classification strategy. Currently, the system can work in both basic and enhanced configuration. Given the parse tree of an input sentence, the basic system applies (1) a boundary classifier to select the nodes associated with correct arguments and (2) a multi-class labeler to assign the role type. For such models, we used some of the linear (e.g. (Gildea and Jurasfky, 2002; Pradhan et al., 2005)) and structural (Moschitti, 2004) features developed in previous studies. In the enhanced configuration, the boundary annotation is subdivided in two steps: a first pass in which we label argument boundary and a second pass in which we apply a simple heuristic to eliminate the argument overlaps. We have also tried some strategies to learn such heuristics automatically. In order to do this we used a tree kernel to classify the subtrees associated with correct predicate argument structures (see (Moschitti et al., 2005)). The rationale behind such an attempt was to exploit"
W05-0630,W05-0407,1,0.90299,"For such models, we used some of the linear (e.g. (Gildea and Jurasfky, 2002; Pradhan et al., 2005)) and structural (Moschitti, 2004) features developed in previous studies. In the enhanced configuration, the boundary annotation is subdivided in two steps: a first pass in which we label argument boundary and a second pass in which we apply a simple heuristic to eliminate the argument overlaps. We have also tried some strategies to learn such heuristics automatically. In order to do this we used a tree kernel to classify the subtrees associated with correct predicate argument structures (see (Moschitti et al., 2005)). The rationale behind such an attempt was to exploit the correlation among potential arguments. Also, the role labeler is divided into two steps: (1) we assign to the arguments one out of four possible class labels: Core Roles, Adjuncts, Continuation Arguments and Co-referring Arguments, and (2) in each of the above class we apply the set of its specific classifiers, e.g. A0,..,A5 within the Core Role class. As such grouping is relatively new, the traditional features may not be sufficient to characterize each class. Thus, to generate a large set of features automatically, we again applied t"
W05-0630,W04-3212,0,0.156334,"ding to the ONE-vs.ALL scheme. To implement the multi-class classifiers we select the argument associated with the maximum among the SVM scores. To represent the Fp,a pairs we used the following features: - the Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice defined in (Gildea and Jurasfky, 2002); - the Partial Path, Compressed Path, No Direction Path, Constituent Tree Distance, Head Word POS, First and Last Word/POS in Constituent, SubCategorization and Head Word of Prepositional Phrases proposed in (Pradhan et al., 2005); and - the Syntactic Frame designed in (Xue and Palmer, 2004). 202 Figure 1: Architecture of the Hierarchical Semantic Role Labeler. 3 Hierarchical Semantic Role Labeler Having two phases for argument labeling provides two main advantages: (1) the efficiency is increased as the negative boundary examples, which are almost all parse-tree nodes, are used with one classifier only (i.e. the boundary classifier), and (2) as arguments share common features that do not occur in the non-arguments, a preliminary classification between arguments and non-arguments advantages the boundary detection of roles with fewer training examples (e.g. A4). Moreover, it may b"
W05-0630,P04-1043,1,\N,Missing
W05-1002,P97-1003,0,0.0257275,"Missing"
W05-1002,W03-1008,0,0.0225516,"nts or semantic roles are arguments of target words that can be verbs or nouns or adjectives. In FrameNet, the argument names are local to the target frames. For example, assuming that attach is the target word and Attaching is the target frame, a typical sentence annotation is the following. [Agent They] attachT gt [Item themselves] [Connector with their mouthparts] and then release a digestive enzyme secretion which eats into the skin. Several machine learning approaches for argument identification and classification have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, ; Gildea and Hockenmaier, 2003; Pradhan et al., 2004). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat feature representation. In the next section we present the common parse tree-based approach to this problem. 2.1 Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem can be divided into two subtasks: (a) the detection of the target argument boundaries, i.e. all its compounding words, and (b) the classification of the argument type, e.g."
W05-1002,J02-3001,0,0.708378,"which a word may be typically used. Frame elements or semantic roles are arguments of target words that can be verbs or nouns or adjectives. In FrameNet, the argument names are local to the target frames. For example, assuming that attach is the target word and Attaching is the target frame, a typical sentence annotation is the following. [Agent They] attachT gt [Item themselves] [Connector with their mouthparts] and then release a digestive enzyme secretion which eats into the skin. Several machine learning approaches for argument identification and classification have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, ; Gildea and Hockenmaier, 2003; Pradhan et al., 2004). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat feature representation. In the next section we present the common parse tree-based approach to this problem. 2.1 Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem can be divided into two subtasks: (a) the detection of the target argument boundaries, i.e. all its compounding words, and"
W05-1002,kingsbury-palmer-2002-treebank,0,0.0180014,"ilarity with such functions applied to the two trees. This approach determines a more syntactically motivated verb partition than the traditional method based on flat SCF representations (e.g. the NP-PP of Figure 1). The subtrees associated with SCF group the verbs which have similar syntactic realizations, in turn, according to Levin’s theories, this would suggest that they are semantically related. A preliminary study on the benefit of such kernels was measured on the classification accuracy of semantic arguments in (Moschitti, 2004). In such work, the improvement on the PropBank arguments (Kingsbury and Palmer, 2002) classification suggests that SK adds information to the prediction of semantic structures. On the contrary, the performance decrease on the FrameNet data classification shows the limit of such approach, i.e. when the syntactic structures are shared among several semantic roles SK seems to be useless. In this article, we use Support Vector Machines (SVMs) to deeply analyze the role of SK in the automatic predicate argument classification. The major novelty of the article relates to the extensive experimentation carried out on the PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 19"
W05-1002,J93-2004,0,0.0383657,"Missing"
W05-1002,P04-1043,1,0.73144,"uments: Arg0, Arg1 and ArgM. The SCF of such verb, i.e. NP-PP, provides a synthesis of the predicate argument structure. Currently, the systems which aim to derive semantic shallow information from texts recognize the SCF of a target verb and represent it as a flat feature (e.g. (Xue and Palmer, 2004; Pradhan et al., 2004)) in the learning algorithm. To achieve this goal, a lexicon which describes the SCFs for each verb, is required. Such a resource is difficult to find especially for specific domains, thus, several methods to automatically extract SCF have been proposed (Korhonen, 2003). In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). The subcategorization frame of verbs was implicitly represented by means of the syntactic subtrees which include the predicate with its arguments. The similarity between such syntactic structures was evaluated by means of convolution kernels. Convolution kernels are machine learning approaches which aim to describe structured data in 10 Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 10–17, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics terms of its substructures. Th"
W05-1002,W04-3212,0,0.0154816,"fferent syntactic alternations, thus, it plays a central role in the linking theory between verb semantics and their syntactic structures. Figure 1 shows the parse tree for the sentence &quot;John rented a room in Boston&quot; along with the semantic shallow information embodied by the verbal predicate to rent and its three arguments: Arg0, Arg1 and ArgM. The SCF of such verb, i.e. NP-PP, provides a synthesis of the predicate argument structure. Currently, the systems which aim to derive semantic shallow information from texts recognize the SCF of a target verb and represent it as a flat feature (e.g. (Xue and Palmer, 2004; Pradhan et al., 2004)) in the learning algorithm. To achieve this goal, a lexicon which describes the SCFs for each verb, is required. Such a resource is difficult to find especially for specific domains, thus, several methods to automatically extract SCF have been proposed (Korhonen, 2003). In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). The subcategorization frame of verbs was implicitly represented by means of the syntactic subtrees which include the predicate with its arguments. The similarity between such syntactic structures was evalua"
W05-1002,P02-1031,0,\N,Missing
W06-2607,W05-0620,0,0.0712622,"Missing"
W06-2607,A00-2018,0,0.00678118,"sign of performant SRL systems entirely based on tree kernels. In the remainder of this paper, Section 2 introduces basic notions on SRL systems and tree kernels. Section 3 illustrates our new kernels for both boundary and classification tasks. Section 4 shows the experiments of SVMs with the above tree kernel based classifiers. 2 Preliminary Concepts In this section we briefly define the SRL model that we intend to design and the kernel function that we use to evaluate the similarity between subtrees. 2.1 Basic SRL approach The SRL approach that we adopt is based on the deep syntactic parse (Charniak, 2000) of the sentence that we intend to annotate semantically. The standard algorithm is to classify the tree node pair hp, ai, where p and a are the nodes that exactly cover the target predicate and a potential argument, respectively. If hp, ai is labeled with an argument, then the terminal nodes dominated by a will be considered as the words constituting such argument. The number of pairs for each sentence can be hundreds, thus, if we consider training corpora of thousands of sentences, we have to deal with millions of training instances. The usual solution to limit such complexity is to divide t"
W06-2607,P02-1034,0,0.633604,"ez, 2005). A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). The idea is to associate with each argument the minimal subtree that includes the target predicate with one of its arguments, and to use a tree kernel function to evaluate the number of common substructures between two such trees. Such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003) and named entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004). Regarding the use of tree kernels for SRL, in (Moschitti, 2004) two main drawbacks have been 49 pointed out: • Highly accurate boundary detection cannot be carried out by a tree kernel model since correct and incorrect arguments may share a large portion of the encoding trees, i.e. they may share many substructures. • Manually derived features (extended with a polynomial kernel) have been shown to be superior to tree kernel approaches. Nevertheless, we believe that mode"
W06-2607,P04-1054,0,0.113272,"es, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). The idea is to associate with each argument the minimal subtree that includes the target predicate with one of its arguments, and to use a tree kernel function to evaluate the number of common substructures between two such trees. Such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003) and named entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004). Regarding the use of tree kernels for SRL, in (Moschitti, 2004) two main drawbacks have been 49 pointed out: • Highly accurate boundary detection cannot be carried out by a tree kernel model since correct and incorrect arguments may share a large portion of the encoding trees, i.e. they may share many substructures. • Manually derived features (extended with a polynomial kernel) have been shown to be superior to tree kernel approaches. Nevertheless, we believe that modeling a completely kernelized SRL system is useful for the following reasons: • We can implement it very quickly as the featu"
W06-2607,J02-3001,0,0.510469,"r the SRL task have shown that (shallow or deep) syntactic information is necessary to achieve a good labeling accuracy. This research brings a wide empirical evidence in favor of the linking theories between semantics and syntax, e.g. (Jackendoff, 1990). However, as no theory provides a sound and complete treatment of such issue, the choice and design of syntactic features for the automatic learning of semantic structures requires remarkable research efforts and intuition. For example, the earlier studies concerning linguistic features suitable for semantic role labeling were carried out in (Gildea and Jurasfky, 2002). Since then, researchers have proposed diverse syntactic feature sets that only slightly enhance the previous ones, e.g. (Xue and Palmer, 2004) or (Carreras and M`arquez, 2005). A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). The idea is to associate with each argument the minimal subtree that includes the target predicate with one of its arguments, and to use a tree kernel function to evaluate the number of common substructu"
W06-2607,A00-2008,0,0.0289784,"etection and argument classification. The comparative experiments on Support Vector Machines with such kernels on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases. Moreover, the use of different classifiers for internal and pre-terminal nodes maintains the same accuracy and highly improves efficiency. 1 Introduction A lot of attention has been recently devoted to the design of systems for the automatic labeling of semantic roles (SRL) as defined in two important projects: FrameNet (Johnson and Fillmore, 2000), inspired by Frame Semantics, and PropBank (Kingsbury and Palmer, 2002) based on Levin’s verb classes. In general, given a sentence in natural language, the annotation of a predicate’s semantic roles requires (1) the detection of the target word that embodies the predicate and (2) the detection and classification of the word sequences constituting the predicate’s arguments. In particular, step (2) can be divided into two different phases: (a) boundary detection, in which the words of the sequence are detected and (b) argument classification, in which the type of the argument is selected. Most"
W06-2607,kingsbury-palmer-2002-treebank,0,0.0340346,"port Vector Machines with such kernels on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases. Moreover, the use of different classifiers for internal and pre-terminal nodes maintains the same accuracy and highly improves efficiency. 1 Introduction A lot of attention has been recently devoted to the design of systems for the automatic labeling of semantic roles (SRL) as defined in two important projects: FrameNet (Johnson and Fillmore, 2000), inspired by Frame Semantics, and PropBank (Kingsbury and Palmer, 2002) based on Levin’s verb classes. In general, given a sentence in natural language, the annotation of a predicate’s semantic roles requires (1) the detection of the target word that embodies the predicate and (2) the detection and classification of the word sequences constituting the predicate’s arguments. In particular, step (2) can be divided into two different phases: (a) boundary detection, in which the words of the sequence are detected and (b) argument classification, in which the type of the argument is selected. Most machine learning models adopted for the SRL task have shown that (shall"
W06-2607,J93-2004,0,0.0293122,"less training data used when applying two distinct type classifiers for internal and pre-terminal nodes and (b) a more adequate feature space which allows SVMs to converge faster to a model containing a smaller number of support vectors, i.e. faster training and classification. 4.1 Experimental set up The empirical evaluations were carried out within the setting defined in the CoNLL-2005 Shared Task (Carreras and M`arquez, 2005). We used as a target dataset the PropBank corpus available at www.cis.upenn.edu/∼ace, along with the Penn TreeBank 2 for the gold trees (www.cis.upenn.edu/∼treebank) (Marcus et al., 1993), which includes about 53,700 sentences. Since the aim of this study was to design a real SRL system we adopted the Charniak parse trees from the CoNLL 2005 Shared Task data (available at www.lsi.upc.edu/∼srlconll/). We used Section 02, 03 and 24 from the Penn TreeBank in most of the experiments. Their characteristics are shown in Table 1. Pos and Neg indicate the number of nodes corresponding or not to a correct argument boundary. Rows 3 and 4 report such number for the internal and pre-terminal nodes separately. We note that the latter are much fewer than the former; this results in a very f"
W06-2607,P04-1043,1,0.927903,"f semantic structures requires remarkable research efforts and intuition. For example, the earlier studies concerning linguistic features suitable for semantic role labeling were carried out in (Gildea and Jurasfky, 2002). Since then, researchers have proposed diverse syntactic feature sets that only slightly enhance the previous ones, e.g. (Xue and Palmer, 2004) or (Carreras and M`arquez, 2005). A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). The idea is to associate with each argument the minimal subtree that includes the target predicate with one of its arguments, and to use a tree kernel function to evaluate the number of common substructures between two such trees. Such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003) and named entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004). Regarding the use of tree kernels for SRL, in (Moschitti, 2004) two main draw"
W06-2607,W05-0407,1,0.828913,"classify the tree node pair hp, ai, where p and a are the nodes that exactly cover the target predicate and a potential argument, respectively. If hp, ai is labeled with an argument, then the terminal nodes dominated by a will be considered as the words constituting such argument. The number of pairs for each sentence can be hundreds, thus, if we consider training corpora of thousands of sentences, we have to deal with millions of training instances. The usual solution to limit such complexity is to divide the labeling task in two subtasks: In this paper, we carry out tree kernel engineering (Moschitti et al., 2005) to increase both accuracy and speed of the boundary detection and argument classification phases. The engineering approach relates to marking the nodes of the encoding subtrees in order to generate substructures more strictly correlated with a particular argument, boundary or predicate. For example, marking the node that exactly covers the target argument helps tree kernels to generate different substructures for correct and incorrect argument boundaries. The other technique that we applied to engineer different kernels is the subdivision of internal and pre-terminal nodes. We show that desig"
W06-2607,E06-1015,1,0.901911,"re-terminal nodes separately. We note that the latter are much fewer than the former; this results in a very fast pre-terminal classifier. As the automatic parse trees contain errors, some arguments cannot be associated with any covering node. This prevents us to extract a tree representation for them. Consequently, we do not consider them in our evaluation. In sections 2, 3 and 24 there are 454, 347 and 731 such cases, respectively. The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes fast tree kernel evaluation (Moschitti, 2006) in the SVM-light software (Joachims, 1999). We used a regularization parameter (option -c) equal to 1 and λ = 0.4 (see (Moschitti, 2004)). 4.2 Boundary Detection Results In these experiments, we used Section 02 for training and Section 24 for testing. The results using the PAF and the MPAF based kernels are reported in Table 2 in rows 2 and 3, respectively. Columns 3 and 4 show the CPU testing time (in seconds) and the F1 of the monolithic boundary classifier. The next 3 columns show the CPU time for the internal (Int) and pre-terminal (Pre) node classifiers, as well as their total (All). The"
W06-2607,W05-0639,0,0.0124545,"ta of the same classifier. However, the use of different classifiers is motivated also by the fact that many argument types can be found mostly in pre-terminal nodes, e.g. modifier or negation arguments, and do not necessitate training data extracted from internal nodes. Consequently, it is more convenient (at least from a computational point of view) to use two different boundary classifiers, hereinafter referred to as combined classifier. 3.2 Kernels on complete predicate argument structures The type of a target argument strongly depends on the type and number of the predicate’s arguments1 (Punyakanok et al., 2005; Toutanova et al., 2005). Consequently, to correctly label an argument, we should extract features from the complete predicate argument structure it belongs to. In contrast, PAFs completely neglect the information (i.e. the tree portions) related to non-target arguments. One way to use this further information with tree kernels is to use the minimum subtree that spans all the predicate’s arguments. The whole parse tree in Figure 1 is an example of such Minimum Spanning Tree (MST) as it includes all and only the argument structures of the predicate ”to deliver”. However, MSTs pose some problem"
W06-2607,P05-1073,0,0.0341479,". However, the use of different classifiers is motivated also by the fact that many argument types can be found mostly in pre-terminal nodes, e.g. modifier or negation arguments, and do not necessitate training data extracted from internal nodes. Consequently, it is more convenient (at least from a computational point of view) to use two different boundary classifiers, hereinafter referred to as combined classifier. 3.2 Kernels on complete predicate argument structures The type of a target argument strongly depends on the type and number of the predicate’s arguments1 (Punyakanok et al., 2005; Toutanova et al., 2005). Consequently, to correctly label an argument, we should extract features from the complete predicate argument structure it belongs to. In contrast, PAFs completely neglect the information (i.e. the tree portions) related to non-target arguments. One way to use this further information with tree kernels is to use the minimum subtree that spans all the predicate’s arguments. The whole parse tree in Figure 1 is an example of such Minimum Spanning Tree (MST) as it includes all and only the argument structures of the predicate ”to deliver”. However, MSTs pose some problems: • We cannot use them f"
W06-2607,W04-3212,0,0.0743486,"empirical evidence in favor of the linking theories between semantics and syntax, e.g. (Jackendoff, 1990). However, as no theory provides a sound and complete treatment of such issue, the choice and design of syntactic features for the automatic learning of semantic structures requires remarkable research efforts and intuition. For example, the earlier studies concerning linguistic features suitable for semantic role labeling were carried out in (Gildea and Jurasfky, 2002). Since then, researchers have proposed diverse syntactic feature sets that only slightly enhance the previous ones, e.g. (Xue and Palmer, 2004) or (Carreras and M`arquez, 2005). A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). The idea is to associate with each argument the minimal subtree that includes the target predicate with one of its arguments, and to use a tree kernel function to evaluate the number of common substructures between two such trees. Such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntac"
W06-2611,W05-0620,0,0.0603058,"Missing"
W06-2611,A00-2018,0,0.136468,"Missing"
W06-2611,J02-3001,0,0.00591774,"th specific participants (Fillmore, 1968). The situations can be fairly simple depicting the entities involved and the roles they play or can be very complex and in this case they are called scenarios. The word that evokes a particular frame is called target word or predicate and can be an 1 A verb sense is an Intersective Levin class in which the verb is listed. adjective, noun or verb. The participant entities are defined using semantic roles and they are called frame elements. Several models have been developed for the automatic detection of the frame elements based on the FrameNet corpus (Gildea and Jurafsky, 2002; Thompson et al., 2003; Litkowski, 2004). While the algorithms used vary, almost all the previous studies divide the task into 1) the identification of the verb arguments to be labeled and 2) the tagging of each argument with a role. Also, most of the models agree on the core features as being: Predicate, Headword, Phrase Type, Governing Category, Position, Voice and Path. These are the initial features adopted by Gildea and Jurafsky (2002) (henceforth G&J) for both frame element identification and role classification. A difference among the previous machinelearning models is whether the fram"
W06-2611,W04-0803,0,0.0578029,"uations can be fairly simple depicting the entities involved and the roles they play or can be very complex and in this case they are called scenarios. The word that evokes a particular frame is called target word or predicate and can be an 1 A verb sense is an Intersective Levin class in which the verb is listed. adjective, noun or verb. The participant entities are defined using semantic roles and they are called frame elements. Several models have been developed for the automatic detection of the frame elements based on the FrameNet corpus (Gildea and Jurafsky, 2002; Thompson et al., 2003; Litkowski, 2004). While the algorithms used vary, almost all the previous studies divide the task into 1) the identification of the verb arguments to be labeled and 2) the tagging of each argument with a role. Also, most of the models agree on the core features as being: Predicate, Headword, Phrase Type, Governing Category, Position, Voice and Path. These are the initial features adopted by Gildea and Jurafsky (2002) (henceforth G&J) for both frame element identification and role classification. A difference among the previous machinelearning models is whether the frame information was used as gold feature. O"
W06-2611,J01-3003,0,0.0126323,"a Levin class is that they share the same participant roles. As FrameNet is annotated with frame-specific semantic roles we manually mapped these roles into the VerbNet set of thematic roles. Given a frame, we assigned thematic roles to all frame elements that are associated with verbal predicates. For example the roles Speaker, Addressee, Message and Topic from the Telling frame were respectively mapped into Agent, Recipient, Theme and Topic. 81 Second, we build a frequency distribution of VerbNet thematic roles on different syntactic position. Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each Levin class has a distinct frequency distribution of roles on different grammatical slots. As we do not have matching grammatical function in FrameNet and VerbNet, we approximate that subjects and direct objects are more likely to appear on positions adjacent to the predicate, while indirect objects appear on more distant positions. The same intuition is used successfully by G&J in the design of the Position feature. We will acquire from the corpus, for each thematic role θi, the frequencies with which it appears on an adjacent (ADJ) or distant (DST) position in a given f"
W06-2611,W04-3212,0,0.0221342,"6 46,734 11,650 92.63 Table 2 . F1 and accuracy of the argument classifiers and the overall multiclassifier for Intersective Levin class 5.3 Automatic semantic role labeling on FrameNet In the experiments involving semantic role labelling, we used a SVM with a polynomial kernel. We adopted the standard features developed for semantic role detection by Gildea and Jurafsky (see Section 2). Also, we considered some of the features designed by (Pradhan et al., 2004): First and Last Word/POS in Constituent, Subcategorization, Head Word of Prepositional Phrases and the Syntactic Frame feature from (Xue and Palmer, 2004). For the rest of the paper we will refer to these features as being literature features (LF). The results obtained when using the literature features alone or in conjunction with the gold frame feature, gold ILC, automatically detected frame feature FN #Train Instances FN #Test Instances LF+Gold Frame LF+Gold ILC LF+Automatic Frame LF+Automatic ILC LF Body_part 1,511 356 90.91 90.80 84.87 85.08 79.76 and automatically detected ILC are depicted in Table 3. The first four columns report the F1 measure of some role classifiers whereas the last column shows the global multiclassifier accuracy. Th"
W06-2611,P98-1046,0,\N,Missing
W06-2611,C98-1046,0,\N,Missing
W06-2909,W05-0620,0,0.0133417,"Missing"
W06-2909,P02-1034,0,0.78843,"respectively. In P|F |l(fi ) turn ∆(n1 , n2 ) = Ii (n1 )Ii (n2 ), where i=1 λ 0 ≤ λ ≤ 1 and l(fi ) is the height of the subtree fi . Thus λl(fi ) assigns a lower weight to larger fragSentence Parse-Tree took{ARG0, ARG1} S S NP VP PRP VP John took NP CC VB read{ARG0, ARG1} NP DT NN the book and read VP PRP VP VB NP S NP VP PRP VB PRP$ NN its title John took VP NP VP VB DT NN the book John read NP PRP$ NN its title Figure 1: A sentence parse tree with two argument spanning trees (AST s) ments. When λ = 1, ∆ is equal to the number of common fragments rooted at nodes n1 and n2 . As described in (Collins and Duffy, 2002), ∆ can be computed in O(|Nt1 |× |Nt2 |). 3 Tree kernel-based classification of Predicate Argument Structures Traditional semantic role labeling systems extract features from pairs of nodes corresponding to a predicate and one of its argument, respectively. Thus, they focus on only binary relations to make classification decisions. This information is poorer than the one expressed by the whole predicate argument structure. As an alternative we can select the set of potential arguments (potential argument nodes) of a predicate and extract features from them. The number of the candidate argument"
W06-2909,J02-3001,0,0.874413,"ructured learning allows SVMs to reach the state-of-the-art accuracy. The paper is organized as follows: Section 2 introduces the Semantic Role Labeling based on SVMs and the tree kernel spaces; Section 3 formally defines the AST s and the algorithm for their classification and re-ranking; Section 4 shows the comparative results between our approach and the traditional one; Section 5 presents the related work; and finally, Section 6 summarizes the conclusions. 2 Semantic Role Labeling In the last years, several machine learning approaches have been developed for automatic role labeling, e.g. (Gildea and Jurasfky, 2002; Pradhan et al., 2005a). Their common characteristic is the adoption of attribute-value representations for predicate-argument structures. Accordingly, our basic system is similar to the one proposed in (Pradhan et al., 2005a) and it is hereby described. We use a boundary detection classifier (for any role type) to derive the words compounding an argument and a multiclassifier to assign the roles (e.g. Arg0 or ArgM) described in PropBank (Kingsbury and Palmer, 2002)). To prepare the training data for both classifiers, we used the following algorithm: 1. Given a sentence from the training-set,"
W06-2909,kingsbury-palmer-2002-treebank,0,0.0968357,"d its arguments. For example, to detect the interesting context, the modeling of syntax/semantics-based features should take into account linguistic aspects like ancestor nodes or semantic dependencies (Toutanova et al., 2004). In a similar way, we can model SRL systems with tree kernels to generate large feature spaces. More in detail, most SRL systems split the labeling process into two different steps: Boundary Detection (i.e. to determine the text boundaries of predicate arguments) and Role Classification (i.e. labeling such arguments with a semantic role, e.g. Arg0 or Arg1 as defined in (Kingsbury and Palmer, 2002)). The former relates to the detection of syntactic parse tree nodes associated with constituents that correspond to arguments, whereas the latter considers the boundary nodes for the assignment of the suitable label. Both steps require the design and extraction of features from parse trees. As capturing the tightly interdependent relations among a predicate and its arguments is a complex task, we can apply tree kernels on the subtrees that span the whole predicate argument structure to generate the feature space of all the possible subtrees. In this paper, we apply the traditional boundary (T"
W06-2909,W05-0630,1,0.831404,"Missing"
W06-2909,P04-1043,1,0.912284,"used to train a boundary classifier (e.g. an SVM). Regarding the argument type classifier, a binary labeler for a role r (e.g. an SVM) can be trained on the Tr+ , i.e. its positive examples and Tr− , i.e. its negative examples, where T + = Tr+ ∪ Tr− , according to the ONE-vsALL scheme. The binary classifiers are then used to build a general role multiclassifier by simply selecting the argument associated with the maximum among the SVM scores. Regarding the design of features for predicateargument pairs, we can use the attribute-values defined in (Gildea and Jurasfky, 2002) or tree structures (Moschitti, 2004). Although we focus on the latter approach, a short description of the former is still relevant as they are used by T BC and T RC. They include the Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice features. For example, the Phrase Type indicates the syntactic type of the phrase labeled as a predicate argument and the Parse Tree Path contains the path in the parse tree between the predicate and the argument phrase, expressed as a sequence of nonterminal labels linked by direction (up or down) symbols, e.g. V ↑ VP ↓ NP. A viable alternative to manual design of synta"
W06-2909,E06-1015,1,0.862676,"Missing"
W06-2909,P05-1072,0,0.142293,"tion of syntactic parse tree nodes associated with constituents that correspond to arguments, whereas the latter considers the boundary nodes for the assignment of the suitable label. Both steps require the design and extraction of features from parse trees. As capturing the tightly interdependent relations among a predicate and its arguments is a complex task, we can apply tree kernels on the subtrees that span the whole predicate argument structure to generate the feature space of all the possible subtrees. In this paper, we apply the traditional boundary (T BC) and role (T RC) classifiers (Pradhan et al., 2005a), which are based on binary predicate/argument relations, to label all parse tree nodes corresponding to potential arguments. Then, we ex61 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 61–68, New York City, June 2006. 2006 Association for Computational Linguistics tract the subtrees which span the predicate-argument dependencies of such arguments, i.e. Argument Spanning Trees (AST s). These are used in a tree kernel function to generate all possible substructures that encode n-ary argument relations, i.e. we carry out an automatic feature e"
W06-2909,W05-0639,0,0.135635,"the AST metaclassifier, which can be used with any of the best figure CoNLL systems. Finally, the overall results suggest that the tree kernel model is robust to parse tree errors since preserves the same improvement across trees derived with different accuracy, i.e. the semi-automatic trees of Section 21 and the automatic tree of Section 23. Moreover, it shows a high accuracy for the classification of correct and incorrect AST s. This last property is quite interesting as the best SRL systems 1 We needed to remove the overlaps from the baseline outcome in order to apply the CoNLL evaluator. (Punyakanok et al., 2005; Toutanova et al., 2005; Pradhan et al., 2005b) were obtained by exploiting the information on the whole predicate argument structure. Next section shows our preliminary experiments on re-ranking using the AST kernel based approach. 4.2 Re-ranking based on Tree Kernels In these experiments, we used the output of T BC and T RC 2 to provide an SVM tree kernel with a ranked list of predicate argument structures. More in detail, we applied a Viterbi-like algorithm to generate the 20 most likely annotations for each predicate structure, according to the joint probabilistic model of T BC and T RC."
W06-2909,W03-1012,0,0.130056,"To make faster the learning process and to try to only capture the most relevant features, we also experimented with a compact version of the AST which is pruned at the level of argument nodes. (b) Attribute value features (standard features) related to the whole predicate structure. These include the features for each arguments (Gildea and Jurasfky, 2002) and global features like the sequence of argument labels, e.g. hArg0, Arg1, ArgM i. Finally, we prepare the training examples for the re-ranker considering the m best annotations of each predicate structure. We use the approach adopted in (Shen et al., 2003), which generates all possible ¡m¢ pairs from the m examples, i.e. 2 pairs. Each pair is assigned to a positive example if the first member of the pair has a higher score than the second member. The score that we use is the F1 measure of the annotated structure with respect to the gold standard. More in detail, given training/testing examples ei = ht1i , t2i , vi1 , vi2 i, where t1i and t2i are two AST s and vi1 and vi2 are two feature vectors associated with two candidate predicate structures s1 and s2 , we define the following kernels: 1) Ktr (e1 , e2 ) = Kt (t11 , t12 ) + Kt (t21 , t22 ) −K"
W06-2909,W04-3222,0,0.120387,"has shown that to achieve high labeling accuracy a joint inference on the whole predicate argument structure should be applied. For this purpose, we need to extract features from the sentence’s syntactic parse tree that encodes the target semantic structure. This task is rather complex since we do not exactly know which are the syntactic clues that capture the relation between the predicate and its arguments. For example, to detect the interesting context, the modeling of syntax/semantics-based features should take into account linguistic aspects like ancestor nodes or semantic dependencies (Toutanova et al., 2004). In a similar way, we can model SRL systems with tree kernels to generate large feature spaces. More in detail, most SRL systems split the labeling process into two different steps: Boundary Detection (i.e. to determine the text boundaries of predicate arguments) and Role Classification (i.e. labeling such arguments with a semantic role, e.g. Arg0 or Arg1 as defined in (Kingsbury and Palmer, 2002)). The former relates to the detection of syntactic parse tree nodes associated with constituents that correspond to arguments, whereas the latter considers the boundary nodes for the assignment of t"
W06-2909,P05-1073,0,0.479237,"ional Linguistics tract the subtrees which span the predicate-argument dependencies of such arguments, i.e. Argument Spanning Trees (AST s). These are used in a tree kernel function to generate all possible substructures that encode n-ary argument relations, i.e. we carry out an automatic feature engineering process. To validate our approach, we experimented with our model and Support Vector Machines for the classification of valid and invalid AST s. The results show that this classification problem can be learned with high accuracy. Moreover, we modeled SRL as a re-ranking task in line with (Toutanova et al., 2005). The large number of complex features provided by tree kernels for structured learning allows SVMs to reach the state-of-the-art accuracy. The paper is organized as follows: Section 2 introduces the Semantic Role Labeling based on SVMs and the tree kernel spaces; Section 3 formally defines the AST s and the algorithm for their classification and re-ranking; Section 4 shows the comparative results between our approach and the traditional one; Section 5 presents the related work; and finally, Section 6 summarizes the conclusions. 2 Semantic Role Labeling In the last years, several machine learn"
W06-3806,A00-2018,0,0.00977345,"Missing"
W06-3806,W05-1203,0,0.0558672,"ching tecniques are not sufficient as these may only detect the structural similarity between sentences of textual entailment pairs. An extension is needed to consider also if two pairs show compatible relations between their sentences. In this paper, we propose to observe textual entailment pairs as pairs of syntactic trees with co-indexed nodes. This shuold help to cosider both the structural similarity between syntactic tree pairs and the similarity between relations among sentences within a pair. Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. We experimented with such kernel using Support Vector Machines on the Recognizing Textual Entailment (RTE) challenge test-beds. The comparative results show that (a) we have designed an effective way to automatically learn entailment rules 33 Workshop on TextGraphs, at HLT-NAACL 2006, pages 33–36, c New York City, June 2006. 2006 Association for Computational Linguistics from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-ofthe-art models. In the remainder of this paper, Sec. 2 introduces the cross-pair simila"
W06-3806,O97-1002,0,0.0604873,"etween T1 and H1 . These placeholders are then used to augment tree nodes. To better take into account argument movements, placeholders are propagated in the syntactic trees following constituent heads (see Fig. 1). In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dectors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivationally related form relation and the verb entailment relation in WordNet, and, fi34 nally, a WordNet-based similarity (Jiang and Conrath, 1997). Each of these detectors gives a different weight to the anchor: the actual computed similarity for the last and 1 for all the others. These weights will be used in the final kernel. 2.2 Similarity between pairs of co-indexed trees Pairs of syntactic trees where nodes are co-indexed with placeholders allow the design a cross-pair similarity that considers both the structural similarity and the intra-pair word movement compatibility. Syntactic trees of texts and hypotheses permit to verify the structural similarity between pairs of sentences. Texts should have similar structures as well as hyp"
W06-3806,P04-1043,1,0.874312,"Missing"
W06-3806,N04-3012,0,0.0623781,"Missing"
W07-1412,P02-1034,0,0.0394357,": E1 and E2 share the following subtrees: T3 “For my younger readers, Chapman killed John Lennon more than twenty years ago.” “John Lennon died more than twenty years ago.” H6 “Cows are vegetarian but, to save money on mass-production, farmers fed cows animal extracts.” “Cows have eaten animal extracts.” (E6 ) but it will clearly fail when used for: (R3 ) T7 This is the rewrite rule they have in common. Then, E2 can be likely classified as a valid entailment, as it shares the rule with the valid entailment E1 . The cross-pair similarity model uses: (1) a tree similarity measure KT (τ1 , τ2 ) (Collins and Duffy, 2002) that counts the subtrees that τ1 and τ2 have in common; (2) a substitution function t(·, c) that changes names of the placeholders in a tree according to a set of correspondences between placeholders c. Given C as the collection of all correspondences between the placeholders of (T 0 , H 0 ) and (T 00 , H 00 ), the cross-pair similarity is computed as: KS ((T 0 , H 0 ), (T 00 , H 00 )) = maxc∈C (KT (t(T 0 , c), t(T 00 , c)) + KT (t(H 0 , c), t(H 00 , c))) (1) The cross-pair similarity KS , used in a kernel-based learning model as the support vector machines, allows the exploitation of implici"
W07-1412,W05-1203,0,0.0239919,"Missing"
W07-1412,E06-1015,1,0.85746,"he computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS ((T 0 , H 0 ), (T 00 , H 00 )). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72–77, c Prague, June 2007. 2007 Association for Computational Linguistics"
W07-1412,P06-1051,1,0.786744,") we reduce the computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS ((T 0 , H 0 ), (T 00 , H 00 )). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72–77, c Prague, June 2007. 2007 Association for Computational Linguistics"
W08-2004,H05-1079,0,0.0150814,"ls for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 ⇒ H 1 T1 “Wanadoo bought KStones” H1 “Wanadoo owns KStones” where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Pro"
W08-2004,P02-1034,0,0.466751,"Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25–32 Manchester, August 2008 For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment"
W08-2004,W05-1203,0,0.0504424,"using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 ⇒ H 1 T1 “Wanadoo bought KStones” H1 “Wanadoo owns KStones” where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceeding"
W08-2004,P06-1117,1,0.782749,"eed to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we re"
W08-2004,W04-2403,1,0.858821,"methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the set of fragments {f1 , f2 , ."
W08-2004,W06-2909,1,0.824703,"roach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the s"
W08-2004,P07-1098,1,0.823458,"pose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and D"
W08-2004,P06-1051,1,0.769809,". it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25–32 Manchester, August 2008 For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment pair were generated causing a remarkable slowdown. In this paper, we describe the feature space of all possible tree fragment pairs and we show that it can be evaluated with a much simpler kernel than the one used in previous work, both in terms"
W09-0505,P98-1013,0,0.607831,"ion levels of words, turns1 , attribute-value pairs, dialog acts, predicate argument structures. The annotation at word level is made with part-of-speech and morphosyntactic information following the recommendations of EAGLES corpora annotation (Leech and Wilson, 2006). The attribute-value annotation uses a predefined domain ontology to specify concepts and their relations. Dialog acts are used to annotate intention in an utterance and can be useful to find relations between different utterances as the next section will show. For predicate structure annotation, we followed the FrameNet model (Baker et al., 1998) (see Section 2.2). 2.1 2. Conventional/Discourse management acts, which maintain dialog cohesion and delimit specific phases, such as opening, continuation, closing, and apologizing; 3. Feedback/Grounding acts,used to elicit and provide feedback in order to establish or restore a common ground in the conversation. Our taxonomy, following the same three-fold partition, is summarized in Table 1. Table 1: Dialog act taxonomy Core dialog acts Speaker wants information from addressee Action-request Speaker wants addressee to perform an action Yes-answer Affirmative answer No-answer Negative answer"
W09-0505,W04-3218,1,0.880684,"lli, Alessandro Moschitti, Giuseppe Riccardi∗ University of Trento 38050 Povo - Trento, Italy {dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu Abstract tities) within one or more frames (frame-slot semantics) that is defined by the application. While this model is simple and clearly insufficient to cope with interpretation and reasoning, it has supported the first generation of spoken dialog systems. Such dialog systems are thus limited by the ability to parse semantic features such as predicates and to perform logical computation in the context of a specific dialog act (Bechet et al., 2004). This limitation is reflected in the type of human-machine interactions which are mostly directed at querying the user for specific slots (e.g. “What is the departure city?”) or implementing simple dialog acts (e.g. confirmation). We believe that an important step in overcoming such limitation relies on the study of models of human-human dialogs at different levels of representation: lexical, syntactic, semantic and discourse. In this paper, we present our results in addressing the above issues in the context of the LUNA research project for next-generation spoken dialog interfaces (De Mori e"
W09-0505,burchardt-etal-2006-salto,0,0.0299062,"Missing"
W09-0505,W03-2117,0,0.0692443,"Missing"
W09-0505,W08-0109,1,\N,Missing
W09-0505,C98-1013,0,\N,Missing
W09-1106,W05-0620,0,0.31106,"Missing"
W09-1106,A00-2018,0,0.0545296,"interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas 34 the second involves a Role Multi-class Classifier (RM). Setup. If the constituency parse tree t of a sentence s is available, we can look at all the pairs hp, ni i, where ni is any node in the tree and p is the node dominating w, an"
W09-1106,P02-1034,0,0.803371,"g hyperplane’s gradient and its bias, respectively. The gradient is a linear combination of the training points x~i , their labels yi and their weights αi . These and the bias are optimized at training time by the learning algorithm. Applying the so-called kernel trick it is possible to replace the scalar product with a kernel function defined over pairs of objects: f (o) = n X overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Implicitly, a TK function establishes a correspondence between distinct fragments and dimensions in some fragment space, i.e. the space of all the possible fragments. To simplify"
W09-1106,P04-1054,0,0.077485,"nel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and"
W09-1106,W04-3233,0,0.442461,"Missing"
W09-1106,J02-3001,0,0.152096,"Missing"
W09-1106,H05-1018,0,0.565047,"rallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the pr"
W09-1106,P03-1004,0,0.816186,"ly represent examples in some high dimensional kernel space, where their similarity is evaluated. Kernel functions can generate a very large number of features, which are then weighted by the SVM optimization algorithm obtaining a feature selection side-effect. Indeed, the weights encoded by the gradient of the separating hyperplane learnt by the SVM implicitly establish a ranking between features in the kernel space. This property has been exploited in feature selection models based on 30 approximations or transformations of the gradient, e.g. (Rakotomamonjy, 2003), (Weston et al., 2003) or (Kudo and Matsumoto, 2003). However, kernel based systems have two major drawbacks: first, new features may be discovered in the implicit space but they cannot be directly observed. Second, since learning is carried out in the dual space, it is not possible to use the faster SVM or perceptron algorithms optimized for linear spaces. Consequently, the processing of large data sets can be computationally very expensive, limiting the use of large amounts of data for our research or applications. We propose an approach that tries to fill in the gap between explicit and implicit feature representations by 1) selecting the mo"
W09-1106,P05-1024,0,0.208414,"ich is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former"
W09-1106,W04-2403,1,0.852575,"idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas 34 the second involves a R"
W09-1106,W06-2909,1,0.788988,"uce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), where"
W09-1106,J08-2003,1,0.937992,"n relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decom"
W09-1106,E06-1015,1,0.895163,"+ b = n X i=1 αi yi x~i · ~x + b (1) where ~x is a classifying example and w ~ and b are the separating hyperplane’s gradient and its bias, respectively. The gradient is a linear combination of the training points x~i , their labels yi and their weights αi . These and the bias are optimized at training time by the learning algorithm. Applying the so-called kernel trick it is possible to replace the scalar product with a kernel function defined over pairs of objects: f (o) = n X overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Implicitly, a TK function establishes a correspondence between disti"
W09-1106,J05-1004,0,0.130088,"(DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas 34 the second involves a Role Multi-class Classifier (RM). Setup. If the constituency parse tree t of a sentence s is available, we can look at all the pairs hp, ni i, where ni is any"
W09-1106,W03-1012,0,0.431357,"rly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that a"
W09-1106,W06-2902,0,0.248263,"ide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Bound"
W09-1106,W04-3222,0,0.40677,"representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas 34 the second involves a Role Multi-class Classifier ("
W10-2910,P98-1013,0,0.0182134,"t al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This result compares favorably with previously published results, which have been biased towards precision and scored l"
W10-2910,D09-1112,1,0.810386,"e use of lexical similarity to reduce data sparseness, e.g. (Basili et al., 2005; Basili et al., 2006; Bloehdorn et al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This resul"
W10-2910,W05-0601,1,0.769097,"r the task of finding opinionated expressions. We note that the performance of our baseline sequence labeler is lower than theirs; this is to be expected since they used a more complex batch learning algorithm (conditional random fields) while we used an online learner, and they spent more effort on feature design. This indicates that we should be able to achieve even higher performance using a stronger base model. The flexible architecture we have presented enables interesting future research: (i) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e.g. (Basili et al., 2005; Basili et al., 2006; Bloehdorn et al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such"
W10-2910,P06-1117,1,0.21832,"jective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This result compares favorably with previously published results, which have been biased towards precision and scored low on recall. The long-distance structural features gives"
W10-2910,P08-1067,0,0.0157514,"e complex approaches requiring advanced search techniques are mainly simplicity and efficiency: this approach is conceptually simple and fairly easy to implement provided that kbest output can be generated efficiently, and features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). 3.1 This is viewed as the main impediment Figure 2: Sequence labeling example. Syntactic and Semantic Structures The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). In the example, viewed is listed as having strong prior subjectivity but no polarity, and impediment has strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essent"
W10-2910,W06-1651,0,0.625063,"p the opinionated expressions, i.e. the text snippets signaling the subjective content of the text. This is necessary for further analysis, such as the determination of opinion holder and the polarity of the opinion. The MPQA corpus (Wiebe et al., 2005), a widely used corpus annotated with subjectivity information, defines two types of subjective expressions: direct subjective expressions (DSEs), which are explicit mentions The task of marking up these expressions has usually been approached using straightforward sequence labeling techniques using simple features in a small contextual window (Choi et al., 2006; Breck et al., 2007). However, due to the simplicity of the feature sets, this approach fails to take into account the fact that the semantic and pragmatic interpretation of sentences is not only determined by words but also by syntactic and shallow-semantic relations. Crucially, taking grammatical relations into account allows us to model how expressions interact in various ways that influence their interpretation as subjective or not. Consider, for instance, the word said in examples (3) and (4) below, where the interpretation as a DSE or an OSE is influenced by the subjective content of th"
W10-2910,W08-2123,1,0.895519,"Missing"
W10-2910,P02-1034,0,0.0607225,"pproach, the learning algorithm thus directly optimizes the measure we are interested in, i.e. the F-measure. 3.5 4 Experiments DT a Structure Learning Approach The Preference Kernel approach reduces the reranking problem to a binary classification task on pairs, after which a standard SVM optimizer is used to train the reranker. A problem with this method is that the optimization problem solved by the SVM – maximizing the classification accuracy on a set of independent pairs – is not directly related to the performance of the reranker. Instead, the method employed by many rerankers following Collins and Duffy (2002) directly learn a scoring function that is trained to maximize performance on the reranking task. We will refer to this approach as the structure learning method. While there are batch learning algorithms that work in this setting (Tsochantaridis et al., 2005), online learning methods have been more popular for efficiency reasons. We investigated two online learning algorithms: the popular structured perceptron Collins and Duffy (2002) and the Passive– Aggressive (PA) algorithm (Crammer et al., 2006). To increase robustness, we averaged the weight vectors seen during training as in the Voted P"
W10-2910,C08-1050,1,0.911231,"as strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are"
W10-2910,ruppenhofer-etal-2008-finding,0,0.0148568,"s using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based o"
W10-2910,W03-0402,0,0.108712,"as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based on reranking, incorporating structural features results in a significant improvement. We investigate two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical info"
W10-2910,P09-2079,0,0.160932,"Missing"
W10-2910,D09-1018,0,0.0280578,"y, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based on reranking, incorporating structural features results in a significant improvement. We investigate two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-bas"
W10-2910,P03-1005,0,0.00900394,"and the argument label. For instance, the ESE liar is connected to the DSE call via an A2 label, and we represent this using a feature DSE:A2:ESE. Apart from the syntactic and semantic features, we also used the score output from the base sequence labeler as a feature. We normalized the scores over the k candidates so that their exponentials summed to 1. 1 70 Available at http://dit.unitn.it/∼moschitt output y and the predicted output yˆ, where Φ is the feature representation function: 1999)2 . It is still an open question how dependency trees should be represented for use with tree kernels (Suzuki et al., 2003; Nguyen et al., 2009); we used the representation shown in Figure 3. Note that we have concatenated the opinion expression labels to the POS tag nodes. We did not use any of the features from Section 3.3 except for the base sequence labeler score. yˆ ← arg maxh w · Φ(x, h) w ← w + Φ(x, y) − Φ(x, yˆ) In the PA algorithm, which is based on the theory of large-margin learning, we instead find the yˆ that violates the margin constraints maximally. The update step length τ is computed based on the margin; this update is bounded by a regularization constant C: TOP ROOT  ρ(y, h) yˆ ← arg max  h w"
W10-2910,W06-0301,0,0.0502376,"the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the disc"
W10-2910,E99-1023,0,0.0195897,"pply over an arbitrarily long distance in the sentence. While it is possible that search algorithms for exact or approximate inference can be constructured for the arg max problem in this model, we sidestepped this issue by using a reranking decomposition of the problem: We first apply a standard Viterbi-based sequence labeler using no structural features and generate a small candidate set of size k. Then, a second and more complex model picks 68 algorithm (Crammer et al., 2006) instead of the perceptron. We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999). Figure 2 shows an example of a sentence with a DSE and an ESE and how they are encoded in the IOB2 encoding. the top candidate from this set without having to search the whole candidate space. The advantages of a reranking approach compared to more complex approaches requiring advanced search techniques are mainly simplicity and efficiency: this approach is conceptually simple and fairly easy to implement provided that kbest output can be generated efficiently, and features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inf"
W10-2910,D07-1114,0,0.0203528,"y found that the most prominent constructional feature for subjectivity analysis was the Tense Shift construction. While the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic id"
W10-2910,P99-1032,0,0.0126318,"Missing"
W10-2910,W04-2705,0,0.0120623,"sential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD 3.3 They [called ]DSE him a [lia"
W10-2910,N10-1121,0,0.148812,"eful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion"
W10-2910,E06-1015,1,0.816723,"been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight (Joachims, C ONNECTING A RGUMENT L ABEL. When a predicate inside some opinion expression is connected to some argument inside another opinion expression, we use a feature consisting of the two expression labels and the argument label. For instance, the ESE liar is connected to the DSE call via an A2 label, and we represent this using a feature DSE:A2:ESE. Apart from the syntactic and semantic features, we also used the score output from the base sequence labeler as a feature. We normalized the scores over"
W10-2910,H05-1044,0,0.0592384,"s affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). 3.1 This is viewed as the main impediment Figure 2: Sequence labeling example. Syntactic and Semantic Structures The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). In the example, viewed is listed as having strong prior subjectivity but no polarity, and impediment has strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a seque"
W10-2910,D09-1159,0,0.0258029,"or subjectivity analysis was the Tense Shift construction. While the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena"
W10-2910,D09-1143,1,0.574103,"ted with the feature DSE/call.01. 3.4.1 Linear Kernel We created feature vectors extracted from the candidate sequences using the features described in Section 3.3. We then trained linear SVMs using the L IBLINEAR software (Fan et al., 2008), using L1 loss and L2 regularization. P REDICATE AND A RGUMENT L ABEL. For every argument of a predicate inside an opinion expression, we create a feature representing the predicate–argument pair: DSE/call.01:A0. 3.4.2 Tree Kernel Tree kernels have been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight (Joachims, C ONNECTING"
W10-2910,W03-1017,0,0.0580957,"ranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as partof-speech tags and functional words (Wiebe et al., 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). However, it has been suggested that subjectivity analysis is inherently more subtle than categorization and that structural linguistic information should therefore be given more attention in this context. For instance, Karlgren et al. (2010) argued from"
W10-2910,J05-1004,0,0.00594556,"is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMO"
W10-2910,N06-1037,0,0.0637993,"so a DSE is represented with the feature DSE/call.01. 3.4.1 Linear Kernel We created feature vectors extracted from the candidate sequences using the features described in Section 3.3. We then trained linear SVMs using the L IBLINEAR software (Fan et al., 2008), using L1 loss and L2 regularization. P REDICATE AND A RGUMENT L ABEL. For every argument of a predicate inside an opinion expression, we create a feature representing the predicate–argument pair: DSE/call.01:A0. 3.4.2 Tree Kernel Tree kernels have been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight ("
W10-2910,W02-1011,0,0.0155002,"te two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as partof-speech tags and functional words (Wiebe et al., 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). However, it has been suggested that subjectivity analysis is inherently more subtle than categorization and that structural linguistic information should therefore be given more attention in this context. For instance, Kar"
W10-2910,W02-1001,0,\N,Missing
W10-2910,C98-1013,0,\N,Missing
W10-2926,H05-1018,0,0.0147352,"polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )"
W10-2926,P03-1054,0,0.0141283,"taxonomy known as coarse grained, which has been described in (Zhang and Lee, 2003) and (Li and Roth, 2006), consisting of six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LOC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates). The TREC 10 QA data set accounts for 6,000 questions. For each question, we generate the full parse of the sentence and use it to train our models. Automatic parses are obtained with the Stanford parser6 (Klein and Manning, 2003), and we actually have only 5,953 sentences in our data set due to parsing issues. During preliminary experiments, we observed an uneven distribution of examples in the traditional training/test split (the same used in P&M). Therefore, we used a random selection to generate an unbiased split, with 5,468 sentences for training and 485 for testing. The resulting data set is available for download at http://danielepighin.net/cms/research/ QC_dataset.tgz. s(fk ) 2 λ . It can also be observed that ti,j ≤ ti,k . Indeed, if ti,k is a subset of ti,j , then it will occur at least as many times as its e"
W10-2926,P03-1004,0,0.310439,"linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoIn the remainder: Section 2 briefly reviews SVMs and TK functions; Section 3 describes the problem of selecting and projecting features from very high onto lower dimensional spaces, and provides the theoretical foundation to our appro"
W10-2926,P05-1024,0,0.127975,"bination of (ii) with (iii) suggests that an extremely aggressive feature selection can be applied. We describe a greedy algorithm that exploits these results. Compared to the one proposed in P&M, the new version of the algorithm has only one parameter (instead of 3), it is more efficient and can be more easily connected with the amount of gradient norm that is lost after feature selection. Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and j"
W10-2926,W05-0620,0,0.0502839,"Missing"
W10-2926,P02-1034,0,0.805592,"nt decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance. 1 In this paper, we present and empirically validate a theory which aims at filling the abovementioned gaps. In particular we provide: (i) a proof of the equation for the exact computation of feature weights induced by TK functions (Collins and Duffy, 2002); (ii) a theoretical characterization of feature selection based on kwk. ~ We show that if feature selection does not sensibly reduces kwk, ~ the margin associated with w ~ does not sensibly decrease as well. Consequently, the theoretical upperbound to the probability error does not sensibly increases; (iii) a proof that the convolutive nature of TK allows for filtering out an exponential number of features with a small kwk ~ decrease. The combination of (ii) with (iii) suggests that an extremely aggressive feature selection can be applied. We describe a greedy algorithm that exploits these re"
W10-2926,P08-2029,1,0.747403,"oach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do       if F 6= ( ∅  do do   Wnext ← Wnext ∪ {f }          Dnext ← Dnext ∪ F then           UPDATE (F )       τ ←τ +1     Wprev ← Wnext   Dprev ← Dnext return (resul"
W10-2926,P04-1054,0,0.190737,"mall kwk ~ decrease. The combination of (ii) with (iii) suggests that an extremely aggressive feature selection can be applied. We describe a greedy algorithm that exploits these results. Compared to the one proposed in P&M, the new version of the algorithm has only one parameter (instead of 3), it is more efficient and can be more easily connected with the amount of gradient norm that is lost after feature selection. Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that"
W10-2926,J08-2003,1,0.888228,"th (iii) suggests that an extremely aggressive feature selection can be applied. We describe a greedy algorithm that exploits these results. Compared to the one proposed in P&M, the new version of the algorithm has only one parameter (instead of 3), it is more efficient and can be more easily connected with the amount of gradient norm that is lost after feature selection. Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more"
W10-2926,W04-3233,0,0.151379,"Missing"
W10-2926,doddington-etal-2004-automatic,0,0.066137,"om the same tree ti , (j) (k) we can conclude that xi &lt; xi ∀fi,j ∈ Efi,k . In other words, for each tree in the model, base fragments are the most relevant, and we can assume that the relevance of the heaviest fragment is an upper bound for the relevance of any fragment 4 . 6 Experiments We ran a set of thorough experiments to support our claims with empirical evidence. We show our results on three very different benchmarks: Question Classification (QC) using TREC 10 data (Voorhees, 2001), Relation Extraction (RE) based on the newswire and broadcast news domain of the ACE 2004 English corpus (Doddington et al., 2004) and Semantic Role Labeling (SRL) on the CoNLL 2005 shared task data (Carreras and M`arquez, 2005). In the next sections we elaborate on the setup and outcome of each set of experiments. As a supervised learning framework we used SVM-Light-TK5 , which extends the SVM-Light optimizer (Joachims, 2000) with support for tree kernel functions. Unless differently stated, all the classifiers are parametrized for optimal Precision and Recall on a development set, obtained by selecting one example in ten from the training set with the same positive-to-negative example ratio. The results that we show ar"
W10-2926,D09-1143,1,0.470778,"VM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do       if F 6= ( ∅  do do   Wnext ← Wnext ∪ {f }          Dnext ← Dnext ∪ F then           UPDATE (F )       τ ←τ +1     Wprev ← Wnext   Dprev ← Dnext return (result) base fragment according to Eq. 6 and the"
W10-2926,J05-1004,0,0.0110788,"assume that the highest weight will be obtained from the heaviest of the base fragments. 5 6 http://disi.unitn.it/˜moschitt/Tree-Kernel.htm 228 http://nlp.stanford.edu/software/lex-parser.shtml Semantic Role Labeling (SRL) SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. For these experiments we concentrated on this latter task and used exactly the same setup as P&M. We considered all the argument nodes of any of the six PropBank (Palmer et al., 2005) core roles7 (i.e. A0, . . . , A5) from all the available training sections, i.e. 2 through 21, for a total of 179,091 training instances. Similarly, we collected 9,277 test instances from the annotations of Section 23. 6.1 0.7 0.65 0.6 1−ρ 0.55 0.5 0.45 0.4 ABBR DESC ENTY HUM LOC NUM 0.35 0.3 0.25 0.2 1 10 100 1000 Number of fragments (log) 10000 Figure 1: Percentage of gradient Norm, i.e. 1 − ρ, according to the number of selected fragments, for different QC classifiers. Model Comparison To show the validity of Lemma 1 in practical scenarios, we compare the accuracy of our linearized models"
W10-2926,P06-1115,0,0.0399295,"an embedded approach to feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do       if F 6= ( ∅"
W10-2926,W09-1106,1,0.878968,"ure of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoIn the remainder: Section 2 briefly reviews SVMs and TK functions; Section 3 describes the problem of selecting and projecting features from very high onto lower dimensional spaces, and provides the theoretical foundation to our approach; Section 4 presents a selection of related work; Section 5 describes our approach to tree fragment selection; Section 6 details the outcome of our experiments; finally, in Section 7 we draw our conclusion"
W10-2926,D09-1012,1,0.882047,"ure of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoIn the remainder: Section 2 briefly reviews SVMs and TK functions; Section 3 describes the problem of selecting and projecting features from very high onto lower dimensional spaces, and provides the theoretical foundation to our approach; Section 4 presents a selection of related work; Section 5 describes our approach to tree fragment selection; Section 6 details the outcome of our experiments; finally, in Section 7 we draw our conclusion"
W10-2926,W03-1012,0,0.162211,"l space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   whi"
W10-2926,P08-1082,0,0.0210775,"r work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do       if F 6= ( ∅  do do   Wnext ← Wnext ∪ {f }          Dnext ← Dnext ∪ F then           UPDATE (F )       τ ←τ +1"
W10-2926,W06-2902,0,0.0472692,"and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do"
W10-2926,W04-3222,0,0.19305,"em to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef"
W10-2926,N06-1037,0,0.0564418,"o feature selection for convolution kernels based on χ2 driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Algorithm 5.1: GREEDY MODEL MINER (M, L) B ← BASE FRAGS(model) B ← REL(BEST(B)) σ ← B/L Dprev ← FILTER(B, σ) UPDATE (Dprev ) while Dprev 6= ∅ Dnext ← ∅    τ ← 1/ ∗ widthf actor ∗ /     W  prev ← Dprev   while Wprev 6= ∅   W   next ← ∅       for each    f ∈ Wprev     Ef ← EXPAND(f, τ )       F ← FILTER(Ef , σ) do       if F 6= ( ∅  do do   Wnext ← Wn"
W11-0216,W08-0601,0,0.0674736,"Missing"
W11-0216,H05-1091,0,0.274085,"PPI task, which adopt different PPI annotations. Consequently the experimental results obtained by different approaches are often difficult to compare. Pyysalo et al. (2008) put together these corpora (including the AIMed corpus used in this paper) in a common format for comparative evaluation. Each of these corpora is known as converted corpus of the corresponding original corpus. Several kernel-based RE approaches have been reported to date for the PPI task. These are based on various methods such as subsequence kernel (Lodhi et al., 2002; Bunescu and Mooney, 2006), dependency graph kernel (Bunescu and Mooney, 2005), etc. Different work exploited dependency analyses with different kernel approaches such as bag-of3 http://projects.ldc.upenn.edu/ace/ Also known as shortest path-enclosed tree or SPT (Zhou et al., 2007). 4 words kernel (e.g. Miwa et al. (2009)), graph based kernel (e.g. Kim et al. (2010)), etc. However, there are only few researches that attempted the exploitation of tree kernels on dependency tree structures. Sætre et al. (2007) used DT kernels on AIMed corpus and achieved an F-score of 37.1. The results were far better when they combined the output of the dependency parser with that of a H"
W11-0216,P04-1054,0,0.231621,"RE have drawn a lot of interest in recent years since they can exploit a huge amount of features without an explicit representation. Some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures. Other kernels simply use techniques such as bag-of-words, subsequences, etc. to map the syntactic and contextual information to flat features, and later compute similarity. One variation of tree kernels is the dependency tree (DT) kernel (Culotta and Sorensen, 2004; Nguyen et al., 2009). A DT kernel (DTK) is a tree kernel that is computed on a dependency tree (or subtree). A dependency tree encodes grammatical relations between words in a sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges. The main advantage of a DT in comparison with phrase structure tree (PST) is that the former allows for relating two words directly (and in more compact substructures than PST) even if they are far apart in the corresponding sentence according to their lexical word order. Seve"
W11-0216,D07-1024,0,0.0572078,"e is only one node for each individual word whereas in their constructed trees (please refer to Fig. 6 of Miwa et al. (2009)), a word (that belongs to the shortest path) has as many node representations as the number of dependency relations with other words (those belonging to the shortest path). Perhaps, this redundancy of information might be the reason their approach achieved higher result. In addition to work on PPI pair extraction, there has been some approaches that exploited dependency parse analyses along with kernel methods for identifying sentences that might contain PPI pairs (e.g. Erkan et al. (2007)). In this paper, we focus on finding the best representation based on a single structure. We speculate that this can be helpful to improve the state-of-theart using several combinations of structures and features. As a first step, we decided to use uPTK, which is more robust to overfitting as the description in the next section unveil. 3 Unlexicalized Partial Tree Kernel (uPTK) The uPTK was firstly proposed in (Severyn and Moschitti, 2010) and experimented with semantic role labeling (SRL). The results showed no improvement for such task but it is well known that in SRL lexical information is"
W11-0216,E06-1051,1,0.41627,"our and their results. A possible explanation could be related to parameter settings. Another source of uncertainty is given by the tool for tree kernel computation, which in their case is not mentioned. Moreover, their description of PT and SST (in Figure 1 of their paper) appears to be imprecise: for example, in (partial or complete) phrase structure trees, words can only appear as leaves but in their figure they appear as nonterminal nodes. The comparison with other kernel approaches (i.e. not necessarily tree kernels on DT or PST) shows that there are model achieving higher results (e.g. Giuliano et al. (2006), Kim et al. (2010), Airola et al. (2008), etc). State-of-the-art results on most of the PPI data sets are obtained by the hybrid kernel presented in Miwa et al. (2009). As noted earlier, our work focuses on the design of an effective DTK 131 for PPI that can be combined with others and that can hopefully be used to design state-of-the-art hybrid kernels. 6 Conclusion In this paper, we have proposed a study of PPI extraction from specific biomedical data based on tree kernels. We have modeled and experimented with new kernels and DT structures, which can be exploited for RE tasks in other doma"
W11-0216,P04-1043,1,0.893934,"llowing three dependency structures to be exploited by convolution tree kernels: • Dependency Words (DW) tree: a DW tree is the minimal subtree of a DT, which includes e1 and e2. An extra node is inserted as parent of the corresponding NE, labeled with the NE category. Only words are considered in this tree. • Grammatical Relation (GR) tree: a GR tree is similar to a DW tree except that words are replaced by their grammatical functions, e.g. prep, nsubj, etc. 2 Convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features (Moschitti, 2004). 126 • Grammatical Relation and Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two"
W11-0216,E06-1015,1,0.809134,"produce better results than the DTK on minimal subtrees. The second is that previously proposed DT structures can be further improved by introducing simplified representation of the entities as well as augmenting nodes in the DT tree structure with relevant features. This paper presents an evaluation of the above assumptions. More specifically, the contributions of this paper are the following: • We propose the use of new DT structures, which are improvement on the structures defined in Nguyen et al. (2009) with the most general (in terms of substructures) DTK, i.e. Partial Tree Kernel (PTK) (Moschitti, 2006). • We firstly propose the use of the Unlexicalized PTK (Severyn and Moschitti, 2010) with our dependency structures, which significantly improves PTK. • We compare the performance of the proposed DTKs on PPI with the one of PST kernels and 125 • Finally, we introduce a novel approach (called mildly extended dependency tree (MEDT) kernel1 , which achieves the best performance among various (both DT and PST) tree kernels. The remainder of the paper is organized as follows. In Section 2, we introduce tree kernels and relation extraction and we also review previous work. Section 3 describes the u"
W11-0216,D09-1143,1,0.146382,"est in recent years since they can exploit a huge amount of features without an explicit representation. Some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures. Other kernels simply use techniques such as bag-of-words, subsequences, etc. to map the syntactic and contextual information to flat features, and later compute similarity. One variation of tree kernels is the dependency tree (DT) kernel (Culotta and Sorensen, 2004; Nguyen et al., 2009). A DT kernel (DTK) is a tree kernel that is computed on a dependency tree (or subtree). A dependency tree encodes grammatical relations between words in a sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges. The main advantage of a DT in comparison with phrase structure tree (PST) is that the former allows for relating two words directly (and in more compact substructures than PST) even if they are far apart in the corresponding sentence according to their lexical word order. Several kernel approaches"
W11-0216,I05-1034,0,0.0447003,"structural information in term of sub-structures, providing a viable alternative to flat features (Moschitti, 2004). 126 • Grammatical Relation and Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two entities involved in a relation. Zhou et al. (2007) proposed the so called contextsensitive tree kernel approach based on PST, which expands PET to include necessary contextual information. The expansion is carried out by some heuristics tuned on the target RE task. Nguyen et al. (2009) improved the PET representation by inserting extra nodes for denoting the NE category of the entities inside the subtree. They also used sequence kernels from tree paths, which provided higher accuracy. 2.3 Relation"
W11-0216,D07-1076,0,0.582512,"Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two entities involved in a relation. Zhou et al. (2007) proposed the so called contextsensitive tree kernel approach based on PST, which expands PET to include necessary contextual information. The expansion is carried out by some heuristics tuned on the target RE task. Nguyen et al. (2009) improved the PET representation by inserting extra nodes for denoting the NE category of the entities inside the subtree. They also used sequence kernels from tree paths, which provided higher accuracy. 2.3 Relation Extraction in the biomedical domain There are several benchmarks for the PPI task, which adopt different PPI annotations. Consequently the experime"
W12-4501,W06-0609,0,0.0197938,"ion of the corpus is all newswire, this had no impact on it. However, for both Chinese and Arabic, since we remove trace tokens corresponding to dropped pronouns, all the other layers of annotation had to be remapped to the remaining sequence of tree tokens. Propositions The propositions in OntoNotes are PropBank-style semantic roles for English, Chinese and Arabic. Most of the verb predicates in the corpus have been annotated with their arguments. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recal"
W12-4501,2011.mtsummit-papers.22,1,0.579383,"s that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedure used to create OntoNotes training, development and test partitions. Procedure: Generate Partitions(OntoNotes) returns Train, Dev, Test 1: Train ← ∅ 2: Dev ← ∅ 3: Test ← ∅ 4: for all Source ∈ OntoNotes do 5: if Source = Wall Street Journal then 6: Train ← Train ∪ S"
W12-4501,P06-1005,0,0.473873,"f WordNet senses, systems could also map from the predicted or gold-standard word senses to the sets of underlying WordNet senses. Another significant piece of knowledge that is particularly useful for coreference but that is not available in the layers of OntoNotes is that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedur"
W12-4501,W10-4305,0,0.0238143,"distribution of the participants by country and the participation by language and task type. 4.5.3 Scoring Metrics Implementation We used the same core scorer implementation25 that was used for the S EM E VAL-2010 task, and which implemented all the different metrics. There were a couple of modifications done to this scorer since then. 6 1. Only exact matches were considered correct. Previously, for S EM E VAL-2010 nonexact matches were judged partially correct with a 0.5 score if the heads were the same and the mention extent did not exceed the gold mention. 2. The modifications suggested by Cai and Strube (2010) have been incorporated in the scorer. Since there are differences in the version used for CoNLL and the one available on the download site, and it is possible that the latter would be revised in the future, we have archived the version of the scorer on the CoNLL-2012 task webpage.26 5 Participants A total of 41 different groups demonstrated interest in the shared task by registering on the task 25 26 http://www.lsi.upc.edu/∼esapena/downloads/index.php?id=3 http://conll.bbn.com/download/scorer.v4.tar.gz 20 Country Participants Brazil China Germany Italy Switzerland USA 1 8 3 1 1 2 Table 13: Pa"
W12-4501,W11-1907,0,0.144398,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,P11-2037,0,0.0414211,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,N01-1016,0,0.027844,"ents. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. tional preferences (LINK - SLC) were added to PropBank. More details can be found in the addendum to the PropBank guidelines16 in the OntoNotes v5.0 release. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the PropBank 1.0 release. This functionality is part of the OntoNotes DB"
W12-4501,P05-1022,0,0.00733886,"have been annotated for other layers. For training models for each of the layers, where feasible, we used all the data that we could 16 17 doc/propbank/english-propbank.pdf http://cemantix.org/ontonotes.html 14 Layer English Chinese Arabic Verb Noun All Verb Noun Sense Inventories 2702 Frames 5672 2194 1335 763 150 20134 2743 111 532 Table 7: Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data wer"
W12-4501,N07-1011,0,0.0270615,"d multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly"
W12-4501,N07-1030,0,0.0061787,"/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used"
W12-4501,doddington-etal-2004-automatic,0,0.253776,"Uryupina University of Trento, 38123 Povo (TN) Italy Yuchen Zhang Brandeis University, Waltham, MA 02453 USA uryupina@gmail.com yuchenz@brandeis.edu Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with decision trees and hand-written rules. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC) (Hirschman and Chinchor, 1997; Chinchor, 2001; Chinchor and Sundheim, 2003). The de facto standard datasets for current coreference studies are the MUC and the ACE 1 (Doddington et al., 2004) corpora. These corpora were tagged with coreferring entities in the form of noun phrases in the text. The MUC corpora cover all noun phrases in text but are relatively small in size. The ACE corpora, on the other hand, cover much more data, but the annotation is restricted to a small subset of entities. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it is a problem that requires world knowledge to solve and word knowledge is hard to define, and partly owing to the lack of substantial annotated data. Aside from"
W12-4501,N10-1061,0,0.0387987,"liarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rath"
W12-4501,N01-1008,0,0.104449,"Missing"
W12-4501,N06-2015,0,0.586083,"enomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly several ablation settings to capture the range of performance. This can then be used as a standard benchmark by the research community. (iii) Continue to improve learning algorithms that better incorporate world knowledge and jointly incorporate information from other layers of syntactic and semantic annotation to improve the state of the art. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) 2 was to explore whether it could fill this void and help push the progress further — not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a subset of entity types. The coreference layer in OntoNotes constitutes just one part of a multilayered, integrated annotation of shallow semantic structures in text with high inter-annotator agreement. This addresses the first issue. In the language"
W12-4501,W11-1902,0,0.174907,"Missing"
W12-4501,H05-1004,0,0.90641,"Missing"
W12-4501,W04-1602,0,0.00799095,"Missing"
W12-4501,J93-2004,0,0.0513136,"Missing"
W12-4501,P00-1023,0,0.125028,"ist.gov/iad/mig/publications/ASRhistory/index.html http://www.itl.nist.gov/iad/mig/tests/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaborative"
W12-4501,P10-1142,0,0.204917,"semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution techniques to the next level. The OntoNotes project has created a large-scale corpus of accurate and integrated annotation of multiple levels of"
W12-4501,J05-1004,0,0.387003,"Missing"
W12-4501,palmer-etal-2008-pilot,0,0.00787827,"Missing"
W12-4501,passonneau-2004-computing,0,0.00467497,"high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has ma"
W12-4501,N07-1051,0,0.00750508,"Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-fold for Arabic) cross-validation. The development and test parses were generated using a model trained on the entire training portion. We used OntoNotes v5.0 training data for training the Chinese and Arabic parser models, but the OntoNotes v4.0 subset of OntoNotes v5.0 data was used for training the English m"
W12-4501,W05-0311,0,0.0332271,"t equally annotatable with high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation sce"
W12-4501,P09-5006,0,0.0290366,"Missing"
W12-4501,N06-1025,0,0.0301668,"tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress,"
W12-4501,D10-1048,0,0.0903952,"tem uses feature templates defined on mention pairs. bj¨orkelund mentions that disallowing transitive closures gave performance improvement of 0.6 and 0.4 respectively for English and Chinese/Arabic. bj¨orkelund also mentions seeing a considerable increase in performance after adding features that correspond to the Shortest Edit Script (Myers, 1986) between surface forms and unvocalised Buckwalter forms, respectively. These could be better at capturing the differences in gender and number signaled by certain morphemes than hand-crafted rules. chen built upon the sieve architecture proposed in Raghunathan et al. (2010) and added one more sieve — head match — for Chinese and modified two sieves. Some participants tried to incorporate peculiarities of the corpus in their systems. For example, martschat excluded adjectival nation names. Unlike English, and especially in absence of an external resource, it is hard to make a gender distinction in Arabic and Chinese. martschat used the information 23 that 先 生(sir) and 女士(lady) often suggest gender information. bo and martschat used plurality markers 们 to identify plurals. For example, 同学 (student) is singular and 同学们 (students) is plural. bo also uses a heuristic"
W12-4501,D09-1101,0,0.142437,"will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorith"
W12-4501,W09-2411,0,0.101275,"Missing"
W12-4501,J01-4004,0,0.987173,"plex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation sc"
W12-4501,P09-1074,0,0.0393009,"rstanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high"
W12-4501,P08-4003,1,0.780554,"P and based pruning; Genre specific selected NE in Arabic. Learning to prune non-referential mentions models NP, PRP and PRP $ in all languages; PN in Logistic Regression Chinese; all NE in English. Exclude pleonastic (LIBLINEAR) it in English. Prune smaller mentions with same head. Eight different mention types for English, and Directed multigraph adjectival use for nations and a few NEs are representation where the filtered as well as embedded mentions and weights are learned over the pleonastic pronouns. Four mention types in training data (on top of BART Chinese. Copulas are also handled (Versley et al., 2008)) appropriately. Latent Structure Learning All noun phrases, pronouns and name entities modification of BART using multi-objective optimization. Standard rules for English and Classifier to Domain specific classifiers for identify markable NPs in Chinese and Arabic. nw and bc genre. Memory based learning NP, PRP and PRP $ in English, and all NP in (T I MBL) Chinese and Arabic. Singleton classifier. All phrase types that are mentions in training MaxEnt are considered as mentions and a classifier is trained to identify potential mentions. C4.5 and deterministic rules All noun phrases, pronouns a"
W12-4501,D07-1052,0,0.0121409,". (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techni"
W12-4501,M95-1005,0,0.953302,"Missing"
W12-4501,J08-2004,1,0.444195,"agging where The NULL arguments are first filtered out, and the remaining NON - NULL arguments are classified into one of the argument types. The argument identification module used an ensemble of ten classifiers — each trained on a tenth of the training data and combined using unweighted voting. This should still give a close to state-of-theart performance given that the argument identification performance tends to start to be asymptotic around 10K training instances (Pradhan et al., 2005). The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on all the training portion of the OntoNotes v5.0 data. No propositional structures were provided for Arabic due to resource constraints. Table 9 shows the detailed performance numbers. The CoNLL-2005 scorer was used to compute the scores. At first glance, the performance on the English newswire genre is much lower than what has been reported for WSJ Section 23. This could be attributed to several factors: i) the fact that we had to compromise on the training method, ii) the newswire in OntoNotes not only contains WSJ data, but also Xinhua news, iii) The WSJ training and test porti"
W12-4501,C10-2158,1,0.696855,"Missing"
W12-4501,W10-1836,1,0.72614,"Missing"
W12-4501,P10-4014,0,0.0186234,"44 74.99 76.19 Table 6: Parser performance on the CoNLL-2012 test set. R English Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Weblogs and Newsgroups [WB] Overall 81.3 81.5 78.8 85.7 77.6 81.2 82.0 79.1 85.7 77.5 81.2 81.7 79.0 85.7 77.5 82.5 82.5 82.5 Chinese Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Overall Arabic Accuracy P F Newswire [NW]19 - - 80.5 85.4 82.4 89.1 - - 84.3 75.2 75.9 75.6 Table 8: Word sense performance over both verbs and nouns in the CoNLL-2012 test set. Word Sense This year we used the IMS (It Makes Sense) (Zhong and Ng, 2010) word sense tagger.20 Word sense information, unlike syntactic parse information is not central to approaches taken by current coreference systems and so we decided to use a better word sense tagger to get a good state of the art accuracy estimate, at the cost of a completely fair (but, still close enough) comparison with English CoNLL-2011 results. This will also allow potential future uses to benefit from it. IMS was trained on all the word sense data that is present in the training portion of the OntoNotes corpus using crossvalidated predictions on the input layers similar to the propositio"
W12-4501,W08-2121,0,\N,Missing
W12-4501,E06-2015,0,\N,Missing
W12-4501,D08-1067,0,\N,Missing
W12-4501,W09-1201,1,\N,Missing
W12-4501,S10-1001,0,\N,Missing
W12-4515,2011.mtsummit-papers.22,0,0.018256,"erence on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this is not a semantic, but syntactic definiteness. Without any experience in Arabic, one can hardly decide how such “syntactic definiteness” might affect coreference. In the present study, we have used the information provided by"
W12-4515,W99-0104,0,0.0553225,"ost commonly pronominal anaphors (cf., for example, (Iida and Poesio, 2011; Arregi et al., 2010) and many others). 1 Statistical EMD approaches have been proved useful for Two new languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on termi"
W12-4515,N01-1008,0,0.015388,"languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this"
W12-4515,P11-1081,1,0.835891,"ble shift A number of high-performance coreference resolution (CR) systems have been created for English in the past decades, implementing both rule-based and statistical approaches. For other languages, however, the situation is far less optimistic. For Romance and German languages, several systems have been developed and evaluated, in particular, at the SemEval-2010 track 1 on Multilingual Coreference Resolution (Recasens et al., 2010). For other languages, individual approaches have been proposed, covering specific subparts of the task, most commonly pronominal anaphors (cf., for example, (Iida and Poesio, 2011; Arregi et al., 2010) and many others). 1 Statistical EMD approaches have been proved useful for Two new languages, Arabic and Chinese, have ACE-style coreference resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for"
W12-4515,N06-1025,0,0.0481714,"resolution, where mentions are basic been proposed for the CoNLL-2012 shared task units belonging to a restricted set of semantic types. 122 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122–128, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics from knowledge-lean approaches of the late nineties (Harabagiu and Maiorano, 1999). In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet (Harabagiu et al., 2001; Huang et al., 2009) or Wikipedia (Ponzetto and Strube, 2006). For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue (2011) have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider. For example, several works on Arabic (Hoyt, 2008) mention that nouns can be made definite with the suffix “Al-”, but this is not a semantic, but syntactic definiteness. Without any exp"
W12-4515,W12-4501,1,0.856641,"ssion to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution. We have extended our CoNLL-2011 submission, based on BART, to cover two additional languages, Arabic and Chinese. This paper focuses on adapting BART to new languages, discussing the problems we have encountered and the solutions adopted. In particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system. 1 Introduction (Pradhan et al., 2012). They present a challenging problem: the systems are required to provide entity mention detection (EMD) and design a proper coreference resolver for both languages. At UniTN/Essex, we have focused on these parts of the task, relying on a modified version of our last-year submission for English. Most state-of-the-art full-scale coreference resolution systems rely on hand-written rules for the mention detection subtask.1 For English, such rules may vary from corpus to corpus, reflecting specifics of particular guidelines (e.g. whether nominal premodifiers can be mentions, as in MUC, or not, as"
W12-4515,W09-2411,0,0.0853079,"Missing"
W12-4515,I11-1011,1,0.835106,"xt Parser Mention Mention Tagger Factory Encoder/ Decoder Coreference chains (entities) Merger Preprocessing Machine Learner Figure 1: BART architecture ing pipeline to operate on the OntoNotes NE-types, mapping them into MUC types required by BART. This allows us to participate in the closed track, as no external material is used any longer. Since last year, we have continued with our experiments on multi-objective optimization, proposed in our CoNLL-2011 paper (Uryupina et al., 2011). We have extended the scope of our work to cover different machine learning algorithms and their parameters (Saha et al., 2011). For CoNLL-2012, we have re-tested all the solutions of our optimization experiments, picking the one with the highest score on the current development set. Finally, our recent experiments on domain selection (Uryupina and Poesio, 2012) suggest that, at least for some subparts of OntoNotes, a system might benefit from training a domain-specific model. We have tested this hypothesis on the CoNLL-2012 data and have consequently trained domain-specific classifiers for the nw and bc domains. 3 Coreference resolution in Arabic and Chinese 3.1 Mention detection Mention detection is rarely considere"
W12-4515,J01-4004,0,0.769784,"Missing"
W12-4515,uryupina-poesio-2012-domain,1,0.827356,"es required by BART. This allows us to participate in the closed track, as no external material is used any longer. Since last year, we have continued with our experiments on multi-objective optimization, proposed in our CoNLL-2011 paper (Uryupina et al., 2011). We have extended the scope of our work to cover different machine learning algorithms and their parameters (Saha et al., 2011). For CoNLL-2012, we have re-tested all the solutions of our optimization experiments, picking the one with the highest score on the current development set. Finally, our recent experiments on domain selection (Uryupina and Poesio, 2012) suggest that, at least for some subparts of OntoNotes, a system might benefit from training a domain-specific model. We have tested this hypothesis on the CoNLL-2012 data and have consequently trained domain-specific classifiers for the nw and bc domains. 3 Coreference resolution in Arabic and Chinese 3.1 Mention detection Mention detection is rarely considered to be a separate task. Only very few studies on coreference resolution report on their EMD techniques. Existing corpora of coreference follow different approaches to mention annotation: this includes defining mention boundaries (basic"
W12-4515,W11-1908,1,0.748386,"orts dual number). The encoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the decoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 2 The English track at CoNLL-2012 can be considered an extension of the last year’s CoNLL task. New data have been added to the corpus, including two additional domains, but the annotation guidelines remain the same. We have therefore mainly relied on the CoNLL2011 version of our system (Uryupina et al., 2011) for the current submission, providing only minor adjustments. Thus, we have modified our preprocessBART Our CoNLL submission is based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and 123 2.1 Coreference resolution in English Language Plugin Feature Extractor POS Tagger Unannotated Text Parser Mention Mention Ta"
W12-4515,P08-4003,1,0.889157,"ing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 2 The English track at CoNLL-2012 can be considered an extension of the last year’s CoNLL task. New data have been added to the corpus, including two additional domains, but the annotation guidelines remain the same. We have therefore mainly relied on the CoNLL2011 version of our system (Uryupina et al., 2011) for the current submission, providing only minor adjustments. Thus, we have modified our preprocessBART Our CoNLL submission is based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and 123 2.1 Coreference resolution in English Language Plugin Feature Extractor POS Tagger Unannotated Text Parser Mention Mention Tagger Factory Encoder/ Decoder Coreference chains (entities) Merger Preprocessing Machine Learner Figure 1: BART architecture ing pipeline to operate on the OntoNotes"
W12-4515,D09-1128,0,\N,Missing
W12-4515,J03-4003,0,\N,Missing
W12-4515,S10-1001,1,\N,Missing
W13-3509,P04-1043,1,0.822983,"er passage trees. More formally, given two hypotheses, hi = hhi (q), hi (a)i and hi = hh0i (q), h0i (a)i, whose members are the question and answer passage trees, we define K(hi , h0i ) as T K(hi (q), h0i (q)) + T K(hi (a), h0i (a)), where T K can be any tree kernel function, e.g., STK or PTK. To enable traditional feature vectors it is enough to add the product (~xh1 − ~xh2 ) · (~xh01 − ~xh02 ) to the structural kernel PK , where ~xh is the feature vector associated with the hypothesis h. We opted for a simple kernel sum over a product, since the latter rarely works in practice. Although in (Moschitti, 2004) the kernel product has been shown to provide some improvement when applied to tree kernels over a subcategorization frame structure, in general, it seems to work well only when the tree structures are small and derived rather accurately (Giordani and Moschitti, 2009; Giordani and Moschitti, 2012). 3 4 Relational Linking The use of a special tag to mark the related fragments in the question and answer tree representations has been shown to yield more accurate relational models (Severyn and Moschitti, 2012). However, previous approach was based on a na¨ıve hard matching between word lemmas. Bel"
W13-3509,E06-1050,0,0.0519429,"tly reduces the search space. Previous work in Question Classification reveals the power of syntactic/semantic tree representations coupled with tree kernels to train the state-of-the-art models (Bloehdorn and Moschitti, 2007). Hence, we opt for an SVM multi-classifier using tree kernels to automatically extract the question class. To build a multi-class classifier we train a binary SVM for each of the classes and apply a one-vs-all strategy to obtain the predicted class. We use constituency trees as our input representation. swers in document passages, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. In particular, to detect the question focus word we train a binary SVM classifier with tree kernels applied to the constituency tree representation. For each given question we generate a set of candidate trees where the parent (node with the POS tag) of each candidate focus word is annotated with a specia"
W13-3509,P02-1034,0,0.0753445,"eature vectors, and finally Sec. 6 reports the experimental results on TREC and Answerbag data. 2 2.1 Tree kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures. We rely on the Partial Tree Kernel (PTK) (Moschitti, 2006) to handle feature engineering over the structural representations. The choice of PTK is motivated by its ability to generate rich feature spaces over both constituency and dependency parse trees. It generalizes a subset tree kernel (STK) (Collins and Duffy, 2002) that maps a tree into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. Different from STK where the nodes in the generated tree fragments are constrained to include none or all of their direct children, PTK fragments can contain any subset of the features, i.e., PTK allows for breaking the production rules. Consequently, PTK generalizes STK, thus generating an extremely rich feature space, which results in higher generalization ability. 2.3 Preference reranking with kernels To enable the use of kernels for learning"
W13-3509,quarteroni-etal-2012-evaluating,0,0.321208,"h tree kernels to train the state-of-the-art models (Bloehdorn and Moschitti, 2007). Hence, we opt for an SVM multi-classifier using tree kernels to automatically extract the question class. To build a multi-class classifier we train a binary SVM for each of the classes and apply a one-vs-all strategy to obtain the predicted class. We use constituency trees as our input representation. swers in document passages, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. In particular, to detect the question focus word we train a binary SVM classifier with tree kernels applied to the constituency tree representation. For each given question we generate a set of candidate trees where the parent (node with the POS tag) of each candidate focus word is annotated with a special FOCUS tag. Trees with the correctly tagged focus word constitute a positive example, while the others are negative examples. To detect the focus for a"
W13-3509,damljanovic-etal-2010-identification,0,0.0551448,"he state-of-the-art models (Bloehdorn and Moschitti, 2007). Hence, we opt for an SVM multi-classifier using tree kernels to automatically extract the question class. To build a multi-class classifier we train a binary SVM for each of the classes and apply a one-vs-all strategy to obtain the predicted class. We use constituency trees as our input representation. swers in document passages, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. In particular, to detect the question focus word we train a binary SVM classifier with tree kernels applied to the constituency tree representation. For each given question we generate a set of candidate trees where the parent (node with the POS tag) of each candidate focus word is annotated with a special FOCUS tag. Trees with the correctly tagged focus word constitute a positive example, while the others are negative examples. To detect the focus for an unseen question we class"
W13-3509,P08-1082,0,0.12356,"Missing"
W13-3509,C12-2040,1,0.441686,"Missing"
W13-3509,C02-1150,0,0.948542,"lps to alleviate the lexical gap problem which makes the n¨aive string matching of words between a question and its answer less reliable. Hence, we propose to exploit a question category (automatically identified by a question type classifier) along with named entities found in the answer to establish relational links between the tree structures of a given q/a pair. In particular, once the question focus and question category Question classification Question classification is the task of assigning a question to one of the pre-specified categories. We use the coarse-grain classes described in (Li and Roth, 2002): six non-overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LOC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates). These categories can be used to determine the Expected Answer Type for a given question and find 78 Table 1: Question classes → named entity types. Question Category HUM LOC NUM ENTY similarity features in QA has been studied elsewhere, e.g., (Surdeanu et al., 2008). Named Entity types Person Location Date, Time, Money, Perce"
W13-3509,P08-2029,1,0.904774,"s are more invariant w.r.t. different domains, thus fostering adaptability. 1 Machine learning has made easier the task of QA engineering by enabling the automatic learning of answer extraction modules. However, new features and training data have to be typically developed when porting a QA system from a domain to another. This is even more critical considering that effective features tend to be as much complex and similar as traditional handcrafted rules. To reduce the burden of manual feature engineering for QA, we proposed structural models based on kernel methods, (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2008) with passages limited to one sentence. Their main idea is to: (i) generate question and passage pairs, where the text passages are retrieved by a search engine; (ii) assuming those containing the correct answer as positive instance pairs and all the others as negative ones; (iii) represent such pairs with two syntactic trees; and (ii) learn to rank answer passages by means of structural kernels applied to two trees. This enables the automatic engineering of structural/lexical semantic patterns. More recently, we showed that such models can be learned for passages constituted"
W13-3509,P07-1098,1,0.92641,"ines; and (ii) structures are more invariant w.r.t. different domains, thus fostering adaptability. 1 Machine learning has made easier the task of QA engineering by enabling the automatic learning of answer extraction modules. However, new features and training data have to be typically developed when porting a QA system from a domain to another. This is even more critical considering that effective features tend to be as much complex and similar as traditional handcrafted rules. To reduce the burden of manual feature engineering for QA, we proposed structural models based on kernel methods, (Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Moschitti, 2008) with passages limited to one sentence. Their main idea is to: (i) generate question and passage pairs, where the text passages are retrieved by a search engine; (ii) assuming those containing the correct answer as positive instance pairs and all the others as negative ones; (iii) represent such pairs with two syntactic trees; and (ii) learn to rank answer passages by means of structural kernels applied to two trees. This enables the automatic engineering of structural/lexical semantic patterns. More recently, we showed that such models can be"
W13-3516,P05-1022,0,0.0426867,"umption. For the spoken genres – BC, BN and TC – we use the manual transcriptions rather than the output of a speech recognizer, as would be the case in real world. The performance on various layers for these genres would therefore be artificially inflated, and should be taken into account while analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-"
W13-3516,P08-1091,1,0.914176,"in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6 1208 2849 92.8 9,261 26,882 87.7 885 2,323 93.3 929 4,419 92.3 451 2,620 96.6 481 2,210 82.2 968 1,622 87.8 758 1,761 90.9 4,472 14,955 8"
W13-3516,W00-1322,0,0.0291123,"Missing"
W13-3516,W06-0609,0,0.0675644,"and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word senses are tagged for the mos"
W13-3516,2011.mtsummit-papers.22,1,0.659518,"reference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic informatio"
W13-3516,P06-1005,0,0.0274234,"able. 4.5 Coreference The task is to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains. The coreference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features i"
W13-3516,W12-4503,1,0.323243,"Missing"
W13-3516,W11-1905,1,0.8623,"Missing"
W13-3516,W10-4305,0,0.00957321,"thm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported in Recasens et al., (2013). As of this writing, the BCUBED metric has been fixed, and the correctness of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performa"
W13-3516,P11-2037,0,0.0435593,"tructure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This meant removing any and"
W13-3516,W05-0620,0,0.0226073,"Missing"
W13-3516,P05-1045,0,0.0391503,"of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performance of the system on the Table 6: Performance of the named entity recognizer on the CoNLL-2012 test set. 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre. In English, BN has the highest performance followed by the NW genre. There is a significant drop from those and the TC and WB genre. Somewhat similar trend is observed in the Chinese data, with Arabic having the lowest scores. Since the Pivot Text portion (PT) of OntoNotes was not tagged with names, we could not compute the accuracy for that cross-section of the data. Previously Finkel and Manning (2009) performed 17 The number of sentences in this table are a subs"
W13-3516,W01-0521,0,0.0213903,"ver the sense inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, develo"
W13-3516,hockenmaier-steedman-2002-acquiring,0,0.0152391,"g5 1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA 2 University of Trento, University of Trento, 38123 Povo (TN), Italy 3 QCRI, Qatar Foundation, 5825 Doha, Qatar 4 Brandeis University, Brandeis University, Waltham, MA 02453, USA 5 National University of Singapore, Singapore, 117417 6 University of Stuttgart, 70174 Stuttgart, Germany Abstract the Penn Discourse Treebank (Prasad et al., 2008), and many other annotation projects, all annotate the same underlying body of text. It was also converted to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the cu"
W13-3516,W04-1602,0,0.0421028,"d to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the current hurdles in language processing is the problem of domain, or genre adaptation. Although genre or domain are popular terms, their definitions are still vague. In OntoNotes, “genre” means a type of source – newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB) or pivot text (PT). Changes in the entity and event profiles across source types, and even in the same source over a time duration, as explicitly expressed by surface lexical forms, usually"
W13-3516,J93-2004,0,0.0664902,"sitions for most verb and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word s"
W13-3516,N06-1020,0,0.0154191,"inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so a"
W13-3516,P02-1014,0,0.0344201,"ce annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported"
W13-3516,J05-1004,0,0.0531944,"rformance. 1 Introduction Roughly a million words of text from the Wall Street Journal newswire (WSJ), circa 1989, has had a significant impact on research in the language processing community — especially those in the area of syntax and (shallow) semantics, the reason for this being the seminal impact of the Penn Treebank project which first selected this text for annotation. Taking advantage of a solid syntactic foundation, later researchers who wanted to annotate semantic phenomena on a relatively large scale, also used it as the basis of their annotation. For example the Proposition Bank (Palmer et al., 2005), BBN Name Entity and Pronoun coreference corpus (Weischedel and Brunstein, 2005), 1 A portion of the English data in the OntoNotes corpus is a selected set of sentences that were annotated for parse and word sense information. These sentences are present in a document of their own, and so the documents for parse layers for English are inflated by about 3655 documents and for the word sense are inflated by about 8797 documents. 143 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143–152, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Comput"
W13-3516,palmer-etal-2008-pilot,0,0.0620251,"Missing"
W13-3516,N07-1051,0,0.0519693,"ile analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-validation. For testing, we used a model trained on the entire training portion. Table 3 shows the precision, recall and F1 -scores of the re-trained parsers on the CoNLL-2012 test along with the part of speech accuracies (POS) using the standard evalb scorer. The performance on the PT ge"
W13-3516,J08-2006,1,0.738119,"of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so as to facilitate future studies. 2.1.3 Proposition The propositions in OntoNo"
W13-3516,W11-1901,1,0.408033,"Missing"
W13-3516,W12-4501,1,0.81236,"he exact same phenomena have been annotated on a broad cross-section of the same language before OntoNotes. The OntoNotes corpus thus provides an opportunity for studying the genre effect on different syntactic, semantic and discourse analyzers. Parts of the OntoNotes Corpus have been used for various shared tasks organized by the language processing community. The word sense layer was the subject of prediction in two SemEval-2007 tasks, and the coreference layer was the subject of prediction in the SemEval-20102 (Recasens et al., 2010), CoNLL-2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). The CoNLL-2012 shared task provided predicted information to the participants, however, that did not include a few layers such as the named entities for Chinese and Arabic, propositions for Arabic, and for better comparison of the English data with the CoNLL-2011 task, a smaller OntoNotes v4.0 portion of the English parse and propositions was used for training. This paper is a first attempt at presenting a coherent high-level picture of the performance of various publicly available state-of-the-art tools on all the layers of OntoNotes in all three languages, so as to pave the way for further"
W13-3516,prasad-etal-2008-penn,0,0.0477485,"Missing"
W13-3516,S10-1001,0,0.0530331,"Missing"
W13-3516,N13-1071,0,0.0213813,"Missing"
W13-3516,J08-2004,1,0.227032,"nd LINK - PCR. Since the community is not used to the new PropBank representation which (i) relies heavily on the trace structure in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6"
W13-3516,C10-2158,1,0.243983,"nd the proposition structure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This mean"
W13-3516,W10-1836,1,0.361229,"Missing"
W13-3516,P10-4014,1,0.579474,"Missing"
W13-3516,D08-1105,1,0.881098,"Missing"
W13-3516,N01-1016,0,\N,Missing
W13-3516,N07-1070,1,\N,Missing
W13-3516,N09-1037,0,\N,Missing
W14-1605,P13-4021,0,0.111609,"Missing"
W14-1605,P02-1034,0,0.0151844,"PK (hs1 , s2 i, hs01 , s02 i) = K(s1 , s01 )+ +K(s2 , s02 ) − K(s1 , s02 ) − K(s2 , s01 ), Tree kernels where sr and s0r refer to two sets of candidates associated with two rankings and K is a kernel applied to pairs of candidates. We represent the latter as pairs of clue and snippet trees. More formally, given two candidates, si = hsi (c), si (s)i and s0i = hs0i (c), s0i (s)i, whose members are the clue and snippet trees, we define We briefly report the different types of kernels (see, e.g., (Moschitti, 2006) for more details). Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. In other words, substructures are composed by atomic building blocks corresponding to nodes along with all of their direct children. These, in case of a syntactic parse tree, are complete production rules of the associated parser grammar. STKb extends STK by allowing leaf nodes to be part of the feature space. Leaf in syntactic trees are words, from this the subscript b (bag-of-words). Subtree Kernel (SbtK) is one of the simplest tree K(si , s0i ) ="
W14-1605,W13-3509,1,0.946016,"plex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 3.1 WebSearch Module (WSM) WSM carries out four different tasks: (i) the retrieval of useful text snippets (TS) and web documents, (ii) the extraction of the answer candidates from such text, (iii) the scoring/filtering of the candidates, and (iv) the estimation of the list confidence. The retrieval of TS is performed by the Bing search engine by simply providing it the clue through its APIs. Then, the latter again are used to access the retrieved TS. The word list generator extracts possible candidate answers from TS or Web documents by picking the terms (also multiwords) of the correct"
W14-1605,D07-1002,0,0.0368523,"es. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 3.1 WebSearch Module (WSM) WSM carries out four different tasks: (i) the retrieval of useful text snippets (TS) and web documents, (ii) the extraction of the answer candidates from such text, (iii) the scoring/filtering of the candidates, and (iv) the estimation of the list confidence. The retrieval of TS is performed by the Bing search engine by simply providing it the clue through its APIs. Then, the latter again are used to access the retrieved TS. The word list generator extracts possibl"
W14-1605,P08-1082,0,0.0296453,"be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 3.1 WebSearch Module (WSM) WSM carries out four different tasks: (i) the retrieval of useful text snippets (TS) and web documents, (ii) the extraction of the answer candidates from such text, (iii) the scoring/filtering of the candidates, and (iv) the estimation of the list confidence. The retrieval of TS is performed by the Bing search engine by simply providing it the clue through its APIs. Then, the latter again are used to access the retrieved TS. The word list generator extracts possible candidate answers from TS or Web documents by"
W14-1605,W06-2909,1,0.810176,"accuracy can impact the quality of our structure and thus the accuracy of our learning to rank algorithms, we preferred to use shallow syntactic trees over full syntactic representations. In the next section, we first describe the structures we used in our kernels, then the tree kernels used as building blocks for our models. Finally, we show the reranking models for both tasks, TS and clue reranking. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled inno3 http://nlp.stanford.edu/software/ corenlp.shtml 4 http://cogcomp.cs.illinois.edu/page/ software_view/13 5 Based on a standard stoplist. 42 NN to the audience Figure 2: Shallow syntactic trees of clue (upper) and snippet (lower) and their relational links. 3.2 Database module (CWDB) NP Rank 1 2 3 4 5 Clue Kind of support for a computer"
W14-1605,P07-1098,1,0.811773,"t automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 3.1 WebSearch Module (WSM) WSM carries out four different tasks: (i) the retrieval of useful text snippets (TS) and web documents, (ii) the extraction of the answer candidates from such text, (iii) the scoring/filtering of the candidates, and (iv) the estimation of the list confidence. The retrieval of TS is performed by the Bing search engine by simply providing it the clue through its APIs. Then, the latter again are used to access the retrieved TS. The word list generator extracts possible candidate answers from"
