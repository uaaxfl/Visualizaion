2021.naacl-main.119,Video-aided Unsupervised Grammar Induction,2021,-1,-1,5,0,3600,songyang zhang,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction."
2021.findings-emnlp.6,Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data,2021,-1,-1,3,0.58137,3415,dian yu,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Despite considerable progress, most machine reading comprehension (MRC) tasks still lack sufficient training data to fully exploit powerful deep neural network models with millions of parameters, and it is laborious, expensive, and time-consuming to create large-scale, high-quality MRC data through crowdsourcing. This paper focuses on generating more training data for MRC tasks by leveraging existing question-answering (QA) data. We first collect a large-scale multi-subject multiple-choice QA dataset for Chinese, ExamQA. We next use incomplete, yet relevant snippets returned by a web search engine as the context for each QA instance to convert it into a weakly-labeled MRC instance. To better use the weakly-labeled data to improve a target MRC task, we evaluate and compare several methods and further propose a self-teaching paradigm. Experimental results show that, upon state-of-the-art MRC baselines, we can obtain +5.1{\%} in accuracy on a multiple-choice Chinese MRC dataset, C{\^{}}3, and +3.8{\%} in exact match on an extractive Chinese MRC dataset, CMRC 2018, demonstrating the usefulness of the generated QA-based weakly-labeled data for different types of MRC tasks as well as the effectiveness of self-teaching. ExamQA will be available at https://dataset.org/examqa/."
2021.emnlp-main.311,Exophoric Pronoun Resolution in Dialogues with Topic Regularization,2021,-1,-1,6,0,9343,xintong yu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns."
2021.emnlp-main.402,{RAST}: Domain-Robust Dialogue Rewriting as Sequence Tagging,2021,-1,-1,6,0,2964,jie hao,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model{'}s outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset."
2021.emnlp-main.457,Instance-adaptive training with noise-robust losses against noisy labels,2021,-1,-1,4,0,3602,lifeng jin,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are not robust to label noise induced in the annotation generation process. Several noise-robust losses have been proposed and evaluated on tasks in computer vision, but they generally use a single dataset-wise hyperparamter to control the strength of noise resistance. This work proposes novel instance-adaptive training frameworks to change single dataset-wise hyperparameters of noise resistance in such losses to be instance-wise. Such instance-wise noise resistance hyperparameters are predicted by special instance-level label quality predictors, which are trained along with the main classification models. Experiments on noisy and corrupted NLP datasets show that proposed instance-adaptive training frameworks help increase the noise-robustness provided by such losses, promoting the use of the frameworks and associated losses in NLP models trained with noisy data."
2021.emnlp-main.610,Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories,2021,-1,-1,6,0,9864,wenlin yao,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one predefined word sense inventory (e.g., WordNet). To address the data sparsity problem and generalize the model to be independent of one predefined inventory, we propose a gloss alignment algorithm that can align definition sentences (glosses) with the same meaning from different sense inventories to collect rich lexical knowledge. We then train a model to identify semantic equivalence between a target word in context and one of its glosses using these aligned inventories, which exhibits strong transfer capability to many WSD tasks. Experiments on benchmark datasets show that the proposed method improves predictions on both frequent and rare word senses, outperforming prior work by 1.2{\%} on the All-Words WSD Task and 4.3{\%} on the Low-Shot WSD Task. Evaluation on WiC Task also indicates that our method can better capture word meanings in context."
2021.ccl-1.49,å­éè¡é´çéå¾·:ä¸­æææ¬éå¾·å¥è¯å«ç ç©¶(Morality Between the Lines: Research on Identification of {C}hinese Moral Sentence),2021,-1,-1,4,0,11774,shiya peng,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}éçäººå·¥æºè½çåå±,è¶æ¥è¶å¤çç ç©¶å¼å§å
³æ³¨äººå·¥æºè½ä¼¦çãå¨NLPé¢å,éå¾·èªå¨è¯å«ä½ä¸ºç ç©¶åæææ¬ä¸­çéå¾·çä¸é¡¹éè¦ä»»å¡,è¿å¹´æ¥å¼å§åå°ç ç©¶è
çå
³æ³¨ãè¯¥ä»»å¡æ¨å¨è¯å«ææ¬ä¸­çéå¾·çæ®µ,å
¶å¯¹èªç¶è¯­è¨å¤ççéå¾·ç¸å
³çä¸æ¸¸ä»»å¡å¦åè§è¯å«æ¶é¤ãå¤å®æ¨¡åéå½¢æ­§è§ç­å
·æéè¦æä¹ãä¸è±æç¸æ¯,ç®åé¢åä¸­æçéå¾·è¯å«ç ç©¶å¼å±ç¼æ
¢,å
¶ä¸»è¦åå æ¯è³ä»è¿æ²¡æè¾å¤§åçéå¾·ä¸­ææ°æ®éä¸ºç ç©¶æä¾æ°æ®ãä¸ºè§£å³ä¸è¿°é®é¢,æ¬æå¨ä¸­æè¯­æä¸è¿è¡äºä¸­æéå¾·å¥çæ æ³¨å·¥ä½,å¹¶åæ­¥å¯¹è¯å«ä¸­æææ¬éå¾·å¥è¿è¡æ¢ç´¢ãæä»¬é¦å
æå»ºäºå½å
é¦ä¸ª10ä¸çº§å«çä¸­æéå¾·å¥æ°æ®é,ç¶åæ¬ææåºäºå©ç¨æµè¡çå ç§æºå¨å­¦ä¹ æ¹æ³æ¢ç©¶è¯å«ä¸­æéå¾·å¥ä»»å¡çææãæ­¤å¤,æä»¬è¿æ¢ç´¢äºå©ç¨é¢å¤ç¥è¯è¾
å©çæ¹æ³,å¯¹ä¸­æéå¾·å¥çè¯å«ä»»å¡è¿è¡äºè¿ä¸æ­¥çæ¢ç©¶ã{''}"
2021.acl-long.445,Importance-based Neuron Allocation for Multilingual Neural Machine Translation,2021,-1,-1,4,0.862069,1878,wanying xie,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method."
2021.acl-demo.1,{T}ex{S}mart: A System for Enhanced Natural Language Understanding,2021,-1,-1,14,0,3591,lemao liu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,0,"This paper introduces TexSmart, a text understanding system that supports fine-grained named entity recognition (NER) and enhanced semantic analysis functionalities. Compared to most previous publicly available text understanding systems and tools, TexSmart holds some unique features. First, the NER function of TexSmart supports over 1,000 entity types, while most other public tools typically support several to (at most) dozens of entity types. Second, TexSmart introduces new semantic analysis functions like semantic expansion and deep semantic representation, that are absent in most previous systems. Third, a spectrum of algorithms (from very fast algorithms to those that are relatively slow but more accurate) are implemented for one function in TexSmart, to fulfill the requirements of different academic and industrial applications. The adoption of unsupervised or weakly-supervised algorithms is especially emphasized, with the goal of easily updating our models to include fresh data with less human annotation efforts."
2020.tacl-1.10,Investigating Prior Knowledge for Challenging {C}hinese Machine Reading Comprehension,2020,12,0,3,1,3620,kai sun,Transactions of the Association for Computational Linguistics,0,"Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations. We present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5{\%}) and human readers (96.0{\%}), especiallyon problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C3 to present great challenges to existing systems as answering 86.8{\%} of questions requires both knowledge within and beyond the accompanying document, and we hope that C3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text. C3 is available at https://dataset.org/c3/."
2020.semeval-1.31,{SHIKEBLCU} at {S}em{E}val-2020 Task 2: An External Knowledge-enhanced Matrix for Multilingual and Cross-Lingual Lexical Entailment,2020,-1,-1,4,0,15018,shike wang,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Lexical entailment recognition plays an important role in tasks like Question Answering and Machine Translation. As important branches of lexical entailment, predicting multilingual and cross-lingual lexical entailment (LE) are two subtasks of SemEval2020 Task2. In previous monolingual LE studies, researchers leverage external linguistic constraints to transform word embeddings for LE relation. In our system, we expand the number of external constraints in multiple languages to obtain more specialised multilingual word embeddings. For the cross-lingual subtask, we apply a bilingual word embeddings mapping method in the model. The mapping method takes specialised embeddings as inputs and is able to retain the embeddings{'} LE features after operations. Our results for multilingual subtask are about 20{\%} and 10{\%} higher than the baseline in graded and binary prediction respectively."
2020.semeval-1.81,{BLCU}-{NLP} at {S}em{E}val-2020 Task 5: Data Augmentation for Efficient Counterfactual Detecting,2020,-1,-1,2,0.752766,8468,chang liu,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Counterfactuals describe events counter to facts and hence naturally involve common sense, knowledge, and reasoning. SemEval 2020 task 5 is focusing on this field. We participate in the subtask 1 and we use BERT as our system. Our Innovations are feature extraction and data augmentation. We extract and summarize features of counterfactual statements, augment counterfactual examples in training set with the help of these features, and two general methods of data augmentation is experimented in our work. We demonstrate the effectiveness of our approaches, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and our official submission achieves F1 0.802, which ranks us 16th in the competition."
2020.emnlp-main.76,Token-level Adaptive Training for Neural Machine Translation,2020,-1,-1,7,0.533333,4168,shuhao gu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation."
2020.emnlp-main.509,Better Highlighting: Creating Sub-Sentence Summary Highlights,2020,-1,-1,4,1,9735,sangwoo cho,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization."
2020.emnlp-main.537,{S}emantic {R}ole {L}abeling {G}uided {M}ulti-turn {D}ialogue {R}e{W}riter,2020,-1,-1,7,1,3603,kun xu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems."
2020.ccl-1.50,"é¢åäººå·¥æºè½ä¼¦çè®¡ç®çä¸­æéå¾·è¯å\
¸æå»ºæ¹æ³ç ç©¶(Construction of a {C}hinese Moral Dictionary for Artificial Intelligence Ethical Computing)",2020,-1,-1,3,0,22067,hongrui wang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"éå¾·è¯å
¸èµæºçå»ºè®¾æ¯äººå·¥æºè½ä¼¦çè®¡ç®çä¸ä¸ªç ç©¶éç¹ãç±äºéå¾·è¡ä¸ºå¤æå¤æ ·,ç°æçè±æéå¾·è¯å
¸åç±»ä½ç³»å¹¶ä¸å®å,èä¸­ææ¹é¢ç®åå°æªæç¸å
³çè¯å
¸èµæº,çè®ºä½ç³»åæå»ºæ¹æ³ä»å¾
æ¢ç©¶ãéå¯¹ä»¥ä¸é®é¢,è¯¥ææåºäºé¢åäººå·¥æºè½ä¼¦çè®¡ç®çä¸­æéå¾·è¯å
¸æå»ºä»»å¡,è®¾è®¡äºåç±»æ ç­¾ååç§ç±»å,å¾å°å
å«25,012ä¸ªè¯çä¸­æéå¾·è¯å
¸èµæºãå®éªç»æè¡¨æ,è¯¥è¯å
¸èµæºä¸ä»
è½å¤ä½¿æºå¨å­¦ä¼éå¾·ç¥è¯,å¤æ­è¯çéå¾·æ ç­¾åç±»å,èä¸è½å¤ä¸ºå¥å­çº§å«çéå¾·ææ¬åææä¾æ°æ®æ¯æã"
2020.ccl-1.68,ç»åæ·±åº¦å­¦ä¹ åè¯­è¨é¾åº¦ç¹å¾çå¥å­å¯è¯»æ§è®¡ç®æ¹æ³(The method of calculating sentence readability combined with deep learning and language difficulty characteristics),2020,-1,-1,2,0,22096,yuling tang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"æ¬ææåºäºå¯è¯»æ§è¯­æåºæå»ºçæ¹è¿æ¹æ³,åºäºè¯¥æ¹æ³,æå»ºäºè§æ¨¡æ´å¤§çæ±è¯­å¥å­å¯è¯»æ§è¯­æåºãè¯¥è¯­æåºå¨å¥å­ç»å¯¹é¾åº¦è¯ä¼°ä»»å¡ä¸çåç¡®çè¾¾å°0.7869,ç¸å¯¹åäººå·¥ä½æåäº0.15ä»¥ä¸,è¯æäºæ¹è¿æ¹æ³çæææ§ãå°æ·±åº¦å­¦ä¹ æ¹æ³åºç¨äºæ±è¯­å¯è¯»æ§è¯ä¼°,æ¢ç©¶äºä¸åæ·±åº¦å­¦ä¹ æ¹æ³èªå¨æè·é¾åº¦ç¹å¾çè½å,å¹¶è¿ä»æ­¥æ¢ç©¶äºåæ·±åº¦å­¦ä¹ ç¹å¾ä¸­èå
¥ä¸åå±é¢çè¯­é¾åº¦ç¹å¾å¯¹æ¨¡åæ´ä½æ§è½çå½±åãå®éªç»ææ¾ç¤º,ä¸åæ·±åº¦å­¦ä¹ æ¨¡åçé¾åº¦ç¹å¾æè·è½åä¸å°½ç¸å,è¯­è¨é¾åº¦ç¹å¾å¯ä»¥ä¸åç¨åº¦å°æé«æ·±åº¦å­¦ä¹ æ¨¡åçé¾åº¦è¡¨å¾è½åã"
2020.acl-main.101,Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints,2020,38,0,4,0,20080,zhenyi wang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin."
2020.acl-main.233,{MART}: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning,2020,34,0,4,0,3841,jie lei,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events."
2020.acl-main.444,Dialogue-Based Relation Extraction,2020,63,0,4,0.947113,3415,dian yu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/."
2020.acl-main.482,{ZPR}2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and {BERT},2020,-1,-1,5,0.481961,3601,linfeng song,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts."
2020.acl-main.603,Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension,2020,30,0,5,0,4925,hongyu gong,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. However, these transformer models can only take as input a fixed-length (e.g., 512) text. To deal with even longer text inputs, previous approaches usually chunk them into \textit{equally-spaced} segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering. Moreover, they are less capable of answering questions that need cross-segment information. We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also apply recurrent mechanisms to enable information to flow across segments. Experiments on three MRC tasks {--} CoQA, QuAC, and TriviaQA {--} demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions."
2020.acl-main.712,Structural Information Preserving for Graph-to-Text Generation,2020,-1,-1,7,0.481961,3601,linfeng song,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline."
S19-2191,{BLCU}{\\_}{NLP} at {S}em{E}val-2019 Task 7: An Inference Chain-based {GPT} Model for Rumour Evaluation,2019,0,0,4,0,25184,ruoyao yang,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Researchers have been paying increasing attention to rumour evaluation due to the rapid spread of unsubstantiated rumours on social media platforms, including SemEval 2019 task 7. However, labelled data for learning rumour veracity is scarce, and labels in rumour stance data are highly disproportionate, making it challenging for a model to perform supervised-learning adequately. We propose an inference chain-based system, which fully utilizes conversation structure-based knowledge in the limited data and expand the training data in minority categories to alleviate class imbalance. Our approach obtains 12.6{\%} improvement upon the baseline system for subtask A, ranks 1st among 21 systems in subtask A, and ranks 4th among 12 systems in subtask B."
S19-2198,{BLCU}{\\_}{NLP} at {S}em{E}val-2019 Task 8: A Contextual Knowledge-enhanced {GPT} Model for Fact Checking,2019,0,0,5,0.862069,1878,wanying xie,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Since the resources of Community Question Answering are abundant and information sharing becomes universal, it will be increasingly difficult to find factual information for questioners in massive messages. SemEval 2019 task 8 is focusing on these issues. We participate in the task and use Generative Pre-trained Transformer (OpenAI GPT) as our system. Our innovations are data extension, feature extraction, and input transformation. For contextual knowledge enhancement, we extend the training set of subtask A, use several features to improve the results of our system and adapt the input formats to be more suitable for this task. We demonstrate the effectiveness of our approaches, which achieves 81.95{\%} of subtask A and 61.08{\%} of subtask B in accuracy on the SemEval 2019 task 8."
Q19-1014,{DREAM}: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension,2019,4,29,4,1,3620,kai sun,Transactions of the Association for Computational Linguistics,0,"We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84{\%} of answers are non-extractive, 85{\%} of questions require reasoning beyond a single sentence, and 34{\%} of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/."
P19-1016,Reliability-aware Dynamic Feature Composition for Name Tagging,2019,0,5,4,0,4847,ying lin,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7{\%} absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5{\%} absolute F-score gain."
P19-1083,Knowledge-aware Pronoun Coreference Resolution,2019,33,0,4,0.320513,4907,hongming zhang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Resolving pronoun coreference requires knowledge support, especially for particular domains (e.g., medicine). In this paper, we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model. To ensure the generalization ability of our model, we directly incorporate knowledge in the format of triplets, which is the most common format of modern knowledge graphs, instead of encoding it with features or rules as that in conventional approaches. Moreover, since not all knowledge is helpful in certain contexts, to selectively use them, we propose a knowledge attention module, which learns to select and use informative knowledge based on contexts, to enhance our model. Experimental results on two datasets from different domains prove the validity and effectiveness of our model, where it outperforms state-of-the-art baselines by a large margin. Moreover, since our model learns to use external knowledge rather than only fitting the training data, it also demonstrates superior performance to baselines in the cross-domain setting."
P19-1304,Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network,2019,0,16,7,1,3603,kun xu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Previous cross-lingual knowledge graph (KG) alignment studies rely on entity embeddings derived only from monolingual KG structural information, which may fail at matching entities that have different facts in two KGs. In this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in KG. From this view, the KB-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. Experiments show that our model outperforms previous state-of-the-art methods by a large margin."
N19-1270,Improving Machine Reading Comprehension with General Reading Strategies,2019,0,30,3,1,3620,kai sun,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Reading strategies have been shown to improve comprehension levels, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a deep language model via pre-training. Inspired by reading strategies identified in cognitive science, and given limited computational resources - just a pre-trained model and a fixed number of training instances - we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8{\%} absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies. We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2{\%} in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of our fine-tuned models that incorporate these strategies. Core code is available at https://github.com/nlpdata/strategy/."
K19-1030,Improving Pre-Trained Multilingual Model with Vocabulary Expansion,2019,72,1,5,0,15981,hai wang,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings."
K19-1065,Evidence Sentence Extraction for Machine Reading Comprehension,2019,57,7,5,0,15981,hai wang,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Remarkable success has been achieved in the last few years on some limited machine reading comprehension (MRC) tasks. However, it is still difficult to interpret the predictions of existing MRC models. In this paper, we focus on extracting evidence sentences that can explain or support the answers of multiple-choice MRC tasks, where the majority of answer options cannot be directly extracted from reference documents. Due to the lack of ground truth evidence sentence labels in most cases, we apply distant supervision to generate imperfect labels and then use them to train an evidence sentence extractor. To denoise the noisy labels, we apply a recently proposed deep probabilistic logic learning framework to incorporate both sentence-level and cross-sentence linguistic indicators for indirect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiple-choice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC."
D19-6012,{BLCU}-{NLP} at {COIN}-Shared Task1: Stagewise Fine-tuning {BERT} for Commonsense Inference in Everyday Narrations,2019,0,0,2,1,11385,chunhua liu,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,0,"This paper describes our system for COIN Shared Task 1: Commonsense Inference in Everyday Narrations. To inject more external knowledge to better reason over the narrative passage, question and answer, the system adopts a stagewise fine-tuning method based on pre-trained BERT model. More specifically, the first stage is to fine-tune on addi- tional machine reading comprehension dataset to learn more commonsense knowledge. The second stage is to fine-tune on target-task (MCScript2.0) with MCScript (2018) dataset assisted. Experimental results show that our system achieves significant improvements over the baseline systems with 84.2{\%} accuracy on the official test dataset."
D19-5804,Improving Question Answering with External Knowledge,2019,55,13,7,0.876653,4904,xiaoman pan,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,0,"We focus on multiple-choice question answering (QA) tasks in subject areas such as science, where we require both broad background knowledge and the facts from the given subject-area reference corpus. In this work, we explore simple yet effective methods for exploiting two sources of external knowledge for subject-area QA. The first enriches the original subject-area reference corpus with relevant text snippets extracted from an open-domain resource (i.e., Wikipedia) that cover potentially ambiguous concepts in the question and answer options. As in other QA research, the second method simply increases the amount of training data by appending additional in-domain subject-area instances. Experiments on three challenging multiple-choice science QA tasks (i.e., ARC-Easy, ARC-Challenge, and OpenBookQA) demonstrate the effectiveness of our methods: in comparison to the previous state-of-the-art, we obtain absolute gains in accuracy of up to 8.1{\%}, 13.0{\%}, and 12.8{\%}, respectively. While we observe consistent gains when we introduce knowledge from Wikipedia, we find that employing additional QA training instances is not uniformly helpful: performance degrades when the added instances exhibit a higher level of difficulty than the original training data. As one of the first studies on exploiting unstructured external knowledge for subject-area QA, we hope our methods, observations, and discussion of the exposed limitations may shed light on further developments in the area."
D19-5605,Generating Diverse Story Continuations with Controllable Semantics,2019,48,0,3,0,14455,lifu tu,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"We propose a simple and effective modeling framework for controlled generation of multiple, diverse outputs. We focus on the setting of generating the next sentence of a story given its context. As controllable dimensions, we consider several sentence attributes, including sentiment, length, predicates, frames, and automatically-induced clusters. Our empirical results demonstrate: (1) our framework is accurate in terms of generating outputs that match the target control values; (2) our model yields increased maximum metric scores compared to standard n-best list generation via beam search; (3) controlling generation with semantic frames leads to a stronger combination of diversity and quality than other control variables as measured by automatic metrics. We also conduct a human evaluation to assess the utility of providing multiple suggestions for creative writing, demonstrating promising results for the potential of controllable, diverse generation in a collaborative writing system."
D19-5412,Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations,2019,27,0,3,1,9735,sangwoo cho,Proceedings of the 2nd Workshop on New Frontiers in Summarization,0,"Emerged as one of the best performing techniques for extractive summarization, determinantal point processes select a most probable set of summary sentences according to a probabilistic measure defined by respectively modeling sentence prominence and pairwise repulsion. Traditionally, both aspects are modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question. Whether, and to what extent, could contextualized sentence representations be used to improve the DPP framework? Our findings suggest that, despite the success of deep semantic representations, it remains necessary to combine them with surface indicators for effective identification of summary-worthy sentences."
D19-1528,Multiplex Word Embeddings for Selectional Preference Acquisition,2019,0,0,8,0.320513,4907,hongming zhang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Conventional word embeddings represent words with fixed vectors, which are usually trained based on co-occurrence patterns among words. In doing so, however, the power of such representations is limited, where the same word might be functionalized separately under different syntactic relations. To address this limitation, one solution is to incorporate relational dependencies of different words into their embeddings. Therefore, in this paper, we propose a multiplex word embedding model, which can be easily extended according to various relations among words. As a result, each word has a center embedding to represent its overall semantics, and several relational embeddings to represent its relational dependencies. Compared to existing models, our model can effectively distinguish words with respect to different relations without introducing unnecessary sparseness. Moreover, to accommodate various relations, we use a small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance."
Y18-1045,{DEMN}: Distilled-Exposition Enhanced Matching Network for Story Comprehension,2018,-1,-1,4,1,11385,chunhua liu,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
S18-1186,{BLCU}{\\_}{NLP} at {S}em{E}val-2018 Task 12: An Ensemble Model for Argument Reasoning Based on Hierarchical Attention,2018,0,0,5,0,28921,meiqian zhao,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"To comprehend an argument and fill the gap between claims and reasons, it is vital to find the implicit supporting warrants behind. In this paper, we propose a hierarchical attention model to identify the right warrant which explains why the reason stands for the claim. Our model focuses not only on the similar part between warrants and other information but also on the contradictory part between two opposing warrants. In addition, we use the ensemble method for different models. Our model achieves an accuracy of 61{\%}, ranking second in this task. Experimental results demonstrate that our model is effective to make correct choices."
D18-1038,{XL}-{NBT}: A Cross-lingual Neural Belief Tracking Framework,2018,26,0,5,0,4648,wenhu chen,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement human agents for customer service in call centers. With globalization, the need for providing cross-lingual customer support becomes more urgent than ever. However, cross-lingual support poses great challenges{---}it requires a large amount of additional annotated data from native speakers. In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework. Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form. Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data. We then distill and transfer its own knowledge to the student state tracker in target languages. We specifically discuss two types of common parallel resources: bilingual corpus and bilingual dictionary, and design different transfer learning strategies accordingly. Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results."
S17-1010,Semantic Frame Labeling with Target-based Neural Model,2017,0,0,2,1,7870,yukun feng,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"This paper explores the automatic learning of distributed representations of the target{'}s context for semantic frame labeling with target-based neural model. We constrain the whole sentence as the model{'}s input without feature extraction from the sentence. This is different from many previous works in which local feature extraction of the targets is widely used. This constraint makes the task harder, especially with long sentences, but also makes our model easily applicable to a range of resources and other similar tasks. We evaluate our model on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15. Finally, we extend the task to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work."
N16-2001,An End-to-end Approach to Learning Semantic Frames with Feedforward Neural Network,2016,12,0,3,1,7870,yukun feng,Proceedings of the {NAACL} Student Research Workshop,0,None
N16-1044,Recurrent Support Vector Machines For Slot Tagging In Spoken Language Understanding,2016,23,7,4,0,34677,yangyang shi,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
S15-2054,{BLCUNLP}: Corpus Pattern Analysis for Verbs Based on Dependency Chain,2015,8,3,3,1,7870,yukun feng,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We implemented a syntactic and semantic tagging system for SemEval 2015 Task 15: Corpus Pattern Analysis. For syntactic tagging, we present a Dependency Chain Search Algorithm that is found to be effective at identifying structurally distant subjects and objects. Other syntactic labels are identified using rules defined over dependency parse structures and the output of a verb classification module. Semantic tagging is performed using a simple lexical mapping table combined with postprocessing rules written over phrase structure constituent types and named entity information. The final score of our system is 0.530 F1, ranking second in this task."
W14-6819,An Introduction to {BLCU} Personal Attributes Extraction System,2014,5,1,1,1,3604,dong yu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"We describe our methods for share task of personal attributes extraction. We divide all 25 attributes into several categories and propose 4 kinds of pipelines to carry out value extraction. There are two stages in the process. The first stage uses CRF model or regular expression based extractor to produce initial answers. In the second stage, we propose two methods to filter out mistake answers: protagonist dependency relationship based filter and attribute keywords based filter."
2012.iwslt-keynotes.3,Who can understand your speech better {--} deep neural network of {G}aussian mixture model,2012,19,0,1,1,3604,dong yu,Proceedings of the 9th International Workshop on Spoken Language Translation: Keynotes,0,None
N07-4016,{V}oice-{R}ate: A Dialog System for Consumer Ratings,2007,11,11,4,0,14678,geoffrey zweig,Proceedings of Human Language Technologies: The Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics ({NAACL}-{HLT}),0,"Voice-Rate is an automated dialog system which provides access to over one million ratings of products and businesses. By calling a toll-free number, consumers can access ratings for products, national businesses such as airlines, and local businesses such as restaurants. Voice-Rate also has a facility for recording and analyzing ratings that are given over the phone. The service has been primed with ratings taken from a variety of web sources, and we are augmenting these with user ratings. Voice-Rate can be accessed by dialing 1-877-456-DATA."
2007.sigdial-1.18,Commute {UX}: Telephone Dialog System for Location-based Services,2007,8,2,4,0,49385,ivan tashev,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"In this paper, we describe a telephone dialog system for location-based services. In such systems, the effectiveness with which both the user can input location information to the system and the system delivers location information to the user is critical. We describe strategies for both of these issues in the context of a dialog system for real-time information about traffic, gas prices, and weather. The strategies employed by our system were evaluated through user studies and a system employing the best strategies was deployed. The system is evaluated through an analysis of 700 calls over a two month period."
