2020.acl-main.319,P19-1425,0,0.0599296,"Missing"
2020.acl-main.319,P18-1163,0,0.0402783,"Missing"
2020.acl-main.319,C18-1055,0,0.042899,"Missing"
2020.acl-main.319,D19-5506,0,0.0890598,"em, especially when applied to less decent real-world inputs compared to training data (Belinkov and Bisk, 2017). For example, typos may severely deteriorate system outputs (Table 1). Moreover, recent studies show that a neural machine translation system can also be broken by noisy synthetic inputs (Belinkov and Bisk, 2017; Lee et al., 2018). Due to the black-box nature of a neural system, it has been a challenge to fathom when and how the system tends to fail. Intuitively, researchers seek to apprehend such failures by the analysis of handcrafted error indicating features (Zhao et al., 2018; Karpukhin et al., 2019). This strategy is costly because it requires expert knowledge for both linguistics and the target neural architecture. Such features are also less applicable because some common errors in deep learning systems are hard to formulate, or very specific to certain architectures. Instead of designing error features, recent researchers adopt ideas from adversarial learning (Goodfellow et al., 2014) to generate adversarial examples for mining pitfalls of NLP systems (Cheng et al., 2018a; Ebrahimi et al., 2018; Zhao et al., 2017). Adversarial examples are minor perturbed inputs that keep the semantic"
2020.acl-main.319,D15-1166,0,0.162662,"Missing"
2020.acl-main.319,N19-1314,0,0.345133,"f of the batch and perturb its source using the current agent as negative samples. During D’s updates, we randomly generate a new batch of pairs from parallel data likewise to test its accuracy. D is updated at most stepD epochs, or until its test accuracy reaches acc bound. Env only1 yields -1 as overall terminal rewards when all sequences in SRC are intermediately terminated. For samples classified as negative during survival, their follow-up rewards and actions are masked as 0. If the agent survives until the end, Env yields additional averaged rd as final rewards for an episode. We follow Michel et al. (2019) to adopt relative degradation: rd = score(y, ref s) − score(y 0 , ref s) score(y, ref s) (10) where y and y 0 denote original and perturbed output, ref s are references, and score is a translation metric. If score(y, ref s) is zero, we return zero as rd . To calculate score we retokenize perturbed SRC by victim models vocabulary and tokenizer before translation. 1 It is commonly accepted that frequent negative rewards result in agents’ tendency to regard zero-reward as optimum and fail exploration, which further leads to training failure. 3489 3.2 Agent As it is shown in Figure 1 (c), the age"
2020.acl-main.319,W18-6319,0,0.0202891,"Missing"
2020.acl-main.319,P19-1020,0,0.0189011,", Ladv is determined by the goal of the attack. However, currently effective adversarial generation for NLP is to search by maximizing a surrogate gradientbased loss: argmax Ladv (x0 , x1 , ...x0i ...xn ) (7) 1≤i≤n,x0 ∈vocab where Ladv is a differentiable function indicating the adversarial object. Due to its formidable search space, this paradigm simply perturbs on a small ratio of token positions and greedy search by brute force among candidates. Note that adversarial example generation is fundamentally different from noised hidden representation in adversarial training (Cheng et al., 2019; Sano et al., 2019), which is not to be concerned in this work. 3 Approach In this section, we will describe our reinforced learning and generation of adversarial examples (Figure 1) in detail. Overall, the victim model is a part of the environment (denoted as Env), which yields rewards indicating overall degradation based on modified inputs. A reinforced agent 3488 learns to modify every source position from left to right sequentially. Meanwhile, a discriminator in Env provides every-step survival signals by determining whether SRC is ill-perturbed. 3.1 Environment We encapsulate the victim translation model wi"
2020.acl-main.319,D18-1397,0,0.0258153,"uto-regressive generation of each yi until the end of sequence symbol (EOS) is generated: P (yi |y&lt;i , X) = softmax(fdec (yi−1 , st , ct ; θdec )) (1) where ct is the attentive result for current decoder state st given H. 2.2 Actor-Critic for Reinforcement Learning Reinforcement learning (Sutton and Barto, 2018, RL) is a widely used machine learning technique following the paradigm of explore and exploit, which is apt for unsupervised policy learning in many challenging tasks (e.g., games (Mnih et al., 2015)). It is also used for direct optimization for non-differentiable learning objectives (Wu et al., 2018; Bahdanau et al., 2016) in NLP. Actor-critic (Konda and Tsitsiklis, 2000) is one of the most popular RL architectures where the agent consists of a separate policy and value networks called actor and critic. They both take in environment state st at each time step as input, while 3487 } TGT N V(st ) softmax softmax critic linear actor linear & victim NMT survival/terminal signals 2 at Y × { } ✓ 3 discriminator SRC 4 { Environment linear+dropout final degradation linear+dropout Mean Mean src bi-GRU tgt bi-GRU Mean Sum src bi-GRU Tokens embedding st agent actor rt 5 Xemb Yemb X Y 1 critic (a) O"
2020.acl-main.319,D18-1036,0,0.0199899,"al translation system, especially when applied to less decent real-world inputs compared to training data (Belinkov and Bisk, 2017). For example, typos may severely deteriorate system outputs (Table 1). Moreover, recent studies show that a neural machine translation system can also be broken by noisy synthetic inputs (Belinkov and Bisk, 2017; Lee et al., 2018). Due to the black-box nature of a neural system, it has been a challenge to fathom when and how the system tends to fail. Intuitively, researchers seek to apprehend such failures by the analysis of handcrafted error indicating features (Zhao et al., 2018; Karpukhin et al., 2019). This strategy is costly because it requires expert knowledge for both linguistics and the target neural architecture. Such features are also less applicable because some common errors in deep learning systems are hard to formulate, or very specific to certain architectures. Instead of designing error features, recent researchers adopt ideas from adversarial learning (Goodfellow et al., 2014) to generate adversarial examples for mining pitfalls of NLP systems (Cheng et al., 2018a; Ebrahimi et al., 2018; Zhao et al., 2017). Adversarial examples are minor perturbed inpu"
2020.acl-main.5,D18-1547,0,0.0447236,"Missing"
2020.acl-main.5,W19-5932,0,0.0508391,"asets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue states consist of constraints and"
2020.acl-main.5,D14-1162,0,0.0820616,"Missing"
2020.acl-main.5,D17-1206,0,0.0765432,"Missing"
2020.acl-main.5,D19-1196,0,0.116686,"Missing"
2020.acl-main.5,W14-4340,0,0.5986,"fits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, di"
2020.acl-main.5,D18-1299,0,0.0248015,"Missing"
2020.acl-main.5,W14-4343,0,0.0757215,"Missing"
2020.acl-main.5,P19-1078,0,0.606512,"MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue states consist o"
2020.acl-main.5,P18-1133,0,0.0410285,"2 Word Copying i:wi =w In this section, we will describe DST-SC model in detail. DST-SC is an open vocabulary model based on the encoder-decoder architecture. As shown in Figure 1, there are three components that contribute to obtain the target slot value: (1) word generation from the vocabulary; (2) word copying from the dialogue history; (3) value copying from the source slot. To reduce the burden on the decoder, DST-SC also equips with a slot gate (Wu et al., 2019) to predict for slot values of none and dontcare. hi = GRU(φemb (wi )). (4) The copy mechanism is shown to be effective in DST (Lei et al., 2018; Xu and Hu, 2018; Wu et al., 2019). Here, we follow Wu et al. (2019) to augment the vanilla attention-based decoder with pointergenerator copying, enabling it to capture slot values that explicitly occur in the dialogue history. X Pwc (yt = w) = ati . (5) • We demonstrate that DST-SC is more effective for handling the related-slot problem and outperforms state-of-the-art baselines. 2.1 (3) i=1 • To the best of our knowledge, this work is the first one to discuss the related-slot problem in multi-domain DST and address it by explicitly modeling slot connections across domains. 2 (2) (1) Word G"
2020.acl-main.5,P18-1134,0,0.405076,"MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue"
2020.acl-main.5,P15-2130,0,0.0603261,"Missing"
2020.acl-main.5,P18-1135,0,0.17312,"ur model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to es"
2020.acl-main.5,P17-1163,0,0.0762936,"Missing"
2020.acl-main.65,W14-3348,0,0.231421,"ce on the validation set and are used across all the experiments. https://wordnet.princeton.edu/ https://en.oxforddictionaries.com/ 712 3. Pip-sem is our intuitive pipeline that consists of a sememe predictor and a definition generator. The sememe predictor is trained on HowNet and is responsible for annotating words in definition generation datasets. The definition generator is used to generate definitions given the word, context, and pseudo annotations of sememes. Metrics We adopt two several automatic metrics that are often used in generation tasks: BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). BLEU considers the exact match between generation results and references and is the most common metric used to evaluate generation systems. Following previous work, we compute Model I-Attention (Gadetsky et al., 2018) LOG-CaD (Ishiwatari et al., 2019) *LOG-CaD † Pip-sem ESD-def † ESD-sem WordNet BLEU METEOR 23.77 / 24.79 / 24.70 8.66 25.52 11.33 25.75 11.52 26.48 12.45 Oxford BLEU METEOR 17.25 / 18.53 / 18.24 8.43 19.89 11.10 19.98 10.79 20.86 11.86 Table 2: BLEU and Meteor scores on WordNet and Oxford dataset. ‘† ’ indicates models that incorporate external sememe annotations while training"
2020.acl-main.65,P18-2043,0,0.275237,"s an important role in natural language understanding for human. It is a common practice for human to consult a dictionary when encountering unfamiliar words (Fraser, 1999). However, it is often the case that we cannot find satisfying definitions for words that are rarely used or newly created. To assist dictionary compilation and help human readers understand unfamiliar texts, generating definitions automatically is of practical significance. Noraset et al. (2017) first propose definition modeling, which is the task of generating the dictionary definition for a given word with its embedding. Gadetsky et al. (2018) extend the work by incorporating word sense disambiguation to generate context-aware word definitions.Both methods adopt a variant of encoder-decoder architecture, ∗ † Equal contribution Corresponding author captain the person in charge of a ship the person who is a member of a ship where the word to be defined is mapped to a lowdimension semantic vector by an encoder, and the decoder is responsible for generating the definition given the semantic vector. Although the existing encoder-decoder architecture (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019) yields reasonable"
2020.acl-main.65,N19-1350,0,0.205989,"tly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition generation, which explicitly decomposes meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that ESD achieves substantial improvements on WordNet and Oxford benchmarks over strong previous baselines. 1 Table 1: An example of the definitions of word “captain”. Reference is from Oxford dictionary and Generated is from the method of Ishiwatari et al. (2019). Introduction Dictionary definition, which provides explanatory sentences for word senses, plays an important role in natural language understanding for human. It is a common practice for human to consult a dictionary when encountering unfamiliar words (Fraser, 1999). However, it is often the case that we cannot find satisfying definitions for words that are rarely used or newly created. To assist dictionary compilation and help human readers understand unfamiliar texts, generating definitions automatically is of practical significance. Noraset et al. (2017) first propose definition modeling,"
2020.acl-main.65,P19-1430,0,0.0448,"Missing"
2020.acl-main.65,Q17-1010,0,0.0410491,"mantic components z, the word representation r∗ and the context representation H. Definition Encoder &&quot; &# &$ ℎ( 2&quot; 2# … 23 +&quot; +# … +1 /&quot; /# … /1 3.2.1 ⊕ )4 &lt;s&gt; )&quot; )# &&quot; Same as Ishiwatari et al. (2019), our encoder consists of two parts, namely word encoder and context encoder. )$ &# Definition Decoder Word Encoder The word encoder is responsible for mapping the word w∗ to a low-dimensional vector r∗ , and consists of a word embedding and a character level encoder. The word embedding is initialized by large-scale pretrained word embeddings such as GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017), and is kept fixed at the training time. Previous works (Noraset et al., 2017; Ishiwatari et al., 2019) also show that morphological information can be helpful for definition generation. We employ a convolutional neural network (Krizhevsky et al., 2012) to encode the character sequence of the word. We concatenate the word embedding and the character encoding to get the word representation r∗ . Figure 1: Neural architecture of ESD, including the word encoder, context encoder, the decoder and the definition encoder for the posterior networks. However, it is generally computationally intractable"
2020.acl-main.65,K16-1002,0,0.0421507,"architecture for a brief understanding. Following the common practice of context-aware definition models (Gadetsky et al., 2018; Ishiwatari et al., 2019), we first encode the source word w∗ 3.2.2 Semantic Components Predictor For the proposed ESD, we need to model both the semantic components posterior qφ (z|w∗ , C, D) and the prior pθ (z|w∗ , C). Semantic Components Posterior Approximator Exactly modeling the true posterior qφ (z|w∗ , C, D) is usually intractable. Therefore, we adopt an approximation method to simplify the posterior inference (Zhang et al., 2016) Following the spirit of VAE (Bowman et al., 2016), we use neural networks for better approximation in this paper. Specifically, we first compute the representation 0 HD =h1:T of the definition D = d1:T with a bidirectional LSTM network. We then obtain the representation of definition D and context C with 710 max-pooling operation. 0 hD = max-pooling(h1:T ) (5) hC = max-pooling(h1:|C |) (6) Finally, we adopt a GRU-like (Cho et al., 2014) gate mechanism to allow the decoder to dynamically fuse information from the word representation r∗ , context vector ct , and semantic context vector ot , which can be calculated as follows: With these repres"
2020.acl-main.65,I17-2070,0,0.188941,"ent codes. 5.3 Case Studies Examples of learned latent codes In Table 6, we show some examples of learned latent codes on WordNet dataset. We can see that our model does learn informative codes, i.e. words with similar meanings are assigned with similar latent codes, and codes of words with different meanings tend to differ. Related Work Definition Generation Definition modeling was firstly proposed by Noraset et al. (2017). They take a word embedding as input and generate a definition of the word. An obvious drawback is that their model cannot handle polysemous words. Recently several works (Ni and Wang, 2017; Gadetsky et al., 2018; Ishiwatari et al., 2019) consider the context-aware definition generation task, where the context is introduced to disambiguate senses of words. They all adopt a encoder-decoder architecture, and rely heavily on the decoder to extract semantic components of the word semantic, thus leading to under-specific definitions. In contrast, we introduce a group of discrete latent variables to model these semantic components explicitly. Examples of generated definitions We also list several generation samples in Table 5. We can see that the definitions generated by our method ar"
2020.acl-main.65,P17-1187,0,0.0537413,"Missing"
2020.acl-main.65,P02-1040,0,0.107702,"s are chosen based on the performance on the validation set and are used across all the experiments. https://wordnet.princeton.edu/ https://en.oxforddictionaries.com/ 712 3. Pip-sem is our intuitive pipeline that consists of a sememe predictor and a definition generator. The sememe predictor is trained on HowNet and is responsible for annotating words in definition generation datasets. The definition generator is used to generate definitions given the word, context, and pseudo annotations of sememes. Metrics We adopt two several automatic metrics that are often used in generation tasks: BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). BLEU considers the exact match between generation results and references and is the most common metric used to evaluate generation systems. Following previous work, we compute Model I-Attention (Gadetsky et al., 2018) LOG-CaD (Ishiwatari et al., 2019) *LOG-CaD † Pip-sem ESD-def † ESD-sem WordNet BLEU METEOR 23.77 / 24.79 / 24.70 8.66 25.52 11.33 25.75 11.52 26.48 12.45 Oxford BLEU METEOR 17.25 / 18.53 / 18.24 8.43 19.89 11.10 19.98 10.79 20.86 11.86 Table 2: BLEU and Meteor scores on WordNet and Oxford dataset. ‘† ’ indicates models that incorporate ext"
2020.acl-main.65,D14-1162,0,0.0870726,"ates the target definition from the semantic components z, the word representation r∗ and the context representation H. Definition Encoder &&quot; &# &$ ℎ( 2&quot; 2# … 23 +&quot; +# … +1 /&quot; /# … /1 3.2.1 ⊕ )4 &lt;s&gt; )&quot; )# &&quot; Same as Ishiwatari et al. (2019), our encoder consists of two parts, namely word encoder and context encoder. )$ &# Definition Decoder Word Encoder The word encoder is responsible for mapping the word w∗ to a low-dimensional vector r∗ , and consists of a word embedding and a character level encoder. The word embedding is initialized by large-scale pretrained word embeddings such as GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017), and is kept fixed at the training time. Previous works (Noraset et al., 2017; Ishiwatari et al., 2019) also show that morphological information can be helpful for definition generation. We employ a convolutional neural network (Krizhevsky et al., 2012) to encode the character sequence of the word. We concatenate the word embedding and the character encoding to get the word representation r∗ . Figure 1: Neural architecture of ESD, including the word encoder, context encoder, the decoder and the definition encoder for the posterior networks. However, it is"
2020.acl-main.65,D19-1357,0,0.298103,"h its embedding. Gadetsky et al. (2018) extend the work by incorporating word sense disambiguation to generate context-aware word definitions.Both methods adopt a variant of encoder-decoder architecture, ∗ † Equal contribution Corresponding author captain the person in charge of a ship the person who is a member of a ship where the word to be defined is mapped to a lowdimension semantic vector by an encoder, and the decoder is responsible for generating the definition given the semantic vector. Although the existing encoder-decoder architecture (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019) yields reasonable generation results, it relies heavily on the decoder to extract thorough semantic components of the word, leading to under-specific definition generation results, i.e. missing some semantic components. As illustrated in Table 1, to generate a precise definition of the word “captain”, one needs to know that “captain” refers to a person, “captain” is related to ship, and “captain” manages or is in charge of the ship, where person, ship, manage are three semantic components of word “captain”. However, due to the lack of explicitly modeling of these semantic components, the mode"
2020.acl-main.65,D17-1013,1,0.830622,"finition. We first propose to leverage sememe annotations of HowNet (Dong and Dong, 2003) as an external signal to guide the learning of latent variables. As we mentioned in Section 2.3, sememes are also known to be helpful for definition generation (Yang et al., 2019). Previously, Xie et al. (2017) show that it is possible to predict sememes of words from large scale pretrained distributional representations. Suppose the set of sememes in HowNet are denoted by S = {s1 , s2 , · · · , sn }, and each word w in HowNet is annotated by a small subset of S, denoted by Sw = {si |si ∈ S}. Inspired by Weng et al. (2017), we adopt a bag-of-word loss to ensure that z is informative enough to be predictive about sememe annotations Sw : X (sem) Lcom = −log p(si |z) (8) i si ∈Sw 711 Our next motivation is that the sememes annotation is still expensive, while definitions of words are off-the-shelf when training. Inspired by Bao et al. (2019) and John et al. (2019), we enforce the model to predict every words in the target definition D=d1:T to ensure that z is informative enough: (def) Lcom = −log T X p(di |z) (9) i=1 Semantic Diversity Objective To achieve the goal of decomposing semantics, it is crucial that ther"
2020.acl-main.65,D16-1050,0,0.0206668,"detailing each component of ESD, we overview the architecture for a brief understanding. Following the common practice of context-aware definition models (Gadetsky et al., 2018; Ishiwatari et al., 2019), we first encode the source word w∗ 3.2.2 Semantic Components Predictor For the proposed ESD, we need to model both the semantic components posterior qφ (z|w∗ , C, D) and the prior pθ (z|w∗ , C). Semantic Components Posterior Approximator Exactly modeling the true posterior qφ (z|w∗ , C, D) is usually intractable. Therefore, we adopt an approximation method to simplify the posterior inference (Zhang et al., 2016) Following the spirit of VAE (Bowman et al., 2016), we use neural networks for better approximation in this paper. Specifically, we first compute the representation 0 HD =h1:T of the definition D = d1:T with a bidirectional LSTM network. We then obtain the representation of definition D and context C with 710 max-pooling operation. 0 hD = max-pooling(h1:T ) (5) hC = max-pooling(h1:|C |) (6) Finally, we adopt a GRU-like (Cho et al., 2014) gate mechanism to allow the decoder to dynamically fuse information from the word representation r∗ , context vector ct , and semantic context vector ot , whi"
2020.acl-srw.7,D18-1176,0,0.0193537,"en the null hypothesis H0 would be: E1 and E2 are independent. Under H0 , RPD(E1 , E2 ) asymptotically follows N (µ, σ 2 ). Then the test statistic z is calculated as follows. RPD(E1 , E2 ) − µ z= σ Experiment The following experiments serve to apply RPD to explore some questions of interest and further demonstrate that RPD is suitable for investigating the relations between embedding spaces. We leave applying RPD to help improve specific NLP tasks to future research. For example, RPD could be used for combining different embeddings together, which could help us produce better metaembeddings (Kiela et al., 2018). 4.1 Methods In our case, we estimate µ = 0.953 and σ = 0.001 with Monte Carlo simulation with randomly initialized embeddings. Take RPD(ESGNS1 ,ESVDPMI ) = 0.511 from Table 1 as an example, the statistic z = 442, which means the p-value  0.01. Thus, we can confidently reject H0 . Notice that we can test any two sets of word embeddings with this method. It is not hard to see that no pair of word embeddings in Table 1 are independent, which suggests that there exists an unified theory behind these methods. Setup If not explicitly stated, the experiments are performed on Text8 corpus (Mahoney,"
2020.acl-srw.7,D14-1162,0,0.112302,"between different embedding spaces systematically, we propose RPD as a measure of the distance between different sets of embeddings. We derive statistical properties of RPD including its asymptotic upper bound and normality under the independence condition. We also provide a geometric interpretation of RPD. Furthermore, we show that RPD is strongly correlated with the performance of word embeddings measured by intrinsic metrics, such as comparing semantic similarity and evaluating analogies. With the help of RPD, we study the relations among several popular embedding methods, including GloVe (Pennington et al., 2014), SGNS1 (Mikolov et al., 2013), Singular Value Decomposition (SVD) factorization of PMI matrix, and SVD factorization of log count (LC) matrix. Results show that these methods are statistically correlated, which suggests that there is an unified theory behind these methods. Additionally, we analyze the influence of training processes, i.e. hyperparameters (negative sampling), random initialization; and the influence of corpora towards word embeddings. Our findings include the fact that different training corpora result in significantly different GloVe embeddings, and that the main difference b"
2020.acl-srw.7,P14-2050,0,0.627212,"embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically, and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding spaces. 1 Introduction Word embeddings are important in Natural language processing (NLP) which map words into a low-dimensional vector space. Many works have been proposed to generate word embeddings (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014a; Bojanowski et al., 2017; Devlin et al., 2019). With many different sets of word embeddings produced by different algorithms and corpora, it is interesting to investigate the relationships between these sets of word embeddings. Intrinsically, this would help us better understand word embeddings (Levy et al., 2015). Practically, knowing the relationship between different sets of word embeddings helps us build better word meta-embeddings (Yin and Sch¨utze, 2016), reduce biases in word embeddings (Bolukbasi et al., 2016), pick better hyperparameters (Yin and Shen, 2018), and choose suitable alg"
2020.acl-srw.7,N18-1202,0,0.028329,"use the Frobenius norm as the norm of matrices. Definition 1. (RPD) The RPD between embedding matrices E1 and E2 is defined as follows: 1 2 embedding is E = U:,1:d D1:d,1:d , where d is the dimension of word embeddings. We denote the method as SVDPMI , if the signal is the PMI matrix, and SVDLC if the signal is the log count matrix. Although the scope of this paper focuses on standard word embeddings that were learned at the word level, RPD could be adapted to analyze embeddings that were learned from word pieces, for example, fastText (Bojanowski et al., 2017) and contextualized embeddings (Peters et al., 2018; Devlin et al., 2019). 2.2 Quantifying Distances between Embeddings RPD(E1 , E2 ) = T T 1 kE˜1 E˜1 − E˜2 E˜2 k2 . 2 kE˜1 E˜1 T kkE˜2 E˜2 T k ˜ comes from dividing each entry of E where E by its standard deviation. For convenience, we let ˜ ≡ E for the following discussion. E The numerator of RPD respects the unitaryinvariant property of word embeddings, which means that unitary transformation (i.e. rotation) preserves the relative geometry of an embedding space. The denominator is a normalization, which allows us to regard the whole embedding matrix as an integrated part (i.e. RPD does not co"
2020.acl-srw.7,P18-2124,0,0.0132546,"information. Transactions of the Association for Computational Linguistics, 5:135–146. RPD and Contextualized Word Embeddings Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4349–4357. Curran Associates, Inc. Contextualized embeddings are popular NLP techniques which significantly improve a wide range of NLP tasks (Bowman et al., 2015; Rajpurkar et al., 2018). To understand why contextualized embeddings are beneficial to those NLP tasks, many works investigate the the nature of syntactic (Liu et al., 2019), semantic (Liu et al., 2019), and commonsense knowledge (Zhou et al., 2019) contained in such representations. However, we still know little about the vector space of contextualized embeddings and their relaSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,"
2020.acl-srw.7,Q15-1016,0,0.213462,"spaces. 1 Introduction Word embeddings are important in Natural language processing (NLP) which map words into a low-dimensional vector space. Many works have been proposed to generate word embeddings (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014a; Bojanowski et al., 2017; Devlin et al., 2019). With many different sets of word embeddings produced by different algorithms and corpora, it is interesting to investigate the relationships between these sets of word embeddings. Intrinsically, this would help us better understand word embeddings (Levy et al., 2015). Practically, knowing the relationship between different sets of word embeddings helps us build better word meta-embeddings (Yin and Sch¨utze, 2016), reduce biases in word embeddings (Bolukbasi et al., 2016), pick better hyperparameters (Yin and Shen, 2018), and choose suitable algorithms in different scenarios (Kozlowski et al., 2019). 2 Background Before introducing RPD, we review the theory behind some static word embedding methods, and 1 Skip-gram with Negative Sampling 42 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, p"
2020.acl-srw.7,P16-2083,0,0.0735078,"Missing"
2020.acl-srw.7,N19-1112,0,0.0178562,", James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4349–4357. Curran Associates, Inc. Contextualized embeddings are popular NLP techniques which significantly improve a wide range of NLP tasks (Bowman et al., 2015; Rajpurkar et al., 2018). To understand why contextualized embeddings are beneficial to those NLP tasks, many works investigate the the nature of syntactic (Liu et al., 2019), semantic (Liu et al., 2019), and commonsense knowledge (Zhou et al., 2019) contained in such representations. However, we still know little about the vector space of contextualized embeddings and their relaSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 47 David Mimno and Laure Thompson. 2017. The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empi"
2020.acl-srw.7,N18-1048,0,0.0408469,"Missing"
2020.acl-srw.7,P16-1128,0,0.0356952,"Missing"
2020.acl-srw.7,D15-1075,0,\N,Missing
2020.acl-srw.7,Q17-1010,0,\N,Missing
2020.acl-srw.7,Q16-1028,0,\N,Missing
2020.acl-srw.7,D17-1308,0,\N,Missing
2020.acl-srw.7,Q18-1008,0,\N,Missing
2020.acl-srw.7,N19-1423,0,\N,Missing
2020.coling-main.526,D16-1250,0,0.053542,"Missing"
2020.coling-main.526,P17-1042,0,0.0446501,"dings. Early work (Mikolov et al., 2013) relies on a seed dictionary to learn the source-target word embedding mapping. Xing et al. (2015) enforce the word embeddings to be of unit length and the orthogonal constraint on the linear mapping. Faruqui and Dyer (2014) on the other hand use Canonical Correlation Analysis (CCA) to project both source and target embeddings to a common low-dimensional space. Artetxe et al. (2016) show that the above methods are variants of the same objective. Smith et al. (2017) further show that this objective is closely related to the orthogonal Procrustes problem. Artetxe et al. (2017) obtain competitive results using the self-learning with a seed dictionary of only 25 word pairs. Adversarial methods. Zhang et al. (2017a) attempt the unsupervised bilingual dictionary induction task using the adversarial network. They use a generator to transform the source word embeddings to the target word embeddings and a discriminator to classify whether the given embedding is sampled from the true target word embeddings or generated by the generator. The generator is trained to fool the discriminator and the discriminator is trained to identify the generated word embeddings. In the end,"
2020.coling-main.526,P18-1073,0,0.416762,"E is the raw embeddings, n is the initial dimension 2: D←∅ . Set the dictionary to empty 3: while n ≤ 300 do . 300 is the dimension of the raw embeddings ¯ with dimension min(n, 300) using PCA and dropmax 4: Reduce E to E 5: if D = ∅ then ¯ 6: Run the initialization and the self-learning on E 7: else ¯ with D as the initial dictionary 8: Run the self-learning on E 9: end if 10: Translate 4K most frequent words and store the results in D 11: n←n×2 12: end while 13: return WX and WY 14: end procedure 5993 System En-Es Es-En En-Fr Fr-En En-It It-En En-De De-En MUSE (Lample et al., 2018) VecMap (Artetxe et al., 2018) C-MUSE (Hartmann et al., 2019) POSTPROC (Vulic et al., 2020) 83.20 82.33 82.33 82.73 83.66 84.60 84.73 85.47 82.66 82.47 82.40 82.73 82.39 83.60 83.73 84.00 78.20 79.13 79.13 79.13 77.90 79.80 79.67 80.60 75.10 75.33 75.27 76.00 72.93 74.27 74.20 75.33 Proposed method (dim 50) Proposed method (dim 100) Proposed method (dim 200) Proposed method (dim 300) 40.33 63.47 80.33 82.40 37.40 61.80 80.27 84.60 53.53 72.73 80.40 82.60 48.07 71.13 79.67 83.67 48.13 68.40 76.47 78.93 45.00 66.73 76.67 79.67 31.60 63.67 71.27 75.33 30.33 61.27 71.20 74.33 Table 3: The accuracy of different UBDI systems on"
2020.coling-main.526,P19-1494,0,0.437928,"to project the word embeddings to a lower-dimensional space. Then they apply a variant of the Iterative Closest Point method to find the source and target word embeddings mapping. Zhou et al. (2019) use normalizing flows to match the distribution of source and target word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the"
2020.coling-main.526,Q17-1010,0,0.0639702,"hod 37.33 35.27 48.87 33.08 47.6 55.53 21.60 13.64 - IDR - Dropmax 40.20 0 41.07 0 47.64 0 34.18 0 0.13 0.33 0.20 0.27 0.07 20.73 0 14.25 Table 5: Ablation study of the proposed method on distant language pairs. reproduce the C-MUSE and POSTPROC system using Python. All these systems are run with the default hyper-parameters settings. Our method is based on the open-sourced VecMap implementation. We evaluate the baseline and our method on 4 similar language pairs, En-{Es, Fr, It, De}, and 4 distant language pairs, En-{Zh, Ja, Vi, Th}. We use the pretrained 300-dimensional fastText embeddings (Bojanowski et al., 2017)3 . The evaluation dictionaries are from Lample et al. (2018). We trim all vocabularies to the 20K most frequent words for training. Specifically, VecMap retains the top-4K words for the initialization, while others use the whole vocabulary. All experiments are done on a single Nvidia GTX 1080Ti. We run each experiment 3 times but with different random seeds, then pick the one with the highest cosine similarity of induced nearest neighbors as the final result. This unsupervised model selection criterion has shown to correlate well with UBDI performance (Hartmann et al., 2019). 5.2 Results Tabl"
2020.coling-main.526,E14-1049,0,0.110886,"Missing"
2020.coling-main.526,P19-1070,0,0.114822,"Missing"
2020.coling-main.526,D18-1043,0,0.387086,"tween the k largest Pkgraphs as the sum 2 eigenvalues λ of L1 and L2 , 4 = i=1 (λ1i − λ2i ) . The higher 4 is, the less similar the graphs are. As shown in Figure 4, the eigenvector similarity drops significantly when the dimension is reduced. This implies that the underlying nearest neighbor graphs of two languages become similar in low-dimensional space. This helps the algorithm to succeed in low-dimensional space as the assumption it makes is held. This phenomenon might be the result that many language pairs share some principle axes of variation, especially the ones with high eigenvalues (Hoshen and Wolf, 2018). 6.3 Hubness Cross-lingual word embeddings are known to suffer from the hubness problem (Lample et al., 2018), where a few points (known as hubs) are the nearest neighbors of many other points in high-dimensional spaces. As suggested in Section 4, distant language pairs might suffer more from this problem and the dropmax trick helps to alleviate this problem. Thus we would like to know to what extent the dropmax trick helps in the hubness problem. Here we adopt the hubness metric proposed by Ormazabal et al. (2019) for evaluation. This metric measures the percentage of target words H that are"
2020.coling-main.526,D19-1328,0,0.436928,"Missing"
2020.coling-main.526,N19-1386,0,0.0551632,"t al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly improves the robustness as well as the performance of self-learning. Hoshen and Wolf (2018) assume that many language pairs share some principle axes of variation. Therefore they first use PCA t"
2020.coling-main.526,2020.acl-main.318,0,0.0343256,"Missing"
2020.coling-main.526,P18-1072,0,0.0848626,"Missing"
2020.coling-main.526,D19-1449,0,0.041175,"Missing"
2020.coling-main.526,2020.repl4nlp-1.7,0,0.0309337,"word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the self-learning to succeed, which is responsible for its failure. We propose Iterative Dimension Reduction to bridge this gap. Our method obtains substantial gains in distant language pairs without scarifying the performance of similar language pairs. It has shown to"
2020.coling-main.526,N15-1104,0,0.0667128,"Missing"
2020.coling-main.526,D18-1268,0,0.134918,"ator and the discriminator is trained to identify the generated word embeddings. In the end, the generator will be used to induce the bilingual dictionary. Their following work (Zhang et al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly"
2020.coling-main.526,P17-1179,0,0.161033,"Missing"
2020.coling-main.526,D17-1207,0,0.0475176,"Missing"
2020.coling-main.526,N19-1161,0,0.645134,"Missing"
2020.wmt-1.115,W17-4761,0,0.014093,"lation is good enough for postediting and indicating what edits are needed. This paper describes our system of the Shared Task on Word and Sentence-Level (QE Tasks 2) at WMT20. With the post-edited translations, all the quality scores can be computed automatically by TERCOM (Snover et al., 2006). Traditional QE models (Kozlova et al., 2016) use some time-consuming and expensive hand-craft features to represent the translation pairs. With the great success of deep neural networks in natural language processing (NLP), some researches have begun to apply automatic neural features to do QE tasks (Chen et al., 2017; Shah et al., 2016). However, the rare QE data can’t fully release the power of deep neural networks. To address this problem, researchers try to transfer bilingual knowledge ∗ Corresponding Author. However, existing predictor-estimator frameworks cannot fully use the information from parallel data because of the discrepancy of data quality between the predictor and the estimator. The predictor is trained on parallel data, which are nearly no errors in translations. While the translations in QE data is generated by a real machine translation system and may have some errors. When the estimator"
2020.wmt-1.115,P19-3020,0,0.0209547,"Missing"
2020.wmt-1.115,W17-4763,0,0.0256222,"Missing"
2020.wmt-1.115,W16-2385,0,0.0194272,"tence-level on both ENZH and EN-DE language pairs. 1 Introduction Quality Estimation (QE) is a task to predict the quality of translations without relying on any references. QE plays a critical role in machine translation to reduce human efforts, such as deciding whether a translation is good enough for postediting and indicating what edits are needed. This paper describes our system of the Shared Task on Word and Sentence-Level (QE Tasks 2) at WMT20. With the post-edited translations, all the quality scores can be computed automatically by TERCOM (Snover et al., 2006). Traditional QE models (Kozlova et al., 2016) use some time-consuming and expensive hand-craft features to represent the translation pairs. With the great success of deep neural networks in natural language processing (NLP), some researches have begun to apply automatic neural features to do QE tasks (Chen et al., 2017; Shah et al., 2016). However, the rare QE data can’t fully release the power of deep neural networks. To address this problem, researchers try to transfer bilingual knowledge ∗ Corresponding Author. However, existing predictor-estimator frameworks cannot fully use the information from parallel data because of the discrepan"
2020.wmt-1.115,W15-3037,0,0.0192437,"encoders of these models in the figure. (a) shows the QE Brain mask system, and it simply enhances the original QE Brain system by simply masking tokens at the target side. (b) uses a masked language model at the target side to obtain deep bidirectional information. 2 Methods As we all know, using different sub-models for ensemble will have better results (Krogh and Vedelsby, 1995). We ensemble different methods in our system, some of them are existing methods, and the others are proposed by us. Next, we will describe these methods. Existing Methods Proposed Methods 2.2.1 2.1.1 QUETCH QUETCH (Kreutzer et al., 2015) (QUality Estimation from scraTCH) is a multilayer perceptron model trained without auxiliary parallel data. The embeddings of input passed through one linear layer with tanh activation functions and then one output layer with softmax activation functions, one linear layer with tanh activation functions, one output layer with softmax activation functions. QUETCH only outputs OK/BAD probabilities for each word in the word-level task. Similar to (Martins et al., 2017), we estimate HTER with the fraction of BAD labels for the sentence-level task. QE Brain QE Brain (Fan et al., 2018) is based on t"
2020.wmt-1.115,W16-2387,0,0.018171,"e target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data, which are from the WMT17 and WMT19 to improve our system’s performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs of the WMT20 QE shared tasks (Specia et al., 2020). 1004 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1004–1009 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics ... 2.1.2 ... NuQE (Martins et al., 2016) (NeUral Quality Estimation) can be seen as a stronger version of QUETCH by using complex neural networks. The architecture of NuQE consists of one embeddings layer, one linear layer, one bi-directional GRU layer, two other linear layers. The input and output of NuQE is the same as QUETCH. We use QUETCH and NuQE as implemented in OpenKiwi (Kepler et al., 2019) 1 . Target Forward BOS MASK ... MASK EOS Target Backward BOS MASK ... MASK EOS (a) QE Brain mask. 2.1.3 Target BOS MASK ... MASK EOS (b) MTLM. Figure 1: To save space, we do not show the source encoders of these models in the figure. (a)"
2020.wmt-1.115,W16-2392,0,0.0175065,"gh for postediting and indicating what edits are needed. This paper describes our system of the Shared Task on Word and Sentence-Level (QE Tasks 2) at WMT20. With the post-edited translations, all the quality scores can be computed automatically by TERCOM (Snover et al., 2006). Traditional QE models (Kozlova et al., 2016) use some time-consuming and expensive hand-craft features to represent the translation pairs. With the great success of deep neural networks in natural language processing (NLP), some researches have begun to apply automatic neural features to do QE tasks (Chen et al., 2017; Shah et al., 2016). However, the rare QE data can’t fully release the power of deep neural networks. To address this problem, researchers try to transfer bilingual knowledge ∗ Corresponding Author. However, existing predictor-estimator frameworks cannot fully use the information from parallel data because of the discrepancy of data quality between the predictor and the estimator. The predictor is trained on parallel data, which are nearly no errors in translations. While the translations in QE data is generated by a real machine translation system and may have some errors. When the estimator is training on the"
2020.wmt-1.115,2006.amta-papers.25,0,0.0565534,". Our system finished fifth in the end at sentence-level on both ENZH and EN-DE language pairs. 1 Introduction Quality Estimation (QE) is a task to predict the quality of translations without relying on any references. QE plays a critical role in machine translation to reduce human efforts, such as deciding whether a translation is good enough for postediting and indicating what edits are needed. This paper describes our system of the Shared Task on Word and Sentence-Level (QE Tasks 2) at WMT20. With the post-edited translations, all the quality scores can be computed automatically by TERCOM (Snover et al., 2006). Traditional QE models (Kozlova et al., 2016) use some time-consuming and expensive hand-craft features to represent the translation pairs. With the great success of deep neural networks in natural language processing (NLP), some researches have begun to apply automatic neural features to do QE tasks (Chen et al., 2017; Shah et al., 2016). However, the rare QE data can’t fully release the power of deep neural networks. To address this problem, researchers try to transfer bilingual knowledge ∗ Corresponding Author. However, existing predictor-estimator frameworks cannot fully use the informati"
2020.wmt-1.115,2020.wmt-1.79,0,0.0644616,"Missing"
2021.acl-short.47,N19-1191,0,0.0546008,"ed with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT,"
2021.acl-short.47,D19-1147,0,0.0961514,"ts, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT, recently proposed in (Khandelwal et al., 2020a), equips a pre-trained NMT model with a kNN classifier over a datastore of cached context representations and corresponding target tokens, providing a simple yet effective strategy to utilize cached contextual information in inference. However, the hyper-parameter k is fixed for all cases, which raises some potential problems. Intuitively"
2021.acl-short.47,2020.findings-emnlp.307,0,0.137873,"Missing"
2021.acl-short.47,W17-3204,0,0.0286012,"Missing"
2021.acl-short.47,W19-5333,0,0.023106,"proposed model as Adaptive kNN-MT (A) and compare it with two baselines. One of that is vanilla kNN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor pred"
2021.acl-short.47,N19-4009,0,0.0149402,"2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor prediction results calculated as Equation (1). Implementation Details. We adopt the fairseq toolkit3 (Ott et al., 2019) and faiss4 (Johnson et al., 2017) to replicate kNN-MT and implement our model. We apply the WMT’19 German-English news translation task winner model (Ng et al., 2019) as the pre-trained NMT model which is also used by Khandelwal et al. (2020a). For kNN-MT, we carefully tune the hyper-parameter λ in Equation (2) and report the best scores for each domain. More details are included in the supplementary materials. For our method, the hidden size of the two-layer FFN in Meta-k Network is set to 32. We Training. We fix the pre-trained NMT model and only optimize the Meta-k Network by minimizing th"
2021.acl-short.47,P02-1040,0,0.113175,"NN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor prediction results calculated as Equation (1). Implementation Details. We adopt the fairseq toolkit3 (Ott e"
2021.acl-short.47,P16-1162,0,0.0804927,"ithout training on any in-domain data. We denote the proposed model as Adaptive kNN-MT (A) and compare it with two baselines. One of that is vanilla kNN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) wher"
2021.acl-short.47,2020.emnlp-main.474,1,0.841791,"he generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT, recently proposed in (Khandelwal et al., 2020a), equips a pre-trained NMT model with a kNN classifier over a datastore of cached context representations and corresponding target tokens, providing a simple yet effective strategy to utilize cached contextual information in inference. However, the hyper-parameter k is fixed for all cases, which raises some potential problems. Intuitively, the retrieved nei"
2021.acl-short.47,N18-1120,0,0.301745,"be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and"
2021.acl-short.69,D18-1461,0,0.0772069,"urface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison between CHAR and oth"
2021.acl-short.69,W02-0603,0,0.290502,"ing with low resource, and adapting to unseen domains. Experimental results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains. 1 Introduction Neural machine translation (NMT) has achieved great success in recent years. Modern NMT systems typically operate on subword level, using segmentation algorithms such as byte pair encoding (BPE) (Sennrich et al., 2016) or Morfessor (Creutz and Lagus, 2002). Compared to word-level models, subword segmentation helps overcome the out-ofvocabulary (OOV) problem and make better use of morphological information in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules"
2021.acl-short.69,N19-1154,0,0.0176604,"T (Cherry et al., 2018; Banar et al., 2020) mainly focus on reducing computation cost of them. Cherry et al. (2018) show that by employing source sequence compression techniques, the quality and efficiency of character-based models can be properly balanced. Banar et al. (2020) share the same idea as Cherry et al. (2018) but build their models using Transformer architecture. Our work differs from theirs in that we aim to analyze the performance of existing models instead of exploring novel architectures. There are also several researches on comparison between CHAR and other subword algorithms (Durrani et al., 2019; Gupta et al., 2019). Durrani et al. (2019) compare character-based models and subword-based models in terms of representation quality, and find that representation learned by the former are more suitable for modeling morphology, and more robust to noisy input. Gupta et al. (2019) investigate the performance of different segmentation algorithms when using Transformer architecture, and find that character-based models can achieve better performance when translating noisy text or text from a different domain. Our finds are consistent with them, yet we conduct a more large-scale and in-depth ana"
2021.acl-short.69,2020.acl-main.145,0,0.0611721,"ctiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison between CHAR and other subword algorithms, e.g. BPE and Mo"
2021.acl-short.69,W17-3204,0,0.158226,"d unknown words. 4 Translation Across Distant Domains Domain robustness (M¨uller et al., 2020), which refers to models’ generalization ability on unseen domains, is important for NMT applications. However, subword algorithms need to learn segmentation rules from a given corpus, which may be domain-specific. When applied to a new domain, they may improperly segment target-domain specific words, hurting the domain robustness. In contrast, CHAR does not suffer from the issue. In this section, we investigate how different segmentation algorithms affect NMT models’ domain robustness. 4.1 Following Koehn and Knowles (2017), each time we train a source domain model on one of four subsets and report results on test sets of the other three domain. We experiment in two settings: No Adapt and Finetune. The first one involves no target domain data, while the latter uses randomly sampled 100k sentence pairs from target domain data to finetune the source domain model. 4.2 Results We report the average out-of-domain (OOD) BLEU scores of NMT systems based on different segmentation algorithms in Figure 3a and Figure 3b. As can be seen from the figure, CHAR surpasses other algorithms in almost all settings, except when fin"
2021.acl-short.69,P18-1007,0,0.0186422,"ect translation of domain-specific and OOV words, which may be segmented improperly by subword algorithms. No adapting Finetune Word Char BPE Morf. BPE-D 11.03 30.26 12.46 40.53 9.02 39.53 9.74 38.49 11.11 40.26 Table 3: Average OOD BLEU of models based on different subword algorithms when adapting from Law to other domains. BPE-D: BPE-dropout (Provilkov et al., 2020) 4.4 Comparison with Advanced Segmentation Algorithms Although we focus on deterministic segmentation algorithms in this paper, there are more advanced ones such as BPE-dropout (Provilkov et al., 2020) and subword regularization (Kudo, 2018), which produce multiple segmentation candidates when training and show improved performance. Therefore, we also conduct experiments comparing CHAR with BPE-dropout in terms of domain adaptation performance. We take the setting of adapting from Law to other domains and report results in Table 3. As can be seen, although BPEdropout surpasses BPE by a large margin, CHAR still achieves the best performance, which again shows the superiority of CHAR. 5 Related Work Character-level neural machine translation has received growing attention in recent years. Lee et al. (2017) first propose a fully cha"
2021.acl-short.69,Q17-1026,0,0.113687,"formation in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison"
2021.acl-short.69,2020.emnlp-main.203,0,0.064111,"Missing"
2021.acl-short.69,2020.amta-research.14,0,0.0522654,"Missing"
2021.acl-short.69,I17-2050,0,0.0184904,"ree of four morphological phenomena on which CHAR falls behind are so-called stability features (Burlot et al., 2018), which are expressed differently in the source language but should be expressed identically in the target language2 . The disadvantage of CHAR in this kind of phenomena shows CHAR-based model may be less robust to lexical changes to source-side changes, and the reason needs to be further researched. 3 Translation with Low Resource of different resource conditions. For validation and test, we use the original development and test split. Previous works (Sennrich and Zhang, 2019; Nguyen and Chiang, 2017) show that in low resource settings the evaluation results can be sensitive to model size (e.g. hidden dimension, layer number) and the number of BPE merges k, so we run an additional search of hidden dimension, layer number and k, and report the best results in this section. See Appendix A for details. 3.2 We evaluate models with BLEU and chrF3. The results are showed in Figure 1. In general, the performances of CHAR and BPE are on par, and are better than Word and Morfessor. In different data conditions, the results varies. Subword algorithms help alleviate the OOV problem. However, most of"
2021.acl-short.69,P02-1040,0,0.109432,"introflexive, and Vietnamese (Vi) and Malaysian (Ml) for isolating. We use OPUS-100 corpus1 (Tiedemann, 2012), which consists of 1M parallel sentences for each language pair. Model and Hyperparameters We use the Transformer architecture (Vaswani et al., 2017) throughout all experiments. To ensure results’ reliability , we run an exhaustive search of hyperparameters including batch size and learning rate. Detailed hyperparameters can be found in Appendix A. 2.2 Results The results are listed in Table 1. We can see that CHAR outperforms other algorithms in 7 out of 8 languages in terms of BLEU (Papineni et al., 2002) and chrF3 (Popovi´c, 2015), showing strong competitiveness of CHAR’s ability across languages. The only exception is the En-Fr language pair, which are known to be quite similar and is beneficial for BPE to learn a joint segmentation model. It is intuitive that BPE and Morfessor cannot outperform CHAR on introflexive languages (Hi, Ar). Introflexive languages follows non-concatenative morphology (McCarthy, 1981), i.e. grammatical 1 http://data.statmt.org/opus-100-corpus/v1.0/supervised/ Word Char BPE Morf. 55.6 49.6 60.6 36.6 73.6 96.6 33.8 51.4 83.2 74.6 48.8 38.4 9.2 65.4 70.8 83.0 67.0 61."
2021.acl-short.69,W15-3049,0,0.0500774,"Missing"
2021.acl-short.69,2020.acl-main.170,0,0.027137,"nt with findings in Section 3. While performances of CHAR and subword-based algorithms are on par on common words, CHAR outperforms the others by a large margin on domainspecific words. This suggests that the advantage of CHAR mainly comes from the correct translation of domain-specific and OOV words, which may be segmented improperly by subword algorithms. No adapting Finetune Word Char BPE Morf. BPE-D 11.03 30.26 12.46 40.53 9.02 39.53 9.74 38.49 11.11 40.26 Table 3: Average OOD BLEU of models based on different subword algorithms when adapting from Law to other domains. BPE-D: BPE-dropout (Provilkov et al., 2020) 4.4 Comparison with Advanced Segmentation Algorithms Although we focus on deterministic segmentation algorithms in this paper, there are more advanced ones such as BPE-dropout (Provilkov et al., 2020) and subword regularization (Kudo, 2018), which produce multiple segmentation candidates when training and show improved performance. Therefore, we also conduct experiments comparing CHAR with BPE-dropout in terms of domain adaptation performance. We take the setting of adapting from Law to other domains and report results in Table 3. As can be seen, although BPEdropout surpasses BPE by a large m"
2021.acl-short.69,P16-1162,0,0.0601632,"ypologically diverse languages, training with low resource, and adapting to unseen domains. Experimental results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains. 1 Introduction Neural machine translation (NMT) has achieved great success in recent years. Modern NMT systems typically operate on subword level, using segmentation algorithms such as byte pair encoding (BPE) (Sennrich et al., 2016) or Morfessor (Creutz and Lagus, 2002). Compared to word-level models, subword segmentation helps overcome the out-ofvocabulary (OOV) problem and make better use of morphological information in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data"
2021.acl-short.69,P19-1021,0,0.0175049,"nguages. Interestingly, three of four morphological phenomena on which CHAR falls behind are so-called stability features (Burlot et al., 2018), which are expressed differently in the source language but should be expressed identically in the target language2 . The disadvantage of CHAR in this kind of phenomena shows CHAR-based model may be less robust to lexical changes to source-side changes, and the reason needs to be further researched. 3 Translation with Low Resource of different resource conditions. For validation and test, we use the original development and test split. Previous works (Sennrich and Zhang, 2019; Nguyen and Chiang, 2017) show that in low resource settings the evaluation results can be sensitive to model size (e.g. hidden dimension, layer number) and the number of BPE merges k, so we run an additional search of hidden dimension, layer number and k, and report the best results in this section. See Appendix A for details. 3.2 We evaluate models with BLEU and chrF3. The results are showed in Figure 1. In general, the performances of CHAR and BPE are on par, and are better than Word and Morfessor. In different data conditions, the results varies. Subword algorithms help alleviate the OOV"
2021.acl-short.69,tiedemann-2012-parallel,0,0.0102544,"only focus on performances of characterlevel models when translating to fusional and agglutinative languages (Gupta et al., 2019; Libovick´y and Fraser, 2020), we conduct a comprehensive study covering all four morphological categories. 2.1 Experiment Setup Dataset We consider the translation from English to eight target languages representing four morphological categories, i.e. French (Fr) and Romanian (Ro) for fusional, Finnish (Fi) and Turkish (Tr) for agglutinative, Hebrew (He) and Arabic (Ar) for introflexive, and Vietnamese (Vi) and Malaysian (Ml) for isolating. We use OPUS-100 corpus1 (Tiedemann, 2012), which consists of 1M parallel sentences for each language pair. Model and Hyperparameters We use the Transformer architecture (Vaswani et al., 2017) throughout all experiments. To ensure results’ reliability , we run an exhaustive search of hyperparameters including batch size and learning rate. Detailed hyperparameters can be found in Appendix A. 2.2 Results The results are listed in Table 1. We can see that CHAR outperforms other algorithms in 7 out of 8 languages in terms of BLEU (Papineni et al., 2002) and chrF3 (Popovi´c, 2015), showing strong competitiveness of CHAR’s ability across la"
2021.acl-short.69,N19-1097,0,0.0435671,"Missing"
2021.emnlp-main.579,2020.acl-main.692,0,0.057278,"ys and [W1 ; b1 ] are trainable parameters. The bandwidth of learnable Laplacian kernel kq −k k Kl (qi , kj ; σ) = exp(− i σ j ) is modeled in the 1 For two d-dimension vectors same way as the bandwidth of learnable Gaussian qxPand y, we compute the d 2 2. L distance between x and y as (x − y ) kernel. i i i=1 7282 Train Dev Test Law 467k 2k 2k Medical 248k 2k 2k Koran 18k 2k 2k IT 223k 2k 2k Subtitles 500k 2k 2k Table 1: The number of training, development and test examples of 5 domain-specific datasets. The training data of Subtitles domain is sampled from the full Subtitles training set by Aharoni and Goldberg (2020). 3.3 Adaptive Mixing of Base Prediction and Retrieved Examples To mix the model-based distribution and examplebased distribution adaptively, we model the mixing weight λ with a learnable neural network. The mixing weight λ is computed by a multilayer perceptron with query qi and weighted sum e as inputs, where [W2 ; b2 ; W3 ; b3 ] are of keys k trainable parameters. ei ] + b2 ) + b3 ) λ = sigmoid(W3 ReLU(W2 [qi ; k (5) ei = k k X wj k j (6) j=1 wj ∝ K(qi , kj ; θ) (7) In this way, kNN-MT (Khandelwal et al., 2021) could be seen as a specific case of KSTER, with fixed Gaussian kernel and mixing"
2021.emnlp-main.579,D19-1165,0,0.0152561,"tperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parame"
2021.emnlp-main.579,N19-1191,0,0.222739,"success (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn K"
2021.emnlp-main.579,P19-1175,0,0.0175103,"ates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examples are retrieved based on fuzzy matching (Bulte and Tezcan, 2019; Xu et al., 2020), embedding similarity, or a mixture of the two approaches (Bapna and Firat, 2019). 3 Methodology In this section, we first formulate the kernelsmoothed machine translation (KSTER), which smooths neural machine translation (NMT) output with retrieved token level examples. Then we introduce the modeling and training of the learnable kernel and adaptive mixing weight. The overview of KSTER is shown in Figure 2. 3.1 Kernel-Smoothed Machine Translation Base Model for Neural Machine Translation The state-of-the-art NMT models are based on the encoder-decoder architecture. The enco"
2021.emnlp-main.579,D18-1340,0,0.13778,"Koran IT domain on which kNN-MT and KSTER are adapted Subtitles Figure 1: The domain-specific and general domain translation performance in EN-DE translation. Base is a Transformer model trained on general domain WMT data. kNN-MT and our proposed KSTER are adapted for domain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of t"
2021.emnlp-main.579,W17-4702,0,0.0162509,"Missing"
2021.emnlp-main.579,2021.acl-long.378,0,0.0648744,"Missing"
2021.emnlp-main.579,2021.findings-emnlp.23,0,0.0311758,"the source. NMT systems are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambi"
2021.emnlp-main.579,2021.acl-short.47,1,0.702718,"ain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7280–7290 c November 7–11, 2021. 2021 Association for Computational Linguistics and retrieved examples brings noise to translation (Bapna and Firat, 2019). Second, completely non-parametric methods"
2021.emnlp-main.579,W04-3250,0,0.250837,".24 IT 22.99 34.48 29.55 31.82 29.21 39.57 37.56 36.90 Subtitles 20.65 25.16 21.80 22.63 23.13 27.73 22.86 25.15 Average-specific 23.54 32.92 33.08 34.53 26.75 36.77 36.64 37.74 Table 5: Test set BLEU scores of multi-domain machine translation. Average-specific is the averaged performance in 5 specific domains. For general domain sentence translation, KSTER outperforms kNN-MT for 3 and 6 BLEU scores in EN-DE and DE-EN direction respectively. For domain-specific translation, KSTER outperforms kNNMT for 1.5 and 1.1 BLEU scores in EN-DE and DE-EN direction. Significance test by paired bootstrap (Koehn, 2004) resampling shows that KSTER outperforms kNN-MT significantly in all domains except for Koran domain in EN-DE translation and IT domain in DE-EN translation. 4.3 Multi-Domain Machine Translation In MDMT, since there is no domain label available in test time, examples from all domains are translated with one model. We build a mixed database with training data of general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 A"
2021.emnlp-main.579,C18-1111,0,0.020353,"nce. We therefore drop the most similar examples during training to reduce this discrepancy. With above improvements, KSTER shows the following advantages: • Extensive experiments show that, KSTER outperforms kNN-MT, a strong competitor, in specific domains for 1.1 to 1.5 BLEU scores while keeping the performance in general domain. • KSTER outperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Faraj"
2021.emnlp-main.579,W17-3204,0,0.0259316,"domain labels of examples are available in test time. In MDMT, the domain labels of examples are not available in test time, so examples from all domains are translated with one model, which is a more practical setting. 4.1 Datasets and Implementation Details Datasets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build"
2021.emnlp-main.579,D19-3019,0,0.0471854,"Missing"
2021.emnlp-main.579,W17-4713,0,0.0203522,"2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empir"
2021.emnlp-main.579,2020.emnlp-main.733,1,0.684475,"ms are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambiguation ability of"
2021.emnlp-main.579,2021.acl-long.25,1,0.667655,"k is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examp"
2021.emnlp-main.579,P02-1040,0,0.109427,"Missing"
2021.emnlp-main.579,2021.tacl-1.2,0,0.0122763,"ion (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced"
2021.emnlp-main.579,W18-6319,0,0.0203748,"Missing"
2021.emnlp-main.579,D18-1104,0,0.0205885,"1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search se"
2021.emnlp-main.579,2020.acl-main.144,0,0.0600508,"Missing"
2021.emnlp-main.579,N18-1120,0,0.09194,"l., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Examp"
2021.emnlp-main.579,P16-1162,0,0.0436248,"asets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build the Since the database is built from the training data and KSTER is trained on the training data, similar examples can constantly be retrieved from the database during training. However, in test time, there may be no example in the database that is 2 similar to"
2021.emnlp-main.579,tiedemann-2012-parallel,0,0.0180797,"nels and different weights for different examples. EN-DE General Specific 24.72 33.08 26.06 33.40 27.80 34.02 27.74 34.38 DE-EN General Specific 25.87 36.64 27.89 37.37 31.88 37.19 31.94 37.61 Table 7: Ablation study of learnable kernel and mixing weight in KSTER with Gaussian kernel in MDMT. Both learnable kernel and learnable mixing weight bring improvement. None represents that both kernel and mixing weight are fixed, in which case KSTER degenerates to kNN-MT. KSTER with Laplacian kernel in unseen domains, which is important in real-world MDMT applications. We take Bible and QED from OPUS (Tiedemann, 2012)6 as unseen domains and randomly sample 2k examples from each domain for test. We directly use the MDMT models to translate sentences from unseen domains. The results of ENDE translation are presented in Table 6. KSTER outperforms all baselines, which shows strong generalization ability. 4.4 Inference Speed A common concern about non-parametric methods in MT is that searching similar examples may slow the inference speed. We test the inference speed KSTER in MDMT in EN-DE translation, which is the setting with the largest database. The averaged inference time in general domain and 5 specific d"
2021.emnlp-main.579,D19-1670,0,0.0233319,"general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 Averaged domain-specific performance 34 BLEU 29 BLEU of kNN-MT changes with the size of database. In this work, we study the performance change of kNN-MT and KSTER with low-quality database. Specifically, we test the robustness of these models in DAMT when the database is noisy. We add token-level noise to the English sentences in parallel training data by EDA (Wei and Zou, 2019) 5 . For each word in a sentence, it is modified with a probability of 0.1. The candidate modifications contain synonym replacement, random insertion, random swap and random deletion with equal probability. Then we use the noisy training data to construct the noisy database. We study the effects of source side noise and target side noise on translation performance. The experiment results are presented in Table 4. Target side noise has more negative effect to translation performance than source side noise. The BLEU scores of KSTER drop less apparently in all settings, which indicates that the p"
2021.emnlp-main.679,2020.emnlp-main.38,0,0.211355,"), for the first time, we propose to investigate the problem of few/zero-shot labels in LMTC tasks from a meta-learning perspective. Illustrated in Fig. 1, we simulate some few/zero-shot scenarios, which are faithful to the LMTC task and thereby provide chances for models to learn how to adapt fast and efficiently with a limited amount of data. However, most meta-learning algorithms are designed for multi-class classification under the fewshot setting (Vinyals et al., 2016), and it is critical for meta-learned models’ generalization to construct faithful and diverse tasks (Snell et al., 2017; Bansal et al., 2020). We argue that the simple extension of these approaches to multi-label classification is sub-optimal for the LMTC tasks in that (1) LMTC tasks need to cope with few- and zero-shot scenarios, while existing methods only consider few-shot ones, which is not faithful to the LMTC tasks. (2) LMTC tasks often face the challenge of long-tailed data distribution. However, these algorithms are not designed for specific data distribution and thereby makes the rare labels in the training set less involved in the meta-learning process, which reduces the diversity of the tasks. • Our method outperforms th"
2021.emnlp-main.679,D16-1076,0,0.0629727,"Missing"
2021.emnlp-main.679,P19-3015,0,0.0281833,"t al. (2019) and Rios and Kavuluru (2018), we also report the harmonic average across all R@K and all nDCG@K scores for methods that can predict zero-shot labels. 5.3 Baselines Following Lu et al. (2020), we compare the following baselines. CNN (Kim, 2014) uses convolutional neural networks with max-pooling to extract text features, which are then used to make the predictions for the labels. RCNN (Lai et al., 2015) uses recurrent neural networks with a convolution layer to consider both long-distance and local dependencies. It achieves best the performances across competitive text encoders in Liu et al. (2019). CAML (Mullenbach et al., 2018) is a model designed for clinical notes and text documents. It more improvement with a smaller threshold. Thus, we believe that a smaller threshold would not affect the conclusions of our experiments. 8637 MIMIC-III Frequent R@10 nDCG@10 Few-shot R@10 nDCG@10 Zero-shot R@10 nDCG@10 Harmonic Average R@10 nDCG@10 CNN RCNN CAML 34.6 43.9 41.2 44.2 56.0 53.3 5.5 14.2 5.9 2.9 9.8 3.9 - - - - ZAGRU + SIMPLE - EXT + META - LMTC 49.0 49.2 49.7* 61.3 61.8 62.6* 26.9 27.2 29.1* 17.7 17.8 20.2* 34.7 35.4 38.8* 22.2 23.5 24.1 34.7 35.2 37.4* 25.5 26.1 28.0* ZAGGRU + SIMPLE"
2021.emnlp-main.679,2020.emnlp-main.235,0,0.323521,"Extensive experiments show that META - LMTC achieves state-of-the-art permation. Specifically, Rios and Kavuluru (2018) formance against strong baselines and can still utilizes label textual descriptors to generate a feaenhance powerful BERTlike models. ture vector for each label. Also, it employs a 2layer graph convolutional neural network (Kipf and 1 Introduction Welling, 2017) to take advantage of the structured Large-scale multi-label text classification (LMTC) knowledge of label spaces to enhance label repreis a fundamental and practical task in natural lan- sentations. Apart from that, Lu et al. (2020) finds guage processing (Tsoumakas et al., 2010). LMTC that label similarity graphs based on pre-trained can be found in several domains, such as organiz- word embeddings and co-occurrence frequency are ing documents in Wikipedia articles (Partalas et al., also beneficial. 2015), annotating medical records with diagnostic Nonetheless, these approaches neglect the potenand procedure labels (Yan et al., 2010; Rios and tial meta-knowledge contained in the dataset that Kavuluru, 2018), assigning legislation with rele- can guide the models to learn with only a small vant legal concepts (Chalkidis e"
2021.emnlp-main.679,N18-1100,0,0.0284997,"Kavuluru (2018), we also report the harmonic average across all R@K and all nDCG@K scores for methods that can predict zero-shot labels. 5.3 Baselines Following Lu et al. (2020), we compare the following baselines. CNN (Kim, 2014) uses convolutional neural networks with max-pooling to extract text features, which are then used to make the predictions for the labels. RCNN (Lai et al., 2015) uses recurrent neural networks with a convolution layer to consider both long-distance and local dependencies. It achieves best the performances across competitive text encoders in Liu et al. (2019). CAML (Mullenbach et al., 2018) is a model designed for clinical notes and text documents. It more improvement with a smaller threshold. Thus, we believe that a smaller threshold would not affect the conclusions of our experiments. 8637 MIMIC-III Frequent R@10 nDCG@10 Few-shot R@10 nDCG@10 Zero-shot R@10 nDCG@10 Harmonic Average R@10 nDCG@10 CNN RCNN CAML 34.6 43.9 41.2 44.2 56.0 53.3 5.5 14.2 5.9 2.9 9.8 3.9 - - - - ZAGRU + SIMPLE - EXT + META - LMTC 49.0 49.2 49.7* 61.3 61.8 62.6* 26.9 27.2 29.1* 17.7 17.8 20.2* 34.7 35.4 38.8* 22.2 23.5 24.1 34.7 35.2 37.4* 25.5 26.1 28.0* ZAGGRU + SIMPLE - EXT + META - LMTC 49.1 49.4 49"
2021.emnlp-main.679,D14-1162,0,0.0840534,"Missing"
2021.findings-acl.252,2020.acl-main.564,0,0.0274512,"Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Zheng et al. (2019) explore this method for unknown intent detection. However, there are two major distinctions between our study and these works. First, they generate OOD utterances according to continuous latent variables, which cannot be easily interpreted. In contrast, our framework generates utterances by performing local replacements to IND utterances, 2853 which is more interpretable to human. Second, our framework additionally contains a weighting module to reform the generated utterances. Our work is also inspired by Cai et al. (2020), which proposes a framework to augment the IND data, while our framework aims to generate OOD data. 3 Preliminary In this section, we formalize unknown intent detection task. Then we introduce the energy score, and its superiority and limitations for this task. 3.1 exp[fy (u)/T ] , y 0 exp[fy 0 (u)/T ] p(y|u) = P y0 Energy-based OOD Detection An energy-based model (LeCun et al., 2006) builds an energy function E(u) that maps an input u to a scalar called energy score (i.e., E : RD → R). Using the energy function, probability density p(u) can be expressed as: p(u) = exp(−E(u)/T ) , Z (1) R whe"
2021.findings-acl.252,N16-1061,0,0.0273089,"es and reweight them. We demonstrate that GOT can further improve the performance of the energy score by explicitly shaping the energy gap and achieves state-of-the-art results. • We show the generality of GOT by applying generated weighted OOD utterances to fine-tune the softmax-based detector, and the fine-tuned softmax-based detector can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of"
2021.findings-acl.252,D19-1131,0,0.0395448,"Missing"
2021.findings-acl.252,P19-1548,0,0.134778,"can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of training data is possible. However, Ren et al. (2019) find that likelihood scores derived from these models can be confounded by background components, and propose a likelihood ratio method to alleviate this issue. Gangal et al. (2019) reformulate and apply this method to unknown intent detection. Different from these methods, we intro"
2021.findings-acl.252,D17-1314,0,0.0804,"er improve the performance of the energy score by explicitly shaping the energy gap and achieves state-of-the-art results. • We show the generality of GOT by applying generated weighted OOD utterances to fine-tune the softmax-based detector, and the fine-tuned softmax-based detector can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of training data is possible. However, Ren et al. (2019)"
2021.findings-emnlp.358,2020.acl-main.692,0,0.0199973,")∈(X ,Y) t (5) where θ is the parameters of all adapter layers. Note that we keep original parameters in the pre-trained NMT model fixed during training to avoid the performance degradation of the NMT model in the inference stage. pre-trained NMT model with adapter layers forward passes these pairs to create an in-domain datastore. When translating in-domain sentences, we utilize the original NMT model and kNN retrieval on the in-domain datastore to perform online domain adaptation as Equation (3). 4 Experiments 4.1 Setup Datasets and Evaluation Metric. We use the same multi-domain dataset as Aharoni and Goldberg (2020) to evaluate the effectiveness of our proposed model and consider domains including IT, Medical, Koran, and Law in our experiments. We extract target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with"
2021.findings-emnlp.358,D19-1147,0,0.0160741,"nario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully app"
2021.findings-emnlp.358,W17-3204,0,0.0648933,"Missing"
2021.findings-emnlp.358,W19-5333,0,0.0206117,"model and consider domains including IT, Medical, Koran, and Law in our experiments. We extract target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with the codes provided by the pre-trained model (Ng et al., 2019). We use SacreBLEU3 to measure all results with casesensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test WMT19’News IT Medical Koran Laws 37, 079, 168 10, 000 - 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. Methods. We compare our proposed approach with several baselines: • Basic NMT: A general domain model is directly used to evaluate on in-domain test sets. • Empty-kNN: The source-side of synthetic bilingual data is always set to <EOS> token. • Copy-kNN: Each target sentence is copied to s"
2021.findings-emnlp.358,P02-1040,0,0.110604,"target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with the codes provided by the pre-trained model (Ng et al., 2019). We use SacreBLEU3 to measure all results with casesensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test WMT19’News IT Medical Koran Laws 37, 079, 168 10, 000 - 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. Methods. We compare our proposed approach with several baselines: • Basic NMT: A general domain model is directly used to evaluate on in-domain test sets. • Empty-kNN: The source-side of synthetic bilingual data is always set to <EOS> token. • Copy-kNN: Each target sentence is copied to source-side to produce synthetic bilingual data. This is a special case of our method without model tra"
2021.findings-emnlp.358,P16-1009,0,0.22508,"f kNN-MT on unsupervised domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021)"
2021.findings-emnlp.358,P16-1162,0,0.272461,"f kNN-MT on unsupervised domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021)"
2021.findings-emnlp.358,N19-1209,0,0.0395328,"Missing"
2021.findings-emnlp.358,2020.emnlp-main.474,1,0.790641,"s large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully applied to neural mach"
2021.findings-emnlp.358,N18-1120,0,0.0193051,"domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In thi"
2021.findings-emnlp.358,2021.acl-short.47,1,0.595004,"nnrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully applied to neural machine transDomain Adaptation framework based on kNN-MT lation (NMT). These approaches complement ad(UDA-kNN). The UDA-kNN aims at directly levervanced NMT models (Sutskever et al., 2014; Bahaging the monolingual target-side data to generate danau et al., 2015; Vaswani et al., 2017; Hassan the corresponding datastore, and encouraging it to et al., 2018) with external memory to alleviate the play a similar role with the real bilingual in-domain performance degradation when translating out-ofdata,"
2021.naacl-main.458,D19-1633,0,0.553828,"y. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficient"
2021.naacl-main.458,P07-2045,0,0.0151311,"Missing"
2021.naacl-main.458,D18-1149,0,0.555263,"arge number (more than 215 , Kaiser et al., 2018; the model capacity. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10"
2021.naacl-main.458,D19-1573,0,0.253454,"Missing"
2021.naacl-main.458,D19-1437,0,0.449018,"to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line we first employ vector quantization (Roy et al., of research works (Kaiser et al., 2018; Roy et al., 2018) to discretize the target language to the la2018; Shu et al., 2019; Ma et al., 2019) introduce tent space with a smaller number (less than 128) latent variable modeling to the non-autoregressive of latent variables, which can serve as the fuzzy Transformer and improves translation quality. The word-class information each target language word. latent variables could be regarded as the spring- We then model the latent variables with conditional board to bridge the modeling gap, introducing random fields (CRF, Lafferty et al., 2001; Sun et al., more informative decoder inputs than the previ- 2019). To avoid the mismatch of the training and 5749 Proceedings of the 2021 Conference"
2021.naacl-main.458,N19-4009,0,0.0191697,"parated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The categorical number K is set to 64 in our experiments. We implement our model based on the open-source framework of fairseq (Ott et al., 2019). Optimization. We optimize the parameter with the Adam (Kingma and Ba, 2015) with β = (0.9, 0.98). We use inverse square root learning rate scheduling (Vaswani et al., 2017) for the WMT tasks and linear annealing schedule (Lee et al., 2018) from 3 × 10−4 to 1 × 10−5 for the IWSLT14 task. Each mini-batch consists of 2048 tokens for IWSLT14 and 32K tokens for WMT tasks. WMT14 EN-DE DE-EN IWSLT14 DE-EN LV-NAR AXE CMLM SynST Flowseq 11.80 20.40 20.74 20.85 / 24.90 25.50 25.40 / / 23.82 24.75 NAT (ours) CNAT (ours) 9.80 21.30 11.02 25.73 17.77 29.81 Table 1: Results of the NAT models with argmax d"
2021.naacl-main.458,P02-1040,0,0.111587,"uces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficiently. versus the autoregressive model. However, the NAT models still lay behind the autoregressive models in In this paper, we propose to learn a set of latent terms of BLEU (Papineni et al., 2002) for machine codes that can act like the syntactic label, which is translation. We attribute the low-quality of NAT learned without using the explicit syntactic trees. models to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line"
2021.naacl-main.458,D07-1043,0,0.0071131,"e a sharp distribution for each latent variable, showing that our learned fuzzy classes are meaningful. 5 Related Work Non-autoregressive Machine Translation. Gu et al. (2018) first develop a non-autoregressive Transformer (NAT) for machine translation, which produces the outputs in parallel, and the inference speed is thus significantly boosted. Due to the missing of dependencies among the target outputs, the translation quality is largely sacrificed. A line of work proposes to mitigate such perQuantitative Results. We first compute the Vformance degradation by enhancing the decoder Measure (Rosenberg and Hirschberg, 2007) score between the latent categories to POS tags and sub- inputs. Lee et al. (2018) propose a method of itwords frequencies. The results are listed in Table 7. erative refinement based on the previous outputs. Overall, the “w/ POS tags” achieves a higher V- Guo et al. (2019) enhance decoder input by introducing the phrase table in statistical machine transMeasure score, indicating that the latent codes are more related to the POS tags than sub-words fre- lation and embedding transformation. There are quencies. The homogeneity score (H-score) evalu- also some work focuses on improving the decod"
2021.naacl-main.458,P16-1162,0,0.0910943,"arg max p(y|z ∗ , x; θ), y y∗ where identifying only requires independently One potential issue is that the mismatch of the maximizing the local probability for each output training and inference stage for the used categorical position. 5752 4 Experiments Model Datasets. We conduct the experiments on the most widely used machine translation benchmarks: WMT14 English-German (WMT14 EN-DE, 4.5M pairs)1 and IWSLT14 German-English (IWSLT14, 160K pairs)2 . The datasets are processed with the Moses script (Koehn et al., 2007), and the words are segmented into subword units using byte-pair encoding (Sennrich et al., 2016, BPE). We use the shared subword embeddings between the source language and target language for the WMT datasets and the separated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The"
2021.naacl-main.458,P19-1125,0,0.679393,"le (a) AT (b)Non-Autoregressive NATDecoding Decoding (c) LT Experiment results on WMT14 and IWSLT14 show that CNAT achieves the new state-of-the- Figure 1: Different inference process of different Transart performance without knowledge distillation. former models. With the sequence-level knowledge distillation and y x reranking techniques, the CNAT is comparable to • Inputs Initialization: With the target sethe current state-of-the-art iterative-based model quence length m, we can compute the dewhile keeping a competitive decoding speedup. coder inputs h = h1:m with Softcopy (Li et al., 2019; Wei et al., 2019) as: 2 Background hj = Neural machine translation (NMT) is formulated as a conditional probability model p(y|x), which models a sentence y = {y1 , y2 , · · · , ym } in the target language given the input x = {x1 , x2 , · · · , xn } from the source language. 2.1 Non-Autoregressive Neural Machine Translation Gu et al. (2018) proposes Non-Autoregressive Transformer (NAT) for machine translation, breaking the dependency among target tokens, thus achieving simultaneous decoding for all tokens. For a source sentence, a non-autoregressive decoder factorizes the probability of its target sentence as:"
2021.naacl-main.458,D19-1072,0,0.0211239,": Approach In this section, we present our proposed CNAT, an extension to the Transformer incorporated with non-autoregressive decoding for target tokens and autoregressive decoding for latent sequences. In brief, CNAT follows the architecture of Latent Transformer (Kaiser et al., 2018), except for the latent variable modeling (in § 3.1 and § 3.2) and inputs initialization (in § 3.3). 3.1 Modeling Target Categorical Information by Vector Quantization Categorical information has achieved great success in neural machine translation, such as partof-speech (POS) tag in autoregressive translation (Yang et al., 2019) and syntactic label in nonautoregressive translation (Akoury et al., 2019). Inspired by the broad application of categorical information, we propose to model the implicit categorical information of target words in a nonautoregressive Transformer. Each target sequence y = y1:m will be assigned to a discrete latent variable sequence z = z1:m . We assume that each zi will capture the fuzzy category of its token yi . Then, the conditional probability p(y|x) is factorized with respect to the categorical latent variable: p(y|x) = X zi = k, qi = Qk , and k = arg min ||repr(yi ) − Qj ||2 , j∈[K] repr"
D15-1145,D13-1106,0,0.0200271,"ncies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been appl"
D15-1145,D07-1007,0,0.758393,"associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance depende"
D15-1145,P07-1005,0,0.747251,"to two factors: 1) associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing"
D15-1145,J90-1003,0,0.339875,"ence, the most ideal translation graph is a graph that includes all source words and their candidate translations. However, this ideal graph has two problems: intensive computation for graph inference and difficulty in modeling dependencies between function and content words. In order to get around these two issues, we only consider lexical selection for source content words2 . We first identify source-side content word pairs using statistical metrics, and then keep word pairs with a high relatedness strength in the translation graph. To be specific, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and co-occurrence frequency to measure the relatedness strength of two source-side words s and s0 within a window ds . Content word pairs will be kept when their co-occurrence frequencies are more than cf times in our training corpus and PMI values are larger than pmi . In this process, we remove noisy word pairs using the following heuristic rules: (1) As an adjective only has relations with its head nouns or dependent adverbs, we remove all word pairs where an adjective is paired with words other than its head nouns or dependent adverbs; (2) We apply a similar constraint to adverbs too, s"
D15-1145,P11-2031,0,0.0127217,"set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermor"
D15-1145,P13-2061,0,0.0248668,"Missing"
D15-1145,P14-1129,0,0.0370286,"Missing"
D15-1145,D08-1039,0,0.140317,"ed translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the"
D15-1145,C08-1041,0,0.290315,"source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the tran"
D15-1145,E14-1003,0,0.0123714,"another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approa"
D15-1145,D13-1176,0,0.0103886,"lations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et"
D15-1145,N03-1017,0,0.0296397,"tion. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇ"
D15-1145,W04-3250,0,0.0291274,"emented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could"
D15-1145,D08-1010,0,0.0188469,"o model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic model"
D15-1145,P14-1030,0,0.100474,"where N (t˜) denotes the set of candidate translations that link to t˜, and RS(t˜, t˜0 ) measures the strength of relatedness between t˜ and t˜0 which is calculated as the average word-level relatedness over all content words in these two translations t˜ and t˜0 . As for the word-level relatedness RS(t, t0 ) for a content word pair (t, t0 ), we estimate it with the following two approaches over collected cooccurring word pairs within a window of size dt : (1) RS(t, t0 ) is computed as a bigram conditional probability plm (t0 |t) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS(t, t0 ) as p(t,t0 ) ln p(t)p(t 0) . 3 Collective Lexical Selection Algorithm Based on the translation graph, we propose a collective lexical selection algorithm to jointly identify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a sourceside content word s should be: 1) associated with s; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association edge with the node of"
D15-1145,D09-1022,0,0.0150969,"among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012"
D15-1145,J03-1002,0,0.00618241,"Missing"
D15-1145,P03-1021,0,0.0293498,"of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)"
D15-1145,P12-1079,1,0.818387,"rget translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctl"
D15-1145,P14-1137,1,0.830823,"Missing"
D15-1145,P11-1129,1,0.870636,"Missing"
D15-1145,D14-1021,0,0.0151054,"n the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the bes"
D15-1145,D13-1050,0,0.0242003,"Missing"
D15-1145,P02-1040,0,0.0939119,"his, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection i"
D15-1145,P08-1066,0,0.228479,"source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇou”(hold) and “l`ıc"
D15-1145,D09-1008,0,0.130528,"luding corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemo"
D15-1145,D14-1003,0,0.0131094,"ide context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weig"
D15-1145,N12-1046,0,0.0463556,"Missing"
D15-1145,2011.mtsummit-papers.13,0,0.158413,"d 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each"
D15-1145,J07-2003,0,\N,Missing
D17-1013,D17-1151,0,0.0303533,"ide. Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state. Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector. On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected. We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures. Due to the large number of parameters, it may be difficult for the NMT system to learn the proper sentence representation as the initial state. Thus, the"
D17-1013,D14-1179,0,0.0372327,"Missing"
D17-1013,P15-1166,0,0.0572008,"the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using en"
D17-1013,P15-1001,0,0.0765902,"he length of x and y, respectively. In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1 , h2 , · · · , h|x |). For each xi , the representation hi is: − → ← − hi = [ hi ; hi ] (1) Related Work Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captio"
D17-1013,2010.amta-papers.33,0,0.0594987,"ns mechanism could be used as a training method and brings no extra computing cost during decoding. Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality. 2 tion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training. 3 Notations and Backgrounds We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism. Denote a source-target sentence pair as {x, y} from the t"
D17-1013,D14-1175,0,0.0505949,"Missing"
D17-1013,P16-1100,0,0.0293479,"y, respectively. In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1 , h2 , · · · , h|x |). For each xi , the representation hi is: − → ← − hi = [ hi ; hi ] (1) Related Work Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared e"
D17-1013,D15-1166,0,0.0629789,"technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using en"
D17-1013,D16-1160,0,0.031291,"ould bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information. However, the purpose and the way of our mechanism are different from them. The word prediction technique has been applied in the research of both statistical machine translawhere [·; ·] denotes the concatenation of column − → ← − vectors; hi"
D17-1013,D09-1022,0,0.0606512,"Missing"
D17-1013,C16-1205,0,0.143249,"Missing"
D17-1013,P16-2021,0,0.0277493,"and brings no extra computing cost during decoding. Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality. 2 tion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training. 3 Notations and Backgrounds We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism. Denote a source-target sentence pair as {x, y} from the training set, where x is the source word seque"
D17-1013,P02-1040,0,0.104925,"97 and 1082 source sentences, respectively, with 4 references for each sentence. For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs. We use newstest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set. These sets have 3000 and 2737 source sentences, respectively, with 1 reference for each sentence. Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017). 5.2 5.4 Translation Experiments To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CH-EN and DE-EN tasks. The detailed results are show in the Table 1 and Table 2. Compared to the baseNMT system, all of our models achieve significant improvements. On the CH-EN experiments, simply adding word predictions to the initial state (WPE ) already brings considerable improvements. The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality. Adding word predicSystems and Techniques We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong"
D17-1013,P07-1020,0,\N,Missing
D17-1079,P14-2131,0,0.0170647,"Missing"
D17-1079,P16-1039,0,0.348057,"ing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarow"
D17-1079,D13-1129,1,0.896381,"Missing"
D17-1079,C14-1078,1,0.843812,"Missing"
D17-1079,P15-1168,0,0.622783,"cter embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra cla"
D17-1079,D14-1093,1,0.462459,"Missing"
D17-1079,D15-1141,0,0.602486,"Missing"
D17-1079,I05-3025,0,0.0362245,"Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sent"
D17-1079,P15-1167,0,0.0923753,"estigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014)"
D17-1079,D16-1257,0,0.0260578,"Missing"
D17-1079,N06-1020,0,0.0153109,"their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empir"
D17-1079,D10-1002,0,0.0159819,"f feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natu"
D17-1079,N16-1118,0,0.0378788,"Missing"
D17-1079,P14-2050,0,0.0361789,"unigram wi and character bigram wi−1 wi , respectively. A forward word representation efi is calculated as follows: ri = concat2 (rilstm−f , rilstm−b ) = tanh(W2 [rilstm−f ; rilstm−b ]) Given the representation ri , we use a scoring unit to score for each potential segment label. Given ri , the score of segment label M is: i fM = WM h, where h = concat3 (ri , eM ), = tanh(W3 [ri ; eM ]) WM is the score matrix for label M, and eM is the label embedding for label M. 3 Word-Context Character Embeddings Our model structure is a derivation from the skipgram model (Mikolov et al., 2013), similar to Levy and Goldberg (2014). Given a sentence with length n: {w1 , w2 , w3 , · · · wn } and its corresponding segment labels: {l1 , l2 , l3 , · · · ln }, the pre-training context of current character wt is the around characters in the windows with size c, together with their corresponding segment labels (Figure 2). Characters wi and labels li in the context are represented by vectors ecwi ∈ Rd and ecli ∈ Rd , respectively, where d is the embedding dimensionality. The word-context embedding of character wt is represented as ewt ∈ Rd , which is trained by predicting the surrounding context representations ecw′ efi = conca"
D17-1079,P14-1043,0,0.01322,"d Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Sco"
D17-1079,D16-1046,0,0.0609597,"Missing"
D17-1079,N15-1142,0,0.0336542,"dicting the surrounding context representations ecw′ efi = concat1 (ewi , ewi−1 wi ), = tanh(W1 [ewi ; ewi−1 wi ]) A backward representation ebi can be obtained in the same way. Then efi and ebi are fed into forward and backward LSTM units at current position, obtaining the corresponding forward and backward LSTM representations rilstm−f and rilstm−b , respectively. In the scoring layer, we first obtain a linear combination of rilstm−f and rilstm−b , which is the final 761 and ecli , parameterizing the labeled segmentation information in the embedding parameters. To capture order information (Ling et al., 2015), we use different embedding matrices for context embedding in different context positions, training different embeddings for the same word when they reside on different locations as the context word. In particular, our context window size is five. As a result, each word has four different versions of ec , namely ec−1 , ec−2 , ec+1 , and ec+2 , each taking a distinct embedding matrix. Given the context window [w−2 , w−1 , w, w+1 , w+2 ], w−1 is the left first context word of the focus word w, ec−1,wi will be selected from embedding matrix E−1 , and w+1 is the right first word of w, ec+1,wi wil"
D17-1079,P14-1028,0,0.295876,"embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li,"
D17-1079,N09-1007,0,0.0471648,"Missing"
D17-1079,I05-3027,0,0.631704,"Missing"
D17-1079,W06-0127,0,0.253287,"in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sentence, and N is the s"
D17-1079,D13-1061,0,0.12946,"endencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-trai"
D17-1079,P13-1043,1,0.546747,"Missing"
D17-1079,P15-1032,0,0.00999897,"Missing"
D17-1079,P16-2092,0,0.103199,"Missing"
D17-1079,O03-4002,0,0.708664,"nference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith cha"
D17-1079,P95-1026,0,0.657468,"2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2"
D17-1079,D13-1031,0,0.0880344,"Missing"
D17-1079,E14-1062,1,0.919559,"Missing"
D17-1079,P16-1040,1,0.632749,"espectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling"
D17-1079,P07-1106,1,0.78255,"Missing"
D18-1062,P15-1081,0,0.030132,"in φt and θt in the target side. The pseudo-code for this process is shown in Algorithm 1. It should be noted that there is no variance once completing training. 623 4.1.1 Experiments on Chinese-English Dataset English are obtained from Multilingual Unsupervised and Supervised Embeddings (MUSE) 1 . For this set of experiments, we use the same data as Zhang et al. (2017). The statistics of the final training data is given in Table 1. We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015), translation matrix (TM) (Mikolov et al., 2013), isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach (Zhang et al., 2017). Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from Zhang et al. (2017). As we can see from the table, our model could achieve superior performance compared with other baseline models. Table 3 lists some word translation examples given by our model. Model MonoGiza w/o emb. MonoGiza w/ emb. TM IA Zhang et al. (2017) Ours #seeds 0 0 50 100 0 0 es-en it-en 铁路 rail railway railroad"
D18-1062,E14-1049,0,0.403621,"ar contexts tend to have similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information. Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision. Artetxe et al. (2017) successfully learn bilingual word embeddings with 2 2.1 Related Work Bilingual Lexicon Induction Extracting bilingual lexica has been studied by researchers for a long time. Mikolov et al. (2013) first observe there is isomorphic structure among word embeddings trained separately on monolingual corpora and they learn the linear transformation between languages. Zhang et al. (2016b) i"
D18-1062,P17-1042,0,0.109336,"s learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision. Artetxe et al. (2017) successfully learn bilingual word embeddings with 2 2.1 Related Work Bilingual Lexicon Induction Extracting bilingual lexica has been studied by researchers for a long time. Mikolov et al. (2013) first observe there is isomorphic structure among word embeddings trained separately on monolingual corpora and they learn the linear transformation between languages. Zhang et al. (2016b) improve the method by constraining the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization during training and 621 Proceedings of the 2018 Conference on Empirical Methods in"
D18-1062,N15-1028,0,0.0275966,"e translation before (Yang et al., 2018; Lample et al., 2018). pθt qфt target Figure 1: Illustration of our model. φs and φt map the source and target word embeddings into latent variables. Discriminator D guides the two latent distributions to be the same. maximize the cosine similarity instead. They point out that adding an orthogonality constraint can improve performance and has a closed-form solution, which was referred to as Procrustes approach in Smith et al. (2017). Canonical correlation analysis has also been used to map both languages to a shared vector space (Faruqui and Dyer, 2014; Lu et al., 2015). To reduce the need for supervision signals, Artetxe et al. (2017) use identical digits and numbers to form an initial seed dictionary and then iteratively refine their results until convergence. Zhang et al. (2017) apply adversarial training to align monolingual word vector spaces with no supervision. Conneau et al. (2018) improve the model by combining adversarial training and Procrustes approach, and their unsupervised approach could reach and even outperform state-of-the-art supervised approaches. In this work, we make further improvements and enhance the model proposed in (Conneau et al."
D18-1062,Q17-1010,0,0.102821,"tionaries via latent variable models and adversarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models. 1 Introduction Learning the representations of languages is a fundamental problem in natural language processing and most existing methods exploit the hypothesis that words occurring in similar contexts tend to have similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information. Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource languages and domains, crosslin"
D18-1062,K16-1002,0,0.0423749,"y learn the linear transformation between languages. Zhang et al. (2016b) improve the method by constraining the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization during training and 621 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 621–626 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics pθs D qфs source a reparameterized variational lower bound. VAEs have been successfully applied in several natural language processing tasks before (Zhang et al., 2016a; Bowman et al., 2016). GANs (Goodfellow et al., 2014) are another framework for estimating generative models via an adversarial process and have attracted huge attention. The basic strategy is to train a generative model and a discriminative model simultaneously via an adversarial process. Adversarial training technique for matching distribution has proven to be powerful in a variety of tasks (Bowman et al., 2016). Adversarial Autoencoder (Makhzani et al., 2015) is a probabilistic autoencoder that uses the GANs to perform variational inference. By combining a VAE with a GAN, Larsen et al. (2016) use learned featur"
D18-1062,D14-1162,0,0.0944644,"builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models. 1 Introduction Learning the representations of languages is a fundamental problem in natural language processing and most existing methods exploit the hypothesis that words occurring in similar contexts tend to have similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information. Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource langu"
D18-1062,N15-1104,0,0.0603586,"similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information. Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision. Artetxe et al. (2017) successfully learn bilingual word embeddings with 2 2.1 Related Work Bilingual Lexicon Induction Extracting bilingual lexica has been studied by researchers for a long time. Mikolov et al. (2013) first observe there is isomorphic structure among word embeddings trained separately on monolingual corpora and they learn the linear transformation between languages. Zhang et al. (2016b) improve the method by"
D18-1062,N18-1122,0,0.0299284,". The basic strategy is to train a generative model and a discriminative model simultaneously via an adversarial process. Adversarial training technique for matching distribution has proven to be powerful in a variety of tasks (Bowman et al., 2016). Adversarial Autoencoder (Makhzani et al., 2015) is a probabilistic autoencoder that uses the GANs to perform variational inference. By combining a VAE with a GAN, Larsen et al. (2016) use learned feature representations in the GAN discriminator as the basis for the VAE reconstruction objective. GANs have been applied in machine translation before (Yang et al., 2018; Lample et al., 2018). pθt qфt target Figure 1: Illustration of our model. φs and φt map the source and target word embeddings into latent variables. Discriminator D guides the two latent distributions to be the same. maximize the cosine similarity instead. They point out that adding an orthogonality constraint can improve performance and has a closed-form solution, which was referred to as Procrustes approach in Smith et al. (2017). Canonical correlation analysis has also been used to map both languages to a shared vector space (Faruqui and Dyer, 2014; Lu et al., 2015). To reduce the need fo"
D18-1062,P17-1179,0,0.494947,"the two latent distributions to be the same. maximize the cosine similarity instead. They point out that adding an orthogonality constraint can improve performance and has a closed-form solution, which was referred to as Procrustes approach in Smith et al. (2017). Canonical correlation analysis has also been used to map both languages to a shared vector space (Faruqui and Dyer, 2014; Lu et al., 2015). To reduce the need for supervision signals, Artetxe et al. (2017) use identical digits and numbers to form an initial seed dictionary and then iteratively refine their results until convergence. Zhang et al. (2017) apply adversarial training to align monolingual word vector spaces with no supervision. Conneau et al. (2018) improve the model by combining adversarial training and Procrustes approach, and their unsupervised approach could reach and even outperform state-of-the-art supervised approaches. In this work, we make further improvements and enhance the model proposed in (Conneau et al., 2018) with latent variable model and iterative training procedure. Lθ,φ = Eqφ (z|x) [log pθ (x|z)] − KL(qφ (z|x)||p0 (z)), (1) where KL refers to Kullback-Leibler divergence. 2.2 3.2 3 Proposed Approach In this sec"
D18-1062,N16-1156,0,0.165386,"Faruqui and Dyer, 2014; Xing et al., 2015). For low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision. Artetxe et al. (2017) successfully learn bilingual word embeddings with 2 2.1 Related Work Bilingual Lexicon Induction Extracting bilingual lexica has been studied by researchers for a long time. Mikolov et al. (2013) first observe there is isomorphic structure among word embeddings trained separately on monolingual corpora and they learn the linear transformation between languages. Zhang et al. (2016b) improve the method by constraining the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization during training and 621 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 621–626 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics pθs D qфs source a reparameterized variational lower bound. VAEs have been successfully applied in several natural language processing tasks before (Zhang et al., 2016a; Bowman et al., 2016). GANs (Goodfellow et al., 2014) are another framew"
D19-1086,D18-1048,0,0.07512,"4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate source contents at every decoding steps. 6 Conclusion In this paper, we propose to recognize the translated PAST and untransl"
D19-1086,D16-1096,0,0.044511,"sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promis"
D19-1086,C18-1232,0,0.0294723,"cally, an encoder first maps the source sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provi"
D19-1086,W17-4123,0,0.0805954,"Missing"
D19-1086,P16-1008,1,0.931586,"et al., 2015). Like human translators, NMT systems should have the ability to know the relevant source-side context for the current word (P RESENT), as well as recognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively,"
D19-1086,D19-1074,0,0.035318,"Missing"
D19-1086,D19-1087,0,0.0241637,"dynamic PAST and F UTURE. (a) Translation length v.s source length (b) BLEU v.s source length Figure 5: Comparison regarding source length. 5 gets a larger improvement when the input sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may b"
D19-1086,D17-1013,1,0.941176,"the encoder leverages N stacked identical layers to map the sentence into contextual representations: The NMT model is now able to employ the dynamic holistic context for better generation. 3.3 Learning PAST and F UTURE as Expected h(l) = EncoderLayer(h(l−1) ), Auxiliary Guided Losses To ensure that the dynamic routing process runs as expected, we introduce the following auxiliary guided signals to assist the learning process. where the superscript l indicates layer depth. Based on the encoded source representations hN , a decoder generates translation word by word. The Bag-of-Word Constraint Weng et al. (2017) propose a multitasking scheme to boost NMT by predicting the bag-of-words of target sentence using the Word Predictions approach. Inspired by 934 trained by minimizing the loss L(θ), where θ is the set of all the parameter of the proposed model: this work, we introduce a B OW constraint to encourage the PAST and F UTURE capsules to be predictive of the preceding and subsequent bag-ofwords regarding each decoding step respectively: L B OW M L(θ) = T 1X = − log pPRE (y≤t |ΩPt ) T t=0  − log pSUB (y≥t |ΩFt ) , where λ1 and λ2 are hyper-parameters. 4 where ppre (y≤t |ΩPt ) and psub (y≥t |ΩFt ) a"
D19-1086,N19-1359,1,0.823548,"ce sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provide a time-dependent holistic view of"
D19-1086,P18-2047,0,0.01658,"ger, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promising numbers of the G"
D19-1086,1983.tc-1.13,0,0.706531,"Missing"
D19-1086,C18-1276,0,0.0187648,"milar to Equation 6. The PAST and F UTURE representations are computed by weighted summation, which is similar to Equation 4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate so"
D19-1086,D15-1166,0,0.0656013,"urce contents in the Section 3.3. Note that we employ G DR at every decoding step t to obtain the time-dependent PAST and F UTURE and omit the subscript t for simplicity. In the dynamic routing process, each vector output of capsule j is calculated with a non-linear (7) where Wb ∈ Rd+dc ∗2 and w ∈ Rdc are learnable parameters. Instead of using simple scalar prod> Ω (Sabour et al., 2017), which uct, i.e., bij = vij j could not consider the current decoding state as a condition signal, we resort to the MLP to take zi into account inspired by MLP-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). That is why we call it “guided” dynamic routing. 2 Note that unlike Sabour et al. (2017), where each pair of input capsule i and output capsule j has a distinct transformation matrix Wij as their numbers are predefined (I × J transformation matrices in total), here we share the transformation matrix Wj of output capsule j among all the input capsules due to the varied amount of the source words. So there are J transformation matrices in our model. 933 Output probabilities Softmax Algorithm 1 Guided Dynamic Routing (G DR) Past Capsules Input: Encoder hidden state h, current decoding hidden Ou"
D19-1086,1981.tc-1.7,0,0.677566,"Missing"
D19-1086,D18-1350,0,0.0406832,"Missing"
D19-1086,Q18-1011,1,0.439939,"ecognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively, this less interpretable nature is probably not the best way to model and exploit the dynamic PAST and F UTURE. We argue that an explicit separation of the source wor"
D19-1086,P02-1040,0,\N,Missing
D19-1086,W04-1013,0,\N,Missing
D19-1086,Q19-1006,0,\N,Missing
D19-1086,P16-1162,0,\N,Missing
D19-1086,D15-1229,0,\N,Missing
D19-1429,W17-6002,0,0.016438,"fusion model (with the blue α, see §4), where the green part belongs to the source model, the orange part belongs to the target model and the white part is common. Better viewed in color. eling schemes to model both the sample-level and element-level domain relevance. • Empirical evidences and analyses are provided on three different tasks in two different languages, which verify the effectiveness of our method. 2 Knowledge Distillation for Adaptation Knowledge distillation (KD), which distills the knowledge from a sophisticated model to a simple model, has been employed in domain adaptation (Bao et al., 2017; Meng et al., 2018). Recently, online knowledge distillation(Furlanello et al., 2018; Zhou et al., 2018) is shown to be more effective, which shares lower layers between the two models and trains them simultaneously. For sequence labeling domain adaptation, we utilize the online knowledge distillation method to distill knowledge from the source model to improve the target model, denoted as basicKD, which is depicted in Figure 2. We use the BiLSTM-CRF architecture (Huang et al., 2015), for both the source model and the target model, and share the embedding layer between them. Notations For the"
D19-1429,P07-1056,0,0.158191,"such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2010), which is suitable to handle this issue. Following Daume III (2007), we focus on the supervised domain adaptation setting, which utilizes large-scale annotated data from the source domain and smallscale annotated data from the target domain. For sequence labeling tasks, each sample is usually a sentence, which consists of a seq"
D19-1429,P17-1110,0,0.252815,"are rising soooo fast! Alas as time goes by, hair’s gone. Rock to 204 Section next week! Table 1: Tweets from the social media domain have different degrees of relevance to the source domain (news). Within each case, the bold part is strongly relevant and the italic part is weakly relevant. Introduction Sequence labeling tasks, such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2"
D19-1429,P07-1033,0,0.344686,"Missing"
D19-1429,P07-1034,0,0.10637,"to unsupervised or semi-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important ch"
D19-1429,P17-1060,0,0.0254509,"n. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test set with varying tar"
D19-1429,W06-0115,0,0.0878241,"Missing"
D19-1429,D18-1226,0,0.208759,"style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Previous methods source !# target &quot; source! target source target"
D19-1429,C12-2073,0,0.029265,"ets similar to the news domain (i.e. strongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfe"
D19-1429,D14-1093,0,0.0232295,"ws domain (i.e. strongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer le"
D19-1429,J93-2004,0,0.0649324,"Missing"
D19-1429,D15-1064,0,0.0877344,"Missing"
D19-1429,W17-2612,0,0.109955,"me tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Previous methods source !# target &quot; source! ta"
D19-1429,C18-1232,0,0.0299762,"tion. Element-level Relevance To acquire the element-level relevance, we employ the domain representation q ∈ R2dh (dh is the dimension of the Bi-LSTM) and calculate the similarity between the element representation and the domain representation. We incorporate two methods to get q: (1) Domain-q: q is a trainable domain specific vector, where every element within a domain share the same q; (2) Sample-q: q is the domain relevant feature extracted from each sample, where every element within a sample share the same q. Because of the superiority of the capsule network modeling abstract features (Gong et al., 2018; Yang et al., 2018), we use it to capture the domain relevant features within a sample. We incorporate the same bottom-up aggregation process as Gong et al. (2018) and the encoded vector is regarded as q: q = Capsule(h) (5) where h is the hidden state matrix of a sample. The similarity calculation formula is the matrix dot 2 : wjelem = q&gt; Bhj (6) where hj is the hidden states of the j th element and wjelem is the relevance weight of it. B ∈ R2dh ×2dh is a trainable matrix. 2 We also try dot and MLP, while matrix dot get better performance with fewer parameters. 4199 3.2 Sample-level Relevance"
D19-1429,D18-1498,0,0.0203295,"m the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test se"
D19-1429,D11-1141,0,0.0158297,"Missing"
D19-1429,D17-1038,0,0.021029,"the target domain lexicons (Liu et al., 2014; Zhang et al., 2014), unlabeled (Liu and Zhang, 2012) or partial-labeled target domain data (Liu et al., 2014) to boost the sequence labeling adaptation performance, which belong to unsupervised or semi-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses o"
D19-1429,D18-1350,0,0.0332048,"Relevance To acquire the element-level relevance, we employ the domain representation q ∈ R2dh (dh is the dimension of the Bi-LSTM) and calculate the similarity between the element representation and the domain representation. We incorporate two methods to get q: (1) Domain-q: q is a trainable domain specific vector, where every element within a domain share the same q; (2) Sample-q: q is the domain relevant feature extracted from each sample, where every element within a sample share the same q. Because of the superiority of the capsule network modeling abstract features (Gong et al., 2018; Yang et al., 2018), we use it to capture the domain relevant features within a sample. We incorporate the same bottom-up aggregation process as Gong et al. (2018) and the encoded vector is regarded as q: q = Capsule(h) (5) where h is the hidden state matrix of a sample. The similarity calculation formula is the matrix dot 2 : wjelem = q&gt; Bhj (6) where hj is the hidden states of the j th element and wjelem is the relevance weight of it. B ∈ R2dh ×2dh is a trainable matrix. 2 We also try dot and MLP, while matrix dot get better performance with fewer parameters. 4199 3.2 Sample-level Relevance To acquire the samp"
D19-1429,D18-1041,0,0.0265729,"in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test set with varying target training data si"
D19-1429,E14-1062,0,0.146862,"rongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Prev"
D19-1429,C18-1269,0,0.0312881,"i-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequenc"
D19-1429,P18-1144,0,0.0412787,"st! Alas as time goes by, hair’s gone. Rock to 204 Section next week! Table 1: Tweets from the social media domain have different degrees of relevance to the source domain (news). Within each case, the bold part is strongly relevant and the italic part is weakly relevant. Introduction Sequence labeling tasks, such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2010), which is suitable"
D19-1429,D17-1079,1,0.870722,"Missing"
K17-1011,P02-1040,0,\N,Missing
K17-1011,D08-1024,1,\N,Missing
K17-1011,P13-1126,0,\N,Missing
K17-1011,P01-1067,0,\N,Missing
K17-1011,C04-1072,0,\N,Missing
K17-1011,P11-2031,0,\N,Missing
K17-1011,P05-1033,1,\N,Missing
K17-1011,J03-1002,0,\N,Missing
K17-1011,K15-1007,0,\N,Missing
K17-1011,2013.mtsummit-papers.7,0,\N,Missing
K17-1011,N12-1026,0,\N,Missing
L16-1104,C12-1018,0,0.0325284,"Missing"
L16-1104,D12-1133,0,0.0180185,"inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentence"
L16-1104,W08-2102,0,0.068385,"Missing"
L16-1104,P05-1022,0,0.122212,"Missing"
L16-1104,A00-2018,0,0.56498,"Missing"
L16-1104,D14-1082,0,0.266842,"al Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural dependency parsing (Table 1). First,"
L16-1104,Q13-1033,0,0.0129105,"). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural d"
L16-1104,P04-1013,0,0.114156,"Missing"
L16-1104,P08-1067,0,0.0719478,"Missing"
L16-1104,J93-2004,0,0.0583985,"on in the action layer, and lj is the jth label in the label layer. We adopt a greedy decoding strategy in the hierarchical parsing process. In each parsing step, the action type ai with the highest probability is first selected, and then the constituent label lj with the highest probability is selected given the optimal action type ai . As in the baseline parser, we adopt the cross-entropy loss as our training objective: L(θ) = − X log p(yi,j |x, Acts) + yi,j ∈A 5. 5.1. λ k θ k2 2 (10) Experiments Set-up We conduct our experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). Following the standard splits of WSJ, sections 2–21 are used as the labeled training data, section 24 is used as the development data and section 23 is used as the evaluation data. Ten-fold jackknifing (Collins, 2000) is used to automatically assign POS tags to the training data. The SVMTool is used as the POS-tagger1 . 5.2. Parameters We carry out a development experiment to measure the correlation between hidden layer size and constituent parsing accuracies. From Table 3, we can see that both the baseline neural parser and the hierarchical neural parser achieve higher parsing accuracies wi"
L16-1104,N06-1020,0,0.105368,"Missing"
L16-1104,J08-4003,0,0.0283774,"89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging"
L16-1104,N07-1051,0,0.151924,"Missing"
L16-1104,W05-1513,0,0.402545,"parser by using a hierarchical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of"
L16-1104,P12-1046,0,0.0380018,"Missing"
L16-1104,P13-1045,0,0.0608923,"Missing"
L16-1104,P07-1080,0,0.167269,"mapping matrix from hidden layer to the action layer and da is the number of action types. W3i ∈ Rdlabel ×dh is the mapping matrix from the hidden layer to the label layer. The probability of a labeled action yi,j , given its history Acts and input x, is computed as: p(yi,j |x, Acts) = p(ai |x, Acts) × p(lj |x, Acts, ai ) (7) Neural where Model Collins (1999) Charniak (2000) Charniak and Johnson (2005)‡ Huang (2008)‡ McClosky et al. (2006)‡ Shindo et al. (2012) Sagae and Lavie (2005) Petrov and Klein (2007) Carreras et al. (2008) Zhu et al. (2013) Zhu et al. (2013) + padding Henderson (2004)‡ Titov and Henderson (2007) Collobert (2011) Billingsley and Curran (2012) Socher et al. (2013)‡ Legrand and Collobert (2014) Watanabe and Sumita (2015) F1 88.2 89.6 91.1 91.7 92.1 92.4 86.0 90.1 91.1 89.9 90.4 90.1 90.0 87.9 84.9 90.4 88.3 90.7 Speed 3.5 5.7 This Work This Work + hierarchical 89.13 89.06 133.6 320.2 3.7 6.2 100.7 89.5 31.7 6.1 22.0 1.8 i p(ai |x, Acts) = eoact P k eoact (8) ak ∈GEN(Acts) j p(lj |x, Acts, ai ) = eolabel (ai ) P k eolabel (ai ) (9) Table 4: Comparisons with previous work. ‡: reranking model. Speed: sentences per second. lk ∈GEN(Acts) Here ai is the ith action in the action layer, and lj"
L16-1104,W09-3825,1,0.909876,"ing node with constituent label X whose child is s0 ; push the new node back onto the stack. • LEFT/RIGHT-X: pop the top two nodes s1 , s0 off the stack; generate a binary-branching node with constituent label X whose left child is s1 and right child is s0 , with the left (LEFT)/right (RIGHT) child as its head; push the new node back to the stack. The shift-reduce actions only build binarized trees. As a result, a binarization process is necessary to convert the Penn Treebank into binarized trees. During this process, temporary nodes are constructed. To accommodate for binarization, we follow Zhang and Clark (2009), adding counterparts to LEFT/RIGHT-X for temporary nodes, namely LEFT/RIGHT-TEMP-X. 3. h = (W1 x + b1 )3 (3) a∈A Here A is the set of all gold labeled actions in the training data. Mini-bached AdaGrad (Duchi et al., 2011) and dropout (Srivastava et al., 2014) are used for optimization. Hierarchical Output Neural Network Due to its large hidden and output layer sizes, the vanilla neural constituent parser is much slower than the dependency counterpart. The main computation cost is the mapping from hidden layer to output layer. Motivated by the hierarchical neural language model (Mnih and Hinto"
L16-1104,P11-2033,1,0.811029,"archical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency par"
L16-1104,P15-1117,1,0.88504,"Missing"
L16-1104,P13-1043,1,0.834198,"Missing"
L16-1104,P15-1113,0,\N,Missing
L18-1145,D14-1179,0,0.03576,"Missing"
L18-1145,C12-1059,0,0.0335631,"Missing"
L18-1145,Q13-1033,0,0.0129566,"the experimental results indicate that our method can achieve +1.06 BLEU improvements. 2. Related Work To the best of our knowledge, Goldberg et. al. (2012) first define the concept of dynamic oracle and propose an online algorithm for parsing problems, , which provides a set of optimal transitions for every valid parser configuration. For configurations which are not part of a gold derivation, their dynamic oracle permits all transitions that can lead to a tree with minimum loss compared to the gold tree. Based on their approach, several other methods using dynamic oracle have been proposed (Goldberg and Nivre, 2013) (G´omezRodrıguez et al., 2014). However, their work in the field of parsing cannot be directly applied in neural machine translation. To mitigate the discrepancy between training and inference, Daume et al. (2009) introduce SEARN, which aims to tackle the problems that training examples might be different from actual test examples. They show that structured prediction can be mapped into a search setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two"
L18-1145,D14-1099,0,0.0274225,"Missing"
L18-1145,D13-1176,0,0.0203397,"setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two methods respectively, namely language model guided scheduled sampling and pre-trained model guided scheduled sampling. 3.1. Neural Machine Translation Neural machine translation aims to directly model the conditional probability p(Y |X) of translating a source sentence, x1 , ..., xn , to a target sentence, y1 , ..., ym . Generally, it accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013). Basically, the encoder generates a context vector for each source sentence and then the decoder outputs a translation, one target word at a time. During training when we are decoding, we always provide the model with true previous token at every time step. Minibatch stochastic gradient descent is applied to look for a set of parameters θ∗ that maximizes the log likelihood of producing the correct target sentence. Specifically, given a batch of training pairs {(X i , Y i )}, we aim to find θ∗ which satisfies: X log p(Y i |X i ; θ) (1) θ∗ = arg max θ (X i ,Y i ) Whereas during inference the mo"
L18-1145,P02-1040,0,0.102773,"Missing"
L18-1145,C14-1008,0,0.0422661,"ever, there are certain limitations in Scheduled Sampling and we propose two dynamic oracle-based methods to improve it. We manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle’s demonstrations. Experimental results show that the proposed approaches improve translation quality over standard NMT system. Keywords: machine translation, dynamic oracle, language model 1. Introduction Neural networks have been widely used contemporarily and have achieved great performance on a variety of fields like sentiment analysis (Santos and Gattit, 2014) and visual object recognition (Ciregan et al., 2012). For sequential problems, recurrent neural networks can be applied to process sequences. To address issues like long term dependencies in the data (Bengio et al., 1994), the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) can be used to tackle the problem (Cho et al., 2014). A straightforward application of the LSTM and GRU architecture have already shown impressive performance in several difficult tasks, including machine translation (Sutskever et al., 2014), and image captioning (Vinyals et a"
N16-1148,E14-2007,0,0.09776,"e actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be"
N16-1148,W02-1020,0,0.713831,""") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be improved after every update, Despite the success of this left-to-right framework, one potential weakness is that it is difficult to modify critical translation errors at the end of a sentence. Critical translation errors are those errors that has large impact on the translation of other words or phrases. When a translation ambiguity occurs at the end of a sentence while it causes translation errors at the beginning, modifying this critical errors first may bring g"
N16-1148,2014.amta-workshop.3,0,0.0144391,"the algorithm in a typical phrase-based machine translation (Koehn et al., 2003). The only exception is that it makes an extra comparison between each translation option and previous PR pairs, which ignores all the phrases that overlap with the source side of a PRP. As a result, a lot of translation options are ignored, which makes the search space much smaller than standard decoding. In this way, we could guarantee that all the PRPs are correctly translated and the whole process can be carried out in real-time. The system could collect all PRPs and adapt the models using methods described in Germann (2014) or Marie (2015). In our current implementation, we mainly focus on the picking and revising step and leave model adaptation as future work. 3 Automatic Suggestion Models To further reduce the human actions, we propose to use automatic suggestion models for the picking and revising step, respectively. Such models can offer suggestions to users in both picking and revising steps. Because both picking and revising actions are performing selections from multiple candidates, we use classifier-based approaches to model these two steps. In the following subsections, we will introduce how we define t"
N16-1148,D13-1025,0,0.0170353,"er the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be improved after every update, Despite the success of this left-to-right framework, one potential weakness is that it is difficult to modify critical translation errors at the end of a sentence. Critical translation errors are those errors that has large impact on the translation of other words or phrases. When a translation ambiguity occurs at the end of a sentence while it causes translation errors at the beginning, modifying this critical errors first may bring great positive effects on previous parts of the translation, which m"
N16-1148,D14-1130,0,0.069783,"ight have a large influence to the translation of their context. To make the picking step easier to be integrated into MT system, we limit the selection of translation errors to be those phrases in the previous PRcycle output. If it's the first PR-cycle, then those errors come from phrases used to generate the baseline translation. For more convenient user interactions, in our PRIMT system, critical errors can be picked from both the source and target side by simply a mouse click on it. The correspondence/alignment between source and target phrases are visualized for easier human observation. Green et al. (2014) demonstrated that performing post-editing, i.e. directly editing the translation errors, could get acceptable translations faster than performing left-to-right IMT. Such result also indicates that identifying critical translation errors is not a difficult task for human to perform. 2.3 Revising In the revising step, the users revise the translation of sji by selecting the correct translation t′ from the translation table, or manually add one if there is no 1 j si is the phrase that covers the source words from index i to j, and translated into t. 1242 A pick-revise pair (PRP), (sji , t′ ), is"
N16-1148,N03-1017,0,0.133198,"for human to perform. 2.3 Revising In the revising step, the users revise the translation of sji by selecting the correct translation t′ from the translation table, or manually add one if there is no 1 j si is the phrase that covers the source words from index i to j, and translated into t. 1242 A pick-revise pair (PRP), (sji , t′ ), is obtained after a PR cycle for a source sentence. We use a constrained decoder to search for the best translation with the previous PRPs as constraints. The constrained search algorithm is similar to the algorithm in a typical phrase-based machine translation (Koehn et al., 2003). The only exception is that it makes an extra comparison between each translation option and previous PR pairs, which ignores all the phrases that overlap with the source side of a PRP. As a result, a lot of translation options are ignored, which makes the search space much smaller than standard decoding. In this way, we could guarantee that all the PRPs are correctly translated and the whole process can be carried out in real-time. The system could collect all PRPs and adapt the models using methods described in Germann (2014) or Marie (2015). In our current implementation, we mainly focus o"
N16-1148,P09-4005,0,0.0210441,"trate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Ala"
N16-1148,W00-0507,0,0.332552,"ation error (Pick) and revising the translation (Revise). The picked phrase could be at any position of the sentence, which improves the efficiency of human computer interaction. We also propose automatic suggestion models for the two actions to further reduce the cost of human interaction. Experiment results demonstrate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which"
N16-1148,D15-1120,0,0.0256354,"Missing"
N16-1148,W07-0737,0,0.0270569,"The users could also type a new translation through a separated input area. Model Adaptation (sji ,t′ ) (sji ,t′ ) no Revising (sji ,t) Picking yes Stop Figure 1: An overview of PRIMT framework. 2.4 Decoder and Model Adaptation 2.2 Picking In the picking step, the users pick the wronglytranslated phrase, (sji ,t)1 , to be revised. The picking process aims at finding critical errors in the translation, caused by errors in the translation table or inherent translation ambiguities. The more critical the error is, the larger translation quality improvement can be achieved by correcting the error (Mohit and Hwa, 2007). Critical errors might have a large influence to the translation of their context. To make the picking step easier to be integrated into MT system, we limit the selection of translation errors to be those phrases in the previous PRcycle output. If it's the first PR-cycle, then those errors come from phrases used to generate the baseline translation. For more convenient user interactions, in our PRIMT system, critical errors can be picked from both the source and target side by simply a mouse click on it. The correspondence/alignment between source and target phrases are visualized for easier"
N16-1148,J03-1002,0,0.00572591,"select all correct translation options as positive instances for the revising step, and randomly sample the same number of wrong translation options to be negative instances. Specifically, translation options that are used by the baseline system are included as negative instances. 3.2.2 RSM Features The features used for RSM are showed in Table 3. For translations of a given source phrase, there is no need to compare their source-side information because these translation options share the same source phrase and context. So these features mainly focus 2 We trained word alignments with Giza++(Och and Ney, 2003) on estimating the translation quality of a given translation option. As a result, features for RSM only including the scores for TM, LM and LRM, etc, which are simpler compared to PSM. Category TM LM LRM count Lexical Description TM scores of current translation option LM score of current translation option LM score of each target word LRM scores of current translation option Target word count Target words 4.2 Methodology Table 3: Features for the RSM 4 Experiments 4.1 4.1.1 Experiment Settings Translation Settings Through out the experiments, we use an in-house implementation of the phrase-b"
N16-1148,P02-1040,0,0.110377,"corporate our PRIMT framework into the translation system. The parallel data for training the translation model includes 8.2 million sentences pairs from LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T10, LDC2007T09. A 5gram language model is trained with MKN smoothing (Chen and Goodman, 1999) on Xinhua portion of Gigaword which contains 14.6 million sentences. We use a combination of NIST02 and NIST03 to tune the MT system parameters and train the suggestion models. We test the system on NIST04 and NIST05 data. The translation results are evaluated with case insensitive 4-gram BLEU (Papineni et al., 2002). Our baseline phrase-based MT system has comparable performance with the open source toolkit Moses (Koehn et al., 2003). 4.1.2 network has one hidden layer of 80 nodes, with sigmoid function as the activation function. We use one-hot representation for the source and target word features when using the maximum entropy and SVM model, and use pre-trained word embeddings (Mikolov et al., 2013) for the neural model. Classification Settings We use three classification models to model the automatic suggestion models: the maximum entropy model, the SVM model and the neural network model. We use a ma"
N18-1116,P16-1100,0,0.233867,"oth word-level and character-level information can be helpful for generating better representations, current research which tries to exploit both word-level and character-level information only composed the word-level representation by character embeddings with the word boundary information (Ling et al., 2015b; Costa-juss`a and 1284 Proceedings of NAACL-HLT 2018, pages 1284–1293 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Fonollosa, 2016) or replaces the word representation with its inside characters when encountering the out-of-vocabulary words (Luong and Manning, 2016; Wu et al., 2016). In this paper, we propose a novel encoder-decoder model that makes use of both character and word information. More specifically, we augment the standard encoder to attend to individual characters to generate better source word representations (§3.1). We also augment the decoder with a second attention that attends to the source-side characters to generate better translations (§3.2). To demonstrate the effectiveness of the proposed model, we carry out experiments on three translation tasks: Chinese-English, EnglishChinese and English-German. Our experiments show that: (1) t"
N18-1116,W17-4712,0,0.0236227,"improve the model by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-wor"
N18-1116,P16-2058,0,0.0380798,"Missing"
N18-1116,W17-4739,0,0.0442815,"Missing"
N18-1116,P16-1162,0,0.204501,"trong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respectively. The meaning of the whole word, “to be wounded”, is fairly compositional. But this compositionality is ignored if the whole word"
N18-1116,D17-1013,1,0.828696,"three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respe"
N18-1116,Q17-1026,0,0.151135,"e word or sub-word boundaries can be non-trivial. For languages like Chinese and Japanese, a word segmentation step is needed, which must usually be trained on labeled data. For languages like English and German, word boundaries are easy to detect, but subword boundaries need to be learned by methods like BPE. In both cases, the segmentation model is trained only in monolingual data, which may result in units that are not suitable for translation. On the other hand, there have been multiple efforts to build models operating purely at the character level (Ling et al., 2015a; Yang et al., 2016; Lee et al., 2017). But splitting this finely can increase potential ambiguities. For example, the Chinese word 红茶 (hong-cha) means “black tea,” but the two characters means “red” and “tea,” respectively. It shows that modeling the character sequence alone may not be able to fully utilize the information at the word or sub-word level, which may also lead to an inaccurate representation. A further problem is that character sequences are longer, making them more costly to process with a recurrent neural network model (RNN). While both word-level and character-level information can be helpful for generating better"
N18-1116,C16-1288,0,0.0317945,"Missing"
N19-1192,D17-1156,0,0.131636,"oints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement on a machine reading experiment as well. 1 Introduction Neural Machine Translation (NMT) (Cho et al., 2014; Sutskever et al., 2014) has been rapidly developed during the past several years. For further performance improvement, deeper and more expressive structures (Johnson et al., 2017; Barone et al., 2017b; Gehring et al., 2017; Vaswani et al., 2017) have been exploited. However, all of these models have more than hundreds of millions of parameters, which makes the training process more challenging. During the training of NMT models, we notice the following two problematic phenomena: First, the training process is unstable. This is evidenced by the decreasing of training loss with ∗ Corresponding Author. fluctuate performance on the validation set. Second, the performance on validation set usually begins to worsen after several epochs, while the training loss keeps decreasing, which suggests t"
N19-1192,W17-4710,0,0.107698,"oints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement on a machine reading experiment as well. 1 Introduction Neural Machine Translation (NMT) (Cho et al., 2014; Sutskever et al., 2014) has been rapidly developed during the past several years. For further performance improvement, deeper and more expressive structures (Johnson et al., 2017; Barone et al., 2017b; Gehring et al., 2017; Vaswani et al., 2017) have been exploited. However, all of these models have more than hundreds of millions of parameters, which makes the training process more challenging. During the training of NMT models, we notice the following two problematic phenomena: First, the training process is unstable. This is evidenced by the decreasing of training loss with ∗ Corresponding Author. fluctuate performance on the validation set. Second, the performance on validation set usually begins to worsen after several epochs, while the training loss keeps decreasing, which suggests t"
N19-1192,Q17-1024,0,0.530165,"0; T0 = −1, Tˆ0 = −1; θT = ∅; θ00 = θ0 ; L0 (θ) = L(θ) while not reach stopping criteria do repeat if Tˆk = Tk then Lt (θ) = L(θ) else Lt (θ) = L(θ) + LW-KD (θ) 20 minimize Lt (θ) and update θt ; 0 0 θt = αθt−1 + (1 − α)θt ; t=t+1; until t mod ∆T == 0; Tk+1 = t; evaluate on validation set; if get better checkpoint then Tˆk+1 = t; if use EMA as teacher then 0 θT = θt ; else θT = θt ; 21 else 9 10 11 12 13 14 15 16 17 18 19 Tˆk+1 = Tˆk ; 22 k = k + 1; 23 Integrated with Mean Teacher Knowledge distillation usually works better when teacher models have better performance. As Tarvainen and Valpola (2017) proposed in their work, averaging model parameters over training steps tends to produce a more accurate model that using final parameters directly. They called this method as Mean Teacher. Following Tarvainen and Valpola (2017), besides updating parameters, we maintain the exponential moving average (EMA) of the model parameters as: 0 0 θt = αθt−1 + (1 − α)θt , (9) where t is the update step, θ is the parameters of the 0 training model and θ the parameters of EMA. α is the decay weight which is close to 1.0, and typically in multiple-nines range, i.e., 0.999, 0.9999. By doing so, at each time"
N19-1192,W18-2705,0,0.0477999,"n NMT Regularization has broad applications in training NMT models to improve performance and avoid over-fitting. There are some common regularization techniques, such as L2 normalization and dropout (Srivastava et al., 2014). These methods are simple and easy to implement but need carefully tuning on the validation set. These methods are also orthogonal to our method. There are also some works to exploit regularization techniques in fine tuning of NMT model. Barone et al. (2017a) proposed a tuneout method which randomly replaces columns of weight matrices of out-of-domain parameter matrices. Khayrallah et al. (2018) shared similar training object with us, as they computed the KL divergence between out-of-domain and in-domain model. Both of their works request a pre-trained teacher model, while we are work on a more general training problem which does not require such kind of model. 6.2 Online Knowledge Distillation While traditional knowledge distillation requires a static, pre-trained teacher model, online knowledge distillation tends to overcome this problem by selecting or generating a teacher dynamically from scratch. To the best of our knowledge, Zhang et al. (2017) is the first trial to replace the"
N19-1192,D16-1139,0,0.131576,"on, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by θ) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model’s output probability p(y|x; θ), knowledge distillation uses the teacher model’s distribution q(·|x) as “soft targets” and optimizes the loss: LKD (θ) = − |V| X q(y = k|x; θT ) (5) k=1 log p(y = k|x; θ), where θT parameterizes the teacher model and p(·|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target 1933 validation score update teacher models (darker means better) knowledge distillation training steps direction Checkpoints: Teacher Models: ?? ′ ?? ′ +1 ?? ′ +2 ?? ′′ ?? ′′ +1 Figure 1: Illustration of online distillation from checkpoints(ODC). Darker color means better performance on validation data. In validation step Tk0 , ODC selects the current best checkpoints as the teacher model; while in the next validation step Tk0 +1 , the training generates a better checkpo"
N19-1192,P84-1044,0,0.354513,"Missing"
N19-1192,2015.iwslt-evaluation.11,0,0.177643,"Missing"
N19-1192,W17-4739,0,0.0208349,"anning (2015) 5 , which has 133K sentence pairs, with 2.70M English words and 3.31M Vietnamese words. We use the released validation and test set, which has 1553 and 1268 sentences respectively. Following the settings in Huang et al. (2017), the Vietnamese and English vocabulary size are 7,709 and 17,191, respectively. For WMT17 English-Turkish translation task, We use the pre-processed data released by WMT176 . It has 207K sentence pairs, with 5.21M English words and 4.63 Turkish words. We use newstest2016 as our validation set and newstest2017 as the test set. We use joint BPE segmentation (Sennrich et al., 2017) to process the whole training data. The merge operations are 16K. Implementation Details Without specific statement, we follow the transformer base v1 hyperparameters settings 7 , with 6 layers in both encoder and decoder, 512 hidden units and 8 attention heads in multi-head attention mechanism and 2048 hidden units in feed-forward layers. Parameters are optimized using Adam(Kingma and Ba, 2014). The initial learning rate is set as 0.1 and scheduled according to the method proposed in Vaswani et al. (2017), with warm-up steps as 4000. We periodically evaluate the training model on the validat"
N19-1192,W16-2323,0,0.135357,"worsen after several epochs, while the training loss keeps decreasing, which suggests the model being at risk of over-fitting. In order to alleviate these issues, the common practice is to periodically evaluate models on a held-out set (with each evaluated model saved as a checkpoint). Training is terminated when m consecutive checkpoints show no improvement and select the checkpoint with best evaluation score as the final model. Further improvement can be achieved by utilizing more checkpoints, by smoothing, which averages these checkpoints’ parameters to generate more desirable parameters (Sennrich et al., 2016a); or by ensemble, which averages these checkpoints’ output probabilities at every step during inference (Chen et al., 2017). However, we notice that all of these methods have a limitation. Once the training process gets parameters with poor performance, selecting, smoothing or ensemble from the checkpoints in this process may have limited generalization performance as well. We impute the limitation to the “offline” property of these methods. In other words, only employing checkpoints after training cannot affect the original training process. In this paper, we propose to utilize checkpoints"
N19-1192,P16-1162,0,0.382409,"worsen after several epochs, while the training loss keeps decreasing, which suggests the model being at risk of over-fitting. In order to alleviate these issues, the common practice is to periodically evaluate models on a held-out set (with each evaluated model saved as a checkpoint). Training is terminated when m consecutive checkpoints show no improvement and select the checkpoint with best evaluation score as the final model. Further improvement can be achieved by utilizing more checkpoints, by smoothing, which averages these checkpoints’ parameters to generate more desirable parameters (Sennrich et al., 2016a); or by ensemble, which averages these checkpoints’ output probabilities at every step during inference (Chen et al., 2017). However, we notice that all of these methods have a limitation. Once the training process gets parameters with poor performance, selecting, smoothing or ensemble from the checkpoints in this process may have limited generalization performance as well. We impute the limitation to the “offline” property of these methods. In other words, only employing checkpoints after training cannot affect the original training process. In this paper, we propose to utilize checkpoints"
N19-1192,N18-1122,0,0.0274376,"former.py 8 We set k = 5 in this experiments. 1936 as the final model. In this case, checkpoints may have better performance but higher variance which could be harmful to parameters averaging. SYSTEM newsdev2017 newstest2017 21.96 22.24 23.01 23.37 24.22 Zhang et al. (2018c) baseline ODC • best-k-ensemble: Do ensemble inference (average the output probabilities) with the best k checkpoints (Chen et al., 2017). Table 2: Case-sensitive BLEU scores on WMT17 Chinese-English Translation As shown in Table 1, our baseline is comparable to the other two recent published results (Zhang et al. (2018b), Yang et al. (2018)). In consistent with Chen et al. (2017), using checkpoints for smoothing or ensemble does improve the baseline system. Using EMA parameters also improve the baseline system as well, which is in consist with (Tarvainen and Valpola, 2017). Compared to the baseline, our approach ODC brings translation improvement across different test sets and achieves 42.48 BLEU scores on average(+1.09 BLEU v.s. baseline). This result confirms that using best checkpoint as teacher indeed helps improving the performance of the translation model. Besides, ODC is comparable to the best results among smoothing and"
N19-1192,P18-1166,0,0.08734,"k checkpoints, instead of the last k, 4 http://data.statmt.org/wmt18/translationtask/preprocessed/zh-en/ 5 https://github.com/tefan-it/nmt-en-vi 6 http://data.statmt.org/wmt17/translationtask/preprocessed/tr-en/ Evaluation on Chinese-English Translation Tasks 7 https://github.com/tensorflow/tensor2tensor/blob/v1.3.0/ tensor2tensor/models/transformer.py 8 We set k = 5 in this experiments. 1936 as the final model. In this case, checkpoints may have better performance but higher variance which could be harmful to parameters averaging. SYSTEM newsdev2017 newstest2017 21.96 22.24 23.01 23.37 24.22 Zhang et al. (2018c) baseline ODC • best-k-ensemble: Do ensemble inference (average the output probabilities) with the best k checkpoints (Chen et al., 2017). Table 2: Case-sensitive BLEU scores on WMT17 Chinese-English Translation As shown in Table 1, our baseline is comparable to the other two recent published results (Zhang et al. (2018b), Yang et al. (2018)). In consistent with Chen et al. (2017), using checkpoints for smoothing or ensemble does improve the baseline system. Using EMA parameters also improve the baseline system as well, which is in consist with (Tarvainen and Valpola, 2017). Compared to the"
N19-1259,D10-1101,0,0.082814,"ons for different targets in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double propagation to expand opinion targets and opinio"
N19-1259,D17-1310,0,0.619841,"s “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting them as pairs is significant for ABSA. Additionally, in Figure 1, the list of pairs extracted from the example review can be considered to be an extractive pair-wise opinion summarization. Considering the significance of the pairs in reviews and promising results of targets extraction in previous works, in this paper, we propose a new subtask for ABSA named T"
N19-1259,D15-1168,0,0.233023,"Missing"
N19-1259,D14-1162,0,0.0808095,"Missing"
N19-1259,S15-2082,0,0.507654,"Missing"
N19-1259,S14-2004,0,0.832884,"] • restaurant : [excellent] Figure 1: The upper part is a restaurant review and the lower part shows the pairs of extracted opinion targets (in red) and opinion words (in blue). Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2007; Liu, 2012), has drawn increasing attention of researchers and industries in recent years. It can provide valuable information from user-generated reviews. However, sentiment analysis at sentence level or document level sometimes cannot provide more detailed information, thus a finer-grained task, Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014), is proposed to identify the opinions of a specific target or aspect ∗ Corresponding author. in reviews. ABSA consists of multiple subtasks including aspect category detection, opinion target extraction, aspect level sentiment classification etc. Opinion target extraction (OTE) and opinion words extraction (OWE) are two such fundamental subtasks. Opinion targets, sometimes called aspect terms, are the words or phrases in the sentence representing features or entities towards which users show attitude. Opinion words (or opinion terms) refer to those terms used to express attitude explicitly. F"
N19-1259,J11-1002,0,0.338747,"erate target-specific context representations for different targets in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double"
N19-1259,W95-0107,0,0.253098,"handcrafted templates. Moreover, in these two methods, the process of detecting opinion words and the process of discovering correspondence is separated into two tasks, which suffers from error propagation. Our model for TOWE aims at detecting the corresponding opinion words in one step with sequence labeling. 3 3.1 Our Methods Task Formulation Given a sentence s = {w1 , w2 , . . . , wi , . . . , wn } consisting of n words, and a opinion target t in the sentence, the task is to make sequence labelling on the sentence to extract the target-oriented opinion words. We use the BIO tagging scheme (Ramshaw and Marcus, 1995) on this task. For each word wi in the sentence s, it should be tagged as yi ∈ {B, I, O} (B: Beginning, I: Inside, O: Others). For example, for different opinion targets, the sentence “Waiters are very friendly and the pasta is out of this world .” is tagged in wi /yi style as follows: 1. Waiters/O are/O very/O [friendly/B] and/O the/O pasta/O is/O out/O of/O this/O world/O ./O (Given opinion target: waiter, extract “friendly” as corresponding opinion word). 2. Waiters/O are/O very/O friendly/O and/O the/O pasta/O is/O [out/B of/I this/I world/I] ./O (Given Opinion target: pasta, extract “out"
N19-1259,P17-2023,0,0.0320155,"in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double propagation to expand opinion targets and opinion words lists in a"
N19-1259,C16-1311,0,0.176544,"So, we first split the sentence into three segments: left context {w1 , w2 , · · · , wl }, target term {wl+1 , · · · , wr−1 } and right context {wr , · · · , wn } and left and right contexts are targetspecific.We use a left LSTM to model the left context plus target and a right LSTM to model the target plus right context respectively. In this way the target-specific contexts could generate targetspecific context representations. However, the direction of the two LSTMs is a crucial problem. 3.3.1 Inward-LSTM We can use a simple strategy called Inward-LSTM, which follows the design of TD-LSTM (Tang et al., 2016). As Figure 2 shows, Inward-LSTM runs the two LSTMs from the two ends of the sentence to the middle target respectively. It runs the left LSTM from the first word to opinion target as a forward-LSTM and a right LSTM from the last word to the opinion target as a backward-LSTM, so we call it as Inward. This is a process of passing the context to target. We obtain left context representations HL and right context representations HR as follows: −−−−→ hLi = LSTM(hLi−1 , ei ), ∀i ∈ [1, · · · , r − 1] , (1) ←−−−− hRi = LSTM(hRi+1 , ei ), ∀i ∈ [l + 1, · · · , n] . (2) It is obvious that the words of o"
N19-1259,D16-1059,0,0.293386,"plicitly. For example, in the sentence “The menu is limited but almost all of the dishes are excellent.”, the words “menu” and “dishes” are two opinion targets, and the words “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting them as pairs is significant for ABSA. Additionally, in Figure 1, the list of pairs extracted from the example review can be considered to be an extractive pair-wise opinion summarization."
N19-1259,P18-2094,0,0.718266,"the words or phrases in the sentence representing features or entities towards which users show attitude. Opinion words (or opinion terms) refer to those terms used to express attitude explicitly. For example, in the sentence “The menu is limited but almost all of the dishes are excellent.”, the words “menu” and “dishes” are two opinion targets, and the words “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting"
N19-1325,H05-1091,0,0.554559,"re heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from n"
N19-1325,P05-1045,0,0.0141379,"n a robust relation classifier which explicitly learns from correctly labeled data and correct incorrectly labeled data implicitly. The training procedure for relation classifier is summarized in Algorithm 2. 4 4.1 Batch size bs Word Dimension dw Position Dimension dp Convolution Filter Dimension dc Convolution Window Size l Latent Variable Dimension dz Dropout p Regulator λ, β 160 50 5×2 230 3 100 0.5 100, 2 Table 2: Hyperparameter settings Freebase with New York Times corpus(NYT)2 and developed by (Riedel et al., 2010). Entity mentions are recognized by the Stanford named entity recognizer (Finkel et al., 2005). The relation facts in Freebase are divided into two parts for training and testing respectively. The sentences from the corpus of the years 2005-2006 are used as the training instances, and sentences from 2007 are used as the testing instances. There are 52 positive relations and a special relation NA. Following previous works, we evaluate our model on the held-out evaluation, which compares relation facts extracted from the test corpus with those in Freebase. We adopt aggregated precision/recall curves and precision@N (P@N) to illustrate the performance of our model. 4.2 Parameter Settings"
N19-1325,D18-1247,0,0.187379,"tperforms the state-of-the-art models. 1 DS LivedIn Gold NA LivedIn BornIn NA LivedIn Table 1: Examples of noisy labeling problem in distant supervision relation classification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relati"
N19-1325,P11-1055,0,0.674899,"pple” and “Steve Jobs” together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2"
N19-1325,P16-1200,0,0.479563,"abeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true"
N19-1325,D17-1189,0,0.442433,"ashed line) than without consideration of false positive instances. Nevertheless, there are still a lot of false negative instances expressing similar semantic information with positive data. These instances also provide evidence for the target relation. The incorrect labels will weaken the discriminative capability of available features and confuse the model if they stay the same. However, when we remedy the label correctly, we indeed possess the optimal decision boundary (red solid line). • There lacks an effective method to fully utilize noisy data of distant supervision. (Xu et al., 2013; Liu et al., 2017) apply methods such as pseudo-labels to directly correct the label of noisy data and Luo et al. (2017) design a dynamic transition matrix to model noise patterns. They still suffer from the drawback of error propagation during training. To tackle the above challenges, we propose a novel framework exploiting noisy data to enhance distant supervision relation classification. We design an instance discriminator with reinforcement learning to recognize both false positive and false negative instances simultaneously, and further split the noisy dataset into two sets, representing correctly labeled"
N19-1325,P17-1040,0,0.0565925,"t of false negative instances expressing similar semantic information with positive data. These instances also provide evidence for the target relation. The incorrect labels will weaken the discriminative capability of available features and confuse the model if they stay the same. However, when we remedy the label correctly, we indeed possess the optimal decision boundary (red solid line). • There lacks an effective method to fully utilize noisy data of distant supervision. (Xu et al., 2013; Liu et al., 2017) apply methods such as pseudo-labels to directly correct the label of noisy data and Luo et al. (2017) design a dynamic transition matrix to model noise patterns. They still suffer from the drawback of error propagation during training. To tackle the above challenges, we propose a novel framework exploiting noisy data to enhance distant supervision relation classification. We design an instance discriminator with reinforcement learning to recognize both false positive and false negative instances simultaneously, and further split the noisy dataset into two sets, representing correctly labeled and incorrectly labeled data respectively. Additionally, we learn a robust relation classifier applyin"
N19-1325,P09-1113,0,0.924258,"S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positiv"
N19-1325,P18-1046,0,0.212871,"The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true positive data DS false positive data side effect of incorrectly labeled data by recognizing them and treating them as unlabeled data. On"
N19-1325,P18-1199,0,0.45276,"The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true positive data DS false positive data side effect of incorrectly labeled data by recognizing them and treating them as unlabeled data. On"
N19-1325,P05-1053,0,0.511935,"s LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from noisy labeling proble"
N19-1325,D12-1042,0,0.619739,"together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association f"
N19-1325,P16-1220,0,0.0253473,"hat our method outperforms the state-of-the-art models. 1 DS LivedIn Gold NA LivedIn BornIn NA LivedIn Table 1: Examples of noisy labeling problem in distant supervision relation classification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For ex"
N19-1325,P13-2117,0,0.0520394,"Missing"
N19-1325,W02-1010,0,0.405364,"ification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together."
N19-1325,D15-1203,0,0.720479,"uffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Li"
N19-1325,C14-1220,0,0.0373944,"data. To address the issue of data sparsity, Mintz et al. (2009) propose distant supervision to automatically annotate large scale training data, which inevitably results in noisy labeling problem. To tolerate noisy instances in positive examples, most early approaches employ multiinstance learning framework, including multiinstance single-label learning (Riedel et al., 2010) and multi-instance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Recently, deep learning has also been introduced to propose an end-to-end convolutional neural network for relation classification (Zeng et al., 2014). In the sentences bag of one entity pair, Zeng et al. (2015) select the most reliable sentence, and Lin et al. (2016) propose attention schemes to de-emphasize unreliable sentences. Han et al. (2018b) incorporate hierarchical information of relations to enhance the attention scheme. But they fail to handle the issue where all sentences in one bag are mislabeled. Feng et al. (2018); Qin et al. (2018b,a) further achieve improvement by using reinforcement 3217 Relation Classifier with Semi-Supervised Learning Instance Discriminator with Reinforcement Learning reward Classifier DPOS DNA Instance"
P11-2066,J93-2003,0,0.0316767,"nd discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment."
P11-2066,W07-0403,0,0.0698784,"nd real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious deriv"
P11-2066,P96-1011,0,0.587138,"tion experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. We will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features. It is worth noting that the definition of spurious ambiguity actually varies for different tasks. In some cases, e.g. bilingu"
P11-2066,P09-1104,0,0.330817,"nts. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious derivations may fill up the"
P11-2066,N03-1017,0,0.0971962,"Missing"
P11-2066,P10-1033,0,0.0590548,"s. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious derivations may fill up the n-best list and sup"
P11-2066,J03-1002,0,0.0152579,"Missing"
P11-2066,P02-1040,0,0.0799889,"Missing"
P11-2066,2006.amta-papers.25,0,0.0393175,"Missing"
P11-2066,J97-3002,0,0.84735,"mmar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity pr"
P11-2066,P05-1059,0,0.0212389,"mars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furth"
P11-2066,N06-1033,0,0.0278782,"will contain at least one aligned word-pair. Comparatively, the grammar in Liu et al. (2010) uses a leftbranching manner. It may generate more spans that only contain null-aligned words, which makes it less efficient than ours. Theorem 2. LGFN has a unique derivation for each ITG alignment, i.e. LGFN is non-spurious. Proof: Derived directly from Definition 4, Theorem 1 and Lemma 1. 4 4.1 Experiments Synthetic Experiments We automatically generated 1000 fully aligned ITG alignments of length 20 by generating random permutations first and checking ITG constraints using a linear time algorithm (Zhang et al., 2006). Sparser alignments were generated by random removal of alignment links according to a given null-aligned word ratio. Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al., 2009), LiuG (Liu et al., 2010) and LGFN (Section 3.3). Table 1 shows the average number of derivations per alignment generated under LG and HaG. The number of derivations produced by LG increased dramatically because LG has no restrictions on nullaligned word attachment. HaG also produced a large number of spurious derivations as the number of null-aligned words increased. Both LiuG"
P12-2056,J93-2003,0,0.0410874,"Missing"
P12-2056,W08-0336,0,0.32287,"Missing"
P12-2056,D09-1075,0,0.180957,"e it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to"
P12-2056,P10-1016,0,0.111291,"ic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word a"
P12-2056,P08-1115,0,0.0415467,"“bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in small288 scale task, we see that both PW and BS underperformed our approach. This may be attributed to the low recall of the learned BS or PW in their approaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation"
P12-2056,N07-2007,0,0.449632,"0), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the 2 We hereafter use “word segmentation” for short. Interestingly, word is also a basic token in syntax-based rules. 285 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285–290, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics experiment, our approach consistently outperformed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fu"
P12-2056,J07-3002,0,0.0638484,"Missing"
P12-2056,P09-1104,0,0.0171604,"ent combination only, no translation results were reported. 287 4 Experiments 4.1 Setup FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST’06 and NIST’08 test sets were used as the development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evalu"
P12-2056,P07-2045,0,0.00620301,"s6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the"
P12-2056,W04-3250,0,0.0723423,"Missing"
P12-2056,2005.mtsummit-papers.11,0,0.00520031,"f Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems (WordSys), i.e. using word as both WSA and WSR, and fully character-based systems (CharSys). Table 5 6 http://www.ictclas.org/ http://nlp.stanford.edu/software/segmenter.shtml S L CTB PKU ICT CTB PKU ICT Word alignment P R F 76.0 81.9 78.9 76.1 82.0 79.0 75.2 80.8 78.0 79.6 85.6 82.5 80.0 85.4 82.6 80.0 85.0 82.4 Character alignment P R F 78.2 85.2 8"
P12-2056,P07-1039,0,0.0187328,"and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, vi"
P12-2056,E09-1063,0,0.743425,"f the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use"
P12-2056,P03-1021,0,0.0209669,"aghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly di"
P12-2056,J03-1002,0,0.0161365,"he development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) wit"
P12-2056,W10-1760,0,0.520071,"us as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (El"
P12-2056,2009.eamt-1.3,0,0.0377949,"Missing"
P12-2056,W07-0705,0,0.0608566,"Missing"
P12-2056,C10-1135,0,0.0705294,"o the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WS"
P12-2056,P11-3001,1,0.883752,"Missing"
P12-2056,W08-0335,0,0.346113,"ity, we will refer to the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specific"
P12-2056,J03-4003,0,\N,Missing
P12-2056,D08-1064,0,\N,Missing
P15-1080,D13-1106,0,0.0259952,"eatures. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue"
P15-1080,J93-2003,0,0.0319196,"lation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; Introduction One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses. Each modeling method defines a score of a target sentence e = e1 e2 ...ei ...eI , given a source sentence f = f1 f2 ...fj ...fJ , where each ei is the ith target word and fj is the jth source word. The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1). The scoring of e decomposes to the calculation of a translation model and a language model. P r(e|f ) = P r(e)P r(f |e)/P r(f ) (2) (1) 825 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 825–835, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sometimes is short but with a high translation probability, as in translating verbs or pronouns. These three features jointly decide the choice of translations. Simply use the weighted"
P15-1080,P05-1033,0,0.676095,"m hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature"
P15-1080,P11-2031,0,0.033909,"e hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different λ for the L1 norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package4 . Although the system performance on the dev set varies, the performa"
P15-1080,N03-1017,0,0.229596,"|f ) 1 ∑M exp[ m=1 λm hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation proba"
P15-1080,Q14-1031,0,0.0710243,"L (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; Introduction One of the core problems in the research of statistical machine translation is the modeling of translatio"
P15-1080,P14-1129,0,0.0270603,"ocal minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue is the parameter learn"
P15-1080,P08-2010,0,0.028251,"ndicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/compon"
P15-1080,P06-1077,0,0.0917127,"∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases th"
P15-1080,P13-1078,0,0.0194805,"presentation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; A"
P15-1080,P14-1012,0,0.0138411,"linear models. (Section 6) 2 Related work Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the b"
P15-1080,P14-1066,0,0.0186922,"ch employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for"
P15-1080,P02-1038,0,0.445544,"Missing"
P15-1080,P03-1021,0,0.706452,"arget sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue is the parameter learning. Log-linear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance. When the scoring function become non-linear, the intersection points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the parameters. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4) The second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; t"
P15-1080,D11-1125,0,0.438376,"ric as eval(·) 1 . Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4. Ideally, the training objective is to select a scoring function sˆ, from all functions S, that scores the ˆ, higher than correct translation (or references) e any other hypotheses (Equation 5). sˆ = {s ∈ S|s(ˆ e) &gt; s(e) ∀e ∈ C} Pairwise (PW) To score the better hypothesis in sampled hypothesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization"
P15-1080,P02-1040,0,0.0946696,"best and worst hypothesis in Cnbest as the ”hope” and ”fear” hypothesis, respectively, in order to avoid multi-pass decoding. 4.1 Training Criteria The task of machine translation is a complex problem with structural output space. Decoding algorithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candidate hypotheses. The purpose of training is to select the scoring function, so that the function score the hypotheses ”correctly”. The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002).  or We denote the scoring function as s(f , e; θ),  denote the simply s, which is parameterized by θ; set of all translation hypotheses as C; denote the extrinsic metric as eval(·) 1 . Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4. Ideally, the training objective is to select a scoring function sˆ, from all functions S, that scores the ˆ, higher than correct translation (or references) e any other hypotheses (Equation 5). sˆ = {s ∈ S|s(ˆ e) &gt; s(e) ∀e ∈ C} Pairwise (PW) To sc"
P15-1080,D13-1140,0,0.0221914,"ting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional l"
P15-1080,N12-1026,0,0.0146753,"thesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions. We define our training objective function in Equation 6. (5) In practice, the candidate set C is exponentially large and hard to enumerate; the correct translation ˆ may not even exist in the current search space for e various reasons, e.g. unknown source word. As a result, we use the n-best set Cnbest to approximate C, use the extrinsic metric eval(·) to evaluate the quality of hypotheses in Cnbest and use the following three"
P15-1080,P01-1067,0,0.651429,"del. 1 P r(e|f ) = pλM (e|f ) 1 ∑M exp[ m=1 λm hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phra"
P15-1080,P14-2023,0,\N,Missing
P15-1117,D12-1133,0,0.0490078,"h size n, parsing stops after performing exactly 2n − 1 actions. MaltParser uses an SVM classifier for deterministic arc-standard parsing. At each step, MaltParser generates a set of successor states according to the current state, and deterministically selects the highest-scored one as the next state. 2.2 Global Learning and Beam Search The drawback of deterministic parsing is error propagation. An incorrect action will have a negative influence to its subsequent actions, leading to an incorrect output parse tree. To address this issue, global learning and beam search (Zhang and Clark, 2011; Bohnet and Nivre, 2012; Choi and McCallum, 2013) are used. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globa"
P15-1117,D14-1082,0,0.123397,"ompetitive greedy neural parser baseline, giving performance comparable to the best linear parser. 1 Introduction Transition-based methods have given competitive accuracies and efficiencies for dependency parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2013). These parsers construct dependency trees by using a sequence of transition actions, such as SHIFT and REDUCE, over input sentences. High accuracies are achieved by using a linear model and millions of binary indicator features. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference"
P15-1117,C14-1078,1,0.675229,"Missing"
P15-1117,P13-1104,0,0.0175535,"Missing"
P15-1117,P04-1015,0,0.959028,"12). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the li"
P15-1117,W02-1001,0,0.857367,"lark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the linear models. As a result, the log probability of an action sequence can not be modeled just as the sum of log probabilities of each action in the sequence, which is the case of structured linear model. We address the challenge by using a softmax function to directly model the distribution of action sequences. Second, for the structured model above, maximum-likelihood training is computationally intractable, requiring summing over all possible action sequen"
P15-1117,P15-1033,0,0.561306,"Missing"
P15-1117,Q13-1033,0,0.0538833,"Missing"
P15-1117,P04-1013,0,0.426907,"Missing"
P15-1117,P10-1110,0,0.0250292,"word embeddings in supervised training, the embeddings of in-vocabulary words become systematically different from these of out-of-vocabulary words after training, and the effect of pre-trained out-ofvocabulary embeddings become uncertain. In this sense, our model can also be regarded as an almost fully supervised model. The same applies to the models of Chen and Manning (2014). We also compare the speed of the structured neural parser on an Intel Core i7 3.40GHz CPU with 16GB RAM. The structured neural parser runs about as fast as Zhang and Nivre (Zhang and Nivre, 2011) and Huang and Sagae (Huang and Sagae, 2010). The results show that our parser combines the benefits of structured models and neural probabilistic models, offering high accuracies, fast speed and slim model size. 5 Related Work Parsing with neural networks. A line of work has been proposed to explore the effect of neural network models for constituent parsing (Henderson, 2004; Mayberry III and Miikkulainen, 2005; Collobert, 2011; Socher et al., 2013; Legrand and Collobert, 2014). Performances of most of these methods are still well below the state-of-the-art, except for Socher et al.(2013), who propose a neural reranker based on a PCFG"
P15-1117,P14-2128,1,0.864478,"Missing"
P15-1117,J93-2004,0,0.0521026,"sequence as a positive example for parameter update, using the training algorithm of Section 3.3. AdaGrad algorithm (Duchi et al., 2011) with mini-batch is adopted for optimization. In this way, the distribution of ot only full action sequences (i.e. complete parse trees), but also partial action sequences (i.e. partial outputs) are modeled, which makes training more challenging. The advantage of early update is that training is used to guide search, minimizing search errors. 1217 4 Description Baseline Experiments 4.1 Set-up Our experiments are performed using the English Penn Treebank (PTB; Marcus et al., (1993)). We follow the standard splits of PTB3, using sections 2-21 for training, section 22 for development testing and section 23 for final testing. For comparison with previous work, we use Penn2Malt1 to convert constituent trees to dependency trees. We use the POS-tagger of Collins (2002) to assign POS automatically. 10-fold jackknifing is performed for tagging the training data. We follow Chen and Manning (2014), and use the set of pre-trained word embeddings2 from Collobert et al. (2011) with a dictionary size of 13,000. The word embeddings were trained on the entire English Wikipedia, which c"
P15-1117,C04-1010,0,0.155583,"Missing"
P15-1117,J08-4003,0,0.367493,"Missing"
P15-1117,P13-1045,0,0.110444,"Missing"
P15-1117,D09-1058,0,0.019485,"Missing"
P15-1117,P15-1032,0,0.244197,"Missing"
P15-1117,W03-3023,0,0.372233,"Missing"
P15-1117,D08-1059,1,0.847959,"Missing"
P15-1117,J11-1005,1,0.774205,"he-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 20"
P15-1117,P11-2033,1,0.638376,"sed. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globally learned. 1214 The parser of Zhang and Nivre (2011) is developed using this framework. The structured perceptron (Collins, 2002) with early update (Collins and Roark, 2004) is applied for training. By utilizing rich manual features, it gives state-of-the-art accuracies in standard Penn Treebank evaluation. We take this method as one baseline. Fw Ft Fl 2.3 Greedy Neural Network Model Chen and Manning (2014) build a greedy neural arc-standard parser. The model can be regarded as an alternative implementation of MaltParser, using a feedforward neural network to replace the SVM classifier for deterministic parsing. 2.3.1 Model The greedy neural mo"
P15-1117,C12-2136,1,0.792212,"eatures. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Col"
P15-1117,P08-1068,0,\N,Missing
P16-1132,D15-1041,0,0.0171462,"speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been use"
P16-1132,P05-1022,0,0.125056,"of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the"
P16-1132,D14-1082,0,0.694626,"s0 ], j is the head of the queue (i.e. [ q0 = wj , q1 = wj+1 · · · ]), and L is a set of dependency arcs that has been built. At each step, the parser chooses one of the following actions: • S HIFT (S): move the front word wj from the queue onto the stacks. • L EFT-l (L): add an arc with label l between the top two trees on the stack (s1 ← s0 ), and remove s1 from the stack. • R IGHT-l (R): add an arc with label l between the top two trees on the stack (s1 → s0 ), and remove s0 from the stack. Given the sentence “John loves Mary”, the gold standard action sequence is S, S, L, S, R. 2.1 Model Chen and Manning (2014) proposed a deterministic neural dependency parser, which rely on dense embeddings to predict the optimal actions at each step. We propose a variation of Chen and Manning 1394 (2014), which splits the output layer into two hierarchical layers: the action layer and dependency label layer. The hierarchical parser determines a action in two steps, first deciding the action type, and then the dependency label (Figure 2). At each step of deterministic parsing, the neural model extracts n atomic features from the parsing state. We adopt the feature templates of Chen and Manning (2014). Every atomic"
P16-1132,D15-1215,0,0.0311275,"Missing"
P16-1132,W02-1001,0,0.0460083,"ee with the best heuristic reranking score yˆi0 . J(Θh ) = 1 |Dh | X (xi ,yi0 ,ˆ yi0 )∈Dh ri (Θh ) = ri (Θh ) + λ ||Θh || 2 max(0, st (xi , yˆi0 , Θh )) − st (xi , yi0 , Θh ) (20) (21) The detailed training algorithm is given by Algorithm 1. AdaGrad (Duchi et al., 2011) updating with subgradient (Ratliff et al., 2007) and minibatch is adopted for optimization. 5 5.1 section 23 for final testing. Following prior work on reranking, we use Penn2Malt1 to convert constituent trees to dependency trees. Ten-fold POS jackknifing is used in the training of the baseline parser. We use the POS-tagger of Collins (2002) to assign POS automatically. Because our reranking model is a dynamic reranking model, which generates training instances during search, we train 10 baseline parsing models on the 10-fold jackknifing data, and load the baseline parser model dynamically for reranking training . We follow Chen and Manning (2014), using the set of pre-trained word embeddings with a dictionary size of 13,0002 from Collobert et al. (2011). The word embeddings were trained on the entire English Wikipedia, which contains about 631 million words. 5.2 There are two different networks in our system, namely a hierarchic"
P16-1132,P15-1033,0,0.0578806,"curacies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural n"
P16-1132,P13-2111,0,0.0549975,"Missing"
P16-1132,J08-4003,0,0.0982509,"Missing"
P16-1132,D11-1137,0,0.0521305,"Missing"
P16-1132,Q13-1012,0,0.383871,"the best tree output. Denote b(i) as the beam at i-th step of search, k-best candidates in the beam of i + 1 step is: b(i + 1) = arg K (st (x, c, Θh ) + sb (x, c)), (14) c∈c(i) where c(i) denotes the set of newly constructed trees by revising trees in b(i), sb (x, c) is the baseline model score and arg K leaves the k best candidate trees to the next beam. Finally, the output tree yi of reranking is selected from all searched trees C in the revising process yi = arg max(st (x, c, Θc ) + sb (x, c)) c∈C (15) Interpolated Reranker In testing, we also adopt the popular mixture reranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which obtains better reranking performance by a linear combination of the reranking score and the baseline model score. yi = arg max (β(st (xi , y, Θc ) + st (x, y, Θh )) y∈τ (xi ) + (1 − β)sb (xi , y)) (16) Here yi is the final output tree for a sentence xi ; τ (xi ) returns all the trees candidates of the dynamic reranking; β ∈[0, 1] is a hyper-parameter. 4.4 Training As k-best neural rerankers (Socher et al., 2013; Zhu et al., 2015), we use the max-margin criterion to train our model in a stage-wise manner (Doppa et al., 2013). Given training data Dc = (xi , yi , yˆ"
P16-1132,W05-1506,0,0.0964815,"rerankers are capable of capturing global syntax features across the tree. In contrast, the most non-local neural parser with LSTM (Dyer et al., 2015) cannot exploit global features. Different to previous neural rerankers, our work in this paper contributes on integrating search and learning for reranking, instead of proposing a new neural model. Forest Reranking Forest reranking (Huang, 2008; Hayashi et al., 2013) offers a different way to extend the coverage of reranking candidates, with computing the reranking score in the trees forests by decomposing non-local features with cube-pruning (Huang and Chiang, 2005). In contrast, the neural reranking score encodes the whole dependency tree, which cannot be decomposed for forest reranking efficiently and accurately. HC-Search Doppa et al. (2013) proposed a structured prediction model with HC-Search strategy and imitation learning, which is closely related to our work in spirit. They used the complete space search (Doppa et al., 2012) for sequence labeling tasks, and the whole search process halts after a specific time bound. Different from them, we propose a dynamic parsing reranking model based on the action revising process, which is a multi-step proces"
P16-1132,P08-1067,0,0.601351,"local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association fo"
P16-1132,D14-1081,0,0.0457333,"literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rat"
P16-1132,P15-1031,0,0.0450277,"Missing"
P16-1132,P13-1045,0,0.206611,"ve accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016."
P16-1132,P15-1032,0,0.0169178,"by replacing the SVM classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively"
P16-1132,P15-1117,1,0.854947,"classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another"
P16-1132,P15-1112,0,0.250311,"S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rather weak feature lo"
P16-1132,J93-2004,0,0.0549301,"Missing"
P17-1177,J07-2003,1,0.560214,"2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X ← → cj = α j,i hi (5) Like Erigu"
P17-1177,W14-4012,0,0.0741933,"Missing"
P17-1177,D14-1179,0,0.0848773,"Missing"
P17-1177,P16-1078,0,0.656145,"017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder (§3.3), we improve the tree encoder of Eriguchi et al. (2016) by introducing a bidirectional tree encoder. For each source tree node (including the source words), we generate a representation containing information both from below (as with the original bottom-up encoder) and from above (using a top-down encoder). Thus, the annotation of each node summarizes the surrounding sequential context, as well as the entire syntactic context. In the decoder (§3.4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of Tu et al. (2016). With this tree-coverage model, we can better guide the generation pha"
P17-1177,C16-1290,0,0.0606452,"inese phrase zhu manila is translated twice because the model attends to the node spanning zhu manila even though both words have already been translated; there is no mechanism to prevent this. (b) + Tree-Coverage Model Figure 4: The attention heapmap plotting the attention weights during different translation steps, for translating the sentence in Figure 1(a). The nodes [7]-[11] correspond to non-leaf nodes indexed in Figure 3. Incorporating Tree-Coverage Model produces more concentrated alignments and alleviates the over-translation problem. Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al. (2016), we propose to use prior knowledge to control the attention mechanism. In our case, the prior knowledge is the source syntactic information. In particular, we build our model on top of the word coverage model proposed by Tu et al. (2016), which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence). The word coverage model makes the attention at a given time step j dependent on the attention at previous time steps via coverage vectors: 1940 C j,i = GRU(C j−1,i , α j,i , d j−1 , hi ). (11) Data LDC MT02"
P17-1177,W02-1039,0,0.307969,"onal Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder (§3.3), we improve the t"
P17-1177,N03-1017,0,0.0640388,"nt unit; see the paper by Cho et al. (2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X"
P17-1177,E17-2093,0,0.0513556,"ce the weight matrices W r , U r , W z , U z , W and U in the standard GRU with PrD , QrD , PzD , QzD , PD , and QD , respectively. The subscript D is either L or R depending on whether node k is a left or right child, respectively. Finally, the annotation of each node is obtained by concatenating its bottom-up hidden state and top-down hidden state: hlk  ↑ h  =  k↓  . hk (a) Tree-GRU Encoder This allows the tree structure information flow from the root to the leaves (words). Thus, all the annotations are based on the full context of word sequence and syntactic tree structure. Kokkinos and Potamianos (2017) propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several respects: in the bottom-up encoder, we use separate reset/update gates for left and right children, analogous to Tree-LSTMs (Tai et al., 2015); in the topdown encoder, we use separate weights for left and right children. Teng and Zhang (2016) also propose a bidirectional Tree-LSTM encoder for classification tasks. They use a more complex head-lexicalization scheme to feed the top-down encoder. We will compare their model with ours in the experiments. 3.4 Tree-Coverage Model We also extend the d"
P17-1177,D15-1278,0,0.0429885,"Missing"
P17-1177,P06-1077,0,0.0158632,"er by Cho et al. (2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X ← → cj = α j,i hi"
P17-1177,D16-1096,0,0.0900282,"hich is especially important for the translation of long sentences. Second, it becomes possible for the decoder to 1936 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In"
P17-1177,P02-1040,0,0.104515,"erage model, respectively. # 120 130 System Seq-LSTM SeqTree-LSTM Coverage no no MT02 34.98 35.28 MT03 32.81 33.56 MT04 34.08 34.94 MT05 31.39 32.64 MT06 28.03 29.26 Average 31.58(+0.82) 32.60(+1.84) Table 3: BLEU scores of different systems based on LSTM. “Seq-LSTM” denotes both the encoder and decoder parts for the sequential model are based on LSTM; “SeqTree-LSTM” means using Tree-LSTM encoder on top of “Seq-LSTM”. We use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32. All other settings are the same as in Bahdanau et al. (2015). We use case insensitive 4-gram BLEU (Papineni et al., 2002) for evaluation, as calculated by multi-bleu.perl in the Moses toolkit.7 4.3 Tree Encoders This set of experiments evaluates the effectiveness of our proposed tree encoders. Table 2, row 2 confirms the finding of Eriguchi et al. (2016) that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.87 BLEU, v.s. row 2). To verify our assumption that model consistency is important for performance, we also conduct experiments to compare TreeLSTM and Tree-GRU on top of LSTM-based encoder-decoder settings. Tree-Lstm with LSTM based sequential model can obtain 1.0"
P17-1177,N07-1051,0,0.0167327,"Missing"
P17-1177,D16-1159,0,0.0797839,"Missing"
P17-1177,P15-1150,0,0.0925457,"Missing"
P17-1177,P16-1008,0,0.3108,"epresentations, which is especially important for the translation of long sentences. Second, it becomes possible for the decoder to 1936 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy"
P17-1177,W09-3825,0,0.044999,"Missing"
P17-1177,P01-1067,0,\N,Missing
P17-2092,P07-2045,0,0.006204,"on in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated withIn typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granular"
P17-2092,P05-1033,0,0.138079,"raged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguisti"
P17-2092,N03-1017,0,0.0576836,"ularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computat"
P17-2092,D14-1179,0,0.106361,"Missing"
P17-2092,P16-1160,0,0.309404,"ut the phrasal component may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, o"
P17-2092,P06-1077,0,0.211161,"Missing"
P17-2092,P16-2058,0,0.0225803,"Missing"
P17-2092,P16-1078,0,0.311835,"h global reordering of phrases and local translation inside phrases. Our model has following benefits: 1. The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible. 2. Our model recognizes phrase structures explicitly. Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words. 3. Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars. 4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart. Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously. 2 bush s"
P17-2092,D15-1166,0,0.0422838,"ine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2092 out attention. The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary"
P17-2092,P08-1114,0,0.19138,"Missing"
P17-2092,P17-1174,0,0.0518004,"f previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results. Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages. Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate. Table 4: Subjective evaluation results. System dl4mt This Work DE-14 16.53 17.40 DE-1213 16.78 17.45 Table 5: Results on German-English the translation is translated by. The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, wh"
P17-2092,P02-1040,0,0.102378,"from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the"
P17-2092,D13-1176,0,0.0742459,"updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work"
P17-2092,W16-2209,0,0.184245,"ponent may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a ch"
P17-2092,Q17-1007,1,0.89772,"Missing"
P17-2092,P16-1162,0,0.0186724,". The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk"
P17-2092,P16-1008,1,0.910907,"Missing"
P17-2092,P08-1066,0,0.0644196,"Missing"
P17-2092,P16-1218,0,0.0232133,"to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the same as the previous step. In the U PDATE operation, ept−1 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016): Ts ! log P (yn |xn ) + log P (ln |xn ) + log P (bn |xn ) bt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not. Two different operations would be executed: # pt−1 , bt = 0 (C OPY ) pt = g(pt−1 , ept−1 , pct ), bt = 1 (U PDATE ) ept−1 = m(st−1 , eyt−1 ) − m(st′ , eyt′ ) N & ! (10) The chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt−1 rather than word state st−1 , and 2) it is only executed at boundary of each chunk rather than at each decoding step. In this way, our model only extracts"
P17-2092,D16-1159,0,0.0691766,"Missing"
P17-2092,1983.tc-1.13,0,0.186514,"Missing"
P17-2092,P16-2049,0,0.0402049,"Missing"
P17-2092,P15-1117,1,0.838643,"Chinese-English translation task. Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag seq"
P17-2092,Q17-1026,0,\N,Missing
P19-1602,P97-1064,0,0.0341154,"onal auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et a"
P19-1602,P17-1177,1,0.860672,"om/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards the syntax of a sen"
P19-1602,W01-0713,0,0.0302084,"ed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John e"
P19-1602,W18-6503,0,0.0200293,"AE could graft the designed syntax to another sentence under certain circumstances. 2 Related Work The variational auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating samplin"
P19-1602,N16-1024,0,0.03024,"lly applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such"
P19-1602,P16-1078,0,0.0219963,"release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely"
P19-1602,P02-1040,0,0.104047,"9.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Weight 1.3 1.2 1.0 0.7 0.5 0.3 0.1 10 9 8 Table 1: BLEU and Forward PPL of VAE with varying KL weights on the PTB test set. The larger↑ (or lower↓ ), the better. 7 31 36 41 46 51 Forward-PPL 1. Reconstruction BLEU. The reconstruction task aims to generate the input sentence itself. In the task, both syntactic and semantic vectors are chosen as the predicted mean of the encoded distribution. We evaluate the reconstruction performance by the BLEU score (Papineni et al., 2002) with input as the reference.3 It reflects how well the model could preserve input information, and is crucial for representation learning and “goal-oriented” text generation. 2. Forward PPL. We then perform unconditioned generation, where both syntactic and semantic vectors are sampled from prior. Forward perplexity (PPL) (Zhao et al., 2018) is the generated sentences’ perplexity score predicted by a pertained language model.4 It shows the fluency of generated sentences from VAE’s prior. We computed Forward PPL based on 100K sampled sentences. 3. Reverse PPL. Unconditioned generation is furth"
P19-1602,D17-1066,0,0.0159685,"ee sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually"
P19-1602,E17-1117,0,0.0226876,"NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such as sentiment. We move one"
P19-1602,N18-1169,0,0.105168,"Missing"
P19-1602,P17-1064,0,0.0150872,"https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards"
P19-1602,D18-1423,0,0.108519,"better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the l"
P19-1602,D17-1013,1,0.827385,"uth training signals; in testing, we do not need external syntactic trees. We build an RNN 1 https://www.sutd.edu.sg/cmsresource/faculty/yuezhang/ zpar.html 6010 with softmax, whose objective is the cross-entropy loss against the groundtruth distribution t, given by: X tw log p(w|zsem ) (3) L(mul) sem = − This is ... S NP .. . /S This is This is ... w∈V ... S NP .. . /S This is ... Figure 2: Overview of our DSS-VAE. Forward dashed arrows are multi-task losses; backward dashed arrows are adversarial losses. where p(w|zsyn ) is the predicted distribution. BoW has been explored by previous work (Weng et al., 2017; John et al., 2018), showing good ability of preserving semantics. For the syntactic space, the multi-task loss trains a model to predict syntax on zsyn . Due to our proposal in §3.2.1, we could build a dedicated RNN, predicting the tokens in the linearized parse tree sequence, whose loss is: Xn log p(si |s1 · · · si−1 , zsyn ) (4) L(mul) syn = − i=1 (independent of the VAE’s decoder) to predict such linearized parse trees, where each parsing token is represented by an embedding (similar to a traditional RNN decoder). Notice that, a node and its backtracking, e.g., NP and /NP, have different"
P19-1602,P18-2053,0,0.0136272,"ters are not updated. 3.2.3 Adversarial Reconstruction Loss Our next intuition is that syntax and semantics are more interwoven to each other than other information such as style and content. Suppose, for example, the syntax and semantics have been perfectly separated by the losses in 6011 §3.2.2, where zsem could predict BoW well, but does not contain any information about the syntactic tree. Even in this ideal case, the decoder can reconstruct the original sentence from zsem by simply learning to re-order words (as zsem does contain BoW). Such word re-ordering knowledge is indeed learnable (Ma et al., 2018), and does not necessarily contain the syntactic information. Therefore, the multi-task and adversarial losses for syntax and semantics do not suffice to regularize DSS-VAE. We now propose an adversarial reconstruction loss to discourage the sentence being predicted by a single subspace zsyn or zsem . When combined, however, they should provide a holistic view of the entire sentence. Formally, let zs be a latent variable (zs = zsyn or zsem ). A decoding adversary is trained to predict the sentence based on zs , denoted by prec (xi |x1 · · · xi−1 , zs ). Then, the adversarial reconstruction los"
P19-1602,J93-2004,0,0.0651646,"er during training. 4 Experiments We evaluate our method on reconstruction and unconditional language generation (§4.1). Then, we apply it two applications, namely, unsupervised paraphrase generation (§4.2) and syntax-transfer generation (§4.3). 4.1 Reconstruction and Unconditional Language Generation First, we compare our model in reconstruction and unconditional language generation with a traditional VAE and a syntactic language model (PRPN, Shen et al., 2017). Dataset We followed previous work (Bowman et al., 2016) and used a standard benchmark, the WSJ sections in the Penn Treebank (PTB) (Marcus et al., 1993). We also followed the standard split: Sections 2–21 for training, Section 24 for validation, and Section 23 for test. Settings We trained VAE and DSS-VAE, both with 100-dimensional RNN states. For the vocabulary, we chose 30k most frequent words. We trained PRPN with the default parameter in the code base.2 Evaluation We evaluate model performance with the following metrics: 6012 2 https://github.com/yikangshen/PRPN 47.33 8.98 45.6 9.6 49.73 9.36 49.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Wei"
P19-1602,P17-2092,1,0.84797,"tion and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and mani"
P19-1602,P01-1017,0,\N,Missing
P19-1602,D14-1179,0,\N,Missing
P19-1602,K16-1002,0,\N,Missing
P19-1616,N18-1020,0,0.020897,"lingual word embedding in an unsupervised manner. Different from this work which maps words in different languages, we perform mappings between representations generated from heterogeneous data, i.e., knowledge base and question-triple pairs. Zero-Shot Learning Zero-shot learning has been studied in the area of natural language process. Hamaguchi et al. (2017) use a neighborhood knowledge graph as a bridge between out of knowledge base entities to train the knowledge graph. Levy et al. (2017) connect nature language question with relation query to tackle zero shot relation extraction problem. Elsahar et al. (2018) extend the copy actions (Luong et al., 2015) to solve the rare words problem in text generation. Some attempts have been made to build machine translation systems for language pairs without direct parallel data, where they relying on one or more other languages as the pivot (Firat et al., 2016; Ha et al., 2016; Chen et al., 2017). In this paper, we use knowledge graph embedding as a bridge between seen and unseen relations, which shares the same spirit with previous work. However, less study has been done in relation detection. 7 Conclusion In this paper, we discuss unseen relation detection"
P19-1616,D13-1160,0,0.0643161,"on mapping for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the mapping performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art.1 1 Introduction The task of Knowledge Base Question Answering (KBQA) has been well developed in recent years (Berant et al., 2013; Bordes et al., 2014; Yao and Van Durme, 2014). It answers questions using an open-domain knowledge base, such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015) or NELL (Carlson et al., 2010). The knowledge base usually contains a large set of triples. 1 Our code and data are available at https://github. com/wudapeng268/KBQA-Adapter. Each triple is in the form of hsubject, relation, objecti, indicating the relation between the subject entity and the object entity. Typical KBQA systems (Yao and Van Durme, 2014; Yin et al., 2016; Dai et al., 2016; Yu et al., 2017; Hao et al.,"
P19-1616,C18-1277,0,0.0334662,"Missing"
P19-1616,D14-1067,0,0.0228382,"een and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the mapping performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art.1 1 Introduction The task of Knowledge Base Question Answering (KBQA) has been well developed in recent years (Berant et al., 2013; Bordes et al., 2014; Yao and Van Durme, 2014). It answers questions using an open-domain knowledge base, such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015) or NELL (Carlson et al., 2010). The knowledge base usually contains a large set of triples. 1 Our code and data are available at https://github. com/wudapeng268/KBQA-Adapter. Each triple is in the form of hsubject, relation, objecti, indicating the relation between the subject entity and the object entity. Typical KBQA systems (Yao and Van Durme, 2014; Yin et al., 2016; Dai et al., 2016; Yu et al., 2017; Hao et al., 2018) can be divided"
P19-1616,P17-1176,0,0.0177963,"ss. Hamaguchi et al. (2017) use a neighborhood knowledge graph as a bridge between out of knowledge base entities to train the knowledge graph. Levy et al. (2017) connect nature language question with relation query to tackle zero shot relation extraction problem. Elsahar et al. (2018) extend the copy actions (Luong et al., 2015) to solve the rare words problem in text generation. Some attempts have been made to build machine translation systems for language pairs without direct parallel data, where they relying on one or more other languages as the pivot (Firat et al., 2016; Ha et al., 2016; Chen et al., 2017). In this paper, we use knowledge graph embedding as a bridge between seen and unseen relations, which shares the same spirit with previous work. However, less study has been done in relation detection. 7 Conclusion In this paper, we discuss unseen relation detection in KBQA, where the main problem lies in the learning of representations. We re-organize the SimpleQuestion dataset as SimpleQuestionBalance to reveal and evaluate the problem, and propose an adapter which significantly improves the results. We emphasize that for any other tasks which contain a large number of unseen samples, train"
P19-1616,K17-1034,0,0.02403,"n this mapping. Zhang et al. (2017a) use Generative Adversarial Nets (Goodfellow et al., 2014) to learn the mapping of bilingual word embedding in an unsupervised manner. Different from this work which maps words in different languages, we perform mappings between representations generated from heterogeneous data, i.e., knowledge base and question-triple pairs. Zero-Shot Learning Zero-shot learning has been studied in the area of natural language process. Hamaguchi et al. (2017) use a neighborhood knowledge graph as a bridge between out of knowledge base entities to train the knowledge graph. Levy et al. (2017) connect nature language question with relation query to tackle zero shot relation extraction problem. Elsahar et al. (2018) extend the copy actions (Luong et al., 2015) to solve the rare words problem in text generation. Some attempts have been made to build machine translation systems for language pairs without direct parallel data, where they relying on one or more other languages as the pivot (Firat et al., 2016; Ha et al., 2016; Chen et al., 2017). In this paper, we use knowledge graph embedding as a bridge between seen and unseen relations, which shares the same spirit with previous work"
P19-1616,P15-1002,0,0.0208483,"r. Different from this work which maps words in different languages, we perform mappings between representations generated from heterogeneous data, i.e., knowledge base and question-triple pairs. Zero-Shot Learning Zero-shot learning has been studied in the area of natural language process. Hamaguchi et al. (2017) use a neighborhood knowledge graph as a bridge between out of knowledge base entities to train the knowledge graph. Levy et al. (2017) connect nature language question with relation query to tackle zero shot relation extraction problem. Elsahar et al. (2018) extend the copy actions (Luong et al., 2015) to solve the rare words problem in text generation. Some attempts have been made to build machine translation systems for language pairs without direct parallel data, where they relying on one or more other languages as the pivot (Firat et al., 2016; Ha et al., 2016; Chen et al., 2017). In this paper, we use knowledge graph embedding as a bridge between seen and unseen relations, which shares the same spirit with previous work. However, less study has been done in relation detection. 7 Conclusion In this paper, we discuss unseen relation detection in KBQA, where the main problem lies in the l"
P19-1616,P16-1076,0,0.198507,"Missing"
P19-1616,D18-1051,0,0.270227,"Missing"
P19-1616,P14-1090,0,0.0875591,"Missing"
P19-1616,C16-1164,0,0.170411,"Missing"
P19-1616,P17-1179,0,0.058388,"Missing"
P19-1616,D17-1207,0,0.0722161,"Missing"
P19-1616,P17-1053,0,\N,Missing
Q18-1011,D17-1151,0,0.0132202,"aseline. Comparison with Other Work. (Rows 9-11). We also conduct experiments with multi-layer decoders (Wu et al., 2016) to see whether the NMT system can automatically model the translated and untranslated contents with additional decoder lay152 ers (Rows 9-10). However, we find that the performance is not improved using a two-layer decoder (Row 9), until a deeper version (three-layer decoder, Row 10) is used. This indicates that enhancing performance by simply adding more RNN layers into the decoder without any explicit instruction is nontrivial, which is consistent with the observation of Britz et al. (2017). Our model also outperforms the word-level C OVERAGE (Tu et al., 2016), which considers the coverage information of the source words independently. Our proposed model can be regarded as a high-level coverage model, which captures higher level coverage information, and gives more specific signals for the decision of attention and target prediction. Our model is more deeply involved in generating target words, by being fed not only to the attention model as in Tu et al. (2016), but also to the decoder state. 5.1.2 Subjective Evaluation Following Tu et al. (2016), we conduct subjective evaluatio"
Q18-1011,W17-3203,0,0.0258306,",000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate"
Q18-1011,W17-4725,0,0.0491104,"Missing"
Q18-1011,D13-1176,0,0.0776419,"TURE contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source. During translation, the decoder implicitly serves several functionalities at the same time: * Equal contributions. Our code can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT)."
Q18-1011,D15-1166,0,0.161208,"de can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT). 3. Maintaining what parts in the source have been translated (PAST) and what parts have not (F UTURE). However, it may be difficult for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation"
Q18-1011,C16-1205,0,0.043448,"del both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work. In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach. Future Modeling. Standard neural sequence decoders"
Q18-1011,D16-1096,0,0.0876841,"ly helps the prediction at the beginning of the sentence. We attribute the vanishing of such signals to the overloaded use of decoder states (e.g., L M, PAST, and F UTURE functionalities), and hence we propose to explicitly model the holistic source summarization by PAST and F UTURE contents at each decoding step. 3 Related Work Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. Coverage Modeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones. Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further. We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) ins"
Q18-1011,D16-1147,0,0.0206683,"o predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rockt¨aschel et al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rockt¨aschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism. In this work, we further separate PAST and F UTURE functionalities from the decoder’s hidden representations. 4 Modeling PAST and F UTURE fo"
Q18-1011,J03-1002,0,0.0224634,") . | {z } PAST loss Dataset. We conduct experiments on ChineseEnglish (Zh-En), German-English (De-En), and English-German (En-De) translation tasks. For Zh-En, the training set consists of 1.6m sentence pairs, which are extracted from the LDC corpora3 . The NIST 2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards port"
Q18-1011,P02-1040,0,0.100819,"rformance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of join"
Q18-1011,E17-2025,0,0.0188295,"14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does"
Q18-1011,W17-4738,0,0.0132824,"uishes the PAST and F UTURE directly, which is a higher level coverage mechanism than the word coverage model. 153 ∆ – -1.00 -3.83 Table 4: Evaluation of the alignment quality. The lower the score, the better the alignment quality. 5.2 Results on German-English We also evaluate our model on the WMT17 benchmarks for both De-En and En-De. As shown in Table 5, our baseline gives comparable BLEU scores to the state-of-the-art NMT systems of WMT17. Our proposed model improves the strong baseline on both De-En and En-De. This shows that our proposed model works well across different language pairs. Rikters et al. (2017) and Sennrich et al. (2017a) obtain higher BLEU scores than our model, because they use additional large scale synthetic data (about 10M) for training. It maybe unfair to compare our model to theirs directly. 5.3 Alignment Quality AER 39.73 38.73 35.90 Analysis We conduct analyses on Zh-En, to better understand our model from different perspectives. Parameters and Speeds. As shown in Table 6, the baseline model (BASE) has 80M parameters. A single F UTURE or PAST layer introduces 15M to 17M parameters, and the corresponding objective introduces 18M parameters. In this work, the most complex mod"
Q18-1011,P16-1162,0,0.121344,"2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For"
Q18-1011,E17-3017,0,0.0569003,"Missing"
Q18-1011,P16-1159,0,0.0842396,"4 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate the proposed model on the Chinese-English translation and alignment tasks. 5.1.1 Translation Quality Table 2 shows the translation performances on Chinese-English. Clearly the proposed approach significantly improves the translation quality in all cases, although there are still considerable differences among different variants. F UTURE Layer. (Rows 1-4). All the activation functions for the F UTURE layer obtain BLEU score im"
Q18-1011,P16-1008,1,0.87499,"election over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (Tu et al., 2016). Ideally, PAST grows and F UTURE declines during the translation process. However, it may be difficult for a single RNN to explicitly model the above processes. In this paper, we propose a novel neural machine translation system that explicitly models PAST and F UTURE contents with two additional RNN layers. The RNN modeling the PAST contents (called PAST layer) starts from scratch and accumulates the in145 Transactions of the Association for Computational Linguistics, vol. 6, pp. 145–157, 2018. Action Editor: Philipp Koehn. Submission batch: 6/2017; Revision batch: 9/2017; Published 3/2018."
Q18-1011,D16-1027,0,0.0173536,"nslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated). In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients"
Q18-1011,D17-1013,1,0.785512,"g feature of the proposed approach. Future Modeling. Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017), in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder’s hidden states to not only generate the current target word, but also predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be allevi"
W10-2917,N06-1013,0,0.217513,"and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multiple alignment results. And we use more features, such as bi-lexical features, w"
W10-2917,J93-2003,0,0.0107643,"od, namely Tri-training, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in A"
W10-2917,P04-1023,0,0.0523745,"Missing"
W10-2917,P06-2014,0,0.0161952,"spite the large improvement in F1 score, our two Tri-training models only get slightly better score than the well-known Model4GDF. This kind of inconsistence between AER or F1 scores and 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the"
W10-2917,P06-1097,0,0.0567277,"Missing"
W10-2917,J07-3002,0,0.0167083,"plotted in Figure 2a. Basically, both accuracy and recall increase with the size of labeled data. However, we also find that the increase of all the scores gets slower when the 140 ModelName Model4C2E Model4E2C BerkeleyAl. Model4GDF Supervised Tri-Bootstrap Tri-Divide Dev04 24.54 26.54 26.19 26.75 27.07 26.88 27.04 Test05 17.10 19.00 20.08 20.67 20.00 20.49 20.96 Test06 17.52 20.18 19.65 20.58 19.47 20.76 20.79 Test08 14.59 16.56 16.70 17.05 16.13 17.31 17.18 Table 4: Experiments on machine translation (BLEU4 scores in percentage) BLEU scores is a known issue in machine translation community (Fraser and Marcu, 2007). One possible explanation is that both AER or F1 are 0-1 loss functions, which means missing one link and adding one redundant link will get the same penalty. And more importantly, every wrong link receives the same penalty under these metrics. However, these different errors may have different effects on the machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. Note that, both experiments on data size show some unsteadiness during the learni"
W10-2917,N03-1017,0,0.00834837,"027). we treat all alignment links as sure links. AER = 1 − |A ∩ P |+ |A ∩ S| |A |+ |S| (3) We also define a F1 score to be the harmonic mean of classifier’s accuracy and recall of correct decisions (Formula 4). F1 = 2 ∗ accuracy ∗ recall accuracy + recall (4) We also evaluate the machine translation quality using unlabeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.3 Experiments of Semi-supervised Models We present our experiment results on semisupervised models in Table 3. The two strategies of generating initial classifiers are compared. TriBootstrap is the model using the original bootstrap sampling initialization; and Tri-Divide is the model using the dividing initialization as described in Section 3.2. Items with superscripts 0 indicate models before the first iteration, i.e. initial models. The scores of BerkeleyAligner and the supervised model are also inclu"
W10-2917,N06-1014,0,0.0238493,"sed model are also included for comparison. In general, all supervised and semi-supervised models achieve better results than the best submodel, which proves the effectiveness of labeled training data. It is also reasonable that initial models are not as good as the supervised model, because they only use part of the labeled data for training. After the iterative training, both the two 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of 139 0.79 0.78 0.8 0.77 F−1 scores Scores(F−1, Accuracy, Recall) 0.9 0.7 0.6 0.76 0.75 F−1 Recall Accuracy 0.5 0.4 0 1000 2000 3000 4000 5000 Training Instances Number Tri−Divide Supervised Tri−Bootstrap 0.74 0.73 0 6000 (a) 0.5 1 1.5 2 Number of sentences 2.5 3 5 x 10 (b) Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b"
W10-2917,P05-1057,0,0.131229,"understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s"
W10-2917,H05-1011,0,0.263803,"093, P.R.China richardlkx@126.com Abstract al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignm"
W10-2917,J03-1002,0,0.0868239,"fiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in AI are always included in the combination"
W10-2917,H05-1010,0,0.350294,"erformance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multipl"
W10-2917,C96-2141,0,0.178455,"ing, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in AI are always include"
W10-2917,2005.mtsummit-papers.41,0,0.0235786,"nly get slightly better score than the well-known Model4GDF. This kind of inconsistence between AER or F1 scores and 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch"
W10-2917,P06-2117,0,0.0643865,"sity Nanjing 210093, P.R.China {huangsj,daixy,chenjj}@nlp.nju.edu.cn 2 School of Foreign Studies, Nanjing University Nanjing 210093, P.R.China richardlkx@126.com Abstract al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier"
W10-2917,P09-1104,0,\N,Missing
W12-6312,C10-2139,0,0.0228617,"if ci is numeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under differe"
W12-6312,D11-1090,0,0.0502279,"umeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic conte"
W12-6312,J04-1004,0,0.0396815,", a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic contexts, it may carry a meaning. The more contexts a string appears in, the more likely it is a independent word. Given a string s, we define the left accessor variety of s as the number of distinct characters that precede s in the corpus, denoted by LAV (s). The higher value LAV (s) is, the more likely that s can be separated at its start position. Similarly, right accessor variety of s is defined as the number of distinct charact"
W12-6312,J09-4006,0,0.027947,"Missing"
W12-6312,O03-4002,0,0.619544,"exist in micro-blog text, and we call it rule-based adaptation. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score, compared with 88.73 points of F-score of the unadapted CRF word segmenter on the pre-released development data. Our system achieved 92.51 points of F-score on the final test data. 1 Introduction Recent years have witnessed the great development of Chinese word segmentation (CWS) techniques. Among various approaches, character labelling via Conditional Random Field (CRF) modelling has become a prevailing technique (Lafferty et al., 2001; Xue, 2003; Zhao et al., 2006), due to its good performance in OOV words recognition and low development cost. Given a large-scale corpus with human annotation, the only issue the developer need to focus on is to design an expressive set of feature templates which 63 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 63–68, Tianjin, China, 20-21 DEC. 2012 characters in emoticons, name entities and special punctuation patterns which extensively exist in micro-blog text. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score,"
W12-6312,W06-0127,0,\N,Missing
W12-6312,W10-4132,0,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
Y11-1003,D11-1033,0,0.059207,"owadays LM adaptation in SMT has been paid lots of attentions. There are two main categories for this problem. The first one is data selection, i.e., when given a test dataset and a large general corpus, which tries to extract sentences from the whole corpus that are relevant to the test dataset under some metric. There are two main approaches for the measurement: One is to apply tf-idf metric (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004), which arises from information retrieval; while for the other approach cross-entropy (perplexity) is adopted for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phr"
Y11-1003,P05-1033,0,0.139229,"ets. It is known that in development dataset each sentence owns references, the reason we use entropy of translation outputs rather than references for development is that in real application we usually translate datasets without references, although it is included for standard SMT evaluation datasets. In fact we could observe that the adaptation result based on the cross-entropy of translation outputs is consistent with that based on cross-entropy of the references, as shown in section 5.2. 23 5 Experiments 5.1 Experiment Settings We implement a hierarchical phrase-based decoder according to Chiang (2005). The development data includes NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2008 (MT08). Besides the above four datasets, the test datasets contain all portions of MT06, including newswire (MT06nw), newsgroup (MT06wg) and weblog (MT06wl), and two portions of MT08, including newswire (MT08nw) and webgroup (MT08wg). The statistics are shown in Table 2. All results are measured in case-insensitive BLEU4 (Papineni et al., 2002). Dataset #Sentence #Word MT03 919 24,900 MT04 1,788 50,061 MT05 1,082 30,512 MT06 1,664 38,984 MT08 1,357 33,259 MT06bc 565 11,884 MT06n"
Y11-1003,2005.eamt-1.19,0,0.126739,"erent datasets and we further present our adaptation method. Experimental results are shown in Section 5. We conclude and present several directions for future work in the last section. 2 Related Work Nowadays LM adaptation in SMT has been paid lots of attentions. There are two main categories for this problem. The first one is data selection, i.e., when given a test dataset and a large general corpus, which tries to extract sentences from the whole corpus that are relevant to the test dataset under some metric. There are two main approaches for the measurement: One is to apply tf-idf metric (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004), which arises from information retrieval; while for the other approach cross-entropy (perplexity) is adopted for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 200"
Y11-1003,W07-0717,0,0.022237,"debrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004), which arises from information retrieval; while for the other approach cross-entropy (perplexity) is adopted for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to its difficulty. The methods mentioned above try to overcome the difference between the training and the test data. H"
Y11-1003,D10-1044,0,0.0180723,"Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to its difficulty. The methods mentioned above try to overcome the difference between the training and the test data. However, the bias between the development and the test data is also an open issue. Not much attention has been paid to the such weight adaptation. In Li et al. (2010), the model weight is tuned on a subset of the development set,"
Y11-1003,W07-0733,0,0.0266058,"he test dataset under some metric. There are two main approaches for the measurement: One is to apply tf-idf metric (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004), which arises from information retrieval; while for the other approach cross-entropy (perplexity) is adopted for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to i"
Y11-1003,C10-1075,1,0.783635,"tsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to its difficulty. The methods mentioned above try to overcome the difference between the training and the test data. However, the bias between the development and the test data is also an open issue. Not much attention has been paid to the such weight adaptation. In Li et al. (2010), the model weight is tuned on a subset of the development set, which is extracted based on the relevance to the test set. In this paper, different from (Li et al., 2010), we focus on the adaptation of LM weight only, as LM is one of the key components of SMT and has its own characteristic. In our work, we adopt cross-entropy as a metric, just as (Axelrod et al., 2011; Moore and Lewis, 2010), to measure the similarity between different datasets. However, only LM weight is adjusted during the adaptation, and no extra model needs to be built. Although our method is quite simple and straightforwa"
Y11-1003,D07-1036,0,0.0432689,"Missing"
Y11-1003,D09-1074,0,0.0219795,"for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to its difficulty. The methods mentioned above try to overcome the difference between the training and the test data. However, the bias between the development and the test data is also an open issue. Not much attention has been paid to the such weight adaptation. In Li et al. (2010), the"
Y11-1003,2010.amta-papers.17,0,0.0205794,"o assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavailable in advance. Such weighting method even could be apply to either each sentence from the training corpus (Matsoukas et al., 2009) or phrase pair from the phrase-table (Foster et al., 2010). Besides, the work of (Mohit et al., 2009; Mohit et al., 2010) also belongs to such scenario, in which they attempt to build a classifier to predict whether or not a phrase is difficult, then the LM weight is updated for each phrase segment according to its difficulty. The methods mentioned above try to overcome the difference between the training and the test data. However, the bias between the development and the test data is also an open issue. Not much attention has been paid to the such weight adaptation. In Li et al. (2010), the model weight is tuned on a subset of the development set, which is extracted based on the relevance to the test set. In t"
Y11-1003,P03-1021,0,0.0249157,"Missing"
Y11-1003,P02-1040,0,0.0845893,"f the references, as shown in section 5.2. 23 5 Experiments 5.1 Experiment Settings We implement a hierarchical phrase-based decoder according to Chiang (2005). The development data includes NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2008 (MT08). Besides the above four datasets, the test datasets contain all portions of MT06, including newswire (MT06nw), newsgroup (MT06wg) and weblog (MT06wl), and two portions of MT08, including newswire (MT08nw) and webgroup (MT08wg). The statistics are shown in Table 2. All results are measured in case-insensitive BLEU4 (Papineni et al., 2002). Dataset #Sentence #Word MT03 919 24,900 MT04 1,788 50,061 MT05 1,082 30,512 MT06 1,664 38,984 MT08 1,357 33,259 MT06bc 565 11,884 MT06nw 616 17,971 MT06ng 483 9,146 MT08nw 691 18,124 MT08wg 666 15,145 Table 2: Statistics on development and test datasets. In the experiments, the training corpus includes LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, and LDC2007T09, which consists of about 8.5M sentence pairs. The word alignment result is trained by GIZA++ in both directions and refined under inter"
Y11-1003,2006.amta-papers.25,0,0.034963,"of test data 2.3 2.2 2.1 2 1.9 1.8 1.7 1.6 0.4 0.6 0.8 1 1.2 1.4 LM weight variation percentage 1.6 1.8 Figure 2: The cross entropy of test vs. LM weight variation in percentage for different dataset pairs the gain from the length penalty decrease could counteract reduction on the precision. While for the case in which MT08 as development and MT03 as test, the length penalty of both baseline and adapted results are equal, while n-gram precision of adapted method is higher than that of baseline, which leads to improvements on final performance. Meanwhile, we also apply another SMT metric TER (Snover et al., 2006) to evaluate the results of the dataset pair MT03 and MT08, as shown in table 4. When we use MT03 as development and MT08 as test, the TER result shows no improvement. This is consistent with observation from above discussion, as improvement for BLEU mainly comes from length penalty, not n-gram precision. Meanwhile, when we use MT08 as development and MT03 as test, we achieve significant improvement on the TER score. This inconsistency shows some potential difference between the TER metric and the BLEU metric. However, for some dataset pairs, the adapted result is not so good as the baseline."
Y11-1003,C04-1059,0,0.125386,"daptation method. Experimental results are shown in Section 5. We conclude and present several directions for future work in the last section. 2 Related Work Nowadays LM adaptation in SMT has been paid lots of attentions. There are two main categories for this problem. The first one is data selection, i.e., when given a test dataset and a large general corpus, which tries to extract sentences from the whole corpus that are relevant to the test dataset under some metric. There are two main approaches for the measurement: One is to apply tf-idf metric (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004), which arises from information retrieval; while for the other approach cross-entropy (perplexity) is adopted for selection, as reported in (Axelrod et al., 2011; Moore and Lewis, 2010). The second is model weighting. The main idea is to assign appropriate weight to each model according to the similarity between the model corpus and test dataset. In this approach, the models could be built from domain-specific corpus (Koehn and Schroeder, 2007) when domain of the test dataset is known, or from datasets that belong to different sources (Foster and Kuhn, 2007; L¨u et al., 2007) when it is unavai"
Y11-1003,P10-2041,0,\N,Missing
Y11-1003,2009.eamt-1.22,0,\N,Missing
