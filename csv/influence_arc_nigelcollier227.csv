2020.acl-main.157,N18-2004,0,0.318085,"ble benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain. The entire dataset is released for future research2 . 1 Introduction Apart from constituting an interesting task on its own, stance detection has been identified as a crucial sub-step towards many other NLP tasks (Mohammad et al., 2017). In fact, stance detection is the core component of fake news detection (Pomerleau and Rao, 2017), fact-checking (Vlachos and Riedel, 2014; Baly et al., 2018), and rumor verification (Zubiaga et al., 2018b). Despite its importance, stance detection suffers from the lack of a large dataset which would allow for reliable comparison between models. We aim at filling this gap by presenting Will-They-Won’tThey (WT– WT), a large dataset of English tweets targeted at stance detection for the rumor verification task. We constructed the dataset based on tweets, since Twitter is a highly relevant platform for rumour verification, which is popular with the public as well as politicians and enterprises (Gorrell et al., 2019). To make the dataset representative"
2020.acl-main.157,S17-2144,0,0.0136497,"r and Perella, 2004; Piesse et al., 2013). In this sense, the analysis of the evolution of opinions and concerns expressed by users about a possible M&A deal, from its early stage to its closing (or its rejection) stage, is a process similar to rumor verification (Zubiaga et al., 2018a). Moreover, despite the wide interest, most research in the intersection of NLP and finance has so far focused on sentiment analysis, text mining and thesauri/taxonomy generation (Fisher et al., 2016; Hahn et al., 2018; El-Haj et al., 2018). While sentiment (Chan and Chong, 2017) and targetedsentiment analysis (Chen et al., 2017) have an undisputed importance for analyzing financial markets, research in stance detection takes on a crucial role: in fact, being able to model the market’s perception of the merger might ultimately contribute to explaining stock price re-valuation. We make the following three contributions. Firstly, we construct and release WT– WT, a large, expert-annotated Twitter stance detection dataset. With its 51,284 tweets, the dataset is an order of magnitude larger than any other stance detection dataset of user-generated data, and could be used to train and robustly compare neural models. To our"
2020.acl-main.157,E17-2088,0,0.202834,"ost operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Using the same annotation schema, Inkpen et al. (2017) released a corpus on the 2016 US election annotated for multi-target stance. In the scope of P HEME, a large project on rumor resolution (Derczynski and Bontcheva, 2014), Zubiaga et al. (2015) stanceannotated 325 conversational trees discussing 9 breaking news events. The dataset was used in RumourEval 2017 (Derczynski et al., 2017) and was later extended with 1,066 tweets for RumourEval 2019 (Gorrell et al., 2019). Following the same procedure, Aker et al. (2017) annotated 401 tweets on mental disorders (Table 3). This makes the proposed dataset by far the largest publicly available dataset"
2020.acl-main.157,S17-2083,0,0.0453654,"Missing"
2020.acl-main.157,C18-1288,0,0.0226635,"efforts on stance detection (Hanselowski et al., 2018). Moreover, w.r.t. stance datasets where unrelated samples were randomly generated (Pomerleau and Rao, 2017; Hanselowski et al., 2018), we report a slightly 6 The average κ was weighted by the number of samples annotated by each pair. The standard deviation of the κ scores between single annotator pairs is 0.074. Label Distribution The distribution of obtained labels for each operation is reported in Table 2. Differences in label distribution between events are usual, and have been observed in other stance corpora (Mohammad et al., 2016a; Kochkina et al., 2018). For most operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Using the same annotation s"
2020.acl-main.157,L16-1623,0,0.53442,"served in other research efforts on stance detection (Hanselowski et al., 2018). Moreover, w.r.t. stance datasets where unrelated samples were randomly generated (Pomerleau and Rao, 2017; Hanselowski et al., 2018), we report a slightly 6 The average κ was weighted by the number of samples annotated by each pair. The standard deviation of the κ scores between single annotator pairs is 0.074. Label Distribution The distribution of obtained labels for each operation is reported in Table 2. Differences in label distribution between events are usual, and have been observed in other stance corpora (Mohammad et al., 2016a; Kochkina et al., 2018). For most operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Us"
2020.acl-main.157,S16-1003,0,0.140981,"Missing"
2020.acl-main.157,D14-1162,0,0.088668,"Missing"
2020.acl-main.157,C18-1203,0,0.0561482,"et al., 2014), which are shared between tweets and targets and kept fixed during training. 3.2 Results and Discussion Results of experiments are reported in Table 4. Despite its simple architecture, SiamNet obtains the best performance in terms of both averaged and weighted averaged F1 scores. In line with previous findings (Mohammad et al., 2017), the SVM model constitutes a very strong and robust baseline. The relative gains in performance of CrossNet w.r.t. BiCE, and of HAN w.r.t. TAN, consistently reflect results obtained by such models on the SemEval 2016-Task 6 corpus (Xu et al., 2018; Sun et al., 2018). Moving to single labels classification, analysis of the confusion matrices shows a relevant number of misclassifications between the support and comment classes. Those classes have been found difficult to discriminate in other datasets as well (Hanselowski et al., 2018). The presence of linguistic features, as in the HAN model, may help in spotting the nuances in the tweet’s argumentative structure which allow for its correct classification. This may hold true also for the refute class, the least common and most difficult to discriminate. Unrelated samples in WT– WT could be about the involv"
2020.acl-main.157,S16-1067,0,0.0688256,"Missing"
2020.acl-main.157,W14-2508,0,0.0159342,"s a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain. The entire dataset is released for future research2 . 1 Introduction Apart from constituting an interesting task on its own, stance detection has been identified as a crucial sub-step towards many other NLP tasks (Mohammad et al., 2017). In fact, stance detection is the core component of fake news detection (Pomerleau and Rao, 2017), fact-checking (Vlachos and Riedel, 2014; Baly et al., 2018), and rumor verification (Zubiaga et al., 2018b). Despite its importance, stance detection suffers from the lack of a large dataset which would allow for reliable comparison between models. We aim at filling this gap by presenting Will-They-Won’tThey (WT– WT), a large dataset of English tweets targeted at stance detection for the rumor verification task. We constructed the dataset based on tweets, since Twitter is a highly relevant platform for rumour verification, which is popular with the public as well as politicians and enterprises (Gorrell et al., 2019). To make the da"
2020.acl-main.157,P18-2118,0,0.048404,"Missing"
2020.acl-main.157,H05-1044,0,0.279691,"Missing"
2020.acl-main.157,N16-1174,0,0.0419379,"Missing"
2020.acl-main.157,C16-1230,0,0.0462877,"Missing"
2020.acl-main.157,I13-1191,0,\N,Missing
2020.acl-main.157,N15-1178,0,\N,Missing
2020.acl-main.157,C18-1158,0,\N,Missing
2020.acl-main.157,S19-2147,0,\N,Missing
2020.acl-main.157,P18-2123,0,\N,Missing
2020.acl-main.157,S17-2089,0,\N,Missing
2020.emnlp-main.253,C10-1096,0,0.0405235,"fic split in parentheses. 4.1 Dictionary and String-based Baselines As a first step, we experimented with a set of na¨ıve systems based on string matching and edit distance.6 These baselines ignore the context around the entities, since they simply try to match entities against SNOMED labels. Dictionary. A lookup table is built by traversing the training data, recording every entity and its corresponding SCTID, and directly applied on the test 6 For this set of experiments, we transform all entities and labels to lower-case. 3126 matching system for medical concept extraction using SimString (Okazaki and Tsujii, 2010) as its back-end. We restrict its search space to the SNOMED CT subset of UMLS. As QuickUMLS predicts UMLS CUI instead of SCTID, we map predicted CUIs to SCTIDs through the UMLS api.8 When multiple plausible mappings exist, we count a hit if anyone of them matches.9 Acc@1 # s.1 Method Dictionary Stratified Split Zero-Shot Split .51 (.45) 0 (0) s.2 Exact matching s.3 Levenshtein ratio s.4 Stoilos distance .40 (.38) .49 (.47) .51 (.49) .37 (.35) .52 (.49) .53 (.51) s.5 s.6 .51 (.48) .31 (.30) .53 (.47) .43 (.38) cTAKES QuickUMLS Table 3: Comparison for Dictionary, String-Matching, cTAKES and Qui"
2020.emnlp-main.253,P14-6004,0,0.0282351,"e challenges and underline some of the key areas that are indispensable for further progress in this domain. 2 Related Work and Datasets Entity Linking. EL (Bunescu and Pasca, 2006) is an important task that has sparked attention in recent years due to its wide-scale potential to aid in knowledge acquisition, e.g. the complementary problems of cross-document coreference resolution (Dredze et al., 2016), semantic relatedness (Dor et al., 2018), geo-coding (Gritta et al., 2017) and relation extraction (Koch et al., 2014). Systems that link entities to Wikipedia (Wikification) (Liu et al., 2013; Roth et al., 2014) and scientific literature to biomedical ontologies (Zheng et al., 2015) have been the focus of attention for many years. Generic EL systems such as Babelfy (Moro et al., 2014) and Tagme (Ferragina and Scaiella, 2011) identify and map entities to Wikipedia and WordNet (Miller et al., 1990) but do not directly integrate the coding standards of healthcare KGs such as SNOMED. Medical EL systems such as cTAKES (Savova et al., 2010) and MetaMap (Aronson and Lang, 2010) were designed to perform medical EL on EHRs but limited evidence e.g. (Denecke, 2014) points to a large drop in recall on UGC such"
2020.emnlp-main.253,D15-1309,0,0.0211257,"ocedure Substance Is a Blood test SCTID: 396550006 Blood SCTID: 87612001 Mention of ? went to get bloods done at 11 30 am (b) Figure 1: Examples of the EL inference challenges for user generated text in the health domain. Social media has become a dominant means for users to share their opinions, emotions and daily experience of life. A large body of work has shown that informal exchanges such as online forums can be leveraged to supplement traditional approaches to a broad range of public health questions such as monitoring suicidal risk and depression (Benton et al., 2017b), domestic abuse (Schrading et al., 2015), cancer (Nzali et al., 2017), and epidemics (Aramaki et al., 2011; Joshi et al., 2019). One of the widely exercised steps to establish a semantic understanding of social media is EnEqual contribution. Benzodiazepine SCTID: 372664007 diagnosed with gad where my benzos at ? Introduction ∗ Glutamate decarboxylase SCTID: 41465008 tity Linking (EL), i.e., the task of linking entities within a text to a suitable concept in a reference Knowledge Graph (KG) (Liu et al., 2013; Yang and Chang, 2015; Yang et al., 2016; Ran et al., 2018). However, it is well-documented that poorly composed contexts, the"
2020.emnlp-main.253,P15-1049,0,0.01582,"alth questions such as monitoring suicidal risk and depression (Benton et al., 2017b), domestic abuse (Schrading et al., 2015), cancer (Nzali et al., 2017), and epidemics (Aramaki et al., 2011; Joshi et al., 2019). One of the widely exercised steps to establish a semantic understanding of social media is EnEqual contribution. Benzodiazepine SCTID: 372664007 diagnosed with gad where my benzos at ? Introduction ∗ Glutamate decarboxylase SCTID: 41465008 tity Linking (EL), i.e., the task of linking entities within a text to a suitable concept in a reference Knowledge Graph (KG) (Liu et al., 2013; Yang and Chang, 2015; Yang et al., 2016; Ran et al., 2018). However, it is well-documented that poorly composed contexts, the ubiquitous presence of colloquialisms, shortened forms, typing/spelling mistakes, and out-of-vocabulary words introduce challenges for effective utilisation of social media text (Baldwin et al., 2013; Michel and Neubig, 2018). These challenges are exacerbated in EL for user generated content (UGC) in the health domain for two main reasons: lack of dedicated annotated resources for training EL models, and entanglement of the aforementioned challenges in general social media with the inheren"
2020.emnlp-main.253,D16-1152,0,0.0424405,"Missing"
2020.findings-emnlp.365,N15-1178,0,0.0169534,"the interplay between social media and news sources has been widely studied in other research fields, such as journalism studies (Johnson et al., 2018; Orellana-Rodriguez and Keane, 2018), very little work exists in computer science (Dredze et al., 2016), and notably, none considering SD. 2 Background The Task. SD is the task of automatically identifying the opinion expressed in a text with respect to a target (Mohammad et al., 2017). Note that SD constitutes a related, but different task than both sentiment analysis and textual entailment. The first considers the emotions conveyed in a text (Alhothali and Hoey, 2015; Tang et al., 2016), while in the second, the goal is to predict whether a logical implication exists between two sentences (Bowman et al., 2015). Consider the following example: • Target: Aetna will merge with Humana • Text: Aetna & Humana CEOs met again to talk about deal, can’t stand those bla-bla people!!! The text’s sentiment is negative, as the author is complaining about the meeting; concerning entailment, it is positive: the target entails the text because, in order to merge, two companies need to discuss the deal; finally, its stance is commenting, as it is just talking about the mer"
2020.findings-emnlp.365,N18-2004,0,0.0513448,"Missing"
2020.findings-emnlp.365,D15-1075,0,0.0379889,"; Orellana-Rodriguez and Keane, 2018), very little work exists in computer science (Dredze et al., 2016), and notably, none considering SD. 2 Background The Task. SD is the task of automatically identifying the opinion expressed in a text with respect to a target (Mohammad et al., 2017). Note that SD constitutes a related, but different task than both sentiment analysis and textual entailment. The first considers the emotions conveyed in a text (Alhothali and Hoey, 2015; Tang et al., 2016), while in the second, the goal is to predict whether a logical implication exists between two sentences (Bowman et al., 2015). Consider the following example: • Target: Aetna will merge with Humana • Text: Aetna & Humana CEOs met again to talk about deal, can’t stand those bla-bla people!!! The text’s sentiment is negative, as the author is complaining about the meeting; concerning entailment, it is positive: the target entails the text because, in order to merge, two companies need to discuss the deal; finally, its stance is commenting, as it is just talking about the merger, without expressing the orientation that it will happen (or not). SD as a Sub-Task. SD is often integrated into rumor verification (Zubiaga et"
2020.findings-emnlp.365,2020.acl-main.157,1,0.917189,"ble 1). The term M&A refers to the process by which the ownership of a company (the target) is transferred to another company (the buyer). An M&A process (merger) ranges from informal talks between the companies to the closing of the deal; high secrecy is involved and discussions are usually not publicly disclosed during its early stages (Bruner and Perella, 2004). Thus, the analysis of the evolution of opinions and concerns about a potential merger is a process similar to rumor verification (Zubiaga et al., 2018b). Notably, the news articles in S TANDER discuss the same targets as in WT– WT (Conforti et al., 2020), a large Twitter SD dataset: thus, their union provides aligned signals from both authoritative (articles) and user-generated (tweets) sources, constituting the first resource of this kind for SD. In this paper, we make the following contributions: (1) We construct S TANDER, a large expertannotated news dataset 1 labeled for SD and finegrained ER. To our knowledge, it is the first news SD dataset to provide evidence snippets, along with their exact location in the corresponding article. (2) We provide detailed statistics of our data, as well as the first diachronic analysis of the sources of"
2020.findings-emnlp.365,W02-0109,0,0.178069,"Missing"
2020.findings-emnlp.365,L16-1623,0,0.0270988,"tion (Lillie and Middelboe, 2019) and automated fact-checking (Popat et al., 2017; Thorne and Vlachos, 2018; Baly et al., 2018): in this context, textual entailment is sometimes preferred to SD as the penultimate sub-step before verification (Thorne et al., 2018). Twitter SD. Traditionally, research on SD focused on user-generated data, such as blogs and commenting sections on websites (Skeppstedt et al., 2017; Hercig et al., 2017), apps (Vamvas and Sennrich, 2020), and Facebook posts (Klenner et al., 2017); above all, mainly due to the handiness of its API, Twitter was used as a data source (Mohammad et al., 2016; Zubiaga et al., 2016; Inkpen et al., 2017; Aker et al., 2017; Conforti et al., 2020). News SD. At the time of writing, a very small number of SD datasets collecting news have been released, usually building on platforms originally developed by professional journalists, like Emergent (Ferreira and Vlachos, 2016) or Snopes (Hanselowski et al., 2019). Note that in Twitter SD, the task consists of defining the stance of a tweet with respect to a short target (usually a named entity like Hillary Clinton (Inkpen et al., 2017), or a concept like feminism (Mohammad et al., 2016)); in news SD, on the"
2020.findings-emnlp.365,D14-1162,0,0.0822115,"Missing"
2021.acl-long.137,N19-1423,0,0.113115,"LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower than 0.3. For the instance-level pacing function pic (t), the value of kT is set as 3, meaning that, after IC is completed, the negative responses of each training instance are sampled"
2021.acl-long.137,P19-1370,0,0.049549,"Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) proposed to alleviate the problem of trivial negatives by sampling stronger negatives. Lin et al. (2020) attempted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning (Bengio et al., 2009) is reminiscent of the cognitive process of human being. Its core idea is first"
2021.acl-long.137,D19-1193,0,0.0320746,"lding intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scor"
2021.acl-long.137,2020.emnlp-main.550,0,0.0218947,"ext-response combinations. In this work, we construct G(·, ·) as an non-interaction matching model with dualencoder structure such that we can precompute all contexts and responses offline and store them in cache. For any context-response pair (c, r), its pairwise relevance G(c, r) is defined as G(c, r) = Ec (c)T Er (r), (4) where Ec (c) and Er (r) are the dense context and response representations produced by a context encoder Ec (·) and a response encoder Er (·)2 . Offline Index. After training the ranking model on the same response selection dataset D using the in-batch negative objective (Karpukhin et al., 2020), we compute the dense representations of all contexts and responses contained in D. Then, as described in Eq. (4), the relevance scores of all possible combinations of the contexts and responses in D can be easily computed through the dot product between their representations. After this step, we can compute the corpus-level and instance-level difficulty of all possible combinations and cache them in memory for a fast access in training. 2 The encoders can be any model, e.g., LSTM (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Select"
2021.acl-long.137,N18-3022,0,0.0122841,"the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics. 1 Table 1: An example dialogue context between speakers A and B, where P1 and P2 are easy and difficult positives; N1 and N2 are easy and difficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the co"
2021.acl-long.137,D19-1128,0,0.384379,"during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scores are higher than the negative ones. Recently, some researchers (Li et al., 2019; Lin et al., 2020) have raised the concern that randomly sampled negative responses are often too trivial (i.e., totally irrelevant to the dialogue context). Models trained with trivial negative responses may fail to handle strong distractors in real-world scenarios. Essentially, the problem stems from the ignorance of the diversity in context-response matching degree. In other words, all random responses are treated as equally negative regardless of their different distracting strengths. For example, Ta1740 Proceedings of the 59th Annual Meeting of the Association for Computational Linguisti"
2021.acl-long.137,2020.emnlp-main.741,1,0.293051,"p at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scores are higher than the negative ones. Recently, some researchers (Li et al., 2019; Lin et al., 2020) have raised the concern that randomly sampled negative responses are often too trivial (i.e., totally irrelevant to the dialogue context). Models trained with trivial negative responses may fail to handle strong distractors in real-world scenarios. Essentially, the problem stems from the ignorance of the diversity in context-response matching degree. In other words, all random responses are treated as equally negative regardless of their different distracting strengths. For example, Ta1740 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Int"
2021.acl-long.137,W15-4640,0,0.0277167,"5 0.775 0.792 0.828 0.825 0.857 0.858 0.842 0.861 0.921 0.933 0.950 0.886 0.937 0.985 0.935 0.968 0.993 Table 2: Experimental results of different models trained with our approach on Douban, Ubuntu, and E-Commerce datasets. All results acquired using HCL outperforms the original results with a significance level p-value < 0.01. we report the results of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at Position 1 (P@1). In addition, we also report the results of R10 @1, R10 @2, R10 @5, where Rn @k means recall at position k in n candidates. Ubuntu Dataset. This dataset (Lowe et al., 2015) contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The training, validation and test size are 500k, 50k and 50k. Each dialogue context is paired with 10 response candidates. Following previous studies, we use R2 @1, R10 @1, R10 @2 and R10 @5 as evaluation metrics. E-Commerce Dataset. This dataset (Zhang et al., 2018) consists of Chinese conversations between customers and customer service staff from Taobao4 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. Rn @k are emplo"
2021.acl-long.137,P19-1006,0,0.0173133,"Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the posi"
2021.acl-long.137,D11-1054,0,0.126157,"Missing"
2021.acl-long.137,N10-1116,0,0.0106068,"ted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning (Bengio et al., 2009) is reminiscent of the cognitive process of human being. Its core idea is first learning easier concepts and then gradually transitioning to more complex concepts based on some predefined learning schemes. Curriculum learning (CL) has demonstrated its benefits in various machine learning tasks (Spitkovsky et al., 2010; Ilg et al., 2017; Li et al., 2017; Svetlik et al., 2017; Liu et al., 2018; Platanios et al., 2019). Recently, Penha and Hauff (2020) employed the idea of CL to tackle the response selection task. However, they only apply curriculum learning for the positive-side response selection, while ignoring the diversity of the negative responses. 5 5.1 Experiment Setups Datasets and Evaluation Metrics We test our approach on three benchmark datasets. Douban Dataset. This dataset (Wu et al., 2017) consists of multi-turn Chinese conversation data crawled from Douban group3 . The size of training, valida"
2021.acl-long.137,D19-1011,0,0.0429294,"utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower"
2021.acl-long.137,P19-1001,0,0.0132239,"utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower"
2021.acl-long.137,D13-1096,0,0.0289406,"cribed in Eq. (4), the relevance scores of all possible combinations of the contexts and responses in D can be easily computed through the dot product between their representations. After this step, we can compute the corpus-level and instance-level difficulty of all possible combinations and cache them in memory for a fast access in training. 2 The encoders can be any model, e.g., LSTM (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) propo"
2021.acl-long.137,N16-1170,0,0.033235,"alogue context is paired with 10 candidate responses. Rn @k are employed as the evaluation metrics. 5.2 Baseline Models In the experiments, we compare our approach with the following models that can be summarized into three categories. Single-turn Matching Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response"
2021.acl-long.137,C18-1317,0,0.0214328,"results of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at Position 1 (P@1). In addition, we also report the results of R10 @1, R10 @2, R10 @5, where Rn @k means recall at position k in n candidates. Ubuntu Dataset. This dataset (Lowe et al., 2015) contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The training, validation and test size are 500k, 50k and 50k. Each dialogue context is paired with 10 response candidates. Following previous studies, we use R2 @1, R10 @1, R10 @2 and R10 @5 as evaluation metrics. E-Commerce Dataset. This dataset (Zhang et al., 2018) consists of Chinese conversations between customers and customer service staff from Taobao4 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. Rn @k are employed as the evaluation metrics. 5.2 Baseline Models In the experiments, we compare our approach with the following models that can be summarized into three categories. Single-turn Matching Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response ca"
2021.acl-long.137,D16-1036,0,0.0232595,"ing Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start trai"
2021.acl-long.137,P18-1103,0,0.273013,"ifficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensu"
2021.acl-long.137,P18-2067,0,0.030971,"r and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) proposed to alleviate the problem of trivial negatives by sampling stronger negatives. Lin et al. (2020) attempted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning"
2021.acl-long.137,P17-1046,0,0.35725,"N2 are easy and difficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the trai"
2021.acl-short.72,W19-1909,0,0.0585707,"Missing"
2021.acl-short.72,Q19-1038,0,0.0287965,"AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for language understanding tasks have been achieved (Artetxe and Schwenk, 2019; Hu et al., 2020). However, it remains unclear whether such transfer techniques can be used to improve domain-specific NLP applications and mitigate the gap between knowledge-enhanced models in resource-rich versus resource-poor languages. In this paper, we thus investigate the current performance gaps in the BEL task beyond English, and propose several cross-lingual transfer techniques to improve domain-specialised representations and BEL in resource-lean languages. In particular, we first present a novel crosslingual BEL (XL - BEL) task and its corresponding evaluation benchmark in 10 typol"
2021.acl-short.72,2020.emnlp-main.253,1,0.890817,"rt. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension o"
2021.acl-short.72,W19-5403,0,0.0578786,"Missing"
2021.acl-short.72,2020.coling-main.118,1,0.788893,"Missing"
2021.acl-short.72,2020.acl-main.423,0,0.0348157,"nowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a f"
2021.acl-short.72,2021.naacl-main.334,1,0.851504,"get languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for l"
2021.acl-short.72,2021.ccl-1.108,0,0.0946509,"Missing"
2021.acl-short.72,I11-1029,0,0.0423863,"our XL - BEL benchmark. The statistics of the benchmark are available in Table 1. We also convert word and phrase translations into the same format (§2.1), where each ‘class’ now contains only two examples. For a translation pair (xp , xq ), we create a unique pseudo-label yxp ,xq and produce two new name-label instances (xp , yxp ,xq ) and (xq , yxp ,xq ),4 and proceed as in §2.1. This allows us to easily combine domainspecific knowledge with general translation knowledge within the same S AP framework. 3 The XL - BEL Task and Evaluation Data A general cross-lingual entity linking (EL) task (McNamee et al., 2011; Tsai and Roth, 2016) aims to map a mention of an entity in free text of any language to a controlled English vocabulary, typically obtained from a knowledge graph (KG). In this work, we propose XL - BEL, a cross-lingual biomedical EL task. Instead of grounding entity mentions to English-specific ontologies, we use UMLS as a language-agnostic KG: the XL - BEL task requires a model to associate a mention in any language to a (language-agnostic) CUI in UMLS. XL - BEL thus serves as an ideal evaluation benchmark for biomedical entity representations: it challenges the capability of both 1) repre"
2021.acl-short.72,W19-5006,0,0.018804,"ta, and pretrained models are available online at: github.com/cambridgeltl/sapbert. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks."
2021.acl-short.72,2020.emnlp-main.365,0,0.0598287,"Missing"
2021.acl-short.72,2020.acl-main.335,0,0.204153,"016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension of the method to include multilingual UMLS synonyms (§2.1), and then introduce another S AP extension which combines domain-specific synonyms with generaldomain translation data (§2.2). Training Examples. Given a min"
2021.acl-short.72,P19-1139,0,0.0265228,"available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are a"
2021.conll-1.44,2021.eacl-main.140,0,0.027205,"different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings"
2021.conll-1.44,D14-1110,0,0.0244753,"C’s effects on embedding properties such as isotropy. ings of M IRRORW I C, and its impact on the contextual representation space. We release our code at github.com/cambridgeltl/MirrorWiC. 2 Related Work and Background Word-in-Context Representations. Modelling context influence on lexical meaning and creating context-aware word representations is a longstanding research goal in lexical semantics. One direction is to create discrete sense embeddings according to a fixed sense inventory such as WordNet. These embeddings can be created from the attributes in the sense inventory such as glosses (Chen et al., 2014) or from the knowledge structure (Camacho-Collados et al., 2016). We point to Camacho-Collados and Pilehvar (2018) for a thorough survey on sense embeddings. Such sense representations require a fixed and discrete sense inventory and might not be sensitive enough to the the dynamic and fluid nature of contextual changes. More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benc"
2021.conll-1.44,N19-1423,0,0.195405,"e two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tail"
2021.conll-1.44,J13-3003,0,0.0344753,"ever, even if the items in the pair happen to have similar meanings, our learning objective still instructs the model to push them away from each other. Our rationale and decision here are based on the following: (1) Such false negative pairs can act as a regularisation; and (2) in essence, one could argue that all distinct word-in-context instances have slightly different meanings since sense is a continuous function of word and context. 4 Experimental Setup tasks: Usim and CoSimLex; two word-in-context classification tasks: WiC and WiC-TSV; and oneshot Word Sense Disambiguation (WSD). Usim (Erk et al., 2013) measures the similarity between two instances of the same word occurring in two different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same tar"
2021.conll-1.44,D19-1006,0,0.0403436,"Missing"
2021.conll-1.44,2021.emnlp-main.552,0,0.0564678,"Missing"
2021.conll-1.44,W19-0423,0,0.0266549,"they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance"
2021.conll-1.44,D19-1533,0,0.0166081,"We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs a"
2021.conll-1.44,2021.acl-long.197,0,0.062921,"Missing"
2021.conll-1.44,2020.acl-main.423,0,0.187269,"states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation"
2021.conll-1.44,2021.naacl-main.334,1,0.803834,"Missing"
2021.conll-1.44,2021.emnlp-main.109,1,0.847856,"Missing"
2021.conll-1.44,2020.emnlp-main.333,1,0.928285,"ao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other words, the contextual representations, typically extracted as the averages over the top four layers of a base PLM (Liu et al., 2020; Garí Soler and Apidianaki, 2021), can be seen as a by-product of training a language model, and are not directly optimised for contextual sensitivity. Inspired by the previous work on adaptive fine-tuning for word and sentence representations (Liu et al., 2021b), we propose a simple self-supervised technique termed M IRRORW I C: it rewires input PLMs to provide improved word-incontext (WiC) representations. Unlike prior work on fine-tuning towards improving WiC representations, our M IRRORW I C procedure disposes of any sense labels, annotated task data, and any external knowledge, and elici"
2021.conll-1.44,2021.emnlp-main.571,1,0.82908,"Missing"
2021.conll-1.44,2021.ccl-1.108,0,0.0776108,"Missing"
2021.conll-1.44,P19-1569,0,0.0183662,"More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benchmarks (Pilehvar and Camacho-Collados, 2019; Wang et al., 2019; Raganato et al., 2020). A body of work has aimed to enrich context-aware and sense information in the PLMs by injecting such knowledge (e.g., sense annotations from predefined sense inventories) at pretraining stage (Levine et al., 2020) or during inference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciti"
2021.conll-1.44,2020.scil-1.35,0,0.0874921,"Missing"
2021.conll-1.44,2020.coling-main.602,0,0.0154305,"M IRROR W I C method, based on contrastive learning, for eliciting better word-in-context (WiC) representations from pretrained language models. We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/"
2021.conll-1.44,N19-1128,0,0.0323547,"Missing"
2021.conll-1.44,E17-1010,0,0.0271206,"a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings for this target word. To evaluate the feature space of the models in WSD, we create a one-shot setting where we provide one context example3 per label and perform nearest neighbour search over contextual word representations from the candidate labels. We directly test the models on the concatenated ALL test set from Raganato et al. (2017) without access to training and development data. We also perform multilingual and cross-lingual evaluation on XL-WiC (Raganato et a"
2021.conll-1.44,2020.emnlp-main.584,0,0.0500662,"Missing"
2021.conll-1.44,2021.acl-short.73,0,0.0937812,"Missing"
2021.conll-1.44,D19-1410,0,0.288673,"ic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance, while off-the-shelf PLMs are not directly effective as universal sentence encoders, it is possible to convert them into such encoders through supervised (Reimers and Gurevych, 2019a; Feng et al., 2020; Liu et al., 2021a) or self-supervised fine-tuning (Carlsson et al., 2021; Liu et al., 2021b; Gao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other"
2021.conll-1.44,2020.emnlp-main.586,1,0.844463,"Missing"
2021.conll-1.44,2021.acl-long.393,0,0.0325412,"nference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive finen"
2021.conll-1.44,2021.acl-long.402,0,0.0262613,"multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive fineness and wide applicability. 3) We offer extensive tuning, the model learns from identical"
2021.eacl-main.18,N19-1423,0,0.233544,"et al., 2018) have emerged as a promising alternative due to their fast inference speed. NAG models omit the sequential dependencies within the output-side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand. While NAG models enjoy full parallelism and faster inference, the generation quality of NAG models often lags behind their autoregressive counterparts. In this work, we explore the potential of largescale pre-trained language models for improving the performance of non-autoregressive generation. Specifically, we utilize BERT (Devlin et al., 2019) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer (Lafferty et al., 2001; Sun et al., 2019) for better capturing the output-side dependencies. In addition, we analyze two significant limitations that NAG models currently suffer from: (1) the inflexibility of prefixed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems. First, prior NAG models require the output length to be determined before token generation, thus an extra module for output length prediction"
2021.eacl-main.18,D15-1042,0,0.0756648,"Missing"
2021.eacl-main.18,D13-1155,0,0.0357203,"for some input documents with length T , the reference summary is longer than [α · T ]. In such cases, ratio-first fails to generate the complete reference summary, leading to the drop of performance. On the other hand, we can see that, ratio-first can notably improve the inference speedup. With α = 0.3, our model achieves the highest inference speedup while still outperforms all compared NAG models. 6.3 Sentence Compression Sentence compression aims at compressing a long sentence into a short one by deleting redundant words. In this experiment, we use the Google sentence compression dataset (Filippova and Altun, 2013) as our benchmark. For evaluation, we use the standard token-kept-F1 (F1) score. In addition, We also report the results of other standard metrics including ROUGE-1, ROUGE-2 and ROUGE-L. We compare the proposed model with the same NAG baselines as in the previous experiment. We also compare our model with several strong autoregressive models, including Bi-LSTM-Dep (Filippova et al., 2015), Tagger and Tagger+ILP (Wang et al., 2017), HiSAN-Dep and HiSAN (Kamigaito et al., 2018). To measure the inference speedup, we include transformer as a baseline model. The results are presented in Table 2, fr"
2021.eacl-main.18,N18-1155,0,0.014324,"ce into a short one by deleting redundant words. In this experiment, we use the Google sentence compression dataset (Filippova and Altun, 2013) as our benchmark. For evaluation, we use the standard token-kept-F1 (F1) score. In addition, We also report the results of other standard metrics including ROUGE-1, ROUGE-2 and ROUGE-L. We compare the proposed model with the same NAG baselines as in the previous experiment. We also compare our model with several strong autoregressive models, including Bi-LSTM-Dep (Filippova et al., 2015), Tagger and Tagger+ILP (Wang et al., 2017), HiSAN-Dep and HiSAN (Kamigaito et al., 2018). To measure the inference speedup, we include transformer as a baseline model. The results are presented in Table 2, from which we see that our model outperforms the best reported NAG baseline (with LPD) in terms of both the generation quality and inference speed. Comparing with the strong autoregressive models, our model can achieve competitive performance with a over 8.42× inference speed up. We also report the results of our model using the ratio-first decoding strategy. By setting α as 0.7, it achieves a 10.00× inference speedup while still outperforming other compared NAG baselines. 6.4"
2021.eacl-main.18,D17-1222,1,0.883593,"Missing"
2021.eacl-main.18,W04-1013,0,0.0535663,"Missing"
2021.eacl-main.18,D15-1166,0,0.0340761,"allel decoding strategy (LPD-k) (Wei et al., 2019), that is, generating k results using the top-k possible output length predictions from the length predictor. The results are then re-ranked by a transformer model to get the final ouput. In the experiment, we report the results of different NAG baselines using LPD-9 decoding. In addition, to better examine the effect of using BERT in NAG models, we add a BNAG-CRF baseline which adopts the same structure of the NAG-CRF model but using BERT as the encoder. We also compare our model with several strong autoregressive models, which are Luong-NMT (Luong et al., 2015), Pointer-Generator (See et al., 2017), DRGD (Li et al., 2017) and Concept Pointer (Wang et al., 2019a). To measure the relative inference speedup, we include transformer as a baseline model. The results are shown in Table 1, from which we can see that, by using length-parallel decoding, the performance of all NAG baselines can be notably improved. However, such procedure significantly increases the inference latency. In contrast, 238 Models R-1 R-2 Autoregressive Luong-NMT 33.10 14.45 Pointer-Generator 35.98 15.99 DRGD 36.25 17.61 36.62 16.40 Concept Pointer Transformer (b = 4) 35.74 16.97 No"
2021.eacl-main.18,D19-1437,0,0.0783369,"we analyze two significant limitations that NAG models currently suffer from: (1) the inflexibility of prefixed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems. First, prior NAG models require the output length to be determined before token generation, thus an extra module for output length prediction is always required. Nevertheless, the most likely length from the prediction module is not necessarily the best-suited one for the token generation model. To this end, previous works (Gu et al., 2018; Ma et al., 2019) usually rely on length-parallel decoding (LPD) (Wei et al., 2019) for performance enhancement; that is, generating and re-ranking the results from different output length candidates. In this work, we propose a simple and elegant decoding mechanism that lets the model determine the output length on-the-fly. Specifically, our model dynamically adjusts the output sequence length via 234 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–243 April 19 - 23, 2021. ©2021 Association for Computational Linguistics emitting an [eos] to"
2021.eacl-main.18,P02-1040,0,0.109194,"ce with a over 8.42× inference speed up. We also report the results of our model using the ratio-first decoding strategy. By setting α as 0.7, it achieves a 10.00× inference speedup while still outperforming other compared NAG baselines. 6.4 Machine Translation Machine translation aims at translating text from the source language to the target language. In this task, we use the IWSLT14 German-to-English (DEEN) dataset as our benchmark. Following previous works, we use the sequence-level knowledge distillation (Gu et al., 2018) during training. For evaluation, we report results in BLEU scores (Papineni et al., 2002). In this experiment, we use the BERT model in German language. We compare our model with a range of strong 239 Models BLEU Autoregressive LSTM-based 28.53 CNN-based 32.84 33.31 Transformer (b = 4) Non-Autoregressive ENAG-E 24.13 (27.30) ENAG-P 25.09 (28.60) 23.89 (28.04) NAG-REG NAG-NMT 23.04 (26.79) 26.39 (29.21) NAG-CRF BNAG-CRF 26.73 (29.67) Ours (α = 0.8) 29.71 Ours (α = 1.0) 30.45 Models w/o CA Ours Transformer Speedup(×) 1.00 CRF X X × × R-1 35.05 32.41 32.16 27.02 R-2 16.48 14.19 11.33 8.81 R-L 33.28 30.53 30.34 25.25 Table 4: Ablation study on Gigawords dataset. NAG models, including"
2021.eacl-main.18,D15-1044,0,0.330098,"e text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.1 1 Introduction Autoregressive generation (AG) models achieve state-of-the-art performance on a wide range of text generation tasks, such as machine translation (Vaswani et al., 2017) and text summarization (Rush et al., 2015). Such models generate a token sequence in a left-to-right, token-by-token fashion. The prediction for the next token is conditioned on all previously generated tokens. This characteristic makes it impossible to parallelize the computational overhead for token predictions in different 1 All related code, data, and models can be found in https://github.com/yxuansu/NAG-BERT. positions, which leads to a relatively high latency in inference. On the other hand, non-autoregressive generation (NAG) models (Gu et al., 2018) have emerged as a promising alternative due to their fast inference speed. NAG"
2021.eacl-main.18,P17-1099,0,0.0530684,"al., 2019), that is, generating k results using the top-k possible output length predictions from the length predictor. The results are then re-ranked by a transformer model to get the final ouput. In the experiment, we report the results of different NAG baselines using LPD-9 decoding. In addition, to better examine the effect of using BERT in NAG models, we add a BNAG-CRF baseline which adopts the same structure of the NAG-CRF model but using BERT as the encoder. We also compare our model with several strong autoregressive models, which are Luong-NMT (Luong et al., 2015), Pointer-Generator (See et al., 2017), DRGD (Li et al., 2017) and Concept Pointer (Wang et al., 2019a). To measure the relative inference speedup, we include transformer as a baseline model. The results are shown in Table 1, from which we can see that, by using length-parallel decoding, the performance of all NAG baselines can be notably improved. However, such procedure significantly increases the inference latency. In contrast, 238 Models R-1 R-2 Autoregressive Luong-NMT 33.10 14.45 Pointer-Generator 35.98 15.99 DRGD 36.25 17.61 36.62 16.40 Concept Pointer Transformer (b = 4) 35.74 16.97 Non-Autoregressive NAG-NMT 27.20 8.96 29"
2021.eacl-main.18,P17-1127,0,0.0338021,"Missing"
2021.eacl-main.18,D19-1304,0,0.122245,"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–243 April 19 - 23, 2021. ©2021 Association for Computational Linguistics emitting an [eos] token at any output position to indicate the ending of the generated sequence. Therefore, we can avoid the additional efforts of output length prediction and results re-ranking. Second, most existing NAG models assume the token predictions in different positions are conditionally independent. As a consequence, they often tend to generate results that are ungrammatical with repetitions (Wang et al., 2019b). To alleviate this problem, we propose a context-aware learning objective which impels the model to output different tokens at adjacent positions, thereby reducing the possibility of repetitive generation. Furthermore, for tasks like text summarization, the output sequence (summary) is known to be shorter than the source sequence (article). In such cases, to further improve the model’s inference efficiency, we introduce a new ratio-first decoding strategy. Specifically, instead of performing inference on all source-side hidden states, ratio-first generates the result only based on a subset"
2021.eacl-main.18,P19-1125,0,0.0610404,"suffer from: (1) the inflexibility of prefixed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems. First, prior NAG models require the output length to be determined before token generation, thus an extra module for output length prediction is always required. Nevertheless, the most likely length from the prediction module is not necessarily the best-suited one for the token generation model. To this end, previous works (Gu et al., 2018; Ma et al., 2019) usually rely on length-parallel decoding (LPD) (Wei et al., 2019) for performance enhancement; that is, generating and re-ranking the results from different output length candidates. In this work, we propose a simple and elegant decoding mechanism that lets the model determine the output length on-the-fly. Specifically, our model dynamically adjusts the output sequence length via 234 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–243 April 19 - 23, 2021. ©2021 Association for Computational Linguistics emitting an [eos] token at any output position to indicate the ending of the generated"
2021.emnlp-main.109,2020.emnlp-main.253,1,0.647974,"similarities between xi and all other strings besides xi (the negatives).2 3 Experimental Setup Evaluation Tasks: Lexical. We evaluate on domain-general and domain-specific tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set (Vuli´c et al., 2020a): it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Do˘gan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, Li et al. 2016), AskAPatient (Limsopatham and Collier, 2016) and COMETA (stratified-general split, Basaldella et al. 2020) as our evaluation datasets. The first three datasets are in the scientific domain (i.e., the data have been extracted from scientific papers), while the latter two 2 We also experimented with another state-of-the-art contrastive learning scheme proposed by Liu et al. (2021). There, hard triplet mining combined with multi-similarity loss (MS loss) is used as the learning objective. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social me"
2021.emnlp-main.109,S14-2010,0,0.0705321,"Missing"
2021.emnlp-main.109,S16-1081,0,0.0605945,"Missing"
2021.emnlp-main.109,D15-1075,0,0.308897,"2021). 1 Introduction In order to address this gap, recent work has Transfer learning with pretrained Masked Lan- trained dual-encoder networks on labelled exterguage Models (MLMs) such as BERT (Devlin et al., nal resources to convert MLMs into universal lan2019) and RoBERTa (Liu et al., 2019) has been guage encoders. Most notably, Sentence-BERT widely successful in NLP, offering unmatched per- (SBERT, Reimers and Gurevych 2019) further formance in a large number of tasks (Wang et al., trains BERT and RoBERTa on Natural Language 2019a). Despite the wealth of semantic knowledge Inference (NLI, Bowman et al. 2015; Williams et al. stored in the MLMs (Rogers et al., 2020), they do 2018) and sentence similarity data (Cer et al., 2017) not produce high-quality lexical and sentence em- to obtain high-quality universal sentence embedbeddings when used off-the-shelf, without further dings. Recently, SapBERT (Liu et al., 2021) self1442 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1442–1459 c November 7–11, 2021. 2021 Association for Computational Linguistics aligns phrasal representations of the same meaning using synonyms extracted from the UMLS (Bodenreider,"
2021.emnlp-main.109,S12-1051,0,0.0536384,"e. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social media domain (i.e., extracted from online forums discussing health-related topics). We report Spearman’s rank correlation coefficients (ρ) for word similarity; accuracy @1/@5 is the standard evaluation measure in the BEL task. Evaluation Tasks: Sentence-Level. Evaluation on the intrinsic sentence textual similarity (STS) task is conducted on the standard SemEval 20122016 datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (STS-b, Cer et al. 2017), SICK-Relatedness (SICK-R, Marelli et al. 2014) for English; STS SemEval-17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-li"
2021.emnlp-main.109,S13-1004,0,0.0904508,"Missing"
2021.emnlp-main.109,Q16-1028,0,0.0521718,"orms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we r"
2021.emnlp-main.109,P18-1073,0,0.0275264,"17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-lingual word similarity (CLWS, Multi-SimLex is used) and bilingual lexicon induction (BLI). We rely on the standard mapping-based BLI setup (Artetxe et al., 2018), and training and test sets from Glavaš et al. (2019), reporting accuracy @1 scores (with CSLS as the word retrieval method, Lample et al. 2018). Mirror-BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021"
2021.emnlp-main.109,D18-2029,0,0.115897,"Missing"
2021.emnlp-main.109,N19-1423,0,0.116457,"Missing"
2021.emnlp-main.109,ehrmann-etal-2014-representing,0,0.0845129,"shing away the negatives (§2.3). 2.1 Training Data through Self-Duplication The key to success of dual-network representation learning (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Liu et al., 2021, inter alia) is the construction of positive and negative pairs. While negative pairs can be easily obtained from randomly sampled texts, positive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store relations such as synonymy or hypernymy (e.g., PPDB, Pavlick et al. 2015; BabelNet, Ehrmann et al. 2014; WordNet, Fellbaum 1998; UMLS). Mirror-BERT, however, does not rely on any external data to construct the positive examples. In a nutshell, given a set of non-duplicated strings X , we assign individual labels (yi ) to each string and build a dataset D = {(xi , yi )|xi ∈ X , yi ∈ {1, . . . , |X |}}. We then create self-duplicated training data D0 simply by repeating every element in D. In other words, let X = {x1 , x2 , . . .}. We then have D = {(x1 , y1 ), (x2 , y2 ), . . .} and D0 = {(x1 , y1 ), (x1 , y 1 ), (x2 , y2 ), (x2 , y 2 ), . . .} where x1 = x1 , y1 = y 1 , x2 = x2 , y2 = y 2 , . ."
2021.emnlp-main.109,D19-1006,0,0.084111,"dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we randomly sample 1,000 sentence pairs from the Quora Qu"
2021.emnlp-main.109,2021.emnlp-main.552,0,0.385338,"Missing"
2021.emnlp-main.109,2021.acl-long.72,0,0.0257597,"ry of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. Very recently, there has been a growing interest in using individual raw sentences for self- as well as on biomedical entity linking. The large gains over base ML"
2021.emnlp-main.109,N16-1162,1,0.835972,"cross-lingual tasks. Self-supervised text representations have a large body of literature. Here, due to space constraints, we provide a highly condensed summary of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. V"
2021.emnlp-main.109,J15-4004,1,0.738393,"BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021). For QNLI, we sample 10k sentences from its training set. Training Setup and Details. The hyperparameters of word-level models are tuned on SimLex-999 (Hill et al., 2015); biomedical models are tuned on COMETA (zero-shot-general split). Sentencelevel models are tuned on the dev set of STS-b. τ in Eq. (1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.1. Sentence-level models use a random span masking 3 github.com/deepmipt/deepPavlovEval We follow the setup of Li et al. (2020) and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, ≈5.4k pairs) using 0/1 labels and cosine similarity scores of QA embeddings. 4 lang.→ EN FR ET AR ZH RU ES PL avg. fastText .528 .560 .447 .409 .428 .435 .488"
2021.emnlp-main.109,P19-1070,1,0.91152,"Missing"
2021.emnlp-main.109,2021.acl-long.197,0,0.292243,"Missing"
2021.emnlp-main.109,2021.eacl-main.270,1,0.773582,"Missing"
2021.emnlp-main.109,P19-1580,0,0.029665,"Probing the impact of dropout. Table 7: Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and random span masking in the English STS tasks. v1 == v¯ 1 controlled dropout controlled dropout dropout( v1 ) == dropout( v¯ 1 ) Figure 6: Under controlled dropout, if two strings are identical, they will have an identical set of dropout masks throughout the encoding process. other augmentation types work as well? Recent work points out that pretrained MLM are heavily overparameterised and most Transformer heads can be pruned without hurting task performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). Zhou et al. (2020) propose a drophead method: it randomly prunes attention heads at MLM training as a regularisation step. We thus evaluate a variant of Mirror-BERT where the dropout layers are replaced with such dropheads:12 this results in even stronger STS performance, cf. Tab. 7. In short, this hints that the Mirror-BERT framework might benefit from other data and feature augmentation techniques in future work.13 Regularisation or Augmentation? (Tab. 8). When using dropout, is it possible that we are simply observing the effect of adding/remov"
2021.emnlp-main.109,2020.cl-4.5,1,0.887829,"Missing"
2021.emnlp-main.109,2021.acl-long.410,1,0.821601,"Missing"
2021.emnlp-main.109,2020.emnlp-main.586,1,0.883623,"Missing"
2021.findings-acl.50,Q17-1010,0,0.00624602,"how much could be improved in this way. 6.2 We implement our model with PyTorch (Paszke et al., 2017). For the primary and secondary encoders, we use a 3-layer transformer with model size of 256 and heads of 8. Since the decoder has to integrate the information from both encoders, we build it with a larger capacity. The number of layers is set to 4. The model size and the attention heads are set to be 512 and 8. For the sequence tagger S that is used in the IANet+S model, we use a 2-layer LSTM with hidden size of 512. In the experiments, we adopt pretrained 300dimensional FastText Embeddings (Bojanowski et al., 2017) to perform the PSI algorithm. During training , we use Adam (Kingma and Ba, 2015) to optimize our model with a learning rate of 1e-4. In all experiments, we set αp in Algorithm 1 as 0.1 based on the performance on the validation set. Model Comparisons 6.3 We compare the proposed model with several representative baselines, including Residual-LSTM (Prakash et al., 2016), β-VAE (Higgins et al., 2017), Transformer (Vaswani et al., 2017), DNPG (Li et al., 2019), LBOW-Topk and LBOW-Gumbel (Fu et al., 2019)3 . To compare different inference approaches, three variants of our model are used. IANet+X:"
2021.findings-acl.50,D14-1179,0,0.0210566,"Missing"
2021.findings-acl.50,D17-1091,0,0.0258231,"a notable margin. We also show that the proposed approach can generate paraphrases in an interpretable and controllable way. 1 Figure 1: Examples of paraphrase pair sampled from Quora and MSCOCO datasets in which the words in red refer to the primary content and the rest of the words make up the secondary content. Introduction Paraphrases refer to text (often sentences) that share the same meaning but use different choices of words and their ordering. Automatic generation of paraphrases is a longstanding problem that is important to many downstream NLP applications such as question answering (Dong et al., 2017; Buck et al., 2018), machine translation (Cho et al., 2014), and semantic parsing (Su and Yan, 2017). Most early research adopts the sequence-to-sequence model (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018) to map the input text to its paraphrase by processing and generating each word in a uniform way. Rather than processing each word uniformly, some recent studies tackles this task in a decomposable manner. For instance, Li et al. (2019) adopt an external word aligner to extract paraphrasing patterns at different levels of granularity and then perform generation. Fu et al. (2019)"
2021.findings-acl.50,N18-1170,0,0.0205869,"nd Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learning (Roy and Grangier, 2019) for this task. While achieving satisfactory results, these above methods do not offer users the way to control the generation process in a fine-grained way. To incorporate controllability into the generation model, different approaches have been proposed. Iyyer et al. (2018) trained the model to produce the paraphrased sentence with a given syntax. Li et al. (2019) proposed to adopt an external word aligner to train the model to generate paraphrases from different levels. In Fu et al. (2019)’s work, the model generates paraphrases by planning the neighbour of words and realizing the complete sentence. 3 Primary/Secondary Identification Given an input sentence, our goal is to identify the primary content that are likely to appear in the paraphrased sentence. To this end, we propose a Primary/Secondary Identification (PSI) approach which dynamically evaluates the i"
2021.findings-acl.50,2020.acl-main.535,0,0.742577,"ntrollable way. 2 Related Work The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches. Early researches included rule-based approaches (McKeown, 1979; Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010). With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task. Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality. Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learning (Roy and Grangier, 2019) for this task. While achieving satisfactory results, these above methods do not offer users the way to control the generation process in a fine-grained way. To incorporate controllability into the generation model, different approaches have been proposed. Iyyer et al. (2018) trained"
2021.findings-acl.50,D18-1421,0,0.0785783,"rds in red refer to the primary content and the rest of the words make up the secondary content. Introduction Paraphrases refer to text (often sentences) that share the same meaning but use different choices of words and their ordering. Automatic generation of paraphrases is a longstanding problem that is important to many downstream NLP applications such as question answering (Dong et al., 2017; Buck et al., 2018), machine translation (Cho et al., 2014), and semantic parsing (Su and Yan, 2017). Most early research adopts the sequence-to-sequence model (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018) to map the input text to its paraphrase by processing and generating each word in a uniform way. Rather than processing each word uniformly, some recent studies tackles this task in a decomposable manner. For instance, Li et al. (2019) adopt an external word aligner to extract paraphrasing patterns at different levels of granularity and then perform generation. Fu et al. (2019) first use source words to predict their neighbors and then organize the predicted neighbors into a complete sentence. In this work, we investigate decomposable paraphrase generation from a different perspective. Specif"
2021.findings-acl.50,P19-1332,0,0.110996,"Automatic generation of paraphrases is a longstanding problem that is important to many downstream NLP applications such as question answering (Dong et al., 2017; Buck et al., 2018), machine translation (Cho et al., 2014), and semantic parsing (Su and Yan, 2017). Most early research adopts the sequence-to-sequence model (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018) to map the input text to its paraphrase by processing and generating each word in a uniform way. Rather than processing each word uniformly, some recent studies tackles this task in a decomposable manner. For instance, Li et al. (2019) adopt an external word aligner to extract paraphrasing patterns at different levels of granularity and then perform generation. Fu et al. (2019) first use source words to predict their neighbors and then organize the predicted neighbors into a complete sentence. In this work, we investigate decomposable paraphrase generation from a different perspective. Specifically, we consider using a non-parametric approach to label each token in an input sentence as either (i) primary, or (ii) secondary. Intuitively, the primary content of a sentence refers to the factual information that defines the sha"
2021.findings-acl.50,W04-1013,0,0.0862739,"and secondary content using the exact PSI(X, Y) algorithm against the reference Y. The reason to include this model is that, besides our proposed alternatives, there are 3 The hyperparameter setups and optimization in all baseline models are the same as their original works. For methods that do not release their code, we directly use the results in their original papers. Implementation Details Evaluation Metrics Following previous studies (Prakash et al., 2016; Fu et al., 2019; Li et al., 2019), we report results on several automatic metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). All lower n-gram metrics (1-4 grams in BLEU and 1-2 grams in ROUGE) are reported. In addition, we include iBLEU (i-B) (Sun and Zhou, 2012) as another evaluation metric, which penalizes repeating the source sentence in its paraphrase. 6.4 Main Results Table 2 lists the results on both datasets. We see that the transformer baseline already achieves pretty strong results. This is because the capacity of transformer model is large enough to fit the datasets quite well. Nonetheless, in most of the evaluation metrics, our model outperforms previous studies by a notable margin, demonstrating the ef"
2021.findings-acl.50,P12-2008,0,0.0272385,"es our proposed alternatives, there are 3 The hyperparameter setups and optimization in all baseline models are the same as their original works. For methods that do not release their code, we directly use the results in their original papers. Implementation Details Evaluation Metrics Following previous studies (Prakash et al., 2016; Fu et al., 2019; Li et al., 2019), we report results on several automatic metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). All lower n-gram metrics (1-4 grams in BLEU and 1-2 grams in ROUGE) are reported. In addition, we include iBLEU (i-B) (Sun and Zhou, 2012) as another evaluation metric, which penalizes repeating the source sentence in its paraphrase. 6.4 Main Results Table 2 lists the results on both datasets. We see that the transformer baseline already achieves pretty strong results. This is because the capacity of transformer model is large enough to fit the datasets quite well. Nonetheless, in most of the evaluation metrics, our model outperforms previous studies by a notable margin, demonstrating the effectiveness of the proposed approach. By comparing different variants of our model, we see that IANet-ref achieves the best results on all m"
2021.findings-acl.50,J10-3003,0,0.0567702,"ggregation decoder which integrates the processed results and generates the paraphrased sentence. We test the proposed approach on two benchmark datasets with automatic and human evaluation. The results show that our approach outperforms previous studies and can generate paraphrases in an interpretable and controllable way. 2 Related Work The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches. Early researches included rule-based approaches (McKeown, 1979; Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010). With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task. Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality. Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learni"
2021.findings-acl.50,P79-1016,0,0.586149,"rocess the identified primary and secondary content; and (2) an aggregation decoder which integrates the processed results and generates the paraphrased sentence. We test the proposed approach on two benchmark datasets with automatic and human evaluation. The results show that our approach outperforms previous studies and can generate paraphrases in an interpretable and controllable way. 2 Related Work The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches. Early researches included rule-based approaches (McKeown, 1979; Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010). With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task. Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality. Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techn"
2021.findings-acl.50,C88-2088,0,0.711158,"tified primary and secondary content; and (2) an aggregation decoder which integrates the processed results and generates the paraphrased sentence. We test the proposed approach on two benchmark datasets with automatic and human evaluation. The results show that our approach outperforms previous studies and can generate paraphrases in an interpretable and controllable way. 2 Related Work The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches. Early researches included rule-based approaches (McKeown, 1979; Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010). With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task. Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality. Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techniques like reinforcement l"
2021.findings-acl.50,P02-1040,0,0.114577,"ts, this model obtains the primary and secondary content using the exact PSI(X, Y) algorithm against the reference Y. The reason to include this model is that, besides our proposed alternatives, there are 3 The hyperparameter setups and optimization in all baseline models are the same as their original works. For methods that do not release their code, we directly use the results in their original papers. Implementation Details Evaluation Metrics Following previous studies (Prakash et al., 2016; Fu et al., 2019; Li et al., 2019), we report results on several automatic metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). All lower n-gram metrics (1-4 grams in BLEU and 1-2 grams in ROUGE) are reported. In addition, we include iBLEU (i-B) (Sun and Zhou, 2012) as another evaluation metric, which penalizes repeating the source sentence in its paraphrase. 6.4 Main Results Table 2 lists the results on both datasets. We see that the transformer baseline already achieves pretty strong results. This is because the capacity of transformer model is large enough to fit the datasets quite well. Nonetheless, in most of the evaluation metrics, our model outperforms previous studies by a notable margin"
2021.findings-acl.50,C16-1275,0,0.0506663,"Missing"
2021.findings-acl.50,P19-1605,0,0.0125868,"th the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task. Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality. Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever. Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learning (Roy and Grangier, 2019) for this task. While achieving satisfactory results, these above methods do not offer users the way to control the generation process in a fine-grained way. To incorporate controllability into the generation model, different approaches have been proposed. Iyyer et al. (2018) trained the model to produce the paraphrased sentence with a given syntax. Li et al. (2019) proposed to adopt an external word aligner to train the model to generate paraphrases from different levels. In Fu et al. (2019)’s work, the model generates paraphrases by planning the neighbour of words and realizing the complete"
2021.findings-acl.50,D17-1127,0,0.0287079,"e and controllable way. 1 Figure 1: Examples of paraphrase pair sampled from Quora and MSCOCO datasets in which the words in red refer to the primary content and the rest of the words make up the secondary content. Introduction Paraphrases refer to text (often sentences) that share the same meaning but use different choices of words and their ordering. Automatic generation of paraphrases is a longstanding problem that is important to many downstream NLP applications such as question answering (Dong et al., 2017; Buck et al., 2018), machine translation (Cho et al., 2014), and semantic parsing (Su and Yan, 2017). Most early research adopts the sequence-to-sequence model (Prakash et al., 2016; Cao et al., 2017; Li et al., 2018) to map the input text to its paraphrase by processing and generating each word in a uniform way. Rather than processing each word uniformly, some recent studies tackles this task in a decomposable manner. For instance, Li et al. (2019) adopt an external word aligner to extract paraphrasing patterns at different levels of granularity and then perform generation. Fu et al. (2019) first use source words to predict their neighbors and then organize the predicted neighbors into a co"
2021.findings-acl.50,D19-1008,0,0.0131377,"We assume that if the token xi belongs to the primary content that is maintained in both X and Y, then removing it from X will cause a significant drop in the pairwise similarity between X and Y. Based on this assumption, we measure the importance of xi as the ratio of change in the pairwise similarity score as G(xi ; X, Y) = F(X, Y) − F(X0 , Y) . F(X, Y) (1) Intuitively, a higher G(xi ; X, Y) means a larger decrease in the pairwise similarity, indicating a higher importance of the token xi and vice versa. Similarity Measurement We now describe the details of the function F(·, ·). Inspired by Zhelezniak et al. (2019), we measure the pairwise similarity between X and Y based on a non-parametric rank correlation coefficient. Specifically, given X and Y, we first transform them into the representation matrices M(X) ∈ R|X|×D and M(Y) ∈ R|Y|×D via a D-dimensional pretrained embeddings. Then, the matrices are mapped into fixed size context vectors x ˆ ∈ R1×D and yˆ ∈ R1×D via an element-wise max-pooling operation. Finally, the pairwise similarity F(X, Y) is measured using Spearman’s correlation coefficient ρˆ of the context vectors x ˆ and yˆ as P 6× D xj ] − r[ˆ yj ])2 j=1 (r[ˆ F(X, Y) = 1 − (2) D × (D2 − 1) w"
2021.findings-emnlp.76,P19-1483,0,0.0347194,"Missing"
2021.findings-emnlp.76,P18-1082,0,0.0274759,"tions are: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automatic and human evaluations reporting state-of-the-art results on two benchmark datasets. (3) In-depth analysis revealing the merits of the proposed approach in terms of controllability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed"
2021.findings-emnlp.76,W18-6521,0,0.0204159,"e PLMs, our performance improvements suggest that the incorporation of an explicit content plan can provide effective guiding signal for the model to achieve better generation results. Evaluation on Content Planning. Next, we compare our content planner with other pipeline models in terms of content planning performance. Following Zhao et al. (2020), we report the results on planning accuracy (P-A) and planning BLEU-2 score (B-2) against the human-generated plans4 . In addition, we examine two ablated variants of our 4 899 The human-generated plans are provided in the enriched WebNLG dataset (Ferreira et al., 2018). Seen Model ADAPT† TILB-SMT† MELBOURNE† GTR-LSTM† Transformer† Step-by-Step† PLANENC† Based on PLMs Switch-GPT T5‡ T5+Prefix‡ Ours Unseen Overall B. 60.59 54.29 54.52 54.00 56.28 53.30 64.42 M. 0.44 0.42 0.41 0.37 0.42 0.44 0.45 B. 10.53 29.88 33.27 29.20 23.04 38.23 38.23 M. 0.19 0.33 0.33 0.28 0.21 0.34 0.37 B. 31.06 44.28 45.13 37.10 47.24 47.24 52.78 M. 0.31 0.38 0.37 0.31 0.39 0.39 0.41 60.98 63.90 64.71 65.42 0.43 0.46 0.45 0.48 40.67 52.80 53.67 54.52 0.34 0.41 0.42 0.44 52.17 57.10 59.70 60.51 0.40 0.44 0.44 0.46 Table 3: Text generation results on WebNLG datasets, where B. and M. rep"
2021.findings-emnlp.76,D19-1052,0,0.0351926,"Missing"
2021.findings-emnlp.76,W17-3518,0,0.13991,"this work, we formulate the intermediate content plan as an ordered list of tokens for its simplicity and wide applicability to data with different structures. For tabular data, each token in the content plan is a slot key from the table. As for graphical data with RDF structure, each token represents the predicate from an RDF triple. In Figure 1, we provide examples for both cases. To fully evaluate our approach, we test the proposed model on two benchmarks with different data structures: (i) ToTTo dataset (Parikh et al., 2020) with tabular data, and (ii) WebNLG dataset (Colin et al., 2016; Gardent et al., 2017) with graphical data. Compared with previous state-of-the-art approaches, our model achieves better performance in terms of generation quality as judged by both human and automatic evaluations. In particular, the results also show that the outputs of our model are highly controllable and contain diverse structures. In summary, our contributions are: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automatic and human evaluations reporting state-of-the-art results on two benchmark datasets."
2021.findings-emnlp.76,W18-6505,0,0.0700864,"systems (Wen et al., 2016), restaurant as- outputs with diverse structures by simply changing sistant (Novikova et al., 2017), and open domain the input planning information (i.e. a content plan), question answering (Chen et al., 2021). which could potentially benefit other applications To address this task, many researchers have de- such as paraphrasing and data augmentation. signed sophisticated neural models based on variTo control the output structure, we need an inous methods, such as soft-template (Wiseman et al., termediate “planning” signal (i.e. a content plan) 2018), copy mechanism (Gehrmann et al., 2018), which informs the model what to generate and in and pre-trained language models (Kale and Rastogi, what order. To this end, we propose a Plan-then2020; Ribeiro et al., 2020). While achieving im- Generate (PlanGen) framework which consists of pressive results, most existing studies only focused two components: a content planner and a sequence on producing results that are close to the references. generator. Given the input data, the content planner On the other hand, the controllability of such mod- first predicts the most plausible content plan that els is still under-explored, i.e. what to"
2021.findings-emnlp.76,2020.inlg-1.14,0,0.512214,"ntrollability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed to first use a classifier to select the key contents. The planning and surface realisation of the selected contents are then addressed by a subsequent Seq2seq model. More related to our work, some researchers studied how neural models can benefit from traditional NLG steps (Kukich, 1983; McKeown, 1992), that is, (i) content planning and (ii) surface reali"
2021.findings-emnlp.76,P13-1138,0,0.413458,"ees (Moryossef et al., 2 Related Work 2019), the ordering of graph nodes (Zhao et al., Data-to-text generation is a long-standing problem 2020), and the multi-step pipeline that includes (Reiter and Dale, 1997) that aims at producing natdiscourse ordering, lexicalization, and regular exural language descriptions of structured data. Trapression generation (Ferreira et al., 2019). While ditional systems are primarily built on templateachieving satisfactory results, these approaches can based algorithms (Oh and Rudnicky, 2000; Stent only be applied to data with graphical structure. et al., 2004; Kondadadi et al., 2013). With recent Compared with previous studies, we show that our advances in deep learning, researchers have shifted content planning approach is more accurate and their attention to neural generation models that can less dependent on the data structure. In addition, be summarized into two categories. by providing the desired content plan, our model End-to-End Models. Many existing studies are can control the output structure on both the intradedicated to building end-to-end neural models sentence and inter-sentence levels (§7.3). 896 Figure 2: PlanGen Framework: Given the structured data (T ),"
2021.findings-emnlp.76,P83-1022,0,0.719178,"s et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed to first use a classifier to select the key contents. The planning and surface realisation of the selected contents are then addressed by a subsequent Seq2seq model. More related to our work, some researchers studied how neural models can benefit from traditional NLG steps (Kukich, 1983; McKeown, 1992), that is, (i) content planning and (ii) surface realisation. To simultaneously select the key contents and arrange their orderings (i.e. content planning), different strategies are proposed such as the most probable traversal of graph trees (Moryossef et al., 2 Related Work 2019), the ordering of graph nodes (Zhao et al., Data-to-text generation is a long-standing problem 2020), and the multi-step pipeline that includes (Reiter and Dale, 1997) that aims at producing natdiscourse ordering, lexicalization, and regular exural language descriptions of structured data. Trapression"
2021.findings-emnlp.76,2020.acl-main.703,0,0.247826,"sis revealing the merits of the proposed approach in terms of controllability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed to first use a classifier to select the key contents. The planning and surface realisation of the selected contents are then addressed by a subsequent Seq2seq model. More related to our work, some researchers studied how neural models can benefit from traditional NLG steps (Kukich, 1983; M"
2021.findings-emnlp.76,2021.ccl-1.108,0,0.0383661,"Missing"
2021.findings-emnlp.76,P19-1197,0,0.114117,"b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed to first use a classifier to select the key contents. The planning and surface realisation of the selected contents are then addressed by a subsequent Seq2seq model. More related to our work, some researchers studied how neural models can benefit from traditional NLG steps (Kukich, 1983; McKeown, 1992), that is, (i) content planning and (ii) surface realisation. To simultaneously select the key contents and arrange their orderings (i.e. content planning), different strategies are proposed such as the most probable traversal of graph trees (Moryossef et al., 2 Related Work 2019), the"
2021.findings-emnlp.76,N19-1236,0,0.031452,"Missing"
2021.findings-emnlp.76,W17-5525,0,0.0690337,"sponse would be “Alma 1 Introduction Jodorowsky played Evelyn in Kids in Love.”. While Generating natural language from structured data both answers are semantically equivalent, produc(Gatt and Krahmer, 2018), i.e. data-to-text genera- ing the answer with the most appropriate structure tion, is a research problem that is crucial to many allows the system to sound less robotic and be easdownstream NLP applications. Some examples are ily understood. (2) It allows the model to generate dialogue systems (Wen et al., 2016), restaurant as- outputs with diverse structures by simply changing sistant (Novikova et al., 2017), and open domain the input planning information (i.e. a content plan), question answering (Chen et al., 2021). which could potentially benefit other applications To address this task, many researchers have de- such as paraphrasing and data augmentation. signed sophisticated neural models based on variTo control the output structure, we need an inous methods, such as soft-template (Wiseman et al., termediate “planning” signal (i.e. a content plan) 2018), copy mechanism (Gehrmann et al., 2018), which informs the model what to generate and in and pre-trained language models (Kale and Rastogi, wh"
2021.findings-emnlp.76,W00-0306,0,0.712604,"planning), different strategies are proposed such as the most probable traversal of graph trees (Moryossef et al., 2 Related Work 2019), the ordering of graph nodes (Zhao et al., Data-to-text generation is a long-standing problem 2020), and the multi-step pipeline that includes (Reiter and Dale, 1997) that aims at producing natdiscourse ordering, lexicalization, and regular exural language descriptions of structured data. Trapression generation (Ferreira et al., 2019). While ditional systems are primarily built on templateachieving satisfactory results, these approaches can based algorithms (Oh and Rudnicky, 2000; Stent only be applied to data with graphical structure. et al., 2004; Kondadadi et al., 2013). With recent Compared with previous studies, we show that our advances in deep learning, researchers have shifted content planning approach is more accurate and their attention to neural generation models that can less dependent on the data structure. In addition, be summarized into two categories. by providing the desired content plan, our model End-to-End Models. Many existing studies are can control the output structure on both the intradedicated to building end-to-end neural models sentence and"
2021.findings-emnlp.76,P02-1040,0,0.110249,"le”, “Notes”, “Title”} from the table, the predicted ordering sequence is {3, 1, 2, ∅, 4}. The content plan {“Name” → “Role” → “Year” → “Title”} can then be predicted by omitting the “Notes” key and re-arranging other keys following the predicted ordering sequence. 4.2 4.3 (3) |S 0 | − R(S, S 0 , T, C) X 0 log PG (Si0 |S<i ; E([T : C])). i=1 The reward function R(S, S 0 , T, C) measures the structure of the sampled sequence S 0 against the input content plan C, and its surface form against the reference text S as R(S, S 0 , T, C) = B(S, S 0 ) + B(C, C 0 ), (4) where B(·, ·) is the BLEU score (Papineni et al., 2002). C 0 = F(T, S 0 ), and F is described in §3. By optimizing Eq. (3), the structure of the output is encouraged to follow the content plan. 4.4 Learning The learning objective of the content planner is LCRF = − log PCRF and PCRF is defined in Eq. (1). For the sequence generator, at the first 10k steps, we train it with LLM as described in Eq. (2). Then, we incorporate the structure-aware RL objective (Eq. (3)) and further train the sequence generator with LLM + LRL for 5k more steps. 5 5.1 Experiment Setup Datasets and Evaluation Metrics ToTTo Dataset (Parikh et al., 2020) consists of Wikipedia"
2021.findings-emnlp.76,2020.emnlp-main.89,0,0.0637426,"ar data) and (b) WebNLG dataset (graphical data with RDF structure). content plan. In this work, we formulate the intermediate content plan as an ordered list of tokens for its simplicity and wide applicability to data with different structures. For tabular data, each token in the content plan is a slot key from the table. As for graphical data with RDF structure, each token represents the predicate from an RDF triple. In Figure 1, we provide examples for both cases. To fully evaluate our approach, we test the proposed model on two benchmarks with different data structures: (i) ToTTo dataset (Parikh et al., 2020) with tabular data, and (ii) WebNLG dataset (Colin et al., 2016; Gardent et al., 2017) with graphical data. Compared with previous state-of-the-art approaches, our model achieves better performance in terms of generation quality as judged by both human and automatic evaluations. In particular, the results also show that the outputs of our model are highly controllable and contain diverse structures. In summary, our contributions are: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automati"
2021.findings-emnlp.76,J04-3003,0,0.164677,"Missing"
2021.findings-emnlp.76,P19-1195,0,0.0985186,"e: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automatic and human evaluations reporting state-of-the-art results on two benchmark datasets. (3) In-depth analysis revealing the merits of the proposed approach in terms of controllability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework. Ma et al. (2019) proposed to first use a classifier"
2021.findings-emnlp.76,2020.tacl-1.18,0,0.0401083,"n subset contains out-of-domain instances. Following previous studies, we report the the automatic result of BLEU and METEOR (Banerjee and Lavie, 2005). 5.2 Implementation Details Our implementation is based on the Huggingface Library (Wolf et al., 2019). We optimize the model using Adam (Kingma and Ba, 2015) with a learning rate of 2e−5 and a batch size of 64. 6 Results In this section, we report the experimental results. 6.1 ToTTo Results We compare our model with the latest models on ToTTo dataset, including NCP (Puduppully et al., 2019a), Pointer-Generator (See et al., 2017), BERTto-BERT (Rothe et al., 2020) and T5-3B (Kale and Rastogi, 2020). Similar to our model, the later two are also based on pre-trained language models. Table 2 lists the results on ToTTo test set. For most of the metrics, our model with 140M parameters outperforms the current state-of-the-art T5-3B model which has over 2.8B parameters. The results 3 PARENT is a word-overlap based metric that reflects the factual accuracy of the generated text in relation to both the input table and the reference sentence. on the PARENT metric suggest that our model can generate more factually accurate text. Moreover, in the Non-Overlap subse"
2021.findings-emnlp.76,P17-1099,0,0.047921,"een and Unseen subset. The Unseen subset contains out-of-domain instances. Following previous studies, we report the the automatic result of BLEU and METEOR (Banerjee and Lavie, 2005). 5.2 Implementation Details Our implementation is based on the Huggingface Library (Wolf et al., 2019). We optimize the model using Adam (Kingma and Ba, 2015) with a learning rate of 2e−5 and a batch size of 64. 6 Results In this section, we report the experimental results. 6.1 ToTTo Results We compare our model with the latest models on ToTTo dataset, including NCP (Puduppully et al., 2019a), Pointer-Generator (See et al., 2017), BERTto-BERT (Rothe et al., 2020) and T5-3B (Kale and Rastogi, 2020). Similar to our model, the later two are also based on pre-trained language models. Table 2 lists the results on ToTTo test set. For most of the metrics, our model with 140M parameters outperforms the current state-of-the-art T5-3B model which has over 2.8B parameters. The results 3 PARENT is a word-overlap based metric that reflects the factual accuracy of the generated text in relation to both the input table and the reference sentence. on the PARENT metric suggest that our model can generate more factually accurate text."
2021.findings-emnlp.76,2020.acl-main.704,0,0.0193839,"Pointer-Generator BERT-to-BERT T5-3B Ours BLEU 19.2 41.6 44.0 49.5 49.2 PARENT 29.2 51.6 52.6 58.4 58.7 Overlap BLEURT -0.576 0.076 0.121 0.230 0.249 BLEU 24.5 50.6 52.7 57.5 56.9 PARENT 32.5 58.0 58.4 62.6 62.8 Non-Overlap BLEURT -0.491 0.244 0.259 0.351 0.371 BLEU 13.9 32.2 35.1 41.4 41.5 PARENT 25.8 45.2 46.8 54.2 54.6 BLEURT -0.662 -0.092 -0.017 0.108 0.126 Table 2: ToTTo test set results: All reported results, including ours, can be found in the official Leaderboard.2 as the model input. We report the automatic result of BLEU-4, PARENT3 (Dhingra et al., 2019), and a learnt metric BLEURT (Sellam et al., 2020). Note that ToTTo features a hidden test set with two splits: Overlap and Non-Overlap. The NonOverlap set contains out-of-domain examples. To get the test set result, a submission must be made to the leaderboard. WebNLG Dataset is used in the WebNLG challenge (Gardent et al., 2017). For each data instance, the input is a set of RDF triples from DBPedia and the output is their textual description. The test set of WebNLG features a Seen and Unseen subset. The Unseen subset contains out-of-domain instances. Following previous studies, we report the the automatic result of BLEU and METEOR (Banerje"
2021.findings-emnlp.76,P04-1011,0,0.768847,"Missing"
2021.findings-emnlp.76,2021.eacl-main.18,1,0.882398,"tructures. In summary, our contributions are: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automatic and human evaluations reporting state-of-the-art results on two benchmark datasets. (3) In-depth analysis revealing the merits of the proposed approach in terms of controllability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework."
2021.findings-emnlp.76,2021.findings-emnlp.77,1,0.87721,"tructures. In summary, our contributions are: (1) A novel Plan-then-Generate (PlanGen) framework that consists of a content planner and a sequence generator for data-to-text generation. (2) Extensive automatic and human evaluations reporting state-of-the-art results on two benchmark datasets. (3) In-depth analysis revealing the merits of the proposed approach in terms of controllability and diversity. with different strategies like soft-templates (Wiseman et al., 2018; Ye et al., 2020), attention awareness (Liu et al., 2018; Colin and Gardent, 2019), and retrieved prototypes (Li et al., 2020; Su et al., 2021b). Gehrmann et al. (2018), Puduppully et al. (2019a,b), and Chen et al. (2020b) adopted copy mechanism for content selection to improve the information coverage of the outputs. With recent advance in pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020), several researchers (Chen et al., 2020a,b; Kale and Rastogi, 2020; Ribeiro et al., 2020) have studied the ways to adapt PLMs into the data-to-text generation task. Pipeline Models. Another line of research investigates ways to tackle the generation problem in a pipeline framework."
2021.findings-emnlp.76,P12-2008,0,0.0370186,"e simply vary the input content plan and use greedy decoding. We use two variants of the input content plan: (1) the content plan predicted by the content planner (Predict), or (2) the reference content plan (Oracle). For each variant, five results are generated by either using the input content plan (CP), or using five randomly shuffled forms of the content plan (Shuffled CP). The outputs are expected to vary in the latter case only. Metric. To measure the output quality, BLEU and PARENT scores are reported. To evaluate the generation diversity, we use Self-BLEU (Zhu et al., 2018) and iBLEU (Sun and Zhou, 2012) metrics8 . Results. Table 6 lists the results in which our model ranks best on all metrics. On the quality metrics, we observe notable performance improvements from our model by using the reference content plan (Oracle), suggesting that the choice of content plan has a significant impact on the outputs. By shuffling the content plan, our model shows the largest decrease in BLEU and PARENT, showing 7 8 For each decoding strategy, five results are generated. For all evaluation metrics, we use the same hyper-parameters as in the original works that proposed the metric. CP × × RL × X 3 X × Ours X"
2021.findings-emnlp.76,P18-1151,0,0.0221447,"ver 2.8B parameters. The results 3 PARENT is a word-overlap based metric that reflects the factual accuracy of the generated text in relation to both the input table and the reference sentence. on the PARENT metric suggest that our model can generate more factually accurate text. Moreover, in the Non-Overlap subset, our model achieves the best result on all metrics, showing its robustness to out-of-domain examples. 6.2 WebNLG Results We compare our approach with two types of models on WebNLG dataset. The first type of models does not use pre-trained language models (PLMs), including GTR-LSTM (Trisedya et al., 2018), Transformer (Ferreira et al., 2019), Step-by-Step (Moryossef et al., 2019), and PLANENC (Zhao et al., 2020). Similar to ours, the latter three are pipeline models that utilize different methods to decide the output planning before generating the result. The second line of research utilizes PLMs, including Switch-GPT (Chen et al., 2020b), T5 (Kale and Rastogi, 2020), and T5+Prefix (Ribeiro et al., 2020). The Switch-GPT model applies a copy mechanism to copy content from the source to the output. We also include the top systems of the WebNLG challenge, including ADAPT, TILBSMT, and MELBOURNE."
2021.findings-emnlp.76,N16-1015,1,0.829728,"rast, to a different query “What role did Alma Jodorowsky play in Kids in Love?”, a natural response would be “Alma 1 Introduction Jodorowsky played Evelyn in Kids in Love.”. While Generating natural language from structured data both answers are semantically equivalent, produc(Gatt and Krahmer, 2018), i.e. data-to-text genera- ing the answer with the most appropriate structure tion, is a research problem that is crucial to many allows the system to sound less robotic and be easdownstream NLP applications. Some examples are ily understood. (2) It allows the model to generate dialogue systems (Wen et al., 2016), restaurant as- outputs with diverse structures by simply changing sistant (Novikova et al., 2017), and open domain the input planning information (i.e. a content plan), question answering (Chen et al., 2021). which could potentially benefit other applications To address this task, many researchers have de- such as paraphrasing and data augmentation. signed sophisticated neural models based on variTo control the output structure, we need an inous methods, such as soft-template (Wiseman et al., termediate “planning” signal (i.e. a content plan) 2018), copy mechanism (Gehrmann et al., 2018), wh"
2021.findings-emnlp.76,D18-1356,0,0.0480523,"Missing"
2021.findings-emnlp.76,2020.acl-main.224,0,0.145139,"e generated text in relation to both the input table and the reference sentence. on the PARENT metric suggest that our model can generate more factually accurate text. Moreover, in the Non-Overlap subset, our model achieves the best result on all metrics, showing its robustness to out-of-domain examples. 6.2 WebNLG Results We compare our approach with two types of models on WebNLG dataset. The first type of models does not use pre-trained language models (PLMs), including GTR-LSTM (Trisedya et al., 2018), Transformer (Ferreira et al., 2019), Step-by-Step (Moryossef et al., 2019), and PLANENC (Zhao et al., 2020). Similar to ours, the latter three are pipeline models that utilize different methods to decide the output planning before generating the result. The second line of research utilizes PLMs, including Switch-GPT (Chen et al., 2020b), T5 (Kale and Rastogi, 2020), and T5+Prefix (Ribeiro et al., 2020). The Switch-GPT model applies a copy mechanism to copy content from the source to the output. We also include the top systems of the WebNLG challenge, including ADAPT, TILBSMT, and MELBOURNE. Evaluation on Text Generation. Table 3 lists the results of different methods in terms of text generation. We"
2021.findings-emnlp.77,2020.coling-main.179,0,0.134773,"4.8 10.4/4.1 26.2/14.7 37.9/28.3 36.1/26.2 36.7/27.1 37.6/28.1 39.1/29.9 41.2/31.7 42.8/33.0 14.3/3.1 36.4/26.1 12.0/5.1 28.0/16.2 39.8/30.1 37.2/28.6 37.8/29.4 38.7/29.2 40.3/30.7 42.7/33.6 45.9/35.7 16.2/4.3 39.0/29.2 11.6/4.7 29.2/17.7 40.3/30.5 39.4/30.1 39.3/30.6 40.0/30.3 41.8/32.0 44.2/34.9 47.6/37.5 17.7/4.9 42.1/31.7 13.1/5.8 31.7/20.0 42.9/33.0 42.2/32.6 42.3/32.8 43.5/33.9 45.0/35.4 47.9/38.1 50.7/40.1 Table 1: Results on datasets from three domains. In each entry, x/y denotes the model performance on BLEU4/ROUGE-4(F-measure). † and ‡ results are copied from Chen et al. (2020b) and Gong et al. (2020). All results acquired with the proposed framework outperform the original model with a significance level p-value < 0.01. a training example (T, S, y) ∈ D, the learning of P|y| g is defined as: LLM = − i=1 log pθ (yi |y<i ; X), where θ denotes the parameters of the generator, and X = [T :S]. Moreover, we introduce a new content-aware learning objective. Our motivation is that the prototypes S is likely to contain information that is irrelevant to the table, thus the generator should learn to ignore the irrelevant part of S and only focus on the useful information. To this end, inspired by Wel"
2021.findings-emnlp.77,Q18-1031,0,0.0237701,"s, a prototype selector then selects the top n prototypes based on the table-text pairwise similarity. Lastly, a sequence generator takes the table and the selected prototypes as input to produce the output. To prevent the model from uncritically copying the information contained in the prototypes that is irrelevant to the table, we introduce a content-aware learning objective when training the generator. In recent years, retrieval-based (i.e. templatebased) text generation has been studied in different NLP areas, including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question"
2021.findings-emnlp.77,2020.inlg-1.14,0,0.161455,"Missing"
2021.findings-emnlp.77,2020.emnlp-main.550,0,0.110198,"Linguistics: EMNLP 2021, pages 910–917 November 7–11, 2021. ©2021 Association for Computational Linguistics Figure 1: An overview of the proposed Prototype-to-Generate (P2G) framework. compared to our approach. Firstly, most previous research (Gu et al., 2017; Wu et al., 2019; Kazemnejad et al., 2020) build their retrieval corpus based on data consisting of aligned source-target pairs, which precludes the use of abundant unlabelled data. Secondly, current retrieval mechanisms are either based on lexical similarity (e.g. BM25) where its accuracy cannot be guaranteed, or large neural networks (Karpukhin et al., 2020) which require a large amount of data to train. Notably, our framework is independent of the choice of generation model. For a comprehensive evaluation, we test our approach on three representative models, including the current state of the art. The experimental results on three datasets show that our framework leads to remarkable performance improvements across all evaluation metrics. 2 Methodology Figure 1 depicts an overview of our framework. Given a linearized table T = {t1 , ..., t|T |}, where ti = {ai , vi } is an attribute-value pair, an IR system first retrieves a set of m candidates R"
2021.findings-emnlp.77,2020.acl-main.535,0,0.153296,"arity. Lastly, a sequence generator takes the table and the selected prototypes as input to produce the output. To prevent the model from uncritically copying the information contained in the prototypes that is irrelevant to the table, we introduce a content-aware learning objective when training the generator. In recent years, retrieval-based (i.e. templatebased) text generation has been studied in different NLP areas, including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of table-to-text generation stems from the structura"
2021.findings-emnlp.77,P13-1138,0,0.452714,"Missing"
2021.findings-emnlp.77,D16-1128,0,0.0257409,"Missing"
2021.findings-emnlp.77,2020.acl-main.703,0,0.494977,"prototypes as input to produce the output. To prevent the model from uncritically copying the information contained in the prototypes that is irrelevant to the table, we introduce a content-aware learning objective when training the generator. In recent years, retrieval-based (i.e. templatebased) text generation has been studied in different NLP areas, including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of table-to-text generation stems from the structural difference between the table and the natural language text. W"
2021.findings-emnlp.77,W04-1013,0,0.0260889,"Retri-Gen (Wu et al., 2019) and RA-Gen (Lewis et al., 2020b), where RA-Gen is based on PLMs. We select three representative models (Switch-GPT, Table-GPT, and T5-Prefix) to test the proposed framework. 3.2 Experiment Setup We conduct experiments on three benchmark fewshot table-to-text datasets (Chen et al., 2020b) from different domains: Humans, Books, and Songs. Following previous studies (Chen et al., 2020b; Gong et al., 2020), we train our model on different settings by varying the training size from {50, 100, 200, 500}, and evaluate our model using BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics. Test sets of Humans, Books, and Songs contain 13587, 5252 and 11879 instances. To build the IR system, we use Lucene1 to pre-index all sentences contained in the English Wikipedia (Dec. 2018 dump). For each table, the IR system retrieves 100 sentences as the candidates R. The prototype selector then select the top 3 reTable 1 lists the experiment results, where P2G+X indicates using model X under our framework. We can see that the proposed framework consistently and significantly improves the performance of all three models on all metrics, showing the robustness and universality of o"
2021.findings-emnlp.77,P19-1197,0,0.48115,"ework: Prototype-to-Generate (P2G), for table-to-text generation under the few-shot scenario. The proposed framework utilizes the retrieved prototypes, which are jointly selected by an IR system and a novel prototype selector to help the model bridging the structural gap between tables and texts. Experimental results on three benchmark datasets with three state-of-the-art models demonstrate that the proposed framework significantly improves the model performance across various evaluation metrics. 1 Introduction et al., 2020b). This motivates us to investigate fewshot table-to-text generation (Ma et al., 2019; Chen et al., 2020b), that allows the model to learn a satisfactory table-to-text mapping with limited labelled training data. In this work, we propose to address this problem by augmenting data-to-text generation models with prototype memory acquired from a large unlabelled corpus. Our motivation is two-fold: (1) Relevant human-authored texts, termed “prototypes”, are informative and can teach the model how to better describe the table when limited training data is available. (2) However, traditional lexical-based IR systems, e.g. BM25, are inaccurate and the quality of their results are not"
2021.findings-emnlp.77,W17-5525,0,0.0554869,", including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of table-to-text generation stems from the structural difference between the table and the natural language text. With recent advances in neural networks, many sophisticated neural models (Liu et al., 2018; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Su et al., 2021b) have been proposed to address this problem. While achieving impressive results, such neural models are data-hungry, i.e. large amounts of training data are required for them to learn the mapping be"
2021.findings-emnlp.77,W00-0306,0,0.625046,"Missing"
2021.findings-emnlp.77,P02-1040,0,0.109672,"etrieval-based approaches include Retri-Gen (Wu et al., 2019) and RA-Gen (Lewis et al., 2020b), where RA-Gen is based on PLMs. We select three representative models (Switch-GPT, Table-GPT, and T5-Prefix) to test the proposed framework. 3.2 Experiment Setup We conduct experiments on three benchmark fewshot table-to-text datasets (Chen et al., 2020b) from different domains: Humans, Books, and Songs. Following previous studies (Chen et al., 2020b; Gong et al., 2020), we train our model on different settings by varying the training size from {50, 100, 200, 500}, and evaluate our model using BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics. Test sets of Humans, Books, and Songs contain 13587, 5252 and 11879 instances. To build the IR system, we use Lucene1 to pre-index all sentences contained in the English Wikipedia (Dec. 2018 dump). For each table, the IR system retrieves 100 sentences as the candidates R. The prototype selector then select the top 3 reTable 1 lists the experiment results, where P2G+X indicates using model X under our framework. We can see that the proposed framework consistently and significantly improves the performance of all three models on all metrics, showing the robustness"
2021.findings-emnlp.77,P19-1195,0,0.090401,"om structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of table-to-text generation stems from the structural difference between the table and the natural language text. With recent advances in neural networks, many sophisticated neural models (Liu et al., 2018; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Su et al., 2021b) have been proposed to address this problem. While achieving impressive results, such neural models are data-hungry, i.e. large amounts of training data are required for them to learn the mapping between tables and texts. This can prohibit these models from being applied to real-world applications due to the huge data curation overhead (Chen 910 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 910–917 November 7–11, 2021. ©2021 Association for Computational Linguistics Figure 1: An overview of the proposed Prototype-to-Generate (P2G) framework."
2021.findings-emnlp.77,P04-1011,0,0.381926,"Missing"
2021.findings-emnlp.77,2021.findings-acl.50,1,0.729524,"pes based on the table-text pairwise similarity. Lastly, a sequence generator takes the table and the selected prototypes as input to produce the output. To prevent the model from uncritically copying the information contained in the prototypes that is irrelevant to the table, we introduce a content-aware learning objective when training the generator. In recent years, retrieval-based (i.e. templatebased) text generation has been studied in different NLP areas, including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of"
2021.findings-emnlp.77,2021.findings-emnlp.76,1,0.791587,"pes based on the table-text pairwise similarity. Lastly, a sequence generator takes the table and the selected prototypes as input to produce the output. To prevent the model from uncritically copying the information contained in the prototypes that is irrelevant to the table, we introduce a content-aware learning objective when training the generator. In recent years, retrieval-based (i.e. templatebased) text generation has been studied in different NLP areas, including machine translation (Gu et al., 2017), unconditional text generation (Guu et al., 2018), dialogue systems (Wu et al., 2019; Su et al., 2021c), paraphrase generation (Kazemnejad et al., 2020; Su et al., 2021a), and question answering (Lewis et al., 2020b). Despite their differences, we identify two major limitations in previous studies Generating natural language from structured table (Gatt and Krahmer, 2018), i.e. table-to-text generation, is an important research problem for various NLP applications, such as biographical descriptions (Lebret et al., 2016), restaurant information (Novikova et al., 2017), basketball game summaries (Wiseman et al., 2017), and open-domain question answering (Chen et al., 2021). The main challenge of"
2021.findings-emnlp.77,D17-1239,0,0.05537,"Missing"
2021.findings-emnlp.77,D18-1356,0,0.0490979,"Missing"
2021.hackashop-1.1,2020.acl-main.370,0,0.0400492,"dversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus. Costanza Conforti1 , Jakob Berndt2 , Mohammad Taher Pilehvar1,3 , Marco Basaldella1 , Chryssi Giannitsarou2 , Flavio Toxvaerd2 , Nigel Collier1 1 Language Technology Lab, University of Cambridge 2 Faculty of Economics, University of Cambridge 3 Tehran Institute for Advanced Studies, Iran {cc918,jb2088}@cam.ac.uk Abstract datasets, which are especially expensive to obtain for items such as news articles. As a consequence, following research on other text classification tasks such as sentiment analysis (Du et al., 2020), research in SD investigated effective methods for cross-domain SD, where the scarcity of data for a specific dataset is supplemented with stance-annotated data from other domains. In this context, preliminary research in adversarial domain adaptation obtained promising results for both Twitter (Wang et al., 2020) and news (Xu et al., 2019) SD. In this paper, we focus on the new task of crossgenre SD: we consider adversarial knowledge transfer from two datasets, WT– WT and S TANDER, which collect samples in the same domain (i.e. the financial domain), but which belong to different genres (i.e"
2021.hackashop-1.1,C18-1158,0,0.153682,"able online; moreover, user-generated data tend to be relatively short and compact, and thus more affordable to annotate and process. Starting from popular shared tasks such as Pomerleau and Rao (2017), SD on complex and articulated input, such as news articles, has gained increasing popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the healthcare industry (Table 2): an M&A operation, or merger, is the process in which a company (the buyer) attempts to acquire the ownership of another company (the"
2021.hackashop-1.1,2020.findings-emnlp.365,1,0.850925,"g popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the healthcare industry (Table 2): an M&A operation, or merger, is the process in which a company (the buyer) attempts to acquire the ownership of another company (the target). A merger succeeds if ownership of the target is transferred, but can fail at any stage of discussions or can be blocked by authorities due to, e.g., antitrust concerns (Bruner and Perella, 2004). 1 Proceedings of the EACL Hackashop on News Media Content Analysis and Aut"
2021.hackashop-1.1,D19-1410,0,0.0290299,"Missing"
2021.hackashop-1.1,C18-1283,0,0.0208441,"sed on user-generated data, such as Twitter or Reddit (Gorrell et al., 2019): this is mainly due to the abundance of such data, which are usually freely available online; moreover, user-generated data tend to be relatively short and compact, and thus more affordable to annotate and process. Starting from popular shared tasks such as Pomerleau and Rao (2017), SD on complex and articulated input, such as news articles, has gained increasing popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the health"
2021.repl4nlp-1.5,D14-1179,0,0.0324408,"Missing"
2021.repl4nlp-1.5,D19-1223,0,0.0639957,"Missing"
2021.repl4nlp-1.5,P15-1144,0,0.0764975,"nal learned representations should ideally be sparse (Bengio et al., 2013; Burgess et al., 2018; Tonolini et al., 2019). In other words it allows us to have varying number of active dimension per sentence2 (Bengio, 2009) in a fixed dimensional vector3 . But if sparsity4 is expected, could it be learned from data without supervision? A handful of studies in NLP that have delved into building sparse representations of words either during the learning phase (Faruqui and Dyer, 2015; Yogatama et al., 2015) or as a post-processing step on top of existing representations (e.g., word2vec embeddings) (Faruqui et al., 2015; Sun et al., 2016; Subramanian et al., 2018; Arora et al., 2018; Li and Hao, 2019). These methods have not been developed for sentence embeddings, with the exception of Trifonov et al. (2018) which makes a strong assumption by forcing the latent sentence representation to be a sparse categorical distribution. In parallel, Variational Autoencoders (VAEs) (Kingma and Welling, 2014) have been effective in capturing semantic closeness of sentences in the learned representation space (Bowman et al., 2016; Prokhorov et al., 2019; Xu et al., 2019; Balasubramanian et al., 2020). Furthermore, methods"
2021.repl4nlp-1.5,2020.emnlp-main.378,0,0.35217,"t of experiments on three text classification corpora: Yelp (sentiment analysis - 5 classes) (Yang et al., 2017), DBpedia and Yahoo (topic classification - 14 and 10 classes respectively) (Zhang et al., 2015). First, we compare performance of the sparse latent representations with their dense counterpart on the text classification tasks (§4.2). Second, the stability of sparsification of HSVAE is compared with the state-of-the-art MAT-VAE (§4.3). Then, to better understand performance of our model on the downstream task, we examine the sparsity patterns (§4.4). BERT vs GRU Encoder. Inspired by Li et al. (2020b), we replace the GRU network used in VAE and HSVAE encoders with a pretrained BERT9 (Devlin et al., 2019), while keeping the GRU decoder. We refer to these models as B-VAE and B-HSVAE, respectively. Also, we compare the 5 https://github.com/jxhe/vae-laggingencoder/blob/master/prepare_data.py. 6 https://github.com/srhrshr/torchData sets/blob/master/dbpedia_csv.tar.gz 7 https://github.com/jxhe/vae-laggingencoder/blob/master/prepare_data.py. 8 https://spacy.io 9 After extracting features from a sequence with BERT, we then applying MLPs to extract features for the posterior distributions, as it"
2021.wassa-1.19,N18-2004,0,0.028242,"Missing"
2021.wassa-1.19,W17-0904,0,0.0176178,"Missing"
2021.wassa-1.19,N10-1004,0,0.0507836,"ity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal fro"
2021.wassa-1.19,P09-1113,0,0.0298862,"f the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a"
2021.wassa-1.19,D18-1045,0,0.0667885,"Missing"
2021.wassa-1.19,P17-2090,0,0.0199156,"imple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an"
2021.wassa-1.19,C14-1168,0,0.0284561,"tional Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data"
2021.wassa-1.19,C18-1158,0,0.0464177,"Missing"
2021.wassa-1.19,P18-1096,0,0.0234355,". ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained"
2021.wassa-1.19,D14-1083,0,0.024934,"We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an important step in the rumor verification pipeline (Zubiaga et al., 2018b): in this framework, popular shared tasks focused on SD of rumorous tweets (Gorrell et al., 2018) and Reddit posts (Gorrell et al., 2018). These works analyze tweets in a tree-shaped stream (Zubiaga et al., 2015). Note that SD constitutes a related but different task than sentiment analysis (Mohammad et al., 2017): the latter focuses on the polarity expressed w.r.t. a topic"
2021.wassa-1.19,P16-1009,0,0.113015,"Missing"
2021.wassa-1.19,E17-2088,0,0.018627,"ments in performance, with gains in F1 scores ranging from +3.4 to +5.1. 1 Introduction Stance Detection (SD) is a widely investigated task (Mohammad et al., 2017), which constitutes an important component of many complex NLP problems, ranging from fake news detection to rumour verification (Vlachos and Riedel, 2014; Baly et al., 2018; Zubiaga et al., 2018b). Since from early works (Agrawal et al.), research on SD focused on user-generated content, ranging from blogs and commenting sections on websites (Hercig et al.), to Reddit or Facebook posts (Klenner et al.) and, above all, Twitter data (Inkpen et al., 2017; Zubiaga et al., 2018a). Recently, Conforti et al. (2020) released WillThey-Won’t-They (WT– WT), a very large corpus of stance-annotated tweets discussing five US mergers and acquisitions (M&A) operations spanning over two industries: healthcare and entertainment. M&A is a general term that refers to the process in which the ownership of companies are transferred. Such process has many stages that range from informal talks to the closing of a deal, and discussions may not be publicly disclosed until a formal agreement is signed (Bruner 2 Cross-Target Generalization with Synthetically Annotate"
2021.wassa-1.19,W17-5801,0,0.0265033,"and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an important step in the rumor verification pipeline (Zubiaga et al., 2018b): in this framework, popular shared tasks focused on SD of rumorous tweets (Gorrell et al., 2018) and Reddit posts (Gorrell et al., 2018). These works analyze tweets in a tree-shaped stream (Zubiaga et al., 2015). Note that SD constitutes"
2021.wassa-1.19,D17-1215,0,0.0232299,"VSHealth buys Aetna CVS #PBM has resulted in delays in therapy, switches, etc all documented. Terrible! The sentiment of the tweet w.r.t. the target is negative: the user believes that the merger would harm patients; however, its stance is comment, as it is Comparison with previous work on Data Augmentation and Domain Adaptation. Note that this framework differs from data augmentation (DAug) strategies adopted to supply for small training data, like in question answering (Kafle et al.), machine translation (Fadaee et al.) distillation (Tang et al., 2019), or for adversarial sample generation (Jia and Liang, 2017). Such techniques, inspired by DAug in speech recognition and computer vision (Chatfield et al., 2014), work by deformating gold samples to generate new artificial samples (for example, by random token masking, or POS- or semantics-based token replacement). Our approach differs in a number of aspects: 1. In DAug the goal is to enlarge a set of initial ID data; here, we assume we don’t have any ID training data, but only OOD; 2. For this reason, while DAug helps to cope with data sparsity, our approach is also useful for domain shifts; 3. In DAug, sample generation might introduce two kinds of"
2021.wassa-1.19,W14-2508,0,0.0225027,"offer great benefits. In this paper, we apply a weakly supervised framework to enhance cross-target generalization through synthetically annotated data. We focus on Twitter SD and show experimentally that integrating synthetic data is helpful for cross-target generalization, leading to significant improvements in performance, with gains in F1 scores ranging from +3.4 to +5.1. 1 Introduction Stance Detection (SD) is a widely investigated task (Mohammad et al., 2017), which constitutes an important component of many complex NLP problems, ranging from fake news detection to rumour verification (Vlachos and Riedel, 2014; Baly et al., 2018; Zubiaga et al., 2018b). Since from early works (Agrawal et al.), research on SD focused on user-generated content, ranging from blogs and commenting sections on websites (Hercig et al.), to Reddit or Facebook posts (Klenner et al.) and, above all, Twitter data (Inkpen et al., 2017; Zubiaga et al., 2018a). Recently, Conforti et al. (2020) released WillThey-Won’t-They (WT– WT), a very large corpus of stance-annotated tweets discussing five US mergers and acquisitions (M&A) operations spanning over two industries: healthcare and entertainment. M&A is a general term that refer"
2021.wnut-1.34,N19-1421,0,0.0610058,"Missing"
C00-1030,W98-1118,0,0.0156778,"For such cases we will need to add postprocessing rules. There are of course many NE models that are not based on HMMs that have had success in the NE task at the MUC conferences. Our main requirement in implementing a model for the domain of molecular-biology has been ease of development, accuracy and portability to other sub-domains since molecular-biology itself is a wide eld. HMMs seemed to be the most favourable option at this time. Alternatives that have also had considerable success are decision trees, e.g. (Nobata et al., 1999) and maximum-entropy. The maximum entropy model shown in (Borthwick et al., 1998) in particular seems a promising approach because of its ability to handle overlapping and large feature sets within a well founded mathematical framework. However this implementation of the method seems to incorporate a number of handcoded domain speci c lexical features and dictionary lists that reduce portability. Undoubtedly we could incorporate richer features into our model and based on the evidence of others we would like to add head nouns as one type of feature in the future. Acknowledgements We would like to express our gratitude to Yuka Tateishi and Tomoko Ohta of the Tsujii laborato"
C00-1030,P96-1041,0,0.0234265,"s taken from a sub-domain of MEDLINE and the results of our experiments on this corpus. 2 Background Recent studies into the use of supervised learning-based models for the named entity task in the micro-biology domain have shown that models based on HMMs and decision trees such as (Nobata et al., 1999) are much more generalisable and adaptable to new classes of words than systems based on traditional hand-built patterns and domain speci c heuristic rules such as (Fukuda et al., 1998), overcoming the problems associated with data sparseness with the help of sophisticated smoothing algorithms (Chen and Goodman, 1996). HMMs can be considered to be stochastic nite state machines and have enjoyed success in a number of elds including speech recognition and part-of-speech tagging (Kupiec, 1992). It has been natural therefore that these models have been adapted for use in other wordclass prediction tasks such as the named-entity task in IE. Such models are often based on ngrams. Although the assumption that a word's part-of-speech or name class can be predicted by the previous n-1 words and their classes is counter-intuitive to our understanding of linguistic structures and long distance dependencies, this sim"
C00-1030,E99-1043,1,0.641132,"ation of technical expressions in these texts. This task can be considered to be similar to the named entity task in the MUC evaluation exercises (MUC, 1995). In our current work we are using abstracts available from PubMed's MEDLINE (MEDLINE, 1999). The MEDLINE database is an online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles. With the rapid growth in the number of published papers in the eld of molecular-biology there has been growing interest in the application of information extraction, (Sekimizu et al., 1998)(Collier et al., 1999)(Thomas et al., 1999)(Craven and Kumlien, 1999), to help solve some of the problems that are associated with information overload. In the remainder of this paper we will rst of all outline the background to the task and then describe the basics of HMMs and the formal model we are using. The following sections give an outline of a new tagged corpus (Ohta et al., 1999) that our team has developed using abstracts taken from a sub-domain of MEDLINE and the results of our experiments on this corpus. 2 Background Recent studies into the use of supervised learning-based models for the named entity ta"
C00-1030,M93-1007,0,\N,Missing
C00-1030,A97-1029,0,\N,Missing
C08-1057,A97-1052,0,0.0209042,"h in section 2 and data in section 3. Experimental evaluation is reported in section 4. Section 5 provides discussion and section 6 concludes. 2 tactic frames (Levin, 1993). Most verb classification approaches have therefore employed shallow syntactic slots or SCFs as basic features. Some have supplemented them with further information about verb tense, voice, and/or semantic selectional preferences on argument heads.2 The preliminary experiment on biomedical verb classification (Korhonen et al., 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). Because such deep syntactic features seem ideally suited for challenging biomedical data, we adopted the same basic approach, but we designed and extracted a range of novel feature sets which include additional syntactic and semantic information. The SCF extraction system assigns each occurrence of a verb in the parsed data as a member of one of the 163 verbal SCFs, builds a lexical entry for each verb (type) and SCF combination, and filters noisy entries out of the lexicon. We d"
C08-1057,briscoe-carroll-2002-robust,0,0.0117721,"ion and section 6 concludes. 2 tactic frames (Levin, 1993). Most verb classification approaches have therefore employed shallow syntactic slots or SCFs as basic features. Some have supplemented them with further information about verb tense, voice, and/or semantic selectional preferences on argument heads.2 The preliminary experiment on biomedical verb classification (Korhonen et al., 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). Because such deep syntactic features seem ideally suited for challenging biomedical data, we adopted the same basic approach, but we designed and extracted a range of novel feature sets which include additional syntactic and semantic information. The SCF extraction system assigns each occurrence of a verb in the parsed data as a member of one of the 163 verbal SCFs, builds a lexical entry for each verb (type) and SCF combination, and filters noisy entries out of the lexicon. We do not employ the filter in our work because its primary aim is to filter out SCFs containing adjuncts (as opposed"
C08-1057,J06-2001,0,0.275819,"Missing"
C08-1057,P06-1044,1,0.927677,"on, predicateargument identification, event extraction and the identification of biomedical (e.g. interaction) relations. However, no such classification is available. Recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (Schulte im Walde, 2006; Joanis et al., 2007; Sun et al., 2008). A number of machine learning (ML) methods have been applied to classify mainly syntactic features (e.g. subcategorization frames (SCFs)) extracted from crossdomain corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Korhonen et al. (2006) have recently applied such an approach to biomedical texts. Their preliminary experiment shows encouraging results but further work is required before such an approach can be used to benefit practical BIO-NLP. We conduct a large-scale investigation to find optimal features for biomedical verb classification. We introduce a range of theoretically-motivated feature sets and evaluate them thoroughly using a robust method new to the task: a cost-based framework for pairwise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained using feature sets which"
C08-1057,kunze-2000-extension,0,0.0703435,"Missing"
C08-1057,C00-2094,0,0.019116,"ACTIVATE GENES : WAF 1). Lexical classes can be used to abstract away from individual words, or to build a lexical organization which predicts much of the behaviour of a new word by associating it with an appropriate class. They have proved useful for various NLP application tasks, e.g. parsing, word sense dis449 1 http://www.nlm.nih.gov/research/umls Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449–456 Manchester, August 2008 ambiguation, semantic role labeling, information extraction, question-answering, machine translation (Dorr, 1997; Prescher et al., 2000; Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005). A large-scale classification specific to the biomedical data could support key BIONLP tasks such as anaphora resolution, predicateargument identification, event extraction and the identification of biomedical (e.g. interaction) relations. However, no such classification is available. Recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (Schulte im Walde, 2006; Joanis et al., 2007; Sun et al., 2008). A number of machine learning (ML) methods have been applied to cl"
C08-1057,W04-3213,0,\N,Missing
C12-1040,A97-1029,0,0.221825,"g The text collections are passed into a module which splits texts into sentences and tokens. This was done using the OpenNLP library with a Maximum Entropy model1 . Abbreviation expansion is then done using BioText (Schwartz and Hearst, 2003) to make a list of local abbreviation occurring in each paper which we then replace with their full form. A similar approach is adopted by Khordad in their staged rules. 3.3.2 Machine learning labeler Within the machine learning module we compare two widely used sequence labeling models: a second order Hidden Markov Models (HMM) (Rabiner and Juang, 1986; Bikel et al., 1997) with Viterbi decoding and a linear chain Conditional Random Fields (CRF) (Lafferty et al., 2001; McDonald and Pereira, 2005). Both are run as fully supervised models. Class labels for tokens follow the standard BIO system, i.e. each token receives the label O if it is not an NE, B plus the entity name when it starts an entity, and I plus the entity name when it is inside an entity. The main advantage of the CRF over the vanilla HMM is that it estimates the conditional probability distribution over labeled sequences. Both use the freely available Java-based MALLET implementation2 with default"
C12-1040,C00-1030,1,0.837361,"ic disorders, text mining. Proceedings of COLING 2012: Technical Papers, pages 647–662, COLING 2012, Mumbai, December 2012. 647 1 Introduction Biomedical named entity recognition (NER) is a computational technique used to identify and classify strings of text (mentions) that designate important concepts in biomedicine. Over the last fourteen years there has been considerable interest in this problem with a variety of generic and entity-specific algorithms applied to extract the names of genes, gene products, cells, chemical compounds and diseases (Fukuda et al., 1998; Rindflesch et al., 1999; Collier et al., 2000; Kazama et al., 2002; Zhou et al., 2003; Settles, 2004; Kim et al., 2004; Leaman and Gonzalez, 2008). As the first stage in the integrated semantic linking of knowledge between literature and structured databases it is critically important to maximise the effectiveness of this step. Despite significant progress in NER there is still no one size fits all solution. Barriers arise because of ambiguity in the text and coding schema. Ambiguity in the text comes in various forms according to the semantic type of the entity but can be caused by a lack of standard nomenclatures, extensive and growing"
C12-1040,W02-0301,0,0.0461033,"ing. Proceedings of COLING 2012: Technical Papers, pages 647–662, COLING 2012, Mumbai, December 2012. 647 1 Introduction Biomedical named entity recognition (NER) is a computational technique used to identify and classify strings of text (mentions) that designate important concepts in biomedicine. Over the last fourteen years there has been considerable interest in this problem with a variety of generic and entity-specific algorithms applied to extract the names of genes, gene products, cells, chemical compounds and diseases (Fukuda et al., 1998; Rindflesch et al., 1999; Collier et al., 2000; Kazama et al., 2002; Zhou et al., 2003; Settles, 2004; Kim et al., 2004; Leaman and Gonzalez, 2008). As the first stage in the integrated semantic linking of knowledge between literature and structured databases it is critically important to maximise the effectiveness of this step. Despite significant progress in NER there is still no one size fits all solution. Barriers arise because of ambiguity in the text and coding schema. Ambiguity in the text comes in various forms according to the semantic type of the entity but can be caused by a lack of standard nomenclatures, extensive and growing nomenclatures for pr"
C12-1040,W04-1213,1,0.850107,"s 647–662, COLING 2012, Mumbai, December 2012. 647 1 Introduction Biomedical named entity recognition (NER) is a computational technique used to identify and classify strings of text (mentions) that designate important concepts in biomedicine. Over the last fourteen years there has been considerable interest in this problem with a variety of generic and entity-specific algorithms applied to extract the names of genes, gene products, cells, chemical compounds and diseases (Fukuda et al., 1998; Rindflesch et al., 1999; Collier et al., 2000; Kazama et al., 2002; Zhou et al., 2003; Settles, 2004; Kim et al., 2004; Leaman and Gonzalez, 2008). As the first stage in the integrated semantic linking of knowledge between literature and structured databases it is critically important to maximise the effectiveness of this step. Despite significant progress in NER there is still no one size fits all solution. Barriers arise because of ambiguity in the text and coding schema. Ambiguity in the text comes in various forms according to the semantic type of the entity but can be caused by a lack of standard nomenclatures, extensive and growing nomenclatures for proteins/genes across multiple organisms or the widesp"
C12-1040,W05-0625,0,0.0359801,"Missing"
C12-1040,W06-0504,0,0.0182927,"n-overlapping’ (Alex et al., 2007). As a basic policy we do not allow embedding of entities within our corpus so annotators have to make a choice of entity class based on the longest matching span even though one entity may contain another entity of the same or a different type. We leave to future work consideration of other approaches, e.g. for handling discontinuous entity mentions. Within our guidelines we describe whether specific, generic, underspecified and negatively quantified mentions qualify. A summary of the rule set (available from the first author) is shown in Table 1. We follow (Magnini et al., 2006) in differentiating between specific, generic and underspecified mentions. 3.2 3.2.1 Annotated data sources Phenominer The Phenominer version 1 corpus contains 112 abstracts we selected from PubMed Central (PMC). 19 auto-immune diseases were selected from OMIM and from these records citations were then chosen. Diseases include Type 1 diabetes, Grave’s disease, Crohn’s disease, autoimmune thyroid disease, multiple sclerosis and inflammatory arthritis. In order to ground the article in discussion about both a disease and a phenotype, citations needed to contain the auto-immune disease term and a"
C12-1040,W04-1221,0,0.0359195,"al Papers, pages 647–662, COLING 2012, Mumbai, December 2012. 647 1 Introduction Biomedical named entity recognition (NER) is a computational technique used to identify and classify strings of text (mentions) that designate important concepts in biomedicine. Over the last fourteen years there has been considerable interest in this problem with a variety of generic and entity-specific algorithms applied to extract the names of genes, gene products, cells, chemical compounds and diseases (Fukuda et al., 1998; Rindflesch et al., 1999; Collier et al., 2000; Kazama et al., 2002; Zhou et al., 2003; Settles, 2004; Kim et al., 2004; Leaman and Gonzalez, 2008). As the first stage in the integrated semantic linking of knowledge between literature and structured databases it is critically important to maximise the effectiveness of this step. Despite significant progress in NER there is still no one size fits all solution. Barriers arise because of ambiguity in the text and coding schema. Ambiguity in the text comes in various forms according to the semantic type of the entity but can be caused by a lack of standard nomenclatures, extensive and growing nomenclatures for proteins/genes across multiple organ"
C12-1093,P12-1056,0,0.0639088,"at it fails to capture the fact that multiple terms may be involved with the same event (Zanzotto et al., 2011), and requires that at least one term undergoes a sufficiently high jump in relative frequency that the event can be identified. Topic models have been proposed as a means of better capturing events, by way of learning clusters of terms that are associated with a given event, as well as modelling changes in term co-occurrence rather than just term frequency. Most work based on topic modelling has been in the form of retrospective event detection models, however (Kireyev et al., 2009; Diao et al., 2012). Moving to the more general area of the machine learning, several online topic models have been proposed (Hoffman et al., 2010; AlSumait et al., 2008). Hoffman et al. (2010) introduced an online LDA variant that uses variational Bayes as the approximate posterior inference algorithm. The model that is closest in spirit to what we propose is On-Line LDA (OLDA) (AlSumait et al., 2008). Using collapsed Gibbs sampling for approximate inference, OLDA processes documents in an on-line fashion by resampling topic assignments for new documents using parameters from a previously learnt model. We retur"
C12-1093,D11-1024,0,0.0327199,"Missing"
C12-1093,N10-1012,1,0.0594681,"Missing"
C12-1093,N10-1021,0,0.0855016,"Missing"
C12-1093,D12-1134,0,0.0399574,"wish to detect events happening presently in 1 http://webtrends.about.com/od/twitter/a/why_twitter_uses_for_twitter.htm 1520 our time, however, we require on-line event detection models. An example application where real-time responsiveness is critical is earthquake detection (Sakaki et al., 2010), and trend analysis also clearly requires on-line processing in order to be of use (Mathioudakis and Koudas, 2010). Most on-line approaches, however, use a relatively simple keyword-based methodology over a pre-defined set of keywords (Culotta, 2010; Lampos and Cristianini, 2010; Weng and Lee, 2011; Zhao et al., 2012) rather than tackling the more challenging task of open-world event detection. Real-time first story detection (Petrovi´c et al., 2010; Osborne et al., 2012) is the task of detecting the mentions of a breaking story as close as possible in time to its first mention. Here, the system should ideally pick up on the breaking story within seconds or minutes of its first mention in order to have impact, e.g. as an alert system for a newswire agency or intelligence organisation. As such, the methods that are standardly applied to the task tend to be based on analysis of local “burstiness” in the data"
C12-1093,D11-1061,0,\N,Missing
C98-1042,P91-1022,0,0.156137,"automatic applications involving the analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 (Church, 1993)(Gale and Church, 1993)(Kay and Rhshcheisen, 1993), based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Vtsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dic"
C98-1042,P93-1002,0,0.138543,"separately. The task of sentence alignment is a critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 (Church, 1993)(Gale and Church, 1993)(Kay and Rhshcheisen, 1993), based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Vtsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from"
C98-1042,P93-1001,0,0.0185462,"nvolving the analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 (Church, 1993)(Gale and Church, 1993)(Kay and Rhshcheisen, 1993), based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Vtsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage li"
C98-1042,P98-1041,1,0.816217,"ath over the set of English and Japanese sentences. Cost being determined by the model&apos;s scores. The alignment space includes all possible combinations of nndtiple matches upto and including 3:3 alignments. The basic ,nodels are now outlined below. 4.1 M o d e l 1: L e x l c a l v e c t o r m a t c h i n g The lexical approach is perhaps the most robust for aligning texts in cognate language pairs, or where there is a large amount of reformatting in translation. It has also been shown to be particularly successful within the vector space model in nmltilingum information retrieval tasks, e.g. (Collier et al., 1998a),(Collier et al., 1998b), for aligning texts in non-cognate languages at the article level. The major limitation with lcxical matching is clearly the assumption of lexical correspondence - El. Taiwan ruling party sees power struggle in China E2. TAIPEI , Feb 9 ( Reuter ) - Taiwan&apos;s ruling Nationalist Party said a str~ggle to succeed Deng Xiaoping as China&apos;s most powerful man may have already begun. E3. &quot;Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is inevitable,&quot; a Nationalist Party report said. E4. China and Taiwan have been rivals since the Nationalists"
C98-1042,P91-1023,0,0.0273645,"slations in Japanese. Of these English words ,~ome 14,000 were proper nouns which were directly relevant to the vocabulary typically found in international news stories. Additionally we perform lexical normalisation before calculating the matching score and remove function words with a stop list. 4.2 M o d e l 2: B y t e - l e n g t h r a t i o s For Asian language pairs we cannot rely entirely on dictionary term matching. Moreover, algorithms which rely on matching cognates cannot be applied easily to English and some Asian language. We were motivated by statistical alignment models such as (Gale and Church, 1991) to investigate whether byte-length probabilities could improve or replace the lexical matching based method. The underlying assumption is that characters in an English sentence are responsible for generating some fraction of each character in the corresponding Japanese sentence. We derived a probability density function by making the assumption that English .and Japanese sentence length ratios are normally distributed. The parameters required for the model arc the mean, IL and variance, or, which we calculated fl&apos;om a training set of 450 hand-aligned sentences. These are then entered into Equ"
C98-1042,J93-1004,0,0.1519,"nalysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 (Church, 1993)(Gale and Church, 1993)(Kay and Rhshcheisen, 1993), based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Vtsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we"
C98-1042,C94-2175,0,0.0360415,"Missing"
C98-1042,P94-1012,0,0.651914,"models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 (Church, 1993)(Gale and Church, 1993)(Kay and Rhshcheisen, 1993), based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Vtsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains. • Dictionary-based approaches are founded on an assumption of lexictd correspondence between language pairs. We cannot alw"
C98-1042,C98-1041,1,\N,Missing
C98-1042,J93-1006,0,\N,Missing
collier-etal-2002-progress,J01-2002,0,\N,Missing
collier-etal-2002-progress,J01-2003,0,\N,Missing
collier-etal-2002-progress,collier-takeuchi-2002-pia,1,\N,Missing
D15-1194,I13-1041,0,0.0479175,"Missing"
D15-1194,N03-1017,0,0.263299,"tween word vector representations to effectively map a Twitter phrase to a medical concept. The main contributions of this paper are threefold: 1. We investigate the adaptation of phrase-based MT to map a Twitter phrase to a SNOMEDCT concept. 2. We propose to combine our adaptation of phrase-based MT and the similarity between word vector representations to map Twitter phrases to formal medical concepts. 3. We thoroughly evaluate the proposed approach using phrases from our collection of tweets related to the topic of adverse drug reactions (ADRs). 2 Related Work Phrase-based MT models, e.g. (Koehn et al., 2003; Och and Ney, 2004), have been shown to be effective in translation between languages, as they learn local term dependencies, such as collocations, reorderings, insertions and deletions. Koehn et al. (2003) showed that a phrase-based MT technique markedly outperformed traditional wordbased MT techniques on several benchmarks. In this work, we adapt the phrase-based MT technique of Koehn et al. (2003) for the medical text normalisation task. In particular, we use the phrase-based MT technique to translate phrases from Twitter language to formal medical language, before mapping the translated p"
D15-1194,P07-2045,0,0.00500875,"We learn the vector representations from the collections of tweets and medical articles, respectively, using window size of 10 words. The tweet collection (denoted Twitter) contains 419,702,147 English tweets, which are related to 11 drug names and 6 cities, while the medical article collection (denoted BMC) includes all medical articles from the BioMed Central8 . For both CBOW and GloVe, we create vector representations with vector sizes 50 and 200, respectively. 4.4 Learning Phrase-based Model We use the phrase-based MT technique of Koehn et al. (2003), as implemented in the Moses toolkit (Koehn et al., 2007)9 with default settings, to learn to translate from the Twitter language to the medical language. In particular, when training the translator, we show the learner pairs of the Twitter phrases and descriptions of the corresponding SNOMED-CT concepts. 5 Experimental Results We evaluate 6 different instantiations of the proposed approach discussed in Section 3, including: 6 https://code.google.com/p/word2vec/ http://nlp.stanford.edu/projects/ glove/ 8 http://www.biomedcentral.com/about/ datamining 9 http://www.statmt.org/moses/ 7 1. bestMT: set k = 1, when finding the translated phrase phrm for a"
D15-1194,J04-4002,0,0.128715,"presentations to effectively map a Twitter phrase to a medical concept. The main contributions of this paper are threefold: 1. We investigate the adaptation of phrase-based MT to map a Twitter phrase to a SNOMEDCT concept. 2. We propose to combine our adaptation of phrase-based MT and the similarity between word vector representations to map Twitter phrases to formal medical concepts. 3. We thoroughly evaluate the proposed approach using phrases from our collection of tweets related to the topic of adverse drug reactions (ADRs). 2 Related Work Phrase-based MT models, e.g. (Koehn et al., 2003; Och and Ney, 2004), have been shown to be effective in translation between languages, as they learn local term dependencies, such as collocations, reorderings, insertions and deletions. Koehn et al. (2003) showed that a phrase-based MT technique markedly outperformed traditional wordbased MT techniques on several benchmarks. In this work, we adapt the phrase-based MT technique of Koehn et al. (2003) for the medical text normalisation task. In particular, we use the phrase-based MT technique to translate phrases from Twitter language to formal medical language, before mapping the translated phrases to medical co"
D15-1194,W14-1609,0,0.0290673,"is on, to represent a particular word (Turian et al., 2010). Recently, techniques for learning high-quality word vector representations (i.e. distributed word representations) that could capture the semantic similarity between words, such as continuous bags of words (CBOW) (Mikolov et al., 2013b) and global vectors (GloVe) (Pennington et al., 2014), have been proposed. Indeed, these distributed word representations have been effectively applied in different systems that achieve state-ofthe-art performances for several NLP tasks, such as MT (Mikolov et al., 2013a) and named entity recognition (Passos et al., 2014). In this work, beside using word vector representations to measure the similarity between translated Twitter phrases and the description of medical concepts, we use the similarity between word vector representations of the original Twitter phrase and the description of a medical concept to augment the adapted phrase-based MT technique. 3 Medical Term Normalisation We discuss our adaptation of phrase-based MT for medical text normalisation in Section 3.1. Section 3.2 introduces our proposed approach for combining similarity score of word vector representations with the adapted phrase-based MT"
D15-1194,D14-1162,0,0.0742782,"Missing"
D15-1194,P10-1040,0,0.110232,"that a phrase-based MT technique markedly outperformed traditional wordbased MT techniques on several benchmarks. In this work, we adapt the phrase-based MT technique of Koehn et al. (2003) for the medical text normalisation task. In particular, we use the phrase-based MT technique to translate phrases from Twitter language to formal medical language, before mapping the translated phrases to medical concepts based on the ranked similarity of their word vector representations. Traditional approaches for creating word vector representations treated words as atomic units (Mikolov et al., 2013b; Turian et al., 2010). For instance, the one-hot representation used a vector with a length of the size of the vocabulary, where one dimension is on, to represent a particular word (Turian et al., 2010). Recently, techniques for learning high-quality word vector representations (i.e. distributed word representations) that could capture the semantic similarity between words, such as continuous bags of words (CBOW) (Mikolov et al., 2013b) and global vectors (GloVe) (Pennington et al., 2014), have been proposed. Indeed, these distributed word representations have been effectively applied in different systems that ach"
D16-1174,W06-3814,0,0.0438754,"Missing"
D16-1174,S14-2098,0,0.111441,"Missing"
D16-1174,J10-4006,0,0.140542,"Missing"
D16-1174,N15-1059,1,0.907262,"Missing"
D16-1174,D14-1110,0,0.440814,"Missing"
D16-1174,P11-1144,0,0.0530254,"Missing"
D16-1174,N15-1184,0,0.102007,"Missing"
D16-1174,N13-1092,0,0.160035,"Missing"
D16-1174,J15-4004,0,0.111179,"Missing"
D16-1174,P12-1092,0,0.738492,"Missing"
D16-1174,P15-1010,1,0.875169,"Missing"
D16-1174,N15-1070,0,0.467518,"Missing"
D16-1174,S14-2003,1,0.911966,"Missing"
D16-1174,S14-2072,0,0.0877215,"Missing"
D16-1174,D14-1113,0,0.443083,"Missing"
D16-1174,W11-0122,0,0.0701096,"Missing"
D16-1174,D14-1162,0,0.077956,"Missing"
D16-1174,S14-2093,0,0.0654737,"Missing"
D16-1174,N10-1013,0,0.382386,"Missing"
D16-1174,P15-1173,0,0.594965,"Missing"
D16-1174,C14-1016,0,0.34374,"Missing"
D16-1174,W09-3206,0,0.091897,"Missing"
D16-1174,P14-2089,0,0.0678899,"Missing"
D16-1174,J16-2003,0,\N,Missing
D18-1169,P07-1056,0,0.0314137,"Missing"
D18-1169,Q17-1010,0,0.131523,"e evaluate the performance of some of recent models on our dataset. These techniques can be broadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the most recent subword models. T"
D18-1169,S17-2002,1,0.857853,"liable dataset. 3.2 Construction Procedure The following four-phase procedure was used to construct the dataset: (1) A set of 660 rare words were carefully selected from a wide range of domains; (2) For each of these initial words, a pairing word was manually selected according to a randomly sampled score from the similarity scale (Section 3.2.2); (3) All pairs were scored by 8 annotators; (4) A final adjudication was performed to address disagreements (Section 3.2.3). 3.2.1 Similarity scale We adopted the five-point Likert scale used for the annotation of the datasets in SemEval-2017 Task 2 (Camacho-Collados et al., 2017). The task reported high IAA scores which reflects the welldefinedness and clarity of the scale. We provided annotators with the concise guideline shown in Table 1, along with several examples. Given the continuity of the scale, the annotators were given flexibility to select values in between the five points, whenever appropriate, with a step size of 0.5. The annotators were asked in the guidelines to make sure they were familiar with all common meanings of the word (as defined by WordNet or other online dictionaries). To facilitate the annotation, the annotators were provided with the defini"
D18-1169,D16-1235,0,0.0863574,"Missing"
D18-1169,D17-1030,0,0.0740015,"Missing"
D18-1169,J15-4004,0,0.122077,"our evaluation of mainstream word embeddings and recent word representation techniques on the dataset. Finally, concluding remarks are mentioned in Section 6. 2 Related Work Word similarity datasets have been one of the oldest, still most prominent, benchmarks for the evaluation and comparison of semantic representation techniques. As a result, several word similarity datasets have been constructed during the past few decades; to name a few: RG-65 (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2002), YP-130 (Yang and Powers, 2005), MEN-3K (Bruni et al., 2014), SimLex-999 (Hill et al., 2015), and SimVerb-3500 (Gerz et al., 2016). Many of these English word similarity datasets have been translated to other languages to create frameworks for multilingual (Leviant and Reichart, 2015) or crosslingual (CamachoCollados et al., 2017) semantic representation techniques. However, these datasets mostly target words that occur frequently in generic texts and, as a result, are not suitable for the evaluation of subword or rare word representation models. One may opt for transforming a frequent-word benchmarks into an artificial rare word dataset by downsampling the dataset’s words in the und"
D18-1169,W04-1213,1,0.707379,"Missing"
D18-1169,2005.mtsummit-papers.11,0,0.0111694,"Missing"
D18-1169,P14-2050,0,0.0581858,"challenging rare word benchmark should ideally reflect this phenomenon. To verify this in our dataset, we experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings constructed by combining the pre-trained Freebase Word2vec, which mostly comprises named entities, with two of the best performing embeddings evalua"
D18-1169,P17-1015,0,0.0255784,"Missing"
D18-1169,W13-3512,0,0.603424,"models in a downstream NLP system, despite being very important, does not provide a solid base for comparing different models, given that small variations in the architecture, parameter setting, or initialisation can lead to performance differences. Moreover, such an evaluation would reflect the “suitability” of representations for that specific configuration and for that particular task, and might not be conclusive for other settings. As far as generic evaluation is concerned, existing benchmarks generally target frequent words. An exception is the Stanford Rare Word (RW) Similarity dataset (Luong et al., 2013) which has been the standard evaluation benchmark for rare word representation techniques for the past few years. In Section 2.1, we will provide an in-depth analysis of RW and highlight that crowdsourcing the annotations, with no rigorous checkpoints, has compromised the reliability of the dataset. This is mainly reflected by the low inter-annotator agreement (IAA), a performance ceiling which is easily surpassed by many existing models. To overcome this barrier and to fill the gap for a reliable benchmark for the evaluation of subword and rare word representation techniques, we introduce a n"
D18-1169,P11-1015,0,0.193517,"Missing"
D18-1169,D14-1162,0,0.0947241,"lier in the Introduction, it is not possible to enumerate the entire vocabulary of a natural language, even if massive corpora are used. A challenging rare word benchmark should ideally reflect this phenomenon. To verify this in our dataset, we experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings construct"
D18-1169,E17-2062,1,0.778842,"and estimate the embedding for a rare word by exploiting different types of lexical knowledge encoded for it in the resource. The definition centroid model of Lazaridou et al. (2017) takes WordNet word glosses (definitions) as semantic clue. An embedding is induced for an unseen word by averaging the content words’ embeddings in its definition.12 The definition LSTM strategy of Bahdanau et al. (2017) extends the centroid model by encoding the definition using an LSTM network (Hochreiter and Schmidhuber, 1997), in order to better capture the semantics and word order in the definition. SemLand (Pilehvar and Collier, 2017) also uses WordNet, but takes a different approach which benefits from the graph structure of WordNet. For an unseen word, SemLand extracts the set of its semantically related words from WordNet and induces an embedding for the unseen word by combining pre-trained embeddings for the related words. 5.3 Experimental Setup We report results for the five techniques discussed in Sections 5.2.2 and 5.2.1. We used two of the best performing embedding sets, i.e., Glove cased CC and ConceptNet Numberbatch, to train the models (except FastText for which we use the pre-trained WikiNews subword embeddings"
D18-1169,D17-1010,0,0.0965811,"In this experiment, we evaluate the performance of some of recent models on our dataset. These techniques can be broadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the mos"
D18-1169,D16-1264,0,0.129264,"Missing"
D18-1169,P16-2068,0,0.0346199,"experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings constructed by combining the pre-trained Freebase Word2vec, which mostly comprises named entities, with two of the best performing embeddings evaluated on the dataset. Given that the word embeddings are not comparable across two different spaces, we"
D18-1169,D15-1033,0,0.0430563,"Missing"
D18-1169,N15-1186,0,0.0191791,"oadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the most recent subword models. The technique learns a mapping function from strings to embeddings by training a Bi-LSTM netw"
D18-1221,D15-1177,1,0.862559,"andom walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with weighted textual features, and an artificial “corpus” of random walks is created and used as input to the skipgram model (Mikolov et al., 2013) for generating an enhanced KB space—this part is covered in §3.1; (2) the transformation from text to entities is performed by a super"
D18-1221,Q16-1002,0,0.134813,"rgescale text-to-entity mapping and entity classification tasks, with state of the art results. 1 Introduction The task of associating a well-defined action, concept or piece of knowledge to a natural language utterance or text is a common problem in natural language processing and generic artificial intelligence (Tellex et al., 2011), and can emerge in many different forms. In NLP, the ability to code text into an entity of a knowledge graph finds applications in tasks such as question answering and information retrieval, or any task that involves some form of mapping a definition to a term (Hill et al., 2016; Rimell et al., 2016). Further, it can be invaluable in providing solutions to domain-specific challenges, for example medical concept normalisation (Limsopatham and Collier, 2016) and identification of adverse drug reactions (O’Connor et al., 2014). This paper details a model for efficiently mapping unrestricted text at the level of phrases and sentences to the entities of a knowledge base ∗ This paper is dedicated to the memory of Euripides Kartsaklis, a man who loved technology. (KB)—a task also referred to as text grounding or normalisation. The model aims at characterising short focused"
D18-1221,D13-1166,1,0.898941,"Missing"
D18-1221,C12-2054,1,0.886801,"Missing"
D18-1221,W13-3513,1,0.849346,"nts, optic, dendrite, rubiaceae, nonparametric, meninges, deviation, anesthetics Sense 2. tableware, meal, expectation, heartily, kitchen, hum, eating, forestay, suitors, croupier, companionship, restaurant, dishes, candles, cup, tea Sense 3. reassigned, projective, ultracentrifuge, polemoniaceous, thyronine, assumptions, lymphocyte, atomic, difficulties, intracellular, virgil, elementary, cartesian form of word sense disambiguation when composing word vectors can provide consistent improvements on end tasks such as sentence similarity and paraphrase detection (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2013). 6 Table 5: Derived senses for word table, visualised as lists of nearest neighbouring words in the vector space. the “body part” sense of the word. In our model, homonymy issues are resolved by design: each point in the target space corresponds to a welldefined unambiguous concept or synset. Further, the attentional mechanism of Fig. 4 handles subtle variations of each distinct sense due to polysemy. The effectiveness of the textual feature mechanism was demonstrated in every task we attempted, but to different extents. As our tuning on the dev sets showed, for tasks closer to textto-entity"
D18-1221,D11-1049,0,0.0303607,"Missing"
D18-1221,D15-1200,0,0.021254,"Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with"
D18-1221,D15-1194,1,0.844041,"results demonstrate the effectiveness of our methods by improving the current state of the art. 2 Background Aligning meaning between text and entities in a knowledge graph is a task traditionally based on heuristic methods exploiting text features such as string matching, word weighting, syntactic relations, or dictionary lookups (McCallum et al., 2005; Lu et al., 2011; O’Connor et al., 2014). Machine learning techniques have been also exploited in various forms, for example Leaman et al. (2013) use a pairwise learning-to-rank technique to learn the similarity between different terms, while Limsopatham and Collier (2015) apply statistical machine translation to “translate” social media text to domain-specific terminology. There is little work based on neural networks; the most relevant to us 1 https://www.snomed.org/snomed-ct is a study by Hill et al. (2016), who tested a number of compositional neural architectures trained to approximate word embeddings on a reverse dictionary task. Compared to their work, this paper proposes the use of a distinct target space for representing ontological knowledge, where every entity in the graph lives. The goal of a graph embedding method is to embed components of a knowle"
D18-1221,P16-1096,1,0.859784,"piece of knowledge to a natural language utterance or text is a common problem in natural language processing and generic artificial intelligence (Tellex et al., 2011), and can emerge in many different forms. In NLP, the ability to code text into an entity of a knowledge graph finds applications in tasks such as question answering and information retrieval, or any task that involves some form of mapping a definition to a term (Hill et al., 2016; Rimell et al., 2016). Further, it can be invaluable in providing solutions to domain-specific challenges, for example medical concept normalisation (Limsopatham and Collier, 2016) and identification of adverse drug reactions (O’Connor et al., 2014). This paper details a model for efficiently mapping unrestricted text at the level of phrases and sentences to the entities of a knowledge base ∗ This paper is dedicated to the memory of Euripides Kartsaklis, a man who loved technology. (KB)—a task also referred to as text grounding or normalisation. The model aims at characterising short focused texts, such as definitions or tweets. Given a medical KB, for example, a tweet of the form “Can’t sleep, too tired to think straight” would be mapped to the entity Insomnia, while i"
D18-1221,P17-1170,1,0.865639,"an et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with weighted textual featur"
D18-1221,N10-1013,0,0.112582,"Missing"
D18-1221,D14-1113,0,0.0162897,"of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al.,"
D18-1221,D14-1167,0,0.0246019,"dom walk-based methods are not the only way to construct graph spaces—alternatives include factorisation (Ahmed et al., 2013) and deep autoencoders (Wang et al., 2016)—they have been found very effective in capturing multiple aspects of the graph structure (Wang et al., 2017; Goyal and Ferrara, 2017). The current paper proposes a random walk generation strategy that improves and complements existing approaches. The idea of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then us"
D18-1221,P16-1219,0,0.0345946,"Missing"
D18-1221,Q17-1028,0,0.0235835,"factorisation (Ahmed et al., 2013) and deep autoencoders (Wang et al., 2016)—they have been found very effective in capturing multiple aspects of the graph structure (Wang et al., 2017; Goyal and Ferrara, 2017). The current paper proposes a random walk generation strategy that improves and complements existing approaches. The idea of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 ."
D18-1250,S17-2097,0,0.412028,"2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations and naturally good at modeling long distance relations within sequential language data. Approaches include Mehryary et al. (2016) with the original RNN and Li et al. (2017); Ammar et al. (2017); Zhou et al. (2018) with RNNs having LSTM units which are used to extend the range of context. Apart from sentences themselves, RNNbased models often take as input information extracted from dependency trees, such as shortest dependency paths (SDP) (Mehryary et al., 2016; Ammar et al., 2017), or even whole trees (Li et al., 2017). Since RNNs and CNNs each have their own distinct advantages, a few models have combined both in a single neural architecture (Cai et al., 2016; Zhang et al., 2018). 3 3.1 Materials and Methods Gold Standard Corpora As noted above, our experiments used six wellknown"
D18-1250,A00-1011,0,0.210112,"presented a deep neural network model, in which each component is capable of taking advantage of a particular type of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural"
D18-1250,S17-2091,0,0.208025,"rformance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations"
D18-1250,S17-2168,0,0.0353796,"Missing"
D18-1250,Q17-1010,0,0.0254341,". These two characteristics indicate the importance of understanding the mechanisms by which neural networks can generalize, i.e. make accurate predictions on novel instances. 3.2 Model Architecture Our ‘Man for All SeasonS’ (MASS) model comprises an embeddings layer, multi-channel bidirectional Long Short-Term Memory (BLSTM) 2268 and, Ddir is the orientation of the dependency vector i.e. from left-to-right or vice versa in the order of the SDP. Both are initialized randomly. For word representation, we take advantage of four types of information, including: • FastText pre-trained embeddings (Bojanowski et al., 2017) are the 300dimensional vectors that represent words as the sum of the skip-gram vector and character n-gram vectors to incorporate sub-word information. • WordNet embeddings are in the form of onehot vectors that determine which sets in the 45 standard WordNet super-senses the tokens belong to. • Character embeddings are denoted by C, containing 76 entries for 26 letters in uppercase and lowercase forms, punctuation, and numbers. Each character cj ∈ C is randomly initialized. They will be used to generate the token’s character-based embeddings. Figure 2: The architecture of MASS model for rel"
D18-1250,H05-1091,0,0.273299,"hes to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, suc"
D18-1250,P16-1072,0,0.430697,"as the possibility of having selection bias raise a question about the true capability of state-of-the-art methods for relation classification. In addition, despite such a wealth of studies, it still remains unclear which approach is superior and which factors set the limits on performance. For example, heuristic post-processing rules have been seen to significantly boost relation classification performance on several benchmarks; yet, they cannot be relied upon to generalize across domains. The novel approach we present in this paper draws inspiration from neural hybrid models such as that of Cai et al. (2016). In this work, we present a large-scale analysis of state-of-theart neural network architectures on six benchmark datasets which represent a variety of language domains and semantic types. As a means of comparison against reported system performance, we propose a novel multi-channel long short term memory (Hochreiter and Schmidhuber, 1997, LSTM) model combined with a Convolutional Neural Network (Kim, 2014, CNN) that takes advantage of all major linguistic and architectural features cur2266 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2266–2277"
D18-1250,S13-2057,0,0.203226,"Missing"
D18-1250,P17-2057,0,0.0176958,"ation types: sentence (i) shows a Synonym-of relation, represented by an abbreviation pattern, which is very different from the predicate relation Cause-effect in (ii). Introduction Determining the semantic relation between pairs of named entity mentions, i.e. relation classification, is useful in many fact extraction applications, ranging from identifying adverse drug reactions (Gurulingappa et al., 2012; Dandala et al., 2017), extracting drug abuse events (Jenhani et al., 2016), improving the access to scientific literature (G´abor et al., 2018), question answering (Lukovnikov et al., 2017; Das et al., 2017) to major life events extraction (Li et al., 2014; Cavalin et al., 2016). With a multitude of possible relation types, it is critical to understand how systems will behave in a variety of settings (see Table 1 for an example). † ∗ &lt;e1>Three-dimensional digital subtraction angiographic&lt;/e1> (&lt;e2>3D-DSA&lt;/e2>) images from diagnostic cerebral angiography were obtained ... Contributed equally & Names are in alphabetical order Corresponding author To the best of our knowledge, almost all relation classification models introduced so far have been experimentally validated on only a few datasets - ofte"
D18-1250,W16-3002,0,0.145651,"Missing"
D18-1250,S18-1111,0,0.0632716,"Missing"
D18-1250,W09-2415,0,0.261995,"Missing"
D18-1250,D15-1176,0,0.0351817,"edding, POS tag embedding and the character embedding. These four types of word-related information are fed into eight separate LSTMs, independently from each other during recurrent propagation. words. These four types of word-related information are fed into eight separate LSTMs (four for each direction) independently from each other during recurrent propagation. These four BLSTM channels are illustrated in Figure 3. The morphological surface information is represented with character-based embedding using a BLSTM, in which the forward and backward LSTM hidden states are jointly concatenated (Ling et al., 2015; Dang et al., 2018). For other layers, the LSTM hidden states are concatenated separately as the forward and the backward vector to form two final embeddings for each token as follows: f wWi = f wF Ti ⊕ f wW Ni ⊕ Chari ⊕ f wP OSi bwWi = bwF Ti ⊕ bwW Ni ⊕ Chari ⊕ bwP OSi 3.2.3 CNN with dependency unit Similar to Cai et al. (2016), the Convolutional Neural Networks (CNNs) in our model utilize Dependency Units (DU) to model the SDP. DU has the form of [wi − dii+1 − wi+1 ], in which wi , wi+1 are two adjacent tokens and dii+1 is the dependency between them. As a result, the low-dimensional forwar"
D18-1250,W16-3009,0,0.267568,"Missing"
D18-1250,D14-1181,0,0.0336199,"ral benchmarks; yet, they cannot be relied upon to generalize across domains. The novel approach we present in this paper draws inspiration from neural hybrid models such as that of Cai et al. (2016). In this work, we present a large-scale analysis of state-of-theart neural network architectures on six benchmark datasets which represent a variety of language domains and semantic types. As a means of comparison against reported system performance, we propose a novel multi-channel long short term memory (Hochreiter and Schmidhuber, 1997, LSTM) model combined with a Convolutional Neural Network (Kim, 2014, CNN) that takes advantage of all major linguistic and architectural features cur2266 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2266–2277 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics rently employed. We designate this as a ‘Man for All SeasonS’ (MASS) model because it incorporates many popular elements reported by state of the art systems on individual datasets. The main contributions of the paper are: 1. We presented a deep neural network model, in which each component is capable of taki"
D18-1250,S17-2171,0,0.224581,"se of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Thr"
D18-1250,W16-3005,0,0.0391341,"Missing"
D18-1250,P09-1113,0,0.139683,"e of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were amon"
D18-1250,W15-1506,0,0.0358419,"ised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are ano"
D18-1250,D14-1214,0,0.0128033,"on, represented by an abbreviation pattern, which is very different from the predicate relation Cause-effect in (ii). Introduction Determining the semantic relation between pairs of named entity mentions, i.e. relation classification, is useful in many fact extraction applications, ranging from identifying adverse drug reactions (Gurulingappa et al., 2012; Dandala et al., 2017), extracting drug abuse events (Jenhani et al., 2016), improving the access to scientific literature (G´abor et al., 2018), question answering (Lukovnikov et al., 2017; Das et al., 2017) to major life events extraction (Li et al., 2014; Cavalin et al., 2016). With a multitude of possible relation types, it is critical to understand how systems will behave in a variety of settings (see Table 1 for an example). † ∗ &lt;e1>Three-dimensional digital subtraction angiographic&lt;/e1> (&lt;e2>3D-DSA&lt;/e2>) images from diagnostic cerebral angiography were obtained ... Contributed equally & Names are in alphabetical order Corresponding author To the best of our knowledge, almost all relation classification models introduced so far have been experimentally validated on only a few datasets - often only one. This is despite the availability of e"
D18-1250,S10-1057,0,0.19607,"Missing"
D18-1250,C16-1238,0,0.15285,"and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the"
D18-1250,N18-1080,0,0.235816,"p learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations and naturally good at modeling long distance relations within sequential language data. Approaches include Mehryary et al. (2016) wi"
D18-1250,D15-1062,0,0.0709902,"eddings layer is followed by multi-channel bi-directional LSTM layers, two parallel CNNs and three softmax classifiers. The model’s input makes use of words and dependencies along the SDP going from the first entity to the second one using both forwards and backwards sequences. layers, two parallel Convolutional Neural Network (CNN) layers and three sof tmax classifiers. The MASS model’s architecture is depicted in Figure 2. MASS makes use of words and dependencies along the SDP going from the first entity to the second one using both forwards and backwards sequences. As is standard practice (Xu et al., 2015; Cai et al., 2016; Mehryary et al., 2016; Panyam et al., 2018) an entity pair is classified as having a relation if and only if the SDP between them is classified as having that relation. 3.2.1 Embeddings layer Despite the presence of inter-sentential relations in the six corpora we make the simplifying assumption that relations occur only between entities (or nominals) in the same sentence. We model each such sentence using a dependency path. In order to classify novel dependency paths we represent a dependency relation di as a vector Di that is the concatenation of two vectors as follow: Di"
D18-1250,P09-1115,0,0.305376,"nent is capable of taking advantage of a particular type of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neu"
D19-5612,P18-1082,0,0.0300316,"(Yelp: 8, Yahoo: 5), which illustrates that the overlap between q (z) and p(z) shrinks further as C grows. 3.3 Text Generation To empirically examine how channel capacity translates into generative capacity of the model, we experimented with the C -VAELSTM models from Table 1. To generate a novel sentence, after a model was trained, a latent variable z is sampled from the prior distribution and then transformed into a sequence of words by the decoder p(x|z). During decoding for generation we try three decoding schemes: (i) Greedy: which selects the most probable word at each step, (ii) Top-k (Fan et al., 2018): which at each step samples from the K most probable words, and (iii) Nucleus Sampling (NS) (Holtzman et al., 2019): which at each step samples from a flexible subset of most probable words chosen based on their cumulative mass (set by a threshold p, where p = 1 means sampling from the full distribution). While similar to Topk, the benefit of NS scheme is that the vocabulary size at each time step of decoding varies, a property that encourages diversity and avoids degenerate text patterns of greedy or beam search decoding (Holtzman et al., 2019). We experiment with NS (p = {0.5, 0.9}) and Top"
D19-5612,D18-1151,0,0.139365,"el capacities. Finally, we run some experiments to find if any form of syntactic information is encoded in the latent space. For all experiments, we use the objective function of eqn. 2 with = 1. We do not use larger s because the constraint KL = C is always satisfied. 3 Corpora We use 5 different corpora covering different domains and size through this section: Yelp and Yahoo Yang et al. (2017) both have (100k,10k,10k) sentences in (train, dev, test) sets and 20k words in vocabulary, Children’s Book Test (CBT; Weston et al. (2016)) has (192k,10k,12k) sentences and 12k vocab, Wikipedia (WIKI; Marvin and Linzen (2018)) has (2m,270k,270k) sentences and 20k vocab, and WebText (Radford et al., 2019) has (1m,23k,24k) sentences and 22k vocab. 4 Models We examine three VAE architectures, covering a range of decoding strengths to examine if the objective function in eqn. 2 is immune to posterior collapse regardless of the choice of 3 can be seen as a Lagrange multiplier and any value that allows for constraint satisfaction (R = C) is fine. 4 Corpora and preprocessing scripts will be released. 5 We attribute the difference in performance across our models to the non-optimal selection of training hyperparameters, a"
D19-5612,D17-1066,0,0.0307558,"h powerful autoregressive decoders, such as LSTMs, the internal decoder’s cells are likely to suffice for representing the sentence, leading to a sub-optimal solution where the decoder ignores the inferred latent code z. This allows the encoder to become independent of x, an issue known as posterior collapse (q (z|x) ⇡ p(z)) where the inference network produces uninformative latent variables. Several solutions have been proposed to address the posterior collapse issue: (i) Modifying the architecture of the model by weakening decoders (Bowman et al., 2016; Miao et al., 2015; Yang et al., 2017; Semeniuta et al., 2017), or introducing additional connections between the encoder and decoder to enforce the dependence between x and z (Zhao et al., 2017; Goyal et al., 2017; Dieng et al., 2018); (ii) Using more flexible or multimodal priors (Tomczak and Welling, 2017; Xu and Durrett, 2018); (iii) Alternating the training by focusing on the inference network in the earlier stages (He et al., 2019), or augmenting amortized optimization of VAEs with instancebased optimization of stochastic variational inference (Kim et al., 2018; Marino et al., 2018). All of the aforementioned approaches impose one or more of the fo"
D19-5612,D18-1480,0,0.0189516,"an issue known as posterior collapse (q (z|x) ⇡ p(z)) where the inference network produces uninformative latent variables. Several solutions have been proposed to address the posterior collapse issue: (i) Modifying the architecture of the model by weakening decoders (Bowman et al., 2016; Miao et al., 2015; Yang et al., 2017; Semeniuta et al., 2017), or introducing additional connections between the encoder and decoder to enforce the dependence between x and z (Zhao et al., 2017; Goyal et al., 2017; Dieng et al., 2018); (ii) Using more flexible or multimodal priors (Tomczak and Welling, 2017; Xu and Durrett, 2018); (iii) Alternating the training by focusing on the inference network in the earlier stages (He et al., 2019), or augmenting amortized optimization of VAEs with instancebased optimization of stochastic variational inference (Kim et al., 2018; Marino et al., 2018). All of the aforementioned approaches impose one or more of the following limitations: restraining the choice of decoder, modifying the training algorithm, or requiring a substantial alternation of the objective function. As exceptions to these, -VAE (Razavi et al., 2019) and -VAE (Higgins et al., 2017) aim to avoid the posterior coll"
D19-6205,N18-1202,0,0.0277241,"esults in Table 2. As expected, all the neural architectures largely improve the results obtained by the CRF and, in line with the literature, Flair performs slightly better than ELMo, which in turn performs better than GloVe. Using our purpose-built embeddings, called BioReddit in the Table, we always obtain an improvement with respect to using embeddings trained on generalpurpose data (Default in Table) or on PubMed, barring the smallest GloVe vectors. Embeddings Using the dataset described in Section 3.1, we trained three word embedding models, namely GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and Flair (Akbik et al., 2018). We choose these models due to their popularity, performance, and relative low resource requirements. In particular, GloVe requires just hours to be trained on a CPU, while ELMo and Flair obtained state-of-the-art results in the NER task at the time of their publication, and both models can be trained in relatively short time (∼1 week) using 1 or 2 GPUs. As general purpose and PubMed embeddings, we use the ones provided or recommended by the respective architecture authors; unfortunately, we are not aware of any GloVe 3 F 64.5 68.9 67.2 70.9 71.2 72.5 73.7 75.3"
D19-6205,C18-1139,0,0.02138,"ll the neural architectures largely improve the results obtained by the CRF and, in line with the literature, Flair performs slightly better than ELMo, which in turn performs better than GloVe. Using our purpose-built embeddings, called BioReddit in the Table, we always obtain an improvement with respect to using embeddings trained on generalpurpose data (Default in Table) or on PubMed, barring the smallest GloVe vectors. Embeddings Using the dataset described in Section 3.1, we trained three word embedding models, namely GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and Flair (Akbik et al., 2018). We choose these models due to their popularity, performance, and relative low resource requirements. In particular, GloVe requires just hours to be trained on a CPU, while ELMo and Flair obtained state-of-the-art results in the NER task at the time of their publication, and both models can be trained in relatively short time (∼1 week) using 1 or 2 GPUs. As general purpose and PubMed embeddings, we use the ones provided or recommended by the respective architecture authors; unfortunately, we are not aware of any GloVe 3 F 64.5 68.9 67.2 70.9 71.2 72.5 73.7 75.3 75.4 75.4 76.4 PubMed pre-train"
D19-6205,D14-1179,0,0.0193654,"Missing"
D19-6205,N16-1030,0,0.120341,"dits, both on general health topics such as ‘r/AskDocs’, or on disease-specific subreddits, such as ‘r/cancer’, ‘r/asthma’, and so on. We then trained word embeddings on this corpus using different off-the-shelf techniques. Then, to evaluate the embeddings, we collected a second dataset of 4800 threads from the health forum HealthUnlocked, which was annotated for the NER task. Then, we analyzed the performance of the embeddings on the tasks of NER and of adverse effect mention detection. For NER, we used Conditional Random Fields as a baseline. We compared them against Bidirectional LSTMCRFs (Lample et al., 2016), on which we analyzed the impact of using our custom-trained word embeddings against embeddings trained on general purpose data and scientific biomedical publications when evaluating on our purpose-built HealthUnlocked dataset and on the PsyTar and CADEC corpora. Finally, we evaluated the performance of a simple architecture for adverse reaction mention detection on the PsyTAR corpus. We conclude the paper explaining our intentions for future research, in other to obtain other results that confirm the preliminary findings we present in this work. Word embeddings, in their different shapes and"
D19-6205,D14-1162,0,0.0916372,"ngs trained on UG-BioNLP data. 2 Related Work The benefit of using in-domain embeddings for the biomedical domain has already been proven effective. For example, (Pakhomov et al., 2016) and (Wang et al., 2018) found that using clinical notes or biomedical articles for training word embeddings has generally a positive impact on down34 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 34–38 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 quite big; for example, GloVE (Pennington et al., 2014) was trained with a 1.2 million big vocabulary and 27 billion tokens when using Twitter, and on a 600,000 word vocabulary and 6 billion tokens when using Wikipedia. stream NLP tasks. (Nikfarjam et al., 2015) trained embeddings on user-generated medical content and used them successfully on the pharmacovigilance task; however, they trained the embeddings an adverse reaction mining corpus, hence making them too task-specific to be considered useful on generic UG-BioNLP tasks. 3 3.1 3.2 HealthUnlocked In order to evaluate our embeddings, as a first step, we decided to focus on the Named Entity Re"
E14-1059,P09-2092,0,0.0400008,"Missing"
E14-1059,guthrie-etal-2006-closer,0,0.0197578,"e ngram-based approach as it requires named entity recognition, and typed dependency parsing (we required roughly 30 minutes per training / test set on our Intel i7 laptop). *NNP*NNP*.* * NO MATCH MATCH *NNP*NNP* * NO MATCH MATCH *NNP*NNP*NN*NN* * MATCH *NN*NN*NN*NNP*IN*NNP* * *NNP*IN*NNP*NN* * NO MATCH NOT-ANALOGY (243.0/8.0 correct) ANALOGY (281.0/3.0 correct) MATCH NOT-ANALOGY (1938.0/22.0 correct) Figure 1: Example Classification Tree Subsequence Pattern Feature Model Basically, this approach aims at creating something similar to the most common sub forests of all snippets, or skip-grams (Guthrie, Allison, Liu, Guthrie, & Wilks, 2006), i.e. results can be seen as a hybrid between “tree patterns” (as e.g. the Shortest Path) and n-grams. The intention is to avoid the problem of overly local patterns, allowing the patterns to work even in the presence of fill words and subsequences added to a sentence. For this, we utilize the PrefixSpan algorithm (Pei et al., 2001) to detect common, reappearing subsequence in the training set, i.e. sequences of words that appear in a given order, ignoring anything in-between. In contrast to the shortest path approach, this model focuses on multiple sentences simultaneously, and therefore is"
E14-1059,C92-2082,0,0.0721451,"sterile” there is a plethora of very similar sentences which do not express an analogy (“Shinjuku is like this: …” or “Tokyo, like the rest of Japan, …”). These subtle differences, which are hard to grasp with handcrafted patterns and are often found in the surrounding context, can be modeled by our approach as outlined in section 5. 561 3 Related Work There exist several works on the semantics of analogies from a cognitive, philosophical, or linguistic perspective, such as (Dedre Gentner, Keith J. Holyoak, & Boicho N. Kokinov, 2001), (Itkonen, 2005), or (Shelley, 2003). Hearst-like patterns (Hearst, 1992), which we use as a first and very crude filter during the construction of the Gold dataset, have frequently been employed in recent years, especially in the area of extracting hyponyms, e.g., (Snow, Jurafsky, & Ng, 2004) which also aims at learning new extraction patterns based on word dependency trees. But also approaches for dealing with analogies are frequently based on patterns applied to text corpora. Most of these approaches are tailored for solving general analogy challenges given in a 4-term multiple-choice format, and are usually evaluated on the US-based SAT challenge dataset (part"
E14-1059,de-marneffe-etal-2006-generating,0,0.00454107,"ning confidence 0.25 and min. leaf distance 2). Shortest Path Feature Model In this subsection we design the Shortest Path feature model, a model aiming at exploiting some of the specific properties of place analogies. By definition, only text snippets featuring two different places can be a place analogy. The Shortest Path model furthermore assumes that both these locations occur in a single sentence (which is tested in 6.3), and that there is a meaningful lexical or grammatical dependency between these occurrences. For actually building our feature space, we rely on typed dependency parses (Marneffe, MacCartney, & Manning, 2006) of the snippets, and extract the shortest path in the resulting dependency tree between both locations (also using Stanford CoreNLP). This path represents the collapsed and propagated dependencies between both locations, i.e. basic tokens as “on” or “by” are integrated in the edge labels and don’t appear as nodes. We considered three variations of this approach: paths built using lexical labels, path with part-of-speech labels, and a combination of both. During the construction of our Gold set, we manually annotated the two relevant places for all analogies. Therefore this approach can be ap"
E14-1059,P08-1052,0,0.0310389,"on patterns applied to text corpora. Most of these approaches are tailored for solving general analogy challenges given in a 4-term multiple-choice format, and are usually evaluated on the US-based SAT challenge dataset (part of the standardized aptitude test for college admission). SAT challenges are in 4-term analogy form, e.g. “ostrich is to bird AS a) cub is to bear OR b) lion is to cat”, and the focus of those approaches is on heuristically assessing similarity of two given words pairs, to find the statistically more plausible answer. For example, (Bollegala, Matsuo, & Ishizuka, 2009), (Nakov & Hearst, 2008), or (Turney, 2008) approach this challenge by using pattern-based Web search and subsequent analysis of the resulting snippets. In contrast to these approaches, we do not focus on word pair similarity, but given one entity, we aim at finding other entities which are seen as analogous in a specific domain (in our case analogies between locations and places). Being focused on a special domain often renders approaches relying on thesauri like WordNet or CoreLex unusable, as many of the words relevant to the domain are simply not contained. Closely related to analogy processing is the detection o"
E14-1059,P10-1071,0,0.0805074,"relying on thesauri like WordNet or CoreLex unusable, as many of the words relevant to the domain are simply not contained. Closely related to analogy processing is the detection of metaphors or metonyms, which are a special form of analogy. Simplified, a metaphor is an analogy between two entities with the additional semantics that one entity can substitute the other and vice versa). While early approaches to metaphor identification relied on hand-crafted patterns (Wilks, 1978), newer ones therefore heavily exploit the interchangeability of the entities (Beust, Ferrari, & Perlerin, 2003) or (Shutova, 2010), and cannot be used for general analogy processing without extensive adoption. These approaches often also rely on some reasoning techniques based on thesauri, but also other approaches based on 1 mining and corpus analysis became popular. For example in (Shutova, Sun, & Korhonen, 2010) a system is presented which, starting from a small seed set of manually annotated metaphorical expressions, is capable of harvesting a large number of metaphors of similar syntactic structure from a corpus. Detecting analogies also has some similarities with relation extraction, e.g. (Bunescu & Mooney, 2006) u"
E14-1059,C10-1113,0,0.0331621,"analogy between two entities with the additional semantics that one entity can substitute the other and vice versa). While early approaches to metaphor identification relied on hand-crafted patterns (Wilks, 1978), newer ones therefore heavily exploit the interchangeability of the entities (Beust, Ferrari, & Perlerin, 2003) or (Shutova, 2010), and cannot be used for general analogy processing without extensive adoption. These approaches often also rely on some reasoning techniques based on thesauri, but also other approaches based on 1 mining and corpus analysis became popular. For example in (Shutova, Sun, & Korhonen, 2010) a system is presented which, starting from a small seed set of manually annotated metaphorical expressions, is capable of harvesting a large number of metaphors of similar syntactic structure from a corpus. Detecting analogies also has some similarities with relation extraction, e.g. (Bunescu & Mooney, 2006) using Subsequence Kernels. However, the task is slightly more difficult than simply mining for a “similar_to” relation, which is addressed by our approach in section 5. 4 Building the Gold Dataset As the goal of this paper is to supply the tools for creating a large corpus of analogies f"
E14-1059,D08-1027,0,0.0713394,"Missing"
E14-1059,C08-1114,0,0.0807184,"t corpora. Most of these approaches are tailored for solving general analogy challenges given in a 4-term multiple-choice format, and are usually evaluated on the US-based SAT challenge dataset (part of the standardized aptitude test for college admission). SAT challenges are in 4-term analogy form, e.g. “ostrich is to bird AS a) cub is to bear OR b) lion is to cat”, and the focus of those approaches is on heuristically assessing similarity of two given words pairs, to find the statistically more plausible answer. For example, (Bollegala, Matsuo, & Ishizuka, 2009), (Nakov & Hearst, 2008), or (Turney, 2008) approach this challenge by using pattern-based Web search and subsequent analysis of the resulting snippets. In contrast to these approaches, we do not focus on word pair similarity, but given one entity, we aim at finding other entities which are seen as analogous in a specific domain (in our case analogies between locations and places). Being focused on a special domain often renders approaches relying on thesauri like WordNet or CoreLex unusable, as many of the words relevant to the domain are simply not contained. Closely related to analogy processing is the detection of metaphors or meto"
E17-2062,N06-2001,0,0.0431668,"e of the comparison work, which generally focus on morphologically complex words, can induce representations for such terms. This advantage enables us to train embeddings in general domain, for which text are available abundantly, and specialise them to specific domains for which large amounts of training data might not be available. We also note that our system did not provide full coverage of the words in the two datasets, missing several words which Related Work Recent research on representation induction for rare words has mainly focused on the case of infrequent morphological variations (Alexandrescu and Kirchhoff, 2006) and has tried to address the problem by resorting to information available for subword units. A morphological analyzer, such as Morfessor (Creutz and Lagus, 2007), is usually used in a pre-processing step to break inflected words into their morphological structures. Representations are then induced for morphologically complex words from their morphemes either by combining recursive neural networks (Luong et al., 2013) or using log-bilinear language models (Botha and Blunsom, 2014). Lazaridou et al. (2013) induced embeddings for complex words by adapting phrase composition models, whereas Sori"
E17-2062,P16-1156,0,0.0370722,"Missing"
E17-2062,N15-1184,0,0.0374917,"rds from their morphemes either by combining recursive neural networks (Luong et al., 2013) or using log-bilinear language models (Botha and Blunsom, 2014). Lazaridou et al. (2013) induced embeddings for complex words by adapting phrase composition models, whereas Soricut and Och (2015) automatically constructed 4 391 http://www.ihtsdo.org/snomed-ct References a morphological graph by exploiting regularities within a word embedding space. In the latter case, the representations were inferred by analyzing morphological transformations in the graph. Also related to our work is the retrofitting (Faruqui et al., 2015) of pre-trained embeddings by exploiting semantic lexical resources. Despite being effective in improving the representations for seen words, the retrofitting approaches are generally unable to induce new embeddings to address the unseen words problem. Cotterell et al. (2016) designed an extension of the retrofitting procedure that uses morphological resources to generate vectors for forms not observed in the training data. A common strand in all these works is that they assume that the training corpus covers the morpheme or other morphological variations of an unseen word. As a result, they f"
E17-2062,N13-1092,0,0.135544,"Missing"
E17-2062,N15-1186,0,0.152514,"to word representation (Turney and Pantel, 2010) is highly reliant on the availability of large amounts of training data and falls short of effectively modeling rare words that appear only a handful of times in the training corpus. Several efforts have been made to address this deficiency by expanding the coverage through inducing representations for rare words. Recent work has mainly focused on morphologically complex rare words has often tried to alleviate the problem by spreading the available knowledge across words that share the same morpheme (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). However, these techniques are unable to induce representations for words whose morphemes are not seen during training, such as infrequent domain specific terms. Importantly, the coverage issue is more evident when representations trained 2 Embeddings for Rare Words The objective is to expand the vocabulary of a given set of pre-trained word embeddings W by adding rare words.1 To achieve this goal, we leverage a lexical resource S that provides a better coverage of rare words or belongs to a specific domain and hence can be used to specialise W to that target domain. Our approach has two phas"
E17-2062,W13-3512,0,0.830758,"words. The prominent distributional approach to word representation (Turney and Pantel, 2010) is highly reliant on the availability of large amounts of training data and falls short of effectively modeling rare words that appear only a handful of times in the training corpus. Several efforts have been made to address this deficiency by expanding the coverage through inducing representations for rare words. Recent work has mainly focused on morphologically complex rare words has often tried to alleviate the problem by spreading the available knowledge across words that share the same morpheme (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). However, these techniques are unable to induce representations for words whose morphemes are not seen during training, such as infrequent domain specific terms. Importantly, the coverage issue is more evident when representations trained 2 Embeddings for Rare Words The objective is to expand the vocabulary of a given set of pre-trained word embeddings W by adding rare words.1 To achieve this goal, we leverage a lexical resource S that provides a better coverage of rare words or belongs to a specific domain and hence can be used to specialise W"
E17-2062,D14-1162,0,0.0834882,"extracted from a lexical resource in that domain. Parameter θ adjusts the contribution of initial embedding. Setting the parameter to zero reduces the formulation to that of inducing an embedding for an unseen word. In the next section, we discuss how the parameters were set in our experiments. 3 Experiments As evaluation framework, we used word similarity. To verify the ability of the approach in inducing embeddings in both general and specific domains, we carried out two different experiments. Embeddings. We used three different pretrained word embeddings: (1) G LOV E embeddings trained by Pennington et al. (2014) on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), (2) W 2 V- GN, Word2vec (Mikolov et al., 2013) trained on the Google News dataset (vocab: 3M, dim: 300), and (3) W 2 V-250 K, the same Word2vec embeddings with a vocabulary of 250K most frequent words. We opted for these embeddings mainly for their popularity but we note that the proposed approach is equally applicable to any other vector representation. Embedding induction Parameters. In experiments, whenever we had access to frequency statistics in the training data, we considered words with frequency < 10K as rare and induced their repre"
E17-2062,D16-1174,1,0.888426,"Missing"
E17-2062,W16-2902,1,0.889807,"Missing"
E17-2062,W09-3204,0,0.083506,"Missing"
E17-2062,P13-1149,0,\N,Missing
I08-2140,W07-1003,1,0.826095,"een and link to biomedilinks to disease names in the BCO cal reference on PubMed, HighWire and Google Scholar. Symbol stands for disease name not in the BCO. The right of the screen shows various user options and symbol to filter the news. We will now describe each of the four modules in turn: * Topic classification. This module identifies news stories with disease-related topics and retains relevant ones for later processing. The module uses ontology-supported text classification with naïve Bayes as the classification algorithm and the BioCaster gold standard corpus as the training data set (Doan et al., 2007). In this module, we used the Rainbow toolkit.2 * NER. Disease-related news stories are automatically analyzed and tagged with NEs like PERSON, ORGANIZATION, DISEASE, LOCATION. This module is implemented by SVM classification al gorithm 3 . For a more detailed description of the schema and NER module, see Kawazoe et al. (2006). * Disease/location detection. This module extracts disease and location information. Details are given in Section 3.2. * Visualization. The detected locations are plotted onto a Google map with ontology links to associated diseases and news stories. 2 3 Rainbow toolkit,"
I08-2140,W07-1000,0,0.147321,"Missing"
kawazoe-etal-2004-annotation,W99-0203,0,\N,Missing
kawazoe-etal-2004-annotation,C94-1086,0,\N,Missing
kawazoe-etal-2004-annotation,M98-1029,0,\N,Missing
mizuta-collier-2004-annotation,E99-1015,0,\N,Missing
mizuta-collier-2004-annotation,W00-1302,0,\N,Missing
mizuta-collier-2004-annotation,P02-1047,0,\N,Missing
mizuta-collier-2004-annotation,J02-4002,0,\N,Missing
N19-1196,P18-1229,0,0.0173912,"ions the model makes to infer a path, i.e A.D = average depth × average branch. Multi-Sense LSTM (MS-LSTM): Kartsaklis et al. (2018) proposed a model that achieves stateof-the-art results on the text-to-entity mapping on the Snomed CT 7 dataset. The approach uses a novel multi-sense LSTM, augmented with an attention mechanism, to project the definition to the ontology vector space. Additionally, for a better alignment between the two vector spaces, the authors augmented the ontology graph with textual features. 3.2 To perform evaluation of the models described above we used Ancestor-F1 score (Mao et al., 2018). This metric compares the ancestors (is − amodel ) of the predicted node with the ancestors (is − agold ) of the gold node in the taxonomy. we constrain each entity to have only one parent node. The edges between the other parent nodes are removed.6 Path Representations. We also experiment with two path representations. Our first approach, text2nodes, uses the label of an entity (cf. Section 1) to represent a path. This is not efficient since the decoder of the model needs to select between all of the entities in an ontology and also requires more parameters in the model. Our second approach,"
N19-1298,S17-2097,0,0.0295946,"esentation of relation based on attentive augmented SDP that overcomes the disadvantages of traditional SDP. • We improved the attention mechanism with kernel filters to capture the features from context vectors. • We proposed an advanced DNN architecture that utilizes the proposed Richer-but-Smarter Shortest Dependency Path (RbSP) and other types of linguistic and architectural features. 2 Related Work RE has been widely studied in NLP community for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-based methods depend strongly on the quality of designed features from an explicit linguistic pre-processing step. Bas"
N19-1298,Q17-1010,0,0.185497,"i = tanh ([ti ⊕ ai ⊕ hi ] Wx + bx ) (3) where Wx and bx are trainable parameters of the network. 4.2 Multi-layer attention with Kernel filters To capture the appropriate augmented information from the child nodes of each token, we propose a novel multi-layer attention with kernel filters architecture. As illustrated in Figure 3, we employ two sequential attention layers on the children of Input Dependency embedding Distance from child to its father token d1 d2 Heuristic attention Self-attention Multi-layer attention αs1 αs2 αh1 αh2 Kernel filters max pooling • Pre-trained fastText embeddings (Bojanowski et al., 2017): which learned the word representation based on its external context. Token embedding Token on SDP Child node Feature selection For token representation, as mentioned above, we assume that each token should be interpreted by itself and its children. Then, the word information ti of each token on the SDP is concatenated with its attentive augmented information ai based on the attached children (which is calculated by Multi-layer attention with Kernel filters, see Section 4.2). In this work, we utilize four types of embeddings to represent the word information of each token, including: Token’s"
N19-1298,H05-1091,0,0.502717,"of relation ∗ Corresponding author Entity-Destination(e1,e2) while nominals ‘students’ and ‘barricade’ in sentence (ii) are of relations Product-Producer(e2,e1). The research history of RE has witnessed the development as well as the competition of a variety of RE methodologies. All of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations. Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph"
N19-1298,P16-1072,0,0.340209,"ethodologies. All of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations. Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly weaker than using the SDP since not all words"
N19-1298,C10-1018,0,0.0369927,"tudied in NLP community for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-based methods depend strongly on the quality of designed features from an explicit linguistic pre-processing step. Based on the idea that SDPs contain the essential information for RE, many studies exploit it with several refinements. Typical refinements include negative sampling (Xu et al., 2015a) and BRCNN (Cai et al., 2016) which model the directed shortest path. Liu et al. (2015) suggested incorporating additional network architectures to further improve the performance of SDP-based methods, which uses a recursive neural network to mod"
N19-1298,P06-1017,0,0.0499706,"contributions: • We proposed a novel representation of relation based on attentive augmented SDP that overcomes the disadvantages of traditional SDP. • We improved the attention mechanism with kernel filters to capture the features from context vectors. • We proposed an advanced DNN architecture that utilizes the proposed Richer-but-Smarter Shortest Dependency Path (RbSP) and other types of linguistic and architectural features. 2 Related Work RE has been widely studied in NLP community for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-based methods depend strongly on the quality of designed features from an e"
N19-1298,P04-1053,0,0.0740491,"N) (LeCun et al., 1989) with a multi-attention layer. Our work has three main contributions: • We proposed a novel representation of relation based on attentive augmented SDP that overcomes the disadvantages of traditional SDP. • We improved the attention mechanism with kernel filters to capture the features from context vectors. • We proposed an advanced DNN architecture that utilizes the proposed Richer-but-Smarter Shortest Dependency Path (RbSP) and other types of linguistic and architectural features. 2 Related Work RE has been widely studied in NLP community for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-b"
N19-1298,W09-2415,0,0.0713247,"Missing"
N19-1298,D18-1250,1,0.771016,"of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations. Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly weaker than using the SDP since not all words in a sentence con"
N19-1298,P15-2047,0,0.135253,"ation for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly weaker than using the SDP since not all words in a sentence contribute equally to classify relations and this leads to unexpected noises (Nguyen and Grishman, 2015). However, the emergence and development of attention mechanism (Bahdanau et al., 2015) has re-vitalized this approach. For RE, the attention mechanism is capable of picking out the relevant words conc"
N19-1298,W16-3009,0,0.0527437,"Missing"
N19-1298,P14-2012,0,0.0245728,"ty for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-based methods depend strongly on the quality of designed features from an explicit linguistic pre-processing step. Based on the idea that SDPs contain the essential information for RE, many studies exploit it with several refinements. Typical refinements include negative sampling (Xu et al., 2015a) and BRCNN (Cai et al., 2016) which model the directed shortest path. Liu et al. (2015) suggested incorporating additional network architectures to further improve the performance of SDP-based methods, which uses a recursive neural network to model the sub-tree. Some works"
N19-1298,W15-1506,0,0.0253887,"e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly weaker than using the SDP since not all words in a sentence contribute equally to classify relations and this leads to unexpected noises (Nguyen and Grishman, 2015). However, the emergence and development of attention mechanism (Bahdanau et al., 2015) has re-vitalized this approach. For RE, the attention mechanism is capable of picking out the relevant words concerning target entities/relations, and then we can find critical words which determine primary useful se2902 Proceedings of NAACL-HLT 2019, pages 2902–2912 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics mantic information (Zhou et al., 2016; Verga et al., 2018). We therefore need to determine the object of attention, i.e., nominals themselves, their"
N19-1298,C16-1238,0,0.0211428,"mance of SDP-based methods, which uses a recursive neural network to model the sub-tree. Some works utilized information over the whole dependency tree, such as Li et al. (2017) used dynamic extended tree conditioned LSTM for RE and Panyam et al. (2018) exploited whole dependency graph for relation extraction in biomedical text. Recently, with the introduction and development of attention mechanism, many works tend to use whole sentence or paragraph and focus on the most relevant information using attention technique. Some studies apply a single attention layer, that focus on the word itself (Shen and Huang, 2016; Zhang et al., 2018a); word position (Zhang et al., 2017) and global relation embedding (Su et al., 2018). Other works apply several attention layers, such as word, relation and pooling attention (Wang et al., 2016), multi-head attention (Verga et al., 2018) and word- and entity-based attention (Jat et al., 2017). Luo et al. (2018) used a bidirectional Long Short-Term Memory architecture with an attention layer and a tensor layer for organizing the context information and detecting the connections between two nominals. 2903 the soured We the butter students nsubj dobj put barricade amod advmo"
N19-1298,N18-1075,0,0.0208628,"nformation over the whole dependency tree, such as Li et al. (2017) used dynamic extended tree conditioned LSTM for RE and Panyam et al. (2018) exploited whole dependency graph for relation extraction in biomedical text. Recently, with the introduction and development of attention mechanism, many works tend to use whole sentence or paragraph and focus on the most relevant information using attention technique. Some studies apply a single attention layer, that focus on the word itself (Shen and Huang, 2016; Zhang et al., 2018a); word position (Zhang et al., 2017) and global relation embedding (Su et al., 2018). Other works apply several attention layers, such as word, relation and pooling attention (Wang et al., 2016), multi-head attention (Verga et al., 2018) and word- and entity-based attention (Jat et al., 2017). Luo et al. (2018) used a bidirectional Long Short-Term Memory architecture with an attention layer and a tensor layer for organizing the context information and detecting the connections between two nominals. 2903 the soured We the butter students nsubj dobj put barricade amod advmod prt det prep:on The agitating also up a highway Wc , b c Figure 1: Examples of SDPs and attached child n"
N19-1298,P16-1123,0,0.0416769,"Missing"
N19-1298,D15-1062,0,0.520542,"ell as the competition of a variety of RE methodologies. All of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations. Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly w"
N19-1298,D15-1206,0,0.409521,"ell as the competition of a variety of RE methodologies. All of them are proven to be effective and have different strengths by leveraging different types of linguistic knowledge, however, also suffer from their own limitations. Some early studies stated that the shortest dependency path (SDP) in dependency tree is usually concise and contains essential information for RE (Bunescu and Mooney, 2005; Fundel et al., 2006). By 2016, this approach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly w"
N19-1298,P09-1115,0,0.0382511,"with a multi-attention layer. Our work has three main contributions: • We proposed a novel representation of relation based on attentive augmented SDP that overcomes the disadvantages of traditional SDP. • We improved the attention mechanism with kernel filters to capture the features from context vectors. • We proposed an advanced DNN architecture that utilizes the proposed Richer-but-Smarter Shortest Dependency Path (RbSP) and other types of linguistic and architectural features. 2 Related Work RE has been widely studied in NLP community for many years. Unsupervised (Hasegawa et al., 2004; Yan et al., 2009; Quan et al., 2014), semi-supervised (Chen et al., 2006; Carlson et al., 2010; Ammar et al., 2017) and distant supervision (Verga et al., 2018; Ji et al., 2017) methods have been proven effective for the task of detecting relations from unstructured text. However, in this paper, we mainly focus on supervised approaches, which usually have higher accuracy. In earlier RE studies, researchers focused on extracting various kinds of linguistic features, including both syntactic features and semantic cues (Chan and Roth, 2010; Nguyen and Grishman, 2014). However, all the feature-based methods depen"
N19-1298,D18-1244,0,0.165586,"oach became dominant with many studies demonstrating that using SDP brings better experimental results than previous approaches that used the whole sentence (Xu et al., 2015a,b; Mehryary et al., 2016; Cai et al., 2016; Le et al., 2018). However, using the SDP may lead to the omission of useful information (i.e., negation, adverbs, prepositions, etc.). Recognizing this disadvantage, some studies have sought to improve SDP approaches, such as adding the information from the sub-tree attached to each node in the SDP (Liu et al., 2015) or applying a graph convolution over pruned dependency trees (Zhang et al., 2018b). Another approach to extract the relation between two entities is using whole sentence in which both are mentioned. This approach seems to be slightly weaker than using the SDP since not all words in a sentence contribute equally to classify relations and this leads to unexpected noises (Nguyen and Grishman, 2015). However, the emergence and development of attention mechanism (Bahdanau et al., 2015) has re-vitalized this approach. For RE, the attention mechanism is capable of picking out the relevant words concerning target entities/relations, and then we can find critical words which deter"
N19-1298,D17-1004,0,0.0510227,"etwork to model the sub-tree. Some works utilized information over the whole dependency tree, such as Li et al. (2017) used dynamic extended tree conditioned LSTM for RE and Panyam et al. (2018) exploited whole dependency graph for relation extraction in biomedical text. Recently, with the introduction and development of attention mechanism, many works tend to use whole sentence or paragraph and focus on the most relevant information using attention technique. Some studies apply a single attention layer, that focus on the word itself (Shen and Huang, 2016; Zhang et al., 2018a); word position (Zhang et al., 2017) and global relation embedding (Su et al., 2018). Other works apply several attention layers, such as word, relation and pooling attention (Wang et al., 2016), multi-head attention (Verga et al., 2018) and word- and entity-based attention (Jat et al., 2017). Luo et al. (2018) used a bidirectional Long Short-Term Memory architecture with an attention layer and a tensor layer for organizing the context information and detecting the connections between two nominals. 2903 the soured We the butter students nsubj dobj put barricade amod advmod prt det prep:on The agitating also up a highway Wc , b c"
N19-1298,P16-2034,0,0.0243824,"all words in a sentence contribute equally to classify relations and this leads to unexpected noises (Nguyen and Grishman, 2015). However, the emergence and development of attention mechanism (Bahdanau et al., 2015) has re-vitalized this approach. For RE, the attention mechanism is capable of picking out the relevant words concerning target entities/relations, and then we can find critical words which determine primary useful se2902 Proceedings of NAACL-HLT 2019, pages 2902–2912 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics mantic information (Zhou et al., 2016; Verga et al., 2018). We therefore need to determine the object of attention, i.e., nominals themselves, their entity types or relation label. However, conventional attention mechanism on sequence of words cannot make use of structural information on dependency tree. Moreover, it is hard for machines to learn the attention weights from a long sequence of input text. In this work we propose an enhanced representation for relations that combines the advantages of the above approaches. Basically, we focus on condensed semantic and syntactic information on the SDP. Compensating for the limitation"
N19-1298,N18-1080,0,0.246223,"ence contribute equally to classify relations and this leads to unexpected noises (Nguyen and Grishman, 2015). However, the emergence and development of attention mechanism (Bahdanau et al., 2015) has re-vitalized this approach. For RE, the attention mechanism is capable of picking out the relevant words concerning target entities/relations, and then we can find critical words which determine primary useful se2902 Proceedings of NAACL-HLT 2019, pages 2902–2912 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics mantic information (Zhou et al., 2016; Verga et al., 2018). We therefore need to determine the object of attention, i.e., nominals themselves, their entity types or relation label. However, conventional attention mechanism on sequence of words cannot make use of structural information on dependency tree. Moreover, it is hard for machines to learn the attention weights from a long sequence of input text. In this work we propose an enhanced representation for relations that combines the advantages of the above approaches. Basically, we focus on condensed semantic and syntactic information on the SDP. Compensating for the limitations of the SDP may stil"
P04-3025,P89-1010,0,0.0436774,"Missing"
P04-3025,C00-1044,0,0.0218697,"ool for classification of vectors of real-valued features (Vapnik, 1998). The present approach emphasizes the use of a variety of diverse information sources. In particular, several classes of features based upon the proximity of the topic with phrases which have been assigned favorability values are described in order to take advantage of situations in which the topic of the text may be explicitly identified. 2 Motivation In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al., 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text. Pang et al. (2002)’s treatment of the task as analogous to topic-classification underscores the difference between the two tasks. A number of rhetorical devices, such as the drawing of contrasts between the reviewed entity and other entities or expectations, sarcasm, understatement, and digressions, all of which are used in abundance in many discourse domains, create challenges for these approaches. It is hope"
P04-3025,J03-3005,0,0.0261644,"Missing"
P04-3025,W02-1011,0,0.0394698,"these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information. 1 Introduction There are a number of challenging aspects in recognizing the favorability of opinion-based texts, the task known as sentiment analysis. Opinions in natural language are very often expressed in subtle and complex ways, presenting challenges which may not be easily addressed by simple text categorization approaches such as n-gram or keyword identification approaches. Although such approaches have been employed effectively (Pang et al., 2002), there appears to remain considerable room for improvement. Moving beyond these approaches can involve addressing the task at several levels. Negative reviews may contain many apparently positive phrases even while maintaining a strongly negative tone, and the opposite is also common. This paper attempts to address this issue using Support Vector Machines (SVMs), a well-known and powerful tool for classification of vectors of real-valued features (Vapnik, 1998). The present approach emphasizes the use of a variety of diverse information sources. In particular, several classes of features base"
P04-3025,A97-1011,0,0.019065,"Missing"
P04-3025,P02-1053,0,0.0802977,"SVMs), a well-known and powerful tool for classification of vectors of real-valued features (Vapnik, 1998). The present approach emphasizes the use of a variety of diverse information sources. In particular, several classes of features based upon the proximity of the topic with phrases which have been assigned favorability values are described in order to take advantage of situations in which the topic of the text may be explicitly identified. 2 Motivation In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al., 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text. Pang et al. (2002)’s treatment of the task as analogous to topic-classification underscores the difference between the two tasks. A number of rhetorical devices, such as the drawing of contrasts between the reviewed entity and other entities or expectations, sarcasm, understatement, and digressions, all of which are used in abundance in many discourse domai"
P04-3025,J90-1003,0,\N,Missing
P04-3025,J04-3002,0,\N,Missing
P04-3025,P97-1023,0,\N,Missing
P06-1044,P03-1009,1,0.901237,"beling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify words using features pertaining to mainly syntactic structure (e.g. statistical distributions of subcategorization frames (SCFs) or general patterns of syntactic behaviour, e.g. transitivity, passivisability) which have been extracted from corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks. While manual construction of such classes is diffi"
P06-1044,I05-1006,0,0.0156539,"een proteins and cell types from biomedical texts (Hirschman et al., 2002). Other tasks, such as the extraction of factual information, remain a bigger challenge. This is partly due to the challenging nature of biomedical texts. They are complex both in terms of syntax and semantics, containing complex nominals, modal subordination, anaphoric links, etc. Researchers have recently began to use deeper NLP techniques (e.g. statistical parsing) in the domain because they are not challenged by the complex structures to the same extent than shallow techniques (e.g. regular expression patterns) are (Lease and Charniak, 2005). However, deeper techniques require richer domain-specific lexical information for optimal performance than is provided by existing lexicons (e.g. UMLS). This is particularly important for verbs, which are central to the structure and meaning of sentences. Where the lexical information is absent, lexical classes can compensate for it or aid in obtaining it in the ways described in section 1. Consider e.g. the INDICATE and ACTIVATE verb classes in Figure 1. They capture the fact that their members are similar in terms of syntax and semantics: they have similar SCFs and selectional preferences,"
P06-1044,J01-3003,0,0.0761921,"ine translation, word sense disambiguation, semantic role labeling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify words using features pertaining to mainly syntactic structure (e.g. statistical distributions of subcategorization frames (SCFs) or general patterns of syntactic behaviour, e.g. transitivity, passivisability) which have been extracted from corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP)"
P06-1044,W02-1016,0,0.385559,"Missing"
P06-1044,A97-1052,0,0.150056,"e calculated the Spearman rank correlation between the 1165 verbs which occurred in both corpora. The result was only a weak correlation: 0.37 ± 0.03. When the scope was restricted to the 100 most frequent verbs in the biomedical data, the correlation was 0.12 ± 0.10 which is only 1.2σ away from zero. The dissimilarity between the distributions is further indicated by the KullbackLeibler distance of 0.97. Table 1 illustrates some of these big differences by showing the list of 15 most frequent verbs in the two corpora. 3 pus data using the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997) (Korhonen, 2002). The system incorporates RASP, a domain-independent robust statistical parser (Briscoe and Carroll, 2002), which tags, lemmatizes and parses data yielding complete though shallow parses and a SCF classifier which incorporates an extensive inventory of 163 verbal SCFs3 . The SCFs abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences. In our work, we parameterized two high frequency SCFs for prepositions (PP and NP + PP SCFs). No filtering of potentially noisy SCFs was done to provide clustering with as much informat"
P06-1044,C00-2094,0,0.407499,"level abstractions they can be used as a means to abstract away from individual words when required. They are also helpful in many operational contexts where lexical information must be acquired from small application-specific corpora. Their predictive power can help compensate for lack of data fully exemplifying the behavior of relevant words. Lexical verb classes have been used to support various (multilingual) tasks, such as computational lexicography, language generation, machine translation, word sense disambiguation, semantic role labeling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify"
P06-1044,briscoe-carroll-2002-robust,0,0.13216,"k correlation: 0.37 ± 0.03. When the scope was restricted to the 100 most frequent verbs in the biomedical data, the correlation was 0.12 ± 0.10 which is only 1.2σ away from zero. The dissimilarity between the distributions is further indicated by the KullbackLeibler distance of 0.97. Table 1 illustrates some of these big differences by showing the list of 15 most frequent verbs in the two corpora. 3 pus data using the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997) (Korhonen, 2002). The system incorporates RASP, a domain-independent robust statistical parser (Briscoe and Carroll, 2002), which tags, lemmatizes and parses data yielding complete though shallow parses and a SCF classifier which incorporates an extensive inventory of 163 verbal SCFs3 . The SCFs abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences. In our work, we parameterized two high frequency SCFs for prepositions (PP and NP + PP SCFs). No filtering of potentially noisy SCFs was done to provide clustering with as much information as possible. Approach We extended the system of Korhonen et al. (2003) with additional clustering techniques (introduce"
P16-1096,W14-4012,0,0.00583455,"Missing"
P16-1096,P14-1062,0,0.00622689,"an effective representation of words when learning the concept normalisation. Neural networks, such as convolutional neural networks (CNN) and recurrent neural networks (RNN), have been effectively applied to NLP tasks, such as NER, sentiment classifications and machine translation (Collobert et al., 2011; Kim, 2014; Bahdanau et al., 2014). For example, Collobert et al. (2011) effectively used a multilayer neural network for chunking, part-ofspeech tagging, NER and semantic role labelling. Kim (2014) effectively used CNN with pre-built word embeddings when performing sentence classifications. Kalchbrenner et al. (2014) learned representation of sentences by using CNN. Meanwhile, Bahdanau et al. (2014) used RNN to encode a sentence written in one language (e.g. French) into a fixed length vector before decoding it to Figure 1: Our CNN architecture for medical concept normalisation. a sentence in another language (e.g. English) for translation. Socher et al. used recursive neural networks to model sentences for different tasks, including paraphrase detection (Socher et al., 2011) and sentence classification (Socher et al., 2013). In this paper, we investigate only the use of CNN and RNN for medical concept no"
P16-1096,D14-1181,0,0.0269148,"sation, Limsopatham and Collier (2015a) showed that effective performance could be achieved by mapping the processed social media messages and medical concepts using the similarity of their embeddings. In this work, we use word embeddings as inputs of deep neural networks, which would allow an effective representation of words when learning the concept normalisation. Neural networks, such as convolutional neural networks (CNN) and recurrent neural networks (RNN), have been effectively applied to NLP tasks, such as NER, sentiment classifications and machine translation (Collobert et al., 2011; Kim, 2014; Bahdanau et al., 2014). For example, Collobert et al. (2011) effectively used a multilayer neural network for chunking, part-ofspeech tagging, NER and semantic role labelling. Kim (2014) effectively used CNN with pre-built word embeddings when performing sentence classifications. Kalchbrenner et al. (2014) learned representation of sentences by using CNN. Meanwhile, Bahdanau et al. (2014) used RNN to encode a sentence written in one language (e.g. French) into a fixed length vector before decoding it to Figure 1: Our CNN architecture for medical concept normalisation. a sentence in another l"
P16-1096,W14-3404,0,0.298462,"incorporated edit-distance when mapping similar texts. The MetaMap system of Aronson (2001) applied a rule-based approach using pre-defined variants of terms when mapping texts to medical concepts in the UMLS Metathesaurus3 . However, as shown in Table 1, existing string matching techniques may not be able to map the social media message “moon face and 30 lbs in 6 weeks” to the medical concept ‘Weight Gain’, or map “head spinning a little” to ‘Dizziness’, as no words in the social media messages and the description of the medical concepts correspond. Recent studies, e.g. (Leaman et al., 2013; Leaman and Lu, 2014; Limsopatham and Collier, 2015a), applied machine learning techniques to take into account relationships between different words (e.g. synonyms) when performing normal3 https://www.nlm.nih.gov/pubs/ factsheets/umlsmeta.html 1014 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1014–1023, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Social media message lose my appetite i don’t hunger or thirst hungry moon face and 30 lbs in 6 weeks gained 7 lbs lose the 10 lbs feeling dizzy ... head spinning a little terribl"
P16-1096,P14-2050,0,0.00833307,"t normalisation task in a different manner. In particular, we use deep neural networks to capture the similarity and/or dependency between terms and effectively represent a given social media message in a low dimensional vector representation, before mapping it to a medical concept. Another research area related to this work is the exploitation of word embeddings (i.e. distributed vector representation of words). It has been empirically shown that word embeddings can capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014). The cosine similarity between vectors of words has a positive correlation with the semantic similarity between them (Mikolov et al., 2013b; Pennington et al., 2014). Importantly, word embeddings have been effectively used for several NLP tasks, such as named entity recognition (Passos et al., 2014), machine translation (Mikolov et al., 2013a) and part-of-speech tagging (Turian et al., 2010). In the context of concept normalisation, Limsopatham and Collier (2015a) showed that effective performance could be achieved by mapping the processed social media messages and medical concepts using the"
P16-1096,I13-1041,0,0.0155556,"Missing"
P16-1096,P14-1023,0,0.00557664,"6 https://dev.twitter.com/streaming/ overview 7 http://sideeffects.embl.de/ 8 From blog posts on http://www.askapatient. com website. 4.2 Pre-trained Word Embeddings As our CNN (Section 3.1) and RNN (Section 3.2) approaches require word vectors as inputs, we investigate the use of two different pre-trained word embeddings. The first word embeddings (denoted, GNews) are the publicly available 300dimension embeddings (vocabulary size of 3M) that were induced from 100 billion words from Google News using word2vec (Mikolov et al., 2013b)9 , which has been shown to be effective for several tasks (Baroni et al., 2014; Kim, 2014). The second word embeddings (denoted, BMC) induced from 854M words of medical articles downloaded from BioMed Central10 by using the skip-gram model from word2vec (with default parameters). The BMC embeddings also have 300 dimension. For the words that do not existing in any embeddings, we use a vector of random values sampled from [−0.25, 0.25]. As an alternative, we also use randomly generated embeddings (denoted, Rand) with 300 dimensions, where a vector representation of each word is randomly sampled from [−0.25, 0.25]. This allows the investigation of the effectiveness of our"
P16-1096,D15-1194,1,0.89246,"tance when mapping similar texts. The MetaMap system of Aronson (2001) applied a rule-based approach using pre-defined variants of terms when mapping texts to medical concepts in the UMLS Metathesaurus3 . However, as shown in Table 1, existing string matching techniques may not be able to map the social media message “moon face and 30 lbs in 6 weeks” to the medical concept ‘Weight Gain’, or map “head spinning a little” to ‘Dizziness’, as no words in the social media messages and the description of the medical concepts correspond. Recent studies, e.g. (Leaman et al., 2013; Leaman and Lu, 2014; Limsopatham and Collier, 2015a), applied machine learning techniques to take into account relationships between different words (e.g. synonyms) when performing normal3 https://www.nlm.nih.gov/pubs/ factsheets/umlsmeta.html 1014 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1014–1023, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Social media message lose my appetite i don’t hunger or thirst hungry moon face and 30 lbs in 6 weeks gained 7 lbs lose the 10 lbs feeling dizzy ... head spinning a little terrible headache!! Description of cor"
P16-1096,J82-2004,0,0.58616,"Missing"
P16-1096,P10-1040,0,0.0477625,"d Zaragoza, 2009) to retrieve relevant concepts. We tackle the concept normalisation task in a different manner. In particular, we use deep neural networks to capture the similarity and/or dependency between terms and effectively represent a given social media message in a low dimensional vector representation, before mapping it to a medical concept. Another research area related to this work is the exploitation of word embeddings (i.e. distributed vector representation of words). It has been empirically shown that word embeddings can capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014). The cosine similarity between vectors of words has a positive correlation with the semantic similarity between them (Mikolov et al., 2013b; Pennington et al., 2014). Importantly, word embeddings have been effectively used for several NLP tasks, such as named entity recognition (Passos et al., 2014), machine translation (Mikolov et al., 2013a) and part-of-speech tagging (Turian et al., 2010). In the context of concept normalisation, Limsopatham and Collier (2015a) showed that effective performance could be achieved by m"
P16-1096,P10-4014,0,0.0165481,"anslations are used to map the medical concepts by taking the ranked position into account. We calculate the cosine similarity using either the GNews or the BMC embeddings. 6. LogisticRegression: A variant of our proposed approaches where we concatenate embeddings of terms (padded where necessary) 11 http://www.ncbi.nlm.nih.gov/ CBBresearch/Lu/Demo/tmTools/#DNorm in each social media phrase into a fixed-size sentence vector, before using this vector as input features for a multi-class logistic regression classifier. Another possible baseline is a word-sense disambiguation system, such as IMS (Zhong and Ng, 2010). Nevertheless, the results from our initial experiments using IMS showed that it could not perform effectively on the three datasets. This is because the performance of IMS depends heavily on the contexts (i.e. words surrounding the input phrase); however, such contexts are not available in any of the three datasets. Therefore, we do not report the performance of IMS in this paper. Note that for the baselines that require training data (i.e. DNorm and P-MT) and our two proposed approaches, apart from the training data provides with each fold of the datasets, we also train them using the descr"
P16-1096,W14-1609,0,0.0103833,"related to this work is the exploitation of word embeddings (i.e. distributed vector representation of words). It has been empirically shown that word embeddings can capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014). The cosine similarity between vectors of words has a positive correlation with the semantic similarity between them (Mikolov et al., 2013b; Pennington et al., 2014). Importantly, word embeddings have been effectively used for several NLP tasks, such as named entity recognition (Passos et al., 2014), machine translation (Mikolov et al., 2013a) and part-of-speech tagging (Turian et al., 2010). In the context of concept normalisation, Limsopatham and Collier (2015a) showed that effective performance could be achieved by mapping the processed social media messages and medical concepts using the similarity of their embeddings. In this work, we use word embeddings as inputs of deep neural networks, which would allow an effective representation of words when learning the concept normalisation. Neural networks, such as convolutional neural networks (CNN) and recurrent neural networks (RNN), hav"
P16-1096,D14-1162,0,0.0781265,"pts. We tackle the concept normalisation task in a different manner. In particular, we use deep neural networks to capture the similarity and/or dependency between terms and effectively represent a given social media message in a low dimensional vector representation, before mapping it to a medical concept. Another research area related to this work is the exploitation of word embeddings (i.e. distributed vector representation of words). It has been empirically shown that word embeddings can capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014). The cosine similarity between vectors of words has a positive correlation with the semantic similarity between them (Mikolov et al., 2013b; Pennington et al., 2014). Importantly, word embeddings have been effectively used for several NLP tasks, such as named entity recognition (Passos et al., 2014), machine translation (Mikolov et al., 2013a) and part-of-speech tagging (Turian et al., 2010). In the context of concept normalisation, Limsopatham and Collier (2015a) showed that effective performance could be achieved by mapping the processed social media messages and m"
P16-1096,D13-1170,0,0.00109195,"NN with pre-built word embeddings when performing sentence classifications. Kalchbrenner et al. (2014) learned representation of sentences by using CNN. Meanwhile, Bahdanau et al. (2014) used RNN to encode a sentence written in one language (e.g. French) into a fixed length vector before decoding it to Figure 1: Our CNN architecture for medical concept normalisation. a sentence in another language (e.g. English) for translation. Socher et al. used recursive neural networks to model sentences for different tasks, including paraphrase detection (Socher et al., 2011) and sentence classification (Socher et al., 2013). In this paper, we investigate only the use of CNN and RNN for medical concept normalisation, as recursive neural networks require parse trees of input sentences while grammatical rules are typically ignored in social media messages. 3 Neural Networks for Concept Normalisation Next, we introduce our medical concept normalisation approaches based on CNN and RNN in Sections 3.1 and 3.2, respectively. 3.1 CNN for Concept Normalisation Our first approach uses CNN to learn the semantic representation of a social media message before mapping it to an appropriate medical concept. We use a CNN archit"
P17-1115,P14-1023,0,0.0792466,"Missing"
P17-1115,S07-1109,0,0.543854,"ink it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a sm"
P17-1115,H05-1091,0,0.0262661,"Missing"
P17-1115,P14-2009,0,0.0385077,"Missing"
P17-1115,S07-1033,0,0.813479,"the current NER technology does not have a solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 feature"
P17-1115,D15-1162,0,0.0470937,"by the whole sentence (or paragraph), rather it is a small and focused “predicate window” pointed to by the entity’s head dependency. In other words, most of the sentence is not only superfluous for the task, it actually lowers the accuracy of the model due to irrelevant input. This is particularly important in metonymy resolution as the entity’s surface form is not taken into consideration, only its context. In Figure 1, we show the process of extracting the Predicate Window from a sample sentence (more examples are available in the Appendix). We start by using the SpaCy dependency parser by Honnibal and Johnson (2015), which is the fastest in the world, open source and highly customisable. Each dependency tree provides the following features: dependency labels and entity head dependency. Rather than using most of the tree, we only use a single local head dependency relationship to point to the predicate. Leveraging a dependency parser helps PreWin with selecting the minimum relevant input to the model while discarding irrelevant input, which may cause the neural model to behave unpredictably. Finally, the entity itself is never used as input in any of our methods, we only rely on context. PreWin then extra"
P17-1115,P09-2079,0,0.0176808,"Missing"
P17-1115,S07-1031,0,0.0140227,"solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable pe"
P17-1115,D12-1017,0,0.781192,"Missing"
P17-1115,P15-2047,0,0.0332998,"Missing"
P17-1115,W02-1027,0,0.8191,"m Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a small neural network typically trained in no more than 5 epochs, minimal training data, a basic dependency parser and the new PreWin method by being highly discriminating in choosing signal over noise. 9 1255 http://homepages.inf.ed.ac.uk/mnissim/mascara/ 7 References Conclusions and Future Work We showed how a minimalist neural approach can replace substantial external resources, handcrafted features and how the PreWin method can even ignore most of the paragraph where the ent"
P17-1115,S07-1007,0,0.904465,"(2009) used dependency parsing to explore how features based on syntactic dependency relations can be used to improve performance on opinion mining. In unsupervised lymphoma (type of cancer) classification, Luo et al. (2014) constructed a sentence graph from the results of a two-phase dependency parse to mine pathology reports for the relationships between medical concepts. Our methods also exploit the versatility of dependency parsing to leverage information about the sentence structure. 2.1 SemEval 2007 Dataset Our main standard for performance evaluation is the SemEval 2007 Shared Task 8 (Markert and Nissim, 2007) dataset first introduced in Nissim and Markert (2003b). Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC). 1249 We only use the locations dataset, which comprises a train (925 samples) and a test (908 samples) partition. For medium evaluation, the classes are literal (geographical territories and political entities), metonymic (place-for-people, place-forproduct, place-for-event, capital-for-government or place-for-organisation) and mixed (metonymic and literal frames invoked simultaneously or unable to distinguish). T"
P17-1115,K16-1006,0,0.0211996,"olov et al., 2013; Mesnil et al., 2013; Baroni et al., 2014; Collobert et al., 2011). We evaluate this method (its 5 and 10-word variant) alongside PreWin and Paragraph. 4.4 Paragraph Baseline The paragraph baseline method extends the “immediate” one by taking 50 words from each side of the entity as the input to the classifier. In practice, this extends the feature window to include extrasentential evidence in the paragraph. This ap6 https://github.com/milangritta/Minimalist-LocationMetonymy-Resolution 7 http://nlp.stanford.edu/projects/glove/ 1252 proach is also popular in machine learning (Melamud et al., 2016; Zhang et al., 2016). 4.5 Ensemble of Models In addition to a single best performing model, we have combined several models trained on different data and/or using different model configurations. For the SemEval test, we combined three separate models trained on the newly annotated CoNLL dataset and the training data for SemEval. For the ReLocaR test, we once again let three models vote, trained on CooNLL and ReLocaR data. 5 Results We evaluate all methods using three datasets for training (ReLocaR, SemEval, CoNLL) and two for testing (ReLocaR, SemEval). Due to inherent randomness in the deep"
P17-1115,D09-1095,0,0.79472,"Missing"
P17-1115,S07-1101,0,0.706817,"nology does not have a solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achie"
P17-1115,P03-1008,0,0.586561,"a-based MR dataset called ReLocaR to address the training data shortage. (3) We make an annotated subset of the CoNLL 2003 (NER) Shared Task available for extra MR training data, alongside models, tools and other data. 1248 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1248–1259 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1115 2 Related Work Some of the earliest work on MR that used an approach similar to our method (machine learning and dependency parsing) was by Nissim and Markert (2003a). The decision list classifier with backoff was evaluated using syntactic head-modifier relations, grammatical roles and a thesaurus to overcome data sparseness and generalisation problems. However, the method was still limited for classifying unseen data. Our method uses the same paradigm but adds more features, a different machine learning architecture and a better usage of the parse tree structure. Much of the later work on MR comes from the SemEval 2007 Shared Task 8 (Markert and Nissim, 2007) and later by Markert and Nissim (2009). The feature set of Nissim and Markert (2003a) was updat"
P17-1115,D14-1162,0,0.0777928,"Missing"
P17-1115,J14-4005,1,0.897615,"Missing"
P17-1115,S07-1093,0,0.726835,"for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a small neural netwo"
P17-1115,W10-4001,0,0.0596201,"Missing"
P17-1170,R13-1022,0,0.0177951,"oth tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been prop"
P17-1170,baccianella-etal-2010-sentiwordnet,0,0.0519029,"Missing"
P17-1170,J15-2004,0,0.0193871,"ce of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie revi"
P17-1170,D15-1041,0,0.0141171,"s of their input. The word level functionality can affect the performance of these systems in two ways: (1) it can hamper their efficiency in handling words that are not encountered frequently during training, such as multiwords, inflections and derivations, and (2) it can restrict their semantic understanding to the level of words, with all their ambiguities, and thereby prevent accurate capture of the intended meanings. The first issue has recently been alleviated by techniques that aim to boost the generalisation power of NLP systems by resorting to sub-word or character-level information (Ballesteros et al., 2015; Kim et al., 2016). The second limitation, however, has not yet been studied sufficiently. A reasonable way to handle word ambiguity, and hence to tackle the second issue, is to semantify the input text: transform it from its surface-level semantics to the deeper level of word senses, i.e. their intended meanings. We take a step in this direction by designing a pipeline that enables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) a"
P17-1170,D16-1041,1,0.0551113,"a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embed"
P17-1170,W13-5003,0,0.0210547,"se distinctions can be beneficial to both tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representati"
P17-1170,N16-1163,0,0.0312474,"were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. C"
P17-1170,D14-1067,0,0.00713165,"onfigurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic netw"
P17-1170,L16-1269,1,0.895177,"bedding of its corresponding word. Owing to its reliance on WordNet’s semantic network, DeConf is limited to generating only those word senses that are covered by this lexical resource. We propose to use Wikipedia in order to expand the vocabulary of the computed word senses. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential"
P17-1170,E17-2036,1,0.840387,"Missing"
P17-1170,D14-1110,0,0.0816486,"Missing"
P17-1170,D13-1184,0,0.0200958,"biguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses i"
P17-1170,W16-2501,0,0.00621644,"ved over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the"
P17-1170,P16-1191,0,0.660744,"r many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimental setup. 5.1 Experimental set"
P17-1170,C14-1048,0,0.0139087,"al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmar"
P17-1170,P82-1020,0,0.792916,"Missing"
P17-1170,Q15-1023,0,0.0247776,"(ˆ s)} 11: return Disambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input d"
P17-1170,P12-1092,0,0.0165482,"unami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has o"
P17-1170,P11-1015,0,0.0188444,"rk for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even"
P17-1170,P15-1010,1,0.869344,"he shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to che"
P17-1170,N15-1070,0,0.0148647,"have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense em"
P17-1170,N15-1164,0,0.0284146,"he word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) propo"
P17-1170,N15-1011,0,0.00802726,"nses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th An"
P17-1170,P14-5010,0,0.00165544,"for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the input semantic network, N . Our algorithm then selects the best candidates iteratively. In each iteration, the 1 As defined in the underlying sense inventory, up to trigrams. We used Stanford CoreNLP (Manning et al., 2014) for tokenization, Part-of-Speech (PoS) tagging and lemmatization. 1858 Figure 2: Simplified graph-based representation of a sample sentence. Figure 3: Text classification model architecture. candidate sense that has the highest graph degree maxDeg is chosen as the winning sense: maxDeg = max |{(s, s0 ) ∈ E : s0 ∈ S}| s∈S (1) After each iteration, when a candidate sense sˆ is selected, all the possible candidate senses of the corresponding word (i.e. getLex(ˆ s)) are removed from E (line 10 in the algorithm). Figure 2 shows a simplified version of the graph for a sample sentence. The algorithm"
P17-1170,K16-1006,0,0.012425,"inking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in"
P17-1170,P14-1062,0,0.00336837,"nables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based mod"
P17-1170,D14-1181,0,0.123887,"of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 P"
P17-1170,D15-1200,0,0.0215655,"14; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to check the benefit that can be gained by replacing word embeddings with sense embeddings in multiple tasks. With the help of two simple disambiguation algorithms, unsupervised sense embeddings were integrated into various downstream applications, with varying degrees of success. Given the interdependency of sense representation and disambiguation in this model, it is very difficult to introduce alternative algorithms into its pipeline, either to benefit from the state of the art, or to carry out an evaluation. Instead, our pipeline provides the ad"
P17-1170,P16-1096,1,0.782735,"can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the i"
P17-1170,P06-1014,1,0.24475,"sent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embeddings. 4.2 Pre-trained Supersense Embeddings It has been argued that WordNet sense distinctions are too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. I"
P17-1170,D14-1113,0,0.0183534,"et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative"
P17-1170,P04-1035,0,0.00916293,"for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases"
P17-1170,P05-1015,0,0.141168,"sults in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose val"
P17-1170,D14-1162,0,0.119517,"mantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al."
P17-1170,D16-1174,1,0.850136,"Missing"
P17-1170,J14-4005,1,0.680885,"isambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retriev"
P17-1170,D16-1018,0,0.0905951,"ifferent NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the pr"
P17-1170,E17-1010,1,0.0268406,"3.6 83.2 IMDB 87.7 87.4 PL05 77.3 76.6 PL04 67.9 67.4 Stanford 91.8 91.3 Wikipedia 83.1 88.0 75.9† 67.1 91.0 WordNet Wikipedia 84.4 83.1 88.0 88.4∗ 75.9 75.8 66.2 69.3∗ 91.4† 91.0 85.5 88.3 80.2 72.5 93.1 83.4 88.3 79.2 69.7† 92.6 Wikipedia 83.8 87.0† 79.2 73.1 92.3 WordNet 85.2 88.8 79.5 73.8 92.7† Wikipedia 84.2 87.9 78.3† 72.6 92.2 Word Pre-trained Sense Supersense WordNet Table 4: Accuracy performance on five polarity detection datasets. Given that polarity datasets are balanced17 , we do not report F1 which would have been identical to accuracy. texts is a known issue (Moro et al., 2014; Raganato et al., 2017), the tackling of which remains an area of exploration. spective of the classification task. We attribute this to two main factors: 1. Sparsity: Splitting a word into multiple word senses can have the negative side effect that the corresponding training data for that word is distributed among multiple independent senses. This reduces the training instances per word sense, which might affect the classifier’s performance, particularly when senses are semantically related (in comparison to fine-grained senses, supersenses address this issue to some extent). 2. Disambiguation quality: As also ment"
P17-1170,N10-1013,0,0.011305,"nce of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research"
P17-1170,P15-1173,0,0.0610169,"Missing"
P17-1170,P11-1097,0,0.160583,"Missing"
P17-1170,N15-1099,0,0.0142975,"these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system by transforming its i"
P17-1170,J98-1004,0,0.448429,"Missing"
P17-1170,P13-2125,0,0.0526578,"re too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimenta"
P17-1170,P13-1045,0,0.0139818,"sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even distribution of positive and negative labels. 5.3.2 Results Table 4 lists accuracy performance of our classification model and all its varian"
P17-1170,N16-1160,0,0.0557682,"Missing"
P17-1170,D15-1167,0,0.00767,"applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th Annual Meeting of the"
P17-1170,C14-1016,0,0.0219186,"fter their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or"
P17-1170,J17-1002,0,0.0225265,"s. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012)."
P17-1170,D15-1243,0,0.0130565,"no improvement is observed over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification whi"
P17-1170,J14-2007,0,0.0411995,"word-level functionality of these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system"
P17-1170,P15-1032,0,0.018956,"ayer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic network of WordNet (Miller, 1995),"
P17-1170,E17-1109,0,0.0294874,"Missing"
P17-1170,P10-4014,0,0.0202993,"ph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges am"
P17-1170,D13-1141,0,0.0158915,"riments with two configurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach explo"
P17-1170,D07-1107,0,\N,Missing
P17-1170,K17-1012,1,\N,Missing
P18-1119,W17-4417,0,0.0177368,"n-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing. 1 Introduction Geocoding1 is a specific case of text geolocation, which aims at disambiguating place references in text. For example, Melbourne can refer to more than ten possible locations and a geocoder’s task is to identify the place coordinates for the intended Melbourne in a context such as “Melbourne hosts one of the four annual Grand Slam tennis tournaments.” This is central to the success of tasks such as indexing and searching documents by geography (Bhargava et al., 2017), geospatial 1 Also called Toponym Resolution in related literature. analysis of social media (Buchel and Pennington, 2017), mapping of disease risk using integrated data (Hay et al., 2013), and emergency response systems (Ashktorab et al., 2014). Previous geocoding methods (Section 2) have leveraged lexical semantics to associate the implicit geographic information in natural language with coordinates. These models have achieved good results in the past. However, focusing only on lexical features, to the exclusion of other feature spaces such as the Cartesian Coordinate System, puts a ceiling"
P18-1119,C12-1064,0,0.0405819,"Missing"
P18-1119,W16-1721,0,0.509013,"Missing"
P18-1119,N16-1122,0,0.0197966,"nd, we introduced MapVec, an algorithm and a container for encoding context locations in geodesic vector space. We showed how CamCoder, using lexical and MapVec features, outperformed both approaches, achieving a new SOTA. MapVec remains effective with various machine learning frameworks (Random Forest, CNN and MLP) and substantially improves accuracy when combined with other neural models (LSTMs). Finally, we introduced GeoVirus, an open-source dataset that helps facilitate geoparsing evaluation across more diverse domains with different lexical-geographic distributions (Flatow et al., 2015; Dredze et al., 2016). Tasks that could benefit from our methods include social media placing tasks (Choi et al., 2014), inferring user location on Twitter (Zheng et al., 2017), geolocation of images based on descriptions (Serdyukov et al., 2009) and detecting/analyzing incidents from social media (Berlingerio et al., 2013). Future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents. Acknowledgements We gratefully acknowledge the funding support of the Natural Environment Research Council (NERC) PhD Studentship NE/M009"
P18-1119,D10-1124,0,0.370464,"Missing"
P18-1119,D16-1235,0,0.0266525,"Missing"
P18-1119,P17-1115,1,0.916968,"(named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Geotagging, is a Named Entity Recognition component which extracts all location references in a given text. This phase may optionally include metonymy resolution, see (Zhang and Gelernter, 2015; Gritta et al., 2017a). The goal of geocoding is to choose the correct coordinates for a location mention from a set of candidates. Gritta et al. (2017b) provided a comprehensive survey of five recent geoparsers. The authors established an evaluation framework, with a new dataset, for their experimental analysis. We use this evaluation framework in our experiments. We briefly describe the methodology of each geocoder featured in our evaluation (names are capitalised and appear in italics) as well as survey the related work in geocoding. Computational methods in geocoding broadly divide into rule-based, statistica"
P18-1119,P80-1024,0,0.366307,"Missing"
P18-1119,D15-1162,0,0.0551982,"Missing"
P18-1119,K16-1006,0,0.0204518,"Missing"
P18-1119,D14-1162,0,0.0808156,"Missing"
P18-1119,D17-1016,0,0.36786,"t al. (2015) supplemented lexical features, represented as a bag-of-words, with an exhaustive set of manually generated geographic features and spatial heuristics such as geospatial containment and geodesic distances between entities. The ranking of locations was learned with LambdaMART (Burges, 2010). Unlike our geocoder, the addition of geographic features did not significantly improve scores, reporting: “The geo-specific features seem to have a limited impact over a strong baseline system.” Unable to obtain a codebase, their results feature in Table 1. The latest neural network approaches (Rahimi et al., 2017) with normalised bag-of-word representations have achieved SOTA scores when augmented with social network data for Twitter document (user’s concatenated tweets) geolocation (Bakerman et al., 2018). 3 Methodology Figure 1 shows our new geocoder CamCoder implemented in Keras (Chollet, 2015). The lexical part of the geocoder has three inputs, from the top: Context Words (location mentions excluded), Location Mentions (context words excluded) and the Target Entity (up to 15 words long) to be Figure 1: The CamCoder neural architecture. It is possible to split CamCoder into a Lexical (top 3 inputs)"
P18-1119,P16-4022,0,0.801543,"Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014"
P18-1119,N15-1153,0,0.188813,"Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objec"
P18-1119,D12-1137,0,0.506942,"15 - 20, 2018. 2018 Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences i"
P18-1119,P13-1144,0,0.766793,"example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Geotagging, is a Named Entity Recognition component which extracts all location references in a given text. This phase may optionally include metonymy resolution, see (Zhang and Gelernter, 2015; Gritta et al., 2017a). The goal of geocoding is to choose the correct coordinates for a location mention from a set of candidates. Gritta et al. (2017b) provided a comprehensiv"
P18-1119,D14-1039,0,0.419312,"GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Ge"
P18-1119,P11-1096,0,0.577584,"SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geopa"
P98-1042,P91-1022,0,0.16622,"y automatic applications involving the analysis of bilingual texts such as extraction of bilingum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 • Dictionary-based approaches are founded on an assumption of lexicul correspondence between language pairs. We cannot always rely on this for non-cognate language pairs, such as English and Japanese. • Texts are often heavily reformatted in translation, so we cannot assume that the corpus will be clean, i.e. contain many one-to-one sentence mappings. In this case statistical methods which rely on structure correspondence such as byte-length ratios may not perform well. These factors suggest that some hybrid method may give us the best combination of coverage and accuracy when we have a v"
P98-1042,P93-1002,0,0.140279,"d separately. The task of sentence alignment is a critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilingum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) 268 • Dictionary-based approaches are founded on an assumption of lexicul correspondence between language pairs. We cannot always rely on this for non-cognate language pairs, such as English and Japanese. • Texts are often heavily reformatted in translation, so we cannot assume that the corpus will be clean, i.e. contain many one-to-one sentence mappings. In this case statistical methods which rely on structure correspondence such as byte-length ratios may not perform well. These factors suggest that some hybr"
P98-1042,P93-1001,0,0.0191023,"been well studied using pure statistical or linguistic models. We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods. 1 (Church, 1993)(Gale and Church, 1903)(Kay and RSsheheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage li"
P98-1042,P98-1041,1,0.817485,"nment path over the set of English and Japanese sentences. Cost being determined by the model's scores. The alignment space includes all possible combinations of multiple matches upto and including 3:3 alignments. The basic models are now outlined below. 4.1 M o d e l 1: L e x i c a l v e c t o r matching The lexical approach is perhaps the most robust for aligning texts in cognate language pairs, or where there is a large amount of reformatting in translation. It has also been shown to be particularly successful within the vector space model in multilingual information retrieval tasks, e.g. (Collier et al., 1998a),(Collier et al., 1998b), for aligning texts in non-cognate languages at the article level. The major limitation with lexical matching is clearly the assumption of lexical correspondence - El. Taiwan ruling party sees power struggle in China E2. TAIPEI , Feb 9 ( Reuter ) - Taiwan's ruling Nationalist Party said a struggle to succeed Deng Xiaoping as China's most powerful man may have already begun. E3. &quot;Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is inevitable,&quot; a Nationalist Party report said. E4. China and Taiwan have been rivals since the Nationalists"
P98-1042,P91-1023,0,0.0271446,"nslations in Japanese. Of these English words some 14,000 were proper nouns which were directly relevant to the vocabulary typically found in international news stories. Additionally we perform lexical normalisation before calculating the matching score and remove function words with a stop list. 4.2 M o d e l 2: B y t e - l e n g t h r a t i o s For Asian language pairs we cannot rely entirely on dictionary term matching. Moreover, algorithms which rely on matching cognates cannot be applied easily to English and some Asian language. We were motivated by statistical alignment models such as (Gale and Church, 1991) to investigate whether byte-length probabilities could improve or replace the lexical matching based method. The underlying assumption is that characters in an English sentence are responsible for generating some fraction of each character in the corresponding Japanese sentence. We derived a probability density function by making the assumption that English .and Japanese sentence length ratios are normally distributed. The parameters required for the model are the mean, p and variance, ~, which we calculated from a training set of 450 hand-aligned sentences. These are then entered into Equati"
P98-1042,J93-1004,0,0.387798,"Missing"
P98-1042,C94-2175,0,0.0187912,"nd sentence offset probabilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods. 1 (Church, 1993)(Gale and Church, 1903)(Kay and RSsheheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains. Introduction There have been many approaches proposed to solve the problem of aligning corresponding sentences"
P98-1042,P94-1012,0,0.643817,"babilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods. 1 (Church, 1993)(Gale and Church, 1903)(Kay and RSsheheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains. Introduction There have been many approaches proposed to solve the problem of aligning corresponding sentences in parallel"
P98-1042,C98-1041,1,\N,Missing
P98-1042,J93-1006,0,\N,Missing
S17-2002,N09-1003,0,0.413176,"Missing"
S17-2002,W13-3520,0,0.0139707,"eline system we included the results of the concept and entity embeddings of NASARI (Camacho-Collados et al., 2016). These embeddings were obtained by exploiting knowledge from Wikipedia and WordNet coupled with general domain corpus-based Word2Vec embeddings (Mikolov et al., 2013). We performed the evaluation with the 300-dimensional English embedded vectors (version 3.0)9 and used them for all languages. For the comparison within and • Subtask 1. The common corpus for subtask 1 was the Wikipedia corpus of the target language. Specifically, systems made use of the Wikipedia dumps released by Al-Rfou et al. (2013).6 • Subtask 2. The common corpus for subtask 2 was the Europarl parallel corpus7 . This corpus is available for all languages except 6 https://sites.google.com/site/rmyeid/ projects/polyglot 7 http://opus.lingfil.uu.se/Europarl. php 8 http://opus.lingfil.uu.se/ OpenSubtitles2016.php 9 http://lcl.uniroma1.it/nasari/ 20 System English r Luminoso run2 0.78 Luminoso run1 0.78 0.78 QLUT run1∗ hhu run1∗ 0.71 HCCL run1∗ 0.68 NASARI (baseline) 0.68 hhu run2∗ 0.66 QLUT run2∗ 0.67 RUFINO run1∗ 0.65 0.60 Citius run2 l2f run2 (a.d.) 0.64 l2f run1 (a.d.) 0.64 Citius run1∗ 0.57 MERALI run1∗ 0.59 Amateur ru"
S17-2002,J06-1003,0,0.0958441,"0 0.60 0.48 0.53 0.44 0.44 0.57 0.61 0.50 0.31 0.40 0.57 0.61 0.05 -0.06 - ρ Final 0.75 0.75 0.72 0.60 0.57 0.64 0.63 0.62 0.41 0.62 -0.06 - 0.74 0.74 0.70 0.60 0.55 0.52 0.51 0.62 0.41 0.62 0.00 - Table 6: Pearson (r), Spearman (ρ) and official (Final) results of participating systems on the five monolingual word similarity datasets (subtask 1). across languages NASARI relies on the lexicalizations provided by BabelNet (Navigli and Ponzetto, 2012) for the concepts and entities in each language. Then, the final score was computed through the conventional closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006), using cosine similarity as the comparison measure. 3.2 Results We present the results of subtask 1 in Section 3.2.1 and subtask 2 in Section 3.2.2. 3.2.1 System Score Official Rank Luminoso run2 Luminoso run1 HCCL run1∗ NASARI (baseline) RUFINO run1∗ SEW run2 (a.d.) SEW run1 RUFINO run2∗ hjpwhuer run1 0.743 0.740 0.658 0.598 0.555 0.552 0.506 0.369 0.018 1 2 3 4 5 6 7 Table 7: Global results of participating systems on subtask 1 (multilingual word similarity). Subtask 1 Table 6 lists the results on all monolingual datasets.10 The systems which made use of the shared Wikipedia corpus are mark"
S17-2002,S17-2034,0,0.0204818,"inds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obta"
S17-2002,W16-2508,1,0.276959,"reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of system"
S17-2002,D16-1235,0,0.0758158,"Missing"
S17-2002,E17-2036,1,0.804232,"Missing"
S17-2002,P15-2001,1,0.838039,"s 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approach"
S17-2002,I05-1067,0,0.020408,"t contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from various issues: 1. The similarity scale used for the annotation of WordSim-353 and MEN (Bruni et al., 2014) does not distinguish between similarity and relatedness, and hence conflates these two. As a result, the datasets contain pairs that are judged to be highly similar even if they are not of similar type or nature. F"
S17-2002,2015.mtsummit-papers.27,0,0.0742157,"Missing"
S17-2002,D09-1124,0,0.0561085,"nce Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets,"
S17-2002,S17-2041,0,0.0352303,"Missing"
S17-2002,S17-2033,0,0.0191197,"arks for evaluation. All kinds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have"
S17-2002,N15-1184,0,0.0720481,"Missing"
S17-2002,J15-4004,0,0.047794,". Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-"
S17-2002,S17-2032,0,0.0235773,"ic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair com"
S17-2002,S17-2037,0,0.0372334,"Missing"
S17-2002,C12-1109,0,0.034733,"24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages ha"
S17-2002,S14-2003,1,0.88311,"Missing"
S17-2002,P11-1076,0,0.00934146,"lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked wit"
S17-2002,P16-2074,0,0.0185427,"ex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from"
S17-2002,S17-2035,0,0.0441697,"Missing"
S17-2002,D14-1162,0,0.118611,"c Word Similarity Jose Camacho-Collados*1 , Mohammad Taher Pilehvar*2 , Nigel Collier2 and Roberto Navigli1 1 2 Department of Computer Science, Sapienza University of Rome Department of Theoretical and Applied Linguistics, University of Cambridge 1 {collados,navigli}@di.uniroma1.it 2 {mp792,nhc30}@cam.ac.uk Abstract word representation, a research field that has recently received massive research attention mainly as a result of the advancements in the use of neural networks for learning dense low-dimensional semantic representations, often referred to as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Almost any application in NLP that deals with semantics can benefit from efficient semantic representation of words (Turney and Pantel, 2010). However, research in semantic representation has in the main focused on the English language only. This is partly due to the limited availability of word similarity benchmarks in languages other than English. Given the central role of similarity datasets in lexical semantics, and given the importance of moving beyond the barriers of the English language and developing languageindependent and multilingual techniques, we felt that this was an appropriat"
S17-2002,S17-2036,0,0.0222596,"ncouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3."
S17-2002,P15-2004,0,0.014103,"oned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarit"
S17-2002,S17-2038,0,0.0206795,"iety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results"
S17-2002,S17-2039,0,0.044791,"Missing"
S17-2002,P14-1044,1,0.822702,"task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations ("
S17-2002,S17-2040,0,0.0437279,"Missing"
S17-2002,S17-2042,0,0.0464227,"Missing"
S17-2002,D13-1141,0,0.039439,"such as RG-65, MC30 (Miller and Charles, 1991), and WS-Sim (Agirre et al., 2009) (the similarity portion of WordSim-353) are relatively small, containing 65, 30, and 200 word pairs, respectively. Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming"
S17-2002,K15-1026,0,0.0981438,"erit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages. 2 Table 1: The set of thirty-four domains. wide range of domains (Section 2.1.1), (2) through translation of these pairs, we obtained word pairs for the other four languages (Section 2.1.2) and, (3) all word pairs of each dataset were manually scored by multiple annotators (Section 2.1.3). 2.1.1 English dataset creation Seed set selection. The dataset creation started with the selection of 500 English words. One of the main objectives o"
S17-2002,S17-2008,0,0.123538,"distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results of the concept and entity embeddings of"
S17-2002,D14-1034,0,\N,Missing
S17-2002,P16-1157,0,\N,Missing
S17-2002,W16-2502,0,\N,Missing
W00-0904,A97-1029,0,0.081688,"Missing"
W00-0904,W98-1118,0,0.0668506,"Missing"
W00-0904,P96-1041,0,0.0313857,"J A P A N B u n k y o - k u , T o k y o , 113-0033 J A P A N nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp Abstract made by these measures against actual system performance. Recently IE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Sekine et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: We present two measures for comparing corpora based on infbrmation theory statistics such as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor. 1 Introduction With the advent of the information society and increasing availability of large m o u n t s of in"
W00-0904,C00-1030,1,0.897199,"annot hope to achieve performance limits. What we aim to do is to compare model performance against the predictions of corpus difficulty made by two different methods. In the rest of this paper we firstly introduce the NE models used for evaluation, the two corpora we examined and then the difficulty comparison metrics. Predictive scores from the metrics are examined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased modeels for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). W e now describe two models used in our experiments based on the decision trees package C4.5 (Quiuian, 1993) and H M M s (Rabiner and Juang, 1986). 2.1 Decision tree n a m e d recogniser:NE-DT entity A decision tree is a type of classifier which has ""leaf nodes"" indicating classes and ""decision nodes"" that specify some test to be carried out, with"
W00-0904,A97-1028,0,0.0545242,"Missing"
W00-0904,W96-0213,0,0.0734193,"ts ( S e i n e et al., 1998). It has two phases, one for creating the decision tree from training d a t a and the other for generating the class-tagged text based on the decision tree. When generating decision trees, trigrams of words were used. For this system, words are considered to be quadruple features. The following features are used to generate conditions in the decision tree: P a r t - o f - s p e e c h i n f o r m a t i o n : There are 45 part-of-speech categories, whose definitions are based on Pennsylvania Treebank&apos;s categories. We use a tagger based on Adwait Ratnaparkhi&apos;s method (Ratnaparkhi, 1996). Character type i n f o r m a t i o n : Orthographic information is considered such as upper case, lower case, capitalization, numerical expressions, symbols. These character features are the same as those used by N E H M M described in the next section and shown in T a b l e 1. W o r d lists s p e c i f i c t o t h e d o m a i n : Word lists are made from the training corpus. Only the 200 highest fxequency words are used. 2.2 Hidden Markov model named entity reco~.iser: NEHMM HMMs are a widely u ~ d class of learning algorithms and can be considered to be stochastic finite state machines. In"
W00-0904,W98-1120,0,0.0369914,"I n f o r m a t i o n Science Communications Research Laboratory G r a d u a t e School o f Science 588-2 I w a o k a , I w a o k a - c h o , N i s h i - k u U n i v e r s i t y o f T o k y o , H o n g o 7-3-1 K o b e , H y o g o , 65].-2492 J A P A N B u n k y o - k u , T o k y o , 113-0033 J A P A N nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp Abstract made by these measures against actual system performance. Recently IE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Sekine et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: We present two measures for comparing corpora based on infbrmation theory statistics such as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - u"
W00-0904,M93-1007,0,\N,Missing
W00-1704,W98-1507,0,0.0605934,"ties” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a tag set based on the ontology which conforms to SGML/XML format. Using the tag set, we annotated the entities such names that appears in the abstracts of research papers taken from the MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are sho"
W00-1704,M98-1001,0,0.0143654,"sentations than the methods based on dictionaries and hand-constructed heuristic rules. We think that a corpus-based, machine-learning approach is quite promising, and to support this we are building a corpus of annotated abstracts taken from National Library of Medicine (NLM)’s MEDLINE database. Corpus annotation is now a key topic for all areas of natural language processing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition tech"
W00-1704,M98-1028,0,0.067007,"sing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a"
W00-1704,M98-1024,0,0.127633,"sing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a"
W00-1704,P99-1032,0,0.0500168,"Missing"
W02-2029,W00-1704,1,\N,Missing
W02-2029,A97-1028,0,\N,Missing
W02-2029,J95-4004,0,\N,Missing
W02-2029,C00-1030,1,\N,Missing
W02-2029,W00-0904,1,\N,Missing
W02-2029,W99-0613,0,\N,Missing
W02-2029,W00-0730,0,\N,Missing
W02-2029,A97-1011,0,\N,Missing
W02-2029,A97-1029,0,\N,Missing
W02-2029,collier-etal-2002-progress,1,\N,Missing
W03-1308,A97-1029,0,0.0752973,"ics that can be captured using IE our basic goal is to join this with deep semantic representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous e"
W03-1308,W98-1118,0,0.0185106,"representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted). This paper explores the underlying"
W03-1308,H92-1022,0,0.0157472,"ssify some of the training data so that the margin between other training points is maximized. This is particularly useful for real world data sets that often contain inseparable data points. Generalising with features In order for the model to be successful it must recognize regularities in the training data that relate preclassified examples of terms with unseen terms that will be encountered in testing. Following on from previous studies in named entity we chose a set of linguistically motivated wordlevel features that include surface word forms, part of speech tags using the Brill tagger (Brill, 1992) and orthographic features. Additionally we used head-noun features that were obtained from preanalysis of the training data set using the FDG shallow parser from Conexor (Tapanainen and J¨arvinen, 1997). A significant proportion of the terms in our corpus undergo a local syntactic transformations such as coordination which introduces ambiguity that needs to be resolved by shallow parsing. For example the c- and v-rel (proto) oncogenes and NF-kappaB and I kappa B protein families. In these cases the head noun features oncogene and family would be added to each word in the constituent phrase. H"
W03-1308,W98-1120,0,0.0313715,"ic goal is to join this with deep semantic representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citation"
W03-1308,W02-2029,1,0.76834,"ed training examples. The number of parameters to be estimated in α therefore never exceeds the number of examples. The influence of αi basically means that training examples with αi &gt; 0 define the decision function (the support vectors) and those examples with αi = 0 have no influence, making the final model very compact and testing (but not training) very fast. The point x is classified as positive (or negative) if f (x) &gt; 0 (or f (x) &lt; 0). The kernel function we explored in our experiments was the polynomial function k(xi , xj ) = (xi · xj + 1)d for d = 2 which was found to be the best by (Takeuchi and Collier, 2002). Once input vectors have been mapped to the feature space the linear discrimination function which is found is the one which gives the maximum the geometric margin between the two classes in the feature space. Besides efficiency of representation, SVMs are known to maximize their generalizability, making them an ideal model for the NE+ task. Generalizability in SVMs is based on statistical learning theory and the observation that it is useful sometimes to misclassify some of the training data so that the margin between other training points is maximized. This is particularly useful for real w"
W03-1308,A97-1011,0,0.0474845,"Missing"
W03-1308,W00-1704,1,0.87942,"Missing"
W03-1308,W00-0904,1,\N,Missing
W04-1205,P02-1047,0,0.0818233,"Missing"
W04-1205,mizuta-collier-2004-annotation,1,0.543142,"of (Teufel and Moens, 2002) and apply ZI to the domain of biology. But our approach is unique in that we focus on experimental results and on a qualitative analysis of ZI as a basis for automatic ZI. Abstract Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance the idea proposed in (Mizuta and Collier, 2004); annotating articles in terms of rhetorical zones with shallow nesting. We give a qualitative analysis of the zone identification (ZI) process in biology articles. Specifically, we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals. We also discuss controversial cases and nested zones, and ZI using multiple features. In doing so, we provide a stronger theoretical and practical support for our framework toward automatic ZI. 1 Introduction Information extraction (IE) in the biomedical domain is now regarded a"
W04-1205,E99-1015,0,0.196085,"Missing"
W04-1205,J02-4002,0,0.0920822,"JCB), 2) discuss controversial cases for ZI and nested annotation to elaborate the scheme, 3) discuss multiple features relevant to ZI, and 4) summarize the investigation and outline future steps related to machine learning and applications. Previous work on rhetorical analysis of scientific articles focus on either; 1) hierarchical discourse relations between sentences (e.g. Mann and Thompson, 1987), 2) genre analysis within a descriptive framework (e.g. Swales 1990), or 3) ZI in a flat structure and a statistical evaluation of the annotation scheme from a machine learning perspective (e.g. Teufel and Moens, 2002). We follow the lines of (Teufel and Moens, 2002) and apply ZI to the domain of biology. But our approach is unique in that we focus on experimental results and on a qualitative analysis of ZI as a basis for automatic ZI. Abstract Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance"
W04-1213,W04-1219,0,0.627413,"ied by the eight participating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performa"
W04-1213,W04-1217,0,0.448851,"ls were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpretation on this may be the CRF is often regarded as a kind of version-upped model of the MEMM (in the sense that both are conditional, exponential models) and thus is replacing MEMM. 5.2 purpose taggers (Lee et al., 2004). BeseNP tags and deep syntactic features were also exploited by several systems but the effectiveness was not clearly examined. The top-ranked two systems incorporated information from gaz"
W04-1213,W04-1221,0,0.551357,"ed as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpretation on this may be the CRF is often regarded as a kind of version-upped model of the MEMM (in the sense that both are conditional, exponential models) and thus is replacing MEMM. 5.2 purpose taggers (Lee et al., 2004). BeseNP tags"
W04-1213,W04-1220,0,0.0495577,"rticipating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpreta"
W04-1213,W04-1218,0,0.0450635,"Missing"
W04-1213,W04-1215,0,0.0331929,"s Roughly four types of classification models were applied by the eight participating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at th"
W04-1213,W04-1214,0,\N,Missing
W04-1213,W04-1216,0,\N,Missing
W04-1213,W03-0419,0,\N,Missing
W04-3253,P89-1010,0,0.10208,"Missing"
W04-3253,C00-1044,0,0.233467,"approach is shown to yield better performance than previous models on the same data. In the case of the latter, results indicate that our approach may allow for further improvements to be gained given knowledge of the topic of the text. 2 Motivation A continual challenge in the task of sentiment analysis of a text is to home in on those aspects of the text which are in some way representative of the tone of the whole text. In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al., 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text. Pang et al. (2002)’s treatment of the task as analogous to topicclassification underscores the difference between the two tasks. Sources of misleading phrases include what Pang et al. (2002) refer to as “thwarted expectations” narrative, where emotive effect is attained by emphasizing the contrast between what the reviewer expected and the actual experience. For example, in the record review data used in the p"
W04-3253,W02-1011,0,0.0913584,"ormation sources Tony Mullen and Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku Tokyo 101-8430 Japan mullen,collier @nii.ac.jp  Abstract This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement. 1 Introduction Recen"
W04-3253,A97-1011,0,0.0343015,"Missing"
W04-3253,P02-1053,0,0.209172,"e case of the former, the present approach is shown to yield better performance than previous models on the same data. In the case of the latter, results indicate that our approach may allow for further improvements to be gained given knowledge of the topic of the text. 2 Motivation A continual challenge in the task of sentiment analysis of a text is to home in on those aspects of the text which are in some way representative of the tone of the whole text. In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al., 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text. Pang et al. (2002)’s treatment of the task as analogous to topicclassification underscores the difference between the two tasks. Sources of misleading phrases include what Pang et al. (2002) refer to as “thwarted expectations” narrative, where emotive effect is attained by emphasizing the contrast between what the reviewer expected and the actual experience"
W04-3253,J90-1003,0,\N,Missing
W04-3253,P97-1023,0,\N,Missing
W07-1003,W04-3239,0,0.0297335,"rmation about Role concepts is integrated into the schema as attributes on NEs. This work takes the investigation one step forward by showing empirical evidence for the usefulness of Role concepts in a practical application. In this paper, we focus on the task of text classification, proceeding under the simplifying assumption that given enough annotated training data for NEs and their Roles both can be automatically tagged with high accuracy. In recent years there have been many studies on text classification using general methods (Sebastiani, 2002; Yang and Liu, 1999) semi-structured texts (Kudo and Matsumoto, 2004), 18 and XML classification (Zaki and Aggarwal, 2003). Other research has investigated the contribution of semantic information in the form of synonyms, syntax, etc. in text representation (Bloehdorn and Hotho, 2004; Hotho et al., 2003; Fr¨urnkranz et al., 1998). Feature selection (Scott and Matwin, 1999) has also been studied. The contribution of this paper is to provide an analysis and evaluation on the Roles of NEs in annotated text classification. The rest of this paper is organized as follows: in Section 2, we outline the BioCaster schema for the annotation of terms in biomedical text; Se"
W09-1318,W08-0607,0,0.120655,"is classified as either relevant (350) or reject (650).3 The corpus is designed to include difficult borderline cases where more advanced understanding of the context is required. For example, an article may be about, say, polio, but not centrally concerned with specific outbreaks of that disease. Instead, the article could report a vaccination campaign or research breakthrough. 3 Hedges According to Hyland (1998), in an extensive study of speculative language in science writing, hedges “are the means by which writers can present a proposition as an opinion rather than a fact.” More recently, Kilicoglu and Bergler (2008) have presented a method for automatically identifying hedges in the biomedical domain. In the current work, we used a science orientated hedge lexicon derived from Mercer et al. (2004). The lexicon consisted of 72 verbs (including appear, appears, appeared, appearing, indicate, indicates, indicated, indicating, and so on) and 32 non-verbs (including, about, quite, poten1 www.biocaster.org ProMED-Mail is a human curated service for monitoring disease outbreak reports (www.promedmail.org.) 3 For copyright reasons, the BioCaster corpus is not publicly available. 2 Proceedings of the Workshop on"
W12-5503,D11-1121,0,0.436527,"), Twitter, Sentiment analysis. Proceedings of the Workshop on Information Extraction and Entity Analytics on Social Media Data, pages 23–38, COLING 2012, Mumbai, December 2012. 23 1 Introduction Recent research into social media has looked at the application of microblogs for predicting the daily rise and fall in stock prices. In many ways microblogs are an ideal early warning about company price movements as they are freely available, rapidly updated and provide spontaneous glimpses into the opinions and sentiments of consumers about their future purchasing behaviors. Previous work such as (Bar-Haim et al., 2011) has assumed that messages containing explicit buy or sell signals about stocks are the most informative for stock price prediction although such messages typically comprise only a fraction of the available information about company sentiment. In this work we approach the task from another perspective, one in which Twitter users’ sentiment, related to the company, influences the next day’s market price movement. This allows us to tap into a much wider base of mood about the company’s future prospects. With the high level of number of Tweets related to tech companies, we focus on predicting sto"
W12-5503,P05-1045,0,0.0137518,"ts on the topic of tech products, For example, although “LOL YES !! RT : *ACCOUNT* : *ACCOUNT* You know we loved your Mac & Cheese Tweet. Can we turn it into a National television ad right now ?.” contains the “mac” keyword, it isn’t a product of Apple corporation. To resolve this problem, we built a Named Entity Recognition(NER) system to identify whether the Tweet contains name entities related to the companies or not based on a linear Conditional Random Fields(CRF) model. The Linear CRF model is used because it is well-studied and has been successfully used in state-of-the-art NER systems (Finkel et al., 2005)(Finkel and Manning, 2009)(Wang, 2009). If the Tweet doesn’t contain any named entities as listed on the company keyword list, it is removed. Twitter users are interested in named entities, such as, famous entrepreneurs, organization names, trendy hardware and software when they talk about tech companies. We collected and labelled manually 3665 randomly sampled Tweets related to the companies based on keywords. These included 280 people names (42 unique), 395 organization names (38 unique), 2528 hardware names (171 unique) and 1401 software names (294 unique). Overall, we have 4604 named entit"
W12-5503,D09-1015,0,0.0223155,"ch products, For example, although “LOL YES !! RT : *ACCOUNT* : *ACCOUNT* You know we loved your Mac & Cheese Tweet. Can we turn it into a National television ad right now ?.” contains the “mac” keyword, it isn’t a product of Apple corporation. To resolve this problem, we built a Named Entity Recognition(NER) system to identify whether the Tweet contains name entities related to the companies or not based on a linear Conditional Random Fields(CRF) model. The Linear CRF model is used because it is well-studied and has been successfully used in state-of-the-art NER systems (Finkel et al., 2005)(Finkel and Manning, 2009)(Wang, 2009). If the Tweet doesn’t contain any named entities as listed on the company keyword list, it is removed. Twitter users are interested in named entities, such as, famous entrepreneurs, organization names, trendy hardware and software when they talk about tech companies. We collected and labelled manually 3665 randomly sampled Tweets related to the companies based on keywords. These included 280 people names (42 unique), 395 organization names (38 unique), 2528 hardware names (171 unique) and 1401 software names (294 unique). Overall, we have 4604 named entities in which 540 entities"
W12-5503,P11-2008,0,0.113974,"Missing"
W12-5503,W11-0704,0,0.0128077,"ttps://dev.twitter.com/docs/streaming-apis/streams/public 25 Figure 1: Daily up and down stock market prediction model 3.1 Data Figure 2: Keyword searching heat map (Apple). Data containing 5,001,460 daily Tweets was crawled by using Twitter online streaming API from 1st April 2011 to 31st May 2011. In this initial investigation we would like to control the market sample to focus on the United States, so we geographically focused our Twitter queries on four large cities: New York, Chicago, Los Angeles and San Francisco. This also has some benefit in harmonising vocabulary – a recent study by (Gouws et al., 2011) noted significant differences in word abbreviation behaviour between British and American microtext authors. The Google Insights heat map2 shown in figure 2 indicates that the people living in these four areas are more interested in the four companies. To compute the daily up and down of stock 2 http://www.google.com/insights/search 26 market changes we employed Yahoo! Finance data using (1).  M ovei = OpenPi − C losePP r e(i) = difference &gt;= 0 difference < 0 Raise Drop (1) where i is a stock market open day, M ovei is stock market change on day i, OpenPi is the opening price on day i, C los"
W12-5503,P11-1038,0,0.0251876,"Missing"
W12-5503,D12-1039,0,0.0560035,"Missing"
W12-5503,C00-1044,0,0.088357,"four different review domains. (Pang et al., 2002) applied three supervised learners for classifying bidirectional sentiment in movie reviews, finding a number of challenges over traditional topic classification such as “thwarted expectations”. (Hu and Liu, 2004) and (Dave et al., 2003) also performed product classification with the former focusing on the qualities of product features. (Mullen and Collier, 24 2004) used lexical clues from Epinion movie reviews and Pitchfork Media music reviews, yielding insights into the value of topical references. With respect to identifying subjectivity, (Hatzivassiloglou and Wiebe, 2000)(Wiebe et al., 2001) examined the role of adjective classes for separating subjective from objective language. The technologies used for determining sentiment orientation commonly include manual or semi-automatic methods for constructing sentiment lexicons, e.g. (Turney, 2002). (Das and Chen, 2007) in the stock analysis domain used a lexicon of finance words to help determine significant correlation between aggregated stock board messages by small investors and the Morgan Stanley High technology 35 Index (MSH35). However, they found the correlation was weak for individual stocks. More recently"
W12-5503,P11-1037,0,0.0482587,"Missing"
W12-5503,W04-3253,1,0.826102,"Missing"
W12-5503,W02-1011,0,0.0208979,"Missing"
W12-5503,N10-1021,0,0.0465785,"Missing"
W12-5503,N12-1034,0,0.0150114,"nance words to help determine significant correlation between aggregated stock board messages by small investors and the Morgan Stanley High technology 35 Index (MSH35). However, they found the correlation was weak for individual stocks. More recently, with the explosion of interest in social networks, a popular microblogging service called Twitter has become a major source for data-driven investigation. (Java et al., 2007)(Kwak et al., 2010) for example showed the social motivations of its users, and others (Zhao et al., 2007)(Lin et al., 2010)(Ritterman et al., 2009)(Petrovi´c et al., 2010)(Petrovic et al., 2012) focused on breaking news or event detection. Sentiment analysis has been found to play an significant role in many applications (Krishnamurthy et al., 2008)(Bollen et al., 2011a)(Kivran-Swaine and Naaman, 2011) complementing evidence from Twitter messages and network structure. In recent work on stock market prediction, (Bar-Haim et al., 2011) used Twitter messages (Tweets) from StockTwits to identify expert investors for predicting stock price rises. They used a support vector machine (SVM) to classify each stock related message to two polarities - “bullish” and “bearish” and then identified"
W12-5503,P02-1053,0,0.0153313,"f NegDiff i < 0 (5) Where negat ivei denotes the number of negatively classified message by TST for a particular company on day i. 3.4.2 Bullish vs. Bearish features To determine whether consumers have market confidence in the company, we made use of a Part-of-speech (POS) tagger to extract adjective, noun, adverb and verb words and fixed them to “bullish” and “bearish” as anchor words. We chose CMU POS Tagger proposed by (Gimpel et al., 2011) because this POS Tagger achieved the state of the art on Tweet data. We then calculated the anchor words using the Semantic Orientation (SO) algorithm (Turney, 2002). This algorithm uses mutual information to measure the association between two words. The 30 Pointwise Mutual Information (PMI) is formulated by function 6. P M I(w1 , w2 ) = log2 ( p(w1 &w2 ) p(w1 )p(w2 ) (6) ) where w1 and w2 are two word strings. p(w1 &w2 ) are the probability that both words cooccurred. p(w1 ) and p(w2 ) are the probability that an isolated word occurs. The ratio shows a metric of statistical dependence between w1 and w2 . Thus, SO can be defined by (7). SO(w) = P M I(w, “bull ish00 ) − P M I(w, “bear ish00 ) (7) Here, based on our anchor words, we redefined SO in (8). SO"
W12-5503,P09-3003,0,0.157008,"although “LOL YES !! RT : *ACCOUNT* : *ACCOUNT* You know we loved your Mac & Cheese Tweet. Can we turn it into a National television ad right now ?.” contains the “mac” keyword, it isn’t a product of Apple corporation. To resolve this problem, we built a Named Entity Recognition(NER) system to identify whether the Tweet contains name entities related to the companies or not based on a linear Conditional Random Fields(CRF) model. The Linear CRF model is used because it is well-studied and has been successfully used in state-of-the-art NER systems (Finkel et al., 2005)(Finkel and Manning, 2009)(Wang, 2009). If the Tweet doesn’t contain any named entities as listed on the company keyword list, it is removed. Twitter users are interested in named entities, such as, famous entrepreneurs, organization names, trendy hardware and software when they talk about tech companies. We collected and labelled manually 3665 randomly sampled Tweets related to the companies based on keywords. These included 280 people names (42 unique), 395 organization names (38 unique), 2528 hardware names (171 unique) and 1401 software names (294 unique). Overall, we have 4604 named entities in which 540 entities are unique."
W12-5503,W01-1626,0,0.0270437,"ang et al., 2002) applied three supervised learners for classifying bidirectional sentiment in movie reviews, finding a number of challenges over traditional topic classification such as “thwarted expectations”. (Hu and Liu, 2004) and (Dave et al., 2003) also performed product classification with the former focusing on the qualities of product features. (Mullen and Collier, 24 2004) used lexical clues from Epinion movie reviews and Pitchfork Media music reviews, yielding insights into the value of topical references. With respect to identifying subjectivity, (Hatzivassiloglou and Wiebe, 2000)(Wiebe et al., 2001) examined the role of adjective classes for separating subjective from objective language. The technologies used for determining sentiment orientation commonly include manual or semi-automatic methods for constructing sentiment lexicons, e.g. (Turney, 2002). (Das and Chen, 2007) in the stock analysis domain used a lexicon of finance words to help determine significant correlation between aggregated stock board messages by small investors and the Morgan Stanley High technology 35 Index (MSH35). However, they found the correlation was weak for individual stocks. More recently, with the explosion"
W13-2019,P81-1022,0,0.719781,"if there are two complex event that relate to the same entity or event. Figure 1 shows an example of representing two complex events as two event trees. To build the event tree, we create a virtual ROOT node; the complex event target will be linked directly to this ROOT node, and triggers and entities that do not belong to sub-structure of the target event will also have links to ROOT node, too. In the event tree, labels of entity classes and event types are retained while terms of triggers and entities are removed. For event tree parsing, we used the Earley parsing algorithm proposed by Jay Earley (1970) to find alternative structures. The event tree is stored in memory in the form of Earley rules. The inputs to the parser are the entities and triggers which have been identified in the trigger detection module, and the outputs are the event tree candidates. To choose the best event tree candidates, we built a probabilistic Earley parser which developed from the idea of Hale (2001). As a first attempt at introducing robustness for edge classifier error our parser used linear interpolation on the probability from the edge detection module and the prior edge probabilities to calculate a score fo"
W13-2019,P11-1163,0,0.0966306,"al., 2013, Pyysalo et al., 2012), aimed at identifying biomedical relations of significance in the development and progress of cancer. Our system explored a multi-stage approach including trigger detection, edge detection and event composition. After trigger edge detection is finished we are left with a semantic graph from which we must select the optimal subset that is consistent with the semantic frames for each event type. Previous approaches have derived sub-graph matching rules using heuristics (Jari Björne et al. 2009) or machine learning using graph kernels (Liu et al., 2013). Based on McClosky et al. (2011)’s observation that event structures have a strong similarity to dependency graphs, we proposed a novel method for the composition of ambiguous events used a probabilistic variation of the Earley chart parsing algorithm (Stolcke 1995) for finding best derived trigger-argument candidates. Our method uses the event templates and named entity classes as grammar rules. As an additional novel step our chart parsing approach incorporates a linear interpolation mechanism for cross-domain adaptivity between the training and testing (development) data. 2 Approach The system consists of five main module"
W13-2019,W09-1402,0,0.0875936,"Missing"
W13-2019,W13-2008,0,\N,Missing
W13-2019,J95-2002,0,\N,Missing
W13-2019,U13-1019,0,\N,Missing
W14-1103,P08-1029,0,0.126474,"line abstracts. Despite the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of"
W14-1103,P07-1033,0,0.137257,"Missing"
W14-1103,P07-1034,0,0.0522826,"ch corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would"
W14-1103,W04-1213,1,0.805633,"Missing"
W14-1103,N09-1068,0,0.0256633,"his paper we aim to empirically test domain transferrence for bioNER under the condition that the test and training data are relatively small and drawn from near domains, i.e. from studies on different types of heritable diseases. To do this we selected Medline abstracts from PubMed that were cited by biocuration experts in the canon13 Abstracts Tokens Av. length ANA CHE DIS GGP ORG PHE C1 110 27,421 32.57 194 (138) 44 (33) 892 (282) 1663 (928) 799 (429) 507 (423) C2 80 26,578 29.93 195 (133) 147 (75) 955 (442) 754 (511) 770 (323) 1430 (1113) a 0.33 b 0.26 0.08 0.07 0.39 0.27 0.41 0.45 As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkit1 with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 0.56 0.67 3.3 Features 0.52 0.33 3.2 Conditional Random Fields We made use of a wide range of features, both conventional features such as word or part of speech, as w"
W14-1103,D10-1044,0,0.0225439,"sed to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in"
W14-1103,P11-1037,0,0.0117148,"made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared by both: {t1 ,t6 ,t"
W14-1103,W03-0430,0,0.00881672,"latively small and drawn from near domains, i.e. from studies on different types of heritable diseases. To do this we selected Medline abstracts from PubMed that were cited by biocuration experts in the canon13 Abstracts Tokens Av. length ANA CHE DIS GGP ORG PHE C1 110 27,421 32.57 194 (138) 44 (33) 892 (282) 1663 (928) 799 (429) 507 (423) C2 80 26,578 29.93 195 (133) 147 (75) 955 (442) 754 (511) 770 (323) 1430 (1113) a 0.33 b 0.26 0.08 0.07 0.39 0.27 0.41 0.45 As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkit1 with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 0.56 0.67 3.3 Features 0.52 0.33 3.2 Conditional Random Fields We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Ta"
W14-1103,W09-1119,0,0.03134,"2) using the Mallet toolkit1 with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 0.56 0.67 3.3 Features 0.52 0.33 3.2 Conditional Random Fields We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies such as (Ratinov and Roth, 2009) have noted that domain gazetteer features play a critical role in aiding classification. In order to show realistic model behaviour consistent with state-of-the-art techniques we have included gazetteers derived from: the Human Phenotype Ontology (HPO: 15,800 terms), the Mammalian Phenotype Ontology (MP: 23,700 terms), the Phenotypic Attribute and Trait Ontology (PATO: 2,200 synonyms), the Brenda Tissue Ontology (BTO: 9,600 synonyms), the Foundation Model of Anatomy (FMA: 120,000 terms), National Library of Medicine gene list (NLM: 9 million terms), UMLS disease terms (UMLS: 275,000 terms), J"
W14-1103,D07-1111,0,0.0349345,"Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared b"
W14-1103,W04-1221,0,0.016103,"from near domains, i.e. from studies on different types of heritable diseases. To do this we selected Medline abstracts from PubMed that were cited by biocuration experts in the canon13 Abstracts Tokens Av. length ANA CHE DIS GGP ORG PHE C1 110 27,421 32.57 194 (138) 44 (33) 892 (282) 1663 (928) 799 (429) 507 (423) C2 80 26,578 29.93 195 (133) 147 (75) 955 (442) 754 (511) 770 (323) 1430 (1113) a 0.33 b 0.26 0.08 0.07 0.39 0.27 0.41 0.45 As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkit1 with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 0.56 0.67 3.3 Features 0.52 0.33 3.2 Conditional Random Fields We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous"
W16-2902,D10-1115,0,0.0965553,"Missing"
W16-2902,D14-1113,0,0.0128517,"urces (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more"
W16-2902,P15-1072,1,0.912999,"ese techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more reliable representation of domain-specific items based on significantly more statistical knowledge. Our experiments show that the proposed model can provide a considerable improv"
W16-2902,D14-1162,0,0.0802104,"rophthalmia. 3.2 3.3 Baselines As baseline, we benchmark our improved representations against Word2vec. We use the 300dimensional vectors trained on the Google News corpus (about 100B tokens). We also report results for the Word2vec vectors when retrofitted using the approach of Faruqui et al. (2015) to the Paraphrase Database (Ganitkevitch et al., 2013, PPDB) and SNOMED-CT1 . The latter is a comprehensive clinical terminology from which we extracted 108K synonymous sets, each comprising an average of 2.7 synonyms. We also compare our representations against the 300-dimensional GloVe vectors (Pennington et al., 2014) trained on the Wikipedia 2014 + Gigaword 5 corpus (6B tokens). We were also interested in verifying how Word2vec and GloVe would perform if trained on Tasks Based on the ontological structure of HPO, we propose two tasks in the framework of semantic similarity measurement. Synonym identification. Let P be the set of all phenotypes in the HPO ontology. Let P ∗ = {p1 , ..., pk } (⊂ P) be the subset of k phenotypes for which at least one synonymous phenotype is provided in HPO and Spi = {s1pi , ..., slpi } be the set of l synonymous phenotypes for phenotype pi . Given a spi , the task here is si"
W16-2902,D14-1110,0,0.0473768,"Missing"
W16-2902,N10-1013,0,0.0359127,"th the help of knowledge derived from other resources (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more"
W16-2902,N15-1184,0,0.192577,"Missing"
W16-2902,P15-1173,0,0.0236663,"Missing"
W16-2902,N13-1092,0,0.0799235,"Missing"
W16-2902,P12-1092,0,0.0274672,"ived from other resources (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our"
W16-2902,P14-2089,0,0.21571,"Missing"
W16-2902,P15-1010,1,0.786101,"14). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more reliable representation of domain-specific items based on significantly more statistical knowledge. Our experiments show that the proposed model c"
W16-2902,Q14-1019,0,\N,Missing
W16-2902,P14-2050,0,\N,Missing
W16-2918,P16-1096,1,0.826959,"entence Classification Nut Limsopatham and Nigel Collier Language Technology Lab Department of Theoretical and Applied Linguistics University of Cambridge Cambridge, UK {nl347,nhc30}@cam.ac.uk Abstract et al., 2015). For example, Turian et al. (2010) used word embeddings as input features for several NLP systems, including a traditional chunking system based on conditional random fields (CRFs) (Lafferty et al., 2001). Collobert et al. (2011) used word embeddings as inputs of a multilayer neural network for part-of-speech tagging, chunking, named entity recognition and semantic role labelling. Limsopatham and Collier (2016) leveraged semantics from word embeddings when identifying medical concepts mentioned in social media messages. Kim (2014) showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec (Mikolov et al., 2013), as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and Lee, 2005) and consumer products, e.g. cameras (Hu and Liu, 2004). The quality of word embeddings (e.g. the ability to capture s"
W16-2918,W15-3820,0,0.0277304,"Missing"
W16-2918,N15-1184,0,0.0166654,"ennington et al., 2014). For instance, when induced from a generic corpus, such as Google News, the vector representation of ‘tissue’ would be similar to the vectors of ‘paper’ and ‘toilet’. However, when induced from medical corpora, such as PubMed1 or BioMed Central2 , the vector of ‘tissue’ would be more similar to those of ‘cell’ and ‘organ’. Hence, word embeddings induced from the corpus related to the task or target domain are likely to be more useful. Meanwhile, it is intuitive that the more training documents used, the more likely that more vocabulary is covered. Recent studies (e.g. (Faruqui et al., 2015; Xu et al., 2014; Yu and Dredze, 2014)) have attempted to improve the quality of word embeddings by Word embeddings have been successfully exploited in systems for NLP tasks, such as parsing and text classification. It is intuitive that word embeddings created from a larger corpus would provide a better coverage of vocabulary. Meanwhile, word embeddings trained on a corpus related to the given task or target domain would more effectively represent the semantics of terms. However, in some emerging domains (e.g. bio-surveillance using social media data), it may be difficult to find a domain cor"
W16-2918,P05-1015,0,0.841489,"neural network for part-of-speech tagging, chunking, named entity recognition and semantic role labelling. Limsopatham and Collier (2016) leveraged semantics from word embeddings when identifying medical concepts mentioned in social media messages. Kim (2014) showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec (Mikolov et al., 2013), as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and Lee, 2005) and consumer products, e.g. cameras (Hu and Liu, 2004). The quality of word embeddings (e.g. the ability to capture semantics of words) highly depends on the corpus from which they are induced (Pennington et al., 2014). For instance, when induced from a generic corpus, such as Google News, the vector representation of ‘tissue’ would be similar to the vectors of ‘paper’ and ‘toilet’. However, when induced from medical corpora, such as PubMed1 or BioMed Central2 , the vector of ‘tissue’ would be more similar to those of ‘cell’ and ‘organ’. Hence, word embeddings induced from the corpus related"
W16-2918,D14-1162,0,0.0916493,"entioned in social media messages. Kim (2014) showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec (Mikolov et al., 2013), as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and Lee, 2005) and consumer products, e.g. cameras (Hu and Liu, 2004). The quality of word embeddings (e.g. the ability to capture semantics of words) highly depends on the corpus from which they are induced (Pennington et al., 2014). For instance, when induced from a generic corpus, such as Google News, the vector representation of ‘tissue’ would be similar to the vectors of ‘paper’ and ‘toilet’. However, when induced from medical corpora, such as PubMed1 or BioMed Central2 , the vector of ‘tissue’ would be more similar to those of ‘cell’ and ‘organ’. Hence, word embeddings induced from the corpus related to the task or target domain are likely to be more useful. Meanwhile, it is intuitive that the more training documents used, the more likely that more vocabulary is covered. Recent studies (e.g. (Faruqui et al., 2015; X"
W16-2918,W15-2608,0,0.265284,"Missing"
W16-2918,P14-1062,0,0.0170603,"olov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). Importantly, word embeddings have been effectively used for several NLP tasks (Turian et al., 2010; Collobert et al., 2011; SeguraBedmar et al., 2015; Limsopatham and Collier, 2015a; Limsopatham and Collier, 2015b; Muneeb 1 136 2 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.biomedcentral.com/ Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 136–140, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics sentence matching (Collobert and Weston, 2008; Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014). In this work, we adapt the CNN model of Kim (2014) to exploit both generic and target domain word embeddings, because of its simplicity and effectiveness. The model architecture of Kim (2014) is shown in Figure 1. In particular, for a given input sentence of length n words (padded where necessary), we create a sentence matrix S ∈ Rd×n , where each column is the ddimensional vector (i.e. embedding) xi ∈ Rd of each word in the sentence: Figure 1: CNN for sentence classification. enhancing the learning algorithm or injecting an existing knowledge-base, e.g. WordNet (Miller, 19"
W16-2918,P10-1040,0,0.131885,"word embeddings. To deal with this problem, we propose novel approaches that use both word embeddings created from generic and target domain corpora. Our experimental results on sentence classification tasks show that our approaches significantly improve the performance of an existing convolutional neural network that achieved state-of-the-art performances on several text classification tasks. 1 Introduction Word embeddings (i.e. distributed vector representation) represent words using dense, lowdimensional and real-valued vectors, where each dimension represents a latent feature of the word (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). It has been empirically shown that word embeddings could capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). Importantly, word embeddings have been effectively used for several NLP tasks (Turian et al., 2010; Collobert et al., 2011; SeguraBedmar et al., 2015; Limsopatham and Collier, 2015a; Limsopatham and Collier, 2015b; Muneeb 1 136 2 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.biomedcentral.com/ Proceedings of the 15th Workshop on Biomed"
W16-2918,D14-1181,0,0.345591,"of Cambridge Cambridge, UK {nl347,nhc30}@cam.ac.uk Abstract et al., 2015). For example, Turian et al. (2010) used word embeddings as input features for several NLP systems, including a traditional chunking system based on conditional random fields (CRFs) (Lafferty et al., 2001). Collobert et al. (2011) used word embeddings as inputs of a multilayer neural network for part-of-speech tagging, chunking, named entity recognition and semantic role labelling. Limsopatham and Collier (2016) leveraged semantics from word embeddings when identifying medical concepts mentioned in social media messages. Kim (2014) showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec (Mikolov et al., 2013), as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and Lee, 2005) and consumer products, e.g. cameras (Hu and Liu, 2004). The quality of word embeddings (e.g. the ability to capture semantics of words) highly depends on the corpus from which they are induced (Pennington et al., 2014). For instance, when"
W16-2918,P14-2050,0,0.0304074,"e the performance of an existing convolutional neural network that achieved state-of-the-art performances on several text classification tasks. 1 Introduction Word embeddings (i.e. distributed vector representation) represent words using dense, lowdimensional and real-valued vectors, where each dimension represents a latent feature of the word (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). It has been empirically shown that word embeddings could capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). Importantly, word embeddings have been effectively used for several NLP tasks (Turian et al., 2010; Collobert et al., 2011; SeguraBedmar et al., 2015; Limsopatham and Collier, 2015a; Limsopatham and Collier, 2015b; Muneeb 1 136 2 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.biomedcentral.com/ Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 136–140, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics sentence matching (Collobert and Weston, 2008; Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014). In this work, we adapt"
W16-2918,P14-2089,0,0.156048,"when induced from a generic corpus, such as Google News, the vector representation of ‘tissue’ would be similar to the vectors of ‘paper’ and ‘toilet’. However, when induced from medical corpora, such as PubMed1 or BioMed Central2 , the vector of ‘tissue’ would be more similar to those of ‘cell’ and ‘organ’. Hence, word embeddings induced from the corpus related to the task or target domain are likely to be more useful. Meanwhile, it is intuitive that the more training documents used, the more likely that more vocabulary is covered. Recent studies (e.g. (Faruqui et al., 2015; Xu et al., 2014; Yu and Dredze, 2014)) have attempted to improve the quality of word embeddings by Word embeddings have been successfully exploited in systems for NLP tasks, such as parsing and text classification. It is intuitive that word embeddings created from a larger corpus would provide a better coverage of vocabulary. Meanwhile, word embeddings trained on a corpus related to the given task or target domain would more effectively represent the semantics of terms. However, in some emerging domains (e.g. bio-surveillance using social media data), it may be difficult to find a domain corpus that is large enough for creating e"
W16-2918,D15-1194,1,0.764292,".e. distributed vector representation) represent words using dense, lowdimensional and real-valued vectors, where each dimension represents a latent feature of the word (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). It has been empirically shown that word embeddings could capture semantic and syntactic similarities between words (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). Importantly, word embeddings have been effectively used for several NLP tasks (Turian et al., 2010; Collobert et al., 2011; SeguraBedmar et al., 2015; Limsopatham and Collier, 2015a; Limsopatham and Collier, 2015b; Muneeb 1 136 2 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.biomedcentral.com/ Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 136–140, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics sentence matching (Collobert and Weston, 2008; Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014). In this work, we adapt the CNN model of Kim (2014) to exploit both generic and target domain word embeddings, because of its simplicity and effectiveness. The model architecture of Kim (2014) is shown in F"
W16-3920,W15-4319,0,0.178888,"Missing"
W16-3920,J81-4005,0,0.771936,"Missing"
W16-3920,P15-1033,0,0.0164897,"rd induced from a character level. 3.2.2 Word Representation Existing studies, e.g. (Mikolov et al., 2013; Pennington et al., 2014), have shown that word embeddings induced from a large corpus could effectively capture semantic and syntactic information of words. Hence, we also use pre-trained word embeddings as word representation. However, any randomly generated word embeddings can also be used. 147 Figure 2: Our bidirectional LSTM for Twitter NER. 3.3 Bidirectional LSTM In this work, we use bidirectional LSTM for modelling social media sentences, as existing work (e.g. (Huang et al., 2015; Dyer et al., 2015; Dyer et al., 2015; Bengio et al., 1994)) has shown that bidirectional LSTM could effectively deal with the variable lengths of sentences. In addition, it could capture past (from the previous words) and future (from the next words) information effectively (Huang et al., 2015; Dyer et al., 2015). Our bidirectional LSTM for Twitter NER is shown in Figure 2. For a given a social media sentence and its orthographic sentence, we firstly extract both character-based word representation and word vector representation corresponding to each word in the social media sentence and the orthographic sente"
W16-3920,W15-4322,0,0.0440387,"Missing"
W16-3920,D15-1194,1,0.835068,"s are difficult to develop and maintain. Examples of hand-crafted features are orthographic features (Bikel et al., 1999), which are based on patterns of characters contained in a given word. In this work, we investigate an approach for could automatically inducing and leveraging orthographic features for named entity recognition for Twitter messages. Neural networks have recently shown to be effective for several NLP tasks, such as NER (Chiu and Nichols, 2015), POS tagging (Huang et al., 2015), sentiment analysis (Limsopatham and Collier, 2016b) and grounding (Limsopatham and Collier, 2016c; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) designed a feed-forward neural network that learned to identify entities in a sentence by using contexts within a fixed number of surrounding words. Chiu and Nichols (2015) showed that modelling both character and word embeddings within a neural network for NER further improve the performance. Huang et al. (2015) introduced a more complex model based on bidirectional LSTM could also take into account hand-crafted features. In this work, we investigate an application of our novel approach (Limsopatham and Collier, 2016a) that enables bidirectional LSTM to"
W16-3920,W16-5102,1,0.649204,"cally for a particular task or domain. Consequently, these hand-crafted features are difficult to develop and maintain. Examples of hand-crafted features are orthographic features (Bikel et al., 1999), which are based on patterns of characters contained in a given word. In this work, we investigate an approach for could automatically inducing and leveraging orthographic features for named entity recognition for Twitter messages. Neural networks have recently shown to be effective for several NLP tasks, such as NER (Chiu and Nichols, 2015), POS tagging (Huang et al., 2015), sentiment analysis (Limsopatham and Collier, 2016b) and grounding (Limsopatham and Collier, 2016c; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) designed a feed-forward neural network that learned to identify entities in a sentence by using contexts within a fixed number of surrounding words. Chiu and Nichols (2015) showed that modelling both character and word embeddings within a neural network for NER further improve the performance. Huang et al. (2015) introduced a more complex model based on bidirectional LSTM could also take into account hand-crafted features. In this work, we investigate an application of our nov"
W16-3920,W16-2918,1,0.807204,"cally for a particular task or domain. Consequently, these hand-crafted features are difficult to develop and maintain. Examples of hand-crafted features are orthographic features (Bikel et al., 1999), which are based on patterns of characters contained in a given word. In this work, we investigate an approach for could automatically inducing and leveraging orthographic features for named entity recognition for Twitter messages. Neural networks have recently shown to be effective for several NLP tasks, such as NER (Chiu and Nichols, 2015), POS tagging (Huang et al., 2015), sentiment analysis (Limsopatham and Collier, 2016b) and grounding (Limsopatham and Collier, 2016c; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) designed a feed-forward neural network that learned to identify entities in a sentence by using contexts within a fixed number of surrounding words. Chiu and Nichols (2015) showed that modelling both character and word embeddings within a neural network for NER further improve the performance. Huang et al. (2015) introduced a more complex model based on bidirectional LSTM could also take into account hand-crafted features. In this work, we investigate an application of our nov"
W16-3920,P16-1096,1,0.475422,"cally for a particular task or domain. Consequently, these hand-crafted features are difficult to develop and maintain. Examples of hand-crafted features are orthographic features (Bikel et al., 1999), which are based on patterns of characters contained in a given word. In this work, we investigate an approach for could automatically inducing and leveraging orthographic features for named entity recognition for Twitter messages. Neural networks have recently shown to be effective for several NLP tasks, such as NER (Chiu and Nichols, 2015), POS tagging (Huang et al., 2015), sentiment analysis (Limsopatham and Collier, 2016b) and grounding (Limsopatham and Collier, 2016c; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) designed a feed-forward neural network that learned to identify entities in a sentence by using contexts within a fixed number of surrounding words. Chiu and Nichols (2015) showed that modelling both character and word embeddings within a neural network for NER further improve the performance. Huang et al. (2015) introduced a more complex model based on bidirectional LSTM could also take into account hand-crafted features. In this work, we investigate an application of our nov"
W16-3920,D15-1104,0,0.0385648,"st effective performance on both the ‘segmentation and categorisation’ and the ‘segmentation only’ sub-tasks. 1 Introduction Named entity recognition (NER), which is one of the first and important stages in a natural language processing (NLP) pipeline, is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; Segura-Bedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Twitter NER is an NER task that aims to identify mentions of entities in Twitter messages (i.e. tweets) (Baldwin et al., 2015; Ritter et al., 2011). Twitter NER is particularly challenging because of the unique characteristics of tweets. For instance, tweets are typically short as the number of characters in a particular twe"
W16-3920,W03-0430,0,0.670792,"hared task, our system achieved the most effective performance on both the ‘segmentation and categorisation’ and the ‘segmentation only’ sub-tasks. 1 Introduction Named entity recognition (NER), which is one of the first and important stages in a natural language processing (NLP) pipeline, is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; Segura-Bedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Twitter NER is an NER task that aims to identify mentions of entities in Twitter messages (i.e. tweets) (Baldwin et al., 2015; Ritter et al., 2011). Twitter NER is particularly challenging because of the unique characteristics of tweets. For instance, tweets are typically short as the number"
W16-3920,D14-1162,0,0.0953486,"g. sigmoid and tanh, and b ∈ R is a bias. In this work, we use 200 different filters with window size h = 3. Then, we follow Collobert et al. (2011) and apply max pooling to capture the most important feature from each filter. Indeed, max pooling takes the maximum value of each row in the matrix C:  cmax  max(C1,: )   .. =  . (3) max(Cd,: ) We use cmax vector as a character-based word representation in bidirectional LSTM (Section 3.3), as it captures important features of a given word induced from a character level. 3.2.2 Word Representation Existing studies, e.g. (Mikolov et al., 2013; Pennington et al., 2014), have shown that word embeddings induced from a large corpus could effectively capture semantic and syntactic information of words. Hence, we also use pre-trained word embeddings as word representation. However, any randomly generated word embeddings can also be used. 147 Figure 2: Our bidirectional LSTM for Twitter NER. 3.3 Bidirectional LSTM In this work, we use bidirectional LSTM for modelling social media sentences, as existing work (e.g. (Huang et al., 2015; Dyer et al., 2015; Dyer et al., 2015; Bengio et al., 1994)) has shown that bidirectional LSTM could effectively deal with the varia"
W16-3920,W09-1119,0,0.149653,"e ‘segmentation and categorisation’ and the ‘segmentation only’ sub-tasks. 1 Introduction Named entity recognition (NER), which is one of the first and important stages in a natural language processing (NLP) pipeline, is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; Segura-Bedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Twitter NER is an NER task that aims to identify mentions of entities in Twitter messages (i.e. tweets) (Baldwin et al., 2015; Ritter et al., 2011). Twitter NER is particularly challenging because of the unique characteristics of tweets. For instance, tweets are typically short as the number of characters in a particular tweet is restricted to 140; hence, the conte"
W16-3920,D11-1141,0,0.0186997,"sed on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; Segura-Bedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Twitter NER is an NER task that aims to identify mentions of entities in Twitter messages (i.e. tweets) (Baldwin et al., 2015; Ritter et al., 2011). Twitter NER is particularly challenging because of the unique characteristics of tweets. For instance, tweets are typically short as the number of characters in a particular tweet is restricted to 140; hence, the contextual information is limited. In addition, the use of colloquial language makes it difficult for existing NER approaches a general domain, such as newswire to be reused (Baldwin et al., 2015). Consequently, state-of-the-art NER software (e.g. Standford NER) is less effective on Twitter NER tasks (Derczynski et al., 2015). For our participation in the Named Entity Recognition in"
W16-3920,W15-2608,0,0.020122,"Missing"
W16-3920,W04-1221,0,0.405987,"achieved the most effective performance on both the ‘segmentation and categorisation’ and the ‘segmentation only’ sub-tasks. 1 Introduction Named entity recognition (NER), which is one of the first and important stages in a natural language processing (NLP) pipeline, is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; Segura-Bedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Twitter NER is an NER task that aims to identify mentions of entities in Twitter messages (i.e. tweets) (Baldwin et al., 2015; Ritter et al., 2011). Twitter NER is particularly challenging because of the unique characteristics of tweets. For instance, tweets are typically short as the number of characters i"
W16-3920,W16-3919,0,0.214436,"Missing"
W16-3920,W03-0419,0,0.400005,"Missing"
W16-5102,Q16-1026,0,0.0140965,"oaches have been effectively used for NER tasks. For example, Collobert et al. (2011) used a feed-forward neural network to effectively identify entities in a newswire corpus (Tjong Kim Sang and De Meulder, 2003) by classifying each word using contexts within a fixed number of surrounding words. Ma and Hovy (2016) and Lample et al. (2016) effectively used both character and word embeddings in a bi-directional LSTM for NER tasks, such as CoNLL03 (Tjong Kim Sang and De Meulder, 2003). Huang et al. (2015) combined hand-crafted features with bi-directional LSTM to further improve the performance. Chiu and Nichols (2016) achieved state-of-the-art performances by modelling both character and word embeddings before combining with hand-crafted features. Nevertheless, the studies of neural network models for biomedical NER tasks are limited. For instance, Chiu et al. (2016) investigated the use of the model of Collobert et al. (2011) with different word embeddings for the BioCreative II Gene Mention task (Smith et al., 2008b) and the JNLPBA task (Kim et al., 2004). In this work, we propose a novel end-to-end neural network model that can learn and leverage orthographic features, which are traditional domain-knowl"
W16-5102,W16-2922,0,0.0633412,"cember 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment analysis and concept normalisation (Collobert et al., 2011; Turian et al., 2010; Limsopatham and Collier, 2016a; Limsopatham and Collier, 2016b; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) effectively used word embeddings as inputs of a feed-forward neural network for sequence labelling tasks, such as NER and POS tagging. Turian et al. (2010) learned a CRF model using word embeddings as input features for NER and chunking tasks. In the biomedical domain, Chiu et al. (2016) investigated the the use of different word embeddings in a feed-forward neural network for biomedical NER tasks. However, when using with word embedding features, traditional features (e.g. orthography and gazetteers) have shown to further improve the performance of an NER system (Segura-Bedmar et al., 2015; Turian et al., 2010; Huang et al., 2015). In this work, we investigate a novel approach that allows an end-to-end neural network system for biomedical NER to explicitly learn and leverage orthographic features. Our approach is based on bidirectional long short-term memory (LSTM) (Hochreit"
W16-5102,C00-1030,1,0.626019,"ture for learning word representation from character embeddings. approaches used in NER tasks (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004). Specifically, CRF is based on an undirected statistical graphical model that aims to learn a latent structure of an input sequence. Examples of effective biomedical NER tools that are based on CRF are ABNER (Settles, 2005), BANNER (Leaman et al., 2008) and Gimli (Campos et al., 2013). However, the performance of these CRF-based tools heavily depend on hand-crafted features, such as orthographic and contextual features (Bikel et al., 1999; Collier et al., 2000), which are task-specific and costly to develop. For example, Segura-Bedmar et al. (2015) manually created orthographic features, such as upperInitial (i.e. whether a given word begins with an upper-case character and then follows by any lower-case characters) and allCaps (i.e. whether all characters in a given word are upper-case), when learning a CRF model for drug name recognition. In this work, we investigate an automatic approach that could automatically induce orthographic features for biomedical named entity recognition. Recently, neural network-based approaches have been effectively us"
W16-5102,J81-4005,0,0.791799,"Missing"
W16-5102,P15-1033,0,0.0165686,"raphic sentence. Target entities Type of data Number of documents for training Number of documents for development Number of documents for testing BC2 Genes MEDLINE abstracts 201 488 58 BioNLP09 Bio-molecular events MEDLINE abstracts 1,436 995 2,200 NCBI Diseases PubMed articles 8,662 2,872 1,036 Table 2: The three datasets used to evaluate our proposed approach. 3.3 Bi-directional LSTM We use bi-directional LSTM to learn to identify named entities in a sentence, because it can capture past (from the previous words) and future (from the next words) information effectively (Huang et al., 2015; Dyer et al., 2015). In addition, LSTM has shown to capture long-distance dependencies more effectively than a vanilla recurrent neural networks (RNNs), since it can cope with the gradient vanishing/exploding problems better (Dyer et al., 2015; Bengio et al., 1994). To enable bi-directional LSTM to learn orthographic features, we create an orthographic pattern of the input sentence (denoted, the orthographic sentence). Specifically, given an input sentence (e.g. ‘interaction between CrkII and A-T2’), we generate an orthographic sentence (e.g. ‘ccccccccccc ccccccc CccCC ccc CpCn’) by using a set of simple rules,"
W16-5102,W04-1213,1,0.916975,"cognition (NER) is one of the first and important stages in a natural language processing (NLP) pipeline. In particular, an NER task is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. In biomedical domain, NER tasks are particularly difficult, since the entities of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieved effective perfor"
W16-5102,W09-1401,0,0.443325,"on Named entity recognition (NER) is one of the first and important stages in a natural language processing (NLP) pipeline. In particular, an NER task is to identify mentions of entities (e.g. persons, locations and organisations) within unstructured text. In biomedical domain, NER tasks are particularly difficult, since the entities of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieve"
W16-5102,N16-1030,0,0.0310513,"l characters in a given word are upper-case), when learning a CRF model for drug name recognition. In this work, we investigate an automatic approach that could automatically induce orthographic features for biomedical named entity recognition. Recently, neural network-based approaches have been effectively used for NER tasks. For example, Collobert et al. (2011) used a feed-forward neural network to effectively identify entities in a newswire corpus (Tjong Kim Sang and De Meulder, 2003) by classifying each word using contexts within a fixed number of surrounding words. Ma and Hovy (2016) and Lample et al. (2016) effectively used both character and word embeddings in a bi-directional LSTM for NER tasks, such as CoNLL03 (Tjong Kim Sang and De Meulder, 2003). Huang et al. (2015) combined hand-crafted features with bi-directional LSTM to further improve the performance. Chiu and Nichols (2016) achieved state-of-the-art performances by modelling both character and word embeddings before combining with hand-crafted features. Nevertheless, the studies of neural network models for biomedical NER tasks are limited. For instance, Chiu et al. (2016) investigated the use of the model of Collobert et al. (2011) w"
W16-5102,D15-1194,1,0.803567,"actic information from word vectors, induced This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 10 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 10–19, Osaka, Japan, December 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment analysis and concept normalisation (Collobert et al., 2011; Turian et al., 2010; Limsopatham and Collier, 2016a; Limsopatham and Collier, 2016b; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) effectively used word embeddings as inputs of a feed-forward neural network for sequence labelling tasks, such as NER and POS tagging. Turian et al. (2010) learned a CRF model using word embeddings as input features for NER and chunking tasks. In the biomedical domain, Chiu et al. (2016) investigated the the use of different word embeddings in a feed-forward neural network for biomedical NER tasks. However, when using with word embedding features, traditional features (e.g. orthography and gazetteers) have shown to further improve the performance of an NE"
W16-5102,W16-2918,1,0.75531,", allow machine learning approaches to exploit semantic and syntactic information from word vectors, induced This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 10 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 10–19, Osaka, Japan, December 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment analysis and concept normalisation (Collobert et al., 2011; Turian et al., 2010; Limsopatham and Collier, 2016a; Limsopatham and Collier, 2016b; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) effectively used word embeddings as inputs of a feed-forward neural network for sequence labelling tasks, such as NER and POS tagging. Turian et al. (2010) learned a CRF model using word embeddings as input features for NER and chunking tasks. In the biomedical domain, Chiu et al. (2016) investigated the the use of different word embeddings in a feed-forward neural network for biomedical NER tasks. However, when using with word embedding features, traditional features (e.g. orthography and g"
W16-5102,P16-1096,1,0.823107,", allow machine learning approaches to exploit semantic and syntactic information from word vectors, induced This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 10 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 10–19, Osaka, Japan, December 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment analysis and concept normalisation (Collobert et al., 2011; Turian et al., 2010; Limsopatham and Collier, 2016a; Limsopatham and Collier, 2016b; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) effectively used word embeddings as inputs of a feed-forward neural network for sequence labelling tasks, such as NER and POS tagging. Turian et al. (2010) learned a CRF model using word embeddings as input features for NER and chunking tasks. In the biomedical domain, Chiu et al. (2016) investigated the the use of different word embeddings in a feed-forward neural network for biomedical NER tasks. However, when using with word embedding features, traditional features (e.g. orthography and g"
W16-5102,D15-1104,0,0.028329,"ularly difficult, since the entities of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieved effective performances for several biomedical NER tasks by learning a CRF model using multiple sets of features, including orthographic, morphological, linguistic-based, conjunctions and dictionary-based. However, these approaches rely heavily on feature engineering and domain knowledge (e.g. gaz"
W16-5102,P16-1101,0,0.01645,"llCaps (i.e. whether all characters in a given word are upper-case), when learning a CRF model for drug name recognition. In this work, we investigate an automatic approach that could automatically induce orthographic features for biomedical named entity recognition. Recently, neural network-based approaches have been effectively used for NER tasks. For example, Collobert et al. (2011) used a feed-forward neural network to effectively identify entities in a newswire corpus (Tjong Kim Sang and De Meulder, 2003) by classifying each word using contexts within a fixed number of surrounding words. Ma and Hovy (2016) and Lample et al. (2016) effectively used both character and word embeddings in a bi-directional LSTM for NER tasks, such as CoNLL03 (Tjong Kim Sang and De Meulder, 2003). Huang et al. (2015) combined hand-crafted features with bi-directional LSTM to further improve the performance. Chiu and Nichols (2016) achieved state-of-the-art performances by modelling both character and word embeddings before combining with hand-crafted features. Nevertheless, the studies of neural network models for biomedical NER tasks are limited. For instance, Chiu et al. (2016) investigated the use of the model of"
W16-5102,W03-0430,0,0.602661,"iomedical domain, NER tasks are particularly difficult, since the entities of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieved effective performances for several biomedical NER tasks by learning a CRF model using multiple sets of features, including orthographic, morphological, linguistic-based, conjunctions and dictionary-based. However, these approaches rely heavily on feature engineeri"
W16-5102,D14-1162,0,0.088271,"pedia. Campos et al. (2013) achieved effective performances for several biomedical NER tasks by learning a CRF model using multiple sets of features, including orthographic, morphological, linguistic-based, conjunctions and dictionary-based. However, these approaches rely heavily on feature engineering and domain knowledge (e.g. gazetteers), which are costly to develop. Consequently, they are difficult to be adapted to a new domain, since hand-engineered features are mostly specific to a target domain. Recent advances in word vector representation (i.e. word embeddings) (Mikolov et al., 2013; Pennington et al., 2014), which represents a word in the form of a low-dimensional vector of real values, allow machine learning approaches to exploit semantic and syntactic information from word vectors, induced This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 10 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 10–19, Osaka, Japan, December 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment a"
W16-5102,W09-1119,0,0.132192,"s of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieved effective performances for several biomedical NER tasks by learning a CRF model using multiple sets of features, including orthographic, morphological, linguistic-based, conjunctions and dictionary-based. However, these approaches rely heavily on feature engineering and domain knowledge (e.g. gazetteers), which are costly to develop. Co"
W16-5102,W15-2608,0,0.14189,"Missing"
W16-5102,W04-1221,0,0.852069,"asks are particularly difficult, since the entities of interests are mainly genes, proteins, and chemical substances, which by nature (1) consist of millions of entities, (2) are created continuously, and (3) are non-standardised and can be referred to using different names (e.g. the use of acronyms and polysemy) (Kim et al., 2009; Kim et al., 2004; Smith et al., 2008a). Traditionally, most of the effective NER approaches are based on machine learning techniques, such as conditional random field (CRF), support vector machine (SVM) and perceptrons (Lafferty et al., 2001; McCallum and Li, 2003; Settles, 2004; Luo et al., 2015; Ju et al., 2011; Ratinov and Roth, 2009; SeguraBedmar et al., 2015). For instance, Ratinov and Roth (2009) effectively learned a perceptron model using features, including word classes induced using Brown clustering (Liang, 2005), and gazetteer extracted from Wikipedia. Campos et al. (2013) achieved effective performances for several biomedical NER tasks by learning a CRF model using multiple sets of features, including orthographic, morphological, linguistic-based, conjunctions and dictionary-based. However, these approaches rely heavily on feature engineering and domain k"
W16-5102,W03-0419,0,0.587646,"Missing"
W16-5102,P10-1040,0,0.0977697,"vector of real values, allow machine learning approaches to exploit semantic and syntactic information from word vectors, induced This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 10 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 10–19, Osaka, Japan, December 12th 2016. from a large dataset, for several NLP tasks, such as NER, part-of-speech (POS) tagging, sentiment analysis and concept normalisation (Collobert et al., 2011; Turian et al., 2010; Limsopatham and Collier, 2016a; Limsopatham and Collier, 2016b; Limsopatham and Collier, 2015). For example, Collobert et al. (2011) effectively used word embeddings as inputs of a feed-forward neural network for sequence labelling tasks, such as NER and POS tagging. Turian et al. (2010) learned a CRF model using word embeddings as input features for NER and chunking tasks. In the biomedical domain, Chiu et al. (2016) investigated the the use of different word embeddings in a feed-forward neural network for biomedical NER tasks. However, when using with word embedding features, traditional f"
W18-5507,aker-etal-2017-simple,0,0.0147379,"cle by attaching a new paragraph with the last updates at the beginning of it. Moreover, putting most newsworthy facts at the beginning of an article allows the impatient readers to quickly decide on their level of interest in the report. After manual analysis of excerpts of the F NC -1 corpus, we concluded that most articles were actuRumor Stance Detection on Tweets. The most commonly used datasets for rumor stance detection, the RumorEval (Derczynski et al., 2017) and the PHEME (Zubiaga et al., 2016b) corpora, collect Tweets. State-of-the-art results on the PHEME corpus has been obtained by Aker et al. (2017), who used a very rich set of problemspecific features. Their model beat the previous state-of-the-art system by Zubiaga et al. (2016a), 42 HEADLINE Sentence 1 w4 Sentence 1 w3 ARTICLE Backward LSTM conditioned on the previous sentence Forward LSTM conditioned on the previous sentence w1 w2 Input features (word embeddings, ...) w3 .... Sentence 2 w2 Sentence 2 ARTICLE w1 LSTM conditioned on the headline LSTM conditioned on the previous sentence Sentence n .... w1 w2 w3 w4 Figure 4: Detail of the forward component of the double-conditional encoding architecture (best seen in color). Dotted arro"
W18-5507,D16-1084,0,0.0356546,"Missing"
W18-5507,N16-1138,0,0.0393041,"peline can be naturally adapted to FND. In recent years, several efforts have been made by the research community toward the automatization of some of these stages, in order to provide effective tools to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). Concerning FND, Pomerleau and Rao (2017) recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge1 (F NC -1). The core of the corpus is constituted by a collection of articles discussing 566 claims, 300 of which come from the E MERGENT dataset (Ferreira and Vlachos, 2016). Each article is summarized in a headline and labeled as agreeing (AGR), disagreeing (DSG) or discussing (DSC) the claim. Additionally, unrelated (UNR) samples were created by pairing headlines with random articles. The goal of the challenge was to classify the pairs constituted by a headline and an article as AGR, DSG , DSC or UNR. Following the pipeline discussed above, it is clear that the F NC -1 actually covers two of the four steps, namely: (1) The tracking step, consisting in filtering out the irrelevant UNR samples; (2) The actual stance detection step, consisting in the classificatio"
W18-5507,P05-1045,0,0.0334323,"standing, without counting on such possibly accidental correlations. In order to avoid the systems to rely on chance correlations, which would not generalize on the test set, we modified the input sequences by substituting all input tokens labeled as <PERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we consider two further input channels: • Named Entities (NE) - NEs were obtained using the Stanford NE Recognizer (Finkel et al., 2005), resulting in a tagset of 13 labels. • Characters - Each input word was split into characters. Only characters occurring more than 100 times in the training set were considered, obtaining a final vocabulary of 149 characters. As in Lample et al. (2016), in we concatenate the output of a BiLSTM run over the character sequence. 5.2.3 Results Results of experiments concatenating the previously mentioned features to the word embedding input to both architectures are reported in Table 4 (even lines). In general, using NE embeddings alone with word embeddings was not beneficial for both models. Con"
W18-5507,C18-1283,0,0.0118432,"stage, where posts concerning the identified rumor are collected; after determining the orientation expressed in each post with respect to the rumor (stance detection), the final truth value of the rumor is obtained by aggregating those single stance judgments (veracity classification). As shown in Figure 1, this pipeline can be naturally adapted to FND. In recent years, several efforts have been made by the research community toward the automatization of some of these stages, in order to provide effective tools to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). Concerning FND, Pomerleau and Rao (2017) recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge1 (F NC -1). The core of the corpus is constituted by a collection of articles discussing 566 claims, 300 of which come from the E MERGENT dataset (Ferreira and Vlachos, 2016). Each article is summarized in a headline and labeled as agreeing (AGR), disagreeing (DSG) or discussing (DSC) the claim. Additionally, unrelated (UNR) samples were created by pairing headlines with random articles. The goal of the challenge was to classify the pairs constituted"
W18-5507,C18-1158,0,0.351489,"ure 1: The rumor verification (RV) pipeline proposed by Zubiaga et al. (2018). The first row describes the corresponding step whereas the second row shows the outputs of each step for both the RV and the fake news detection (FND) tasks. The red rectangle indicates steps covered by the FNC-1 corpus. Figure adapted from Zubiaga et al. (2018). claim, leveraging only word embeddings as input. Note that the amount of semantic understanding needed for the second task is much higher than for the first. In fact, even humans struggle in the related sample classification, as empirically demonstrated by Hanselowski et al. (2018): the interannotator agreement of five human judges drops from Fleiss’ κ of .686 to .218, after filtering out the UNR samples. For this reason, we concentrate on the stance detection step, and we make the following contributions: 2 2.1 Related Work Stance Detection Stance Detection (SD) has been defined as the task of determining the attitude expressed in a short piece of text with respect to a target, usually expressed with one or few words (as Feminism or Climate Change, Mohammad et al. (2016)). In fact, most of the available corpora for SD consider very short samples, as Tweets. SD became v"
W18-5507,P18-2118,0,0.132447,"time, reduce the number of parameters. Double-conditional Encoding. As a first method, we modeled the relationship between the headline and the article using conditional encoding. First, the headline is encoded using a bidirectional LSTM. Then, we separately process each sentence of the article with BiLSTMH , a BiLSTM conditioned on the last states of the BiLSTM which processed the headline. We finally stack BiLSTMA on top of BiLSTMH . In this way, we obtain a matrix H Si ∈ Rl×si for each sentence Si . 43 the final vector representation of the ith sentence Si is obtained as follows: Following Wang et al. (2018), we notate this as: HSi = Bi-LSTMH (ESi ) ∀i ∈ {1, ..., n} (1) uit = tanh(Ws hit + bs ) H Si = Bi-LSTMA (HSi ) ∀i ∈ {1, ..., n} (2) si = H Hi = HH Ai H Si = ReLU(Ws 3.3 (8) Decoding Following the inverted pyramid principles, according to which the most relevant information is concentrated at the beginning of the article, we aggregate the sentence vector representations {s1 , ..., sn } using a backward LSTM. The final prediction yˆ is finally obtained with a softmax operation over the tagset. 4 Experimental Setup 4.1 Data and Preprocessing We downloaded the F NC -1 corpus from the challenge we"
W18-5507,S14-2003,1,0.716238,"taset collecting long documents, we briefly mention some of the most relevant works on SD using Twitter data. 1. We identify asymmetry in length between headlines and articles as a key characteristic of the FNC-1 corpus: on average, an article contains more than 30 times the number of words contained in its associated headline. This is peculiar with respect to most of the commonly used datasets for stance detection (Mohammad et al., 2017) and require the development of architectures specifically tailored to this considerable asymmetry. Following on the terminology introduced by Jurgens et al. (2014) for Semantic Similarity, we propose to handle the problem as a CrossLevel Stance Detection task. To our knowledge, it is the first time that this task is investigated in isolation. 2. Inspired by theoretical principles in the field of Journalism Studies, we propose two simple neural architectures to model the argumentative structure of an article, and its complex interplay with a headline. We demonstrate that our systems can beat a strong feature-based baseline, based on one of the F NC -1 winning architectures, and that they can successfully model the internal structure of a news article and"
W18-5507,P17-2067,0,0.0356218,"hreshold of 4 considered sentences, simulations using forward encoding perform always consistently worse than using backwards encoding (the red violins in Figure 5). Reasonably, below this threshold, we do not observe a considerable difference in performance between backward and forward models. 5.2 5.2.1 5.2.2 Anonymizing the input After manual analysis of the predictions, we suspected that some models could have spotted some correlations between certain Named Entities and a specific stance in the training set. Some of those correlations are well known and can be useful in veracity detection (Wang, 2017). In this paper, however, we wanted to train a model for stance detection only based on its language understanding, without counting on such possibly accidental correlations. In order to avoid the systems to rely on chance correlations, which would not generalize on the test set, we modified the input sequences by substituting all input tokens labeled as <PERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we conside"
W18-5507,C18-1288,0,0.0444741,"Missing"
W18-5507,N16-1174,0,0.0297861,"Missing"
W18-5507,N16-1030,0,0.0214577,"ERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we consider two further input channels: • Named Entities (NE) - NEs were obtained using the Stanford NE Recognizer (Finkel et al., 2005), resulting in a tagset of 13 labels. • Characters - Each input word was split into characters. Only characters occurring more than 100 times in the training set were considered, obtaining a final vocabulary of 149 characters. As in Lample et al. (2016), in we concatenate the output of a BiLSTM run over the character sequence. 5.2.3 Results Results of experiments concatenating the previously mentioned features to the word embedding input to both architectures are reported in Table 4 (even lines). In general, using NE embeddings alone with word embeddings was not beneficial for both models. Considering the architecture based on double-conditional encoding, using both 47 Acknowledgments characters and NE features actually lead to (sometimes small) improvements in almost all considered evaluation metrics. Moving to the architecture using co-mat"
W18-5507,C16-1230,0,0.0152684,"This style is particularly suited for rapidly evolving breaking news event, where a journalist can update an article by attaching a new paragraph with the last updates at the beginning of it. Moreover, putting most newsworthy facts at the beginning of an article allows the impatient readers to quickly decide on their level of interest in the report. After manual analysis of excerpts of the F NC -1 corpus, we concluded that most articles were actuRumor Stance Detection on Tweets. The most commonly used datasets for rumor stance detection, the RumorEval (Derczynski et al., 2017) and the PHEME (Zubiaga et al., 2016b) corpora, collect Tweets. State-of-the-art results on the PHEME corpus has been obtained by Aker et al. (2017), who used a very rich set of problemspecific features. Their model beat the previous state-of-the-art system by Zubiaga et al. (2016a), 42 HEADLINE Sentence 1 w4 Sentence 1 w3 ARTICLE Backward LSTM conditioned on the previous sentence Forward LSTM conditioned on the previous sentence w1 w2 Input features (word embeddings, ...) w3 .... Sentence 2 w2 Sentence 2 ARTICLE w1 LSTM conditioned on the headline LSTM conditioned on the previous sentence Sentence n .... w1 w2 w3 w4 Figure 4: D"
W18-5507,S16-1003,0,0.0899858,"Missing"
