2009.mtsummit-papers.20,D08-1064,0,0.259405,"core? Callison-Burch et al. (2009) used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the WMT09 and found that in general, system combinations performed as well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose"
2009.mtsummit-papers.20,P05-1066,0,0.0871819,"and LCM, the automatic evaluation metrics of CWMT2008 evaluation also include: BLEU, NIST, GTM, mWER, mPER and ICT (a metric developed by the Institute of Computing Technology, CAS). All these metrics are case-sensitive. The evaluation of Chinese translation is based on Chinese characters instead of words. 2.5 Evaluation Results Figures 1-4 show the evaluation results. 3 BLEU-SBP 3.1 BLEU’s Deficiency We encountered the following two practical cases in our evaluation. In this paper, if not specified particularly, all the BLEU means NIST-BLEU. 3.1.1 The Sign Test When we applied the sign test (Collins et al., 2005) for significance testing with BLEU, we encountered such problem (Table 4): when comparing system A and system B, if we select A as the baseline system, we found B is significantly better than A, but if we select B as the baseline system, we found A is significantly better than B. We tried two kinds of BLEU: NIST-BLUE and IBM-BLEU, the results are similar (Table 4). This is because the sign test requires a function (ai,bi) that indicates whether bi is better, worse or same quality translation relative to ai. Because BLEU is not defined on single sentences, Collins et al. (2005) use an approxim"
2009.mtsummit-papers.20,P02-1040,0,0.0823026,"s well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose linguistic checkpoint method (LCM) as another alternative metric with an attempt to detect and report richer linguistic information on the system. Now most MT evaluation methods only generate a general similarity score. At the pre"
2009.mtsummit-papers.20,H93-1040,0,0.212743,"Missing"
2009.mtsummit-papers.20,W09-0401,0,\N,Missing
2009.mtsummit-papers.20,W03-1709,1,\N,Missing
2009.mtsummit-papers.20,C08-1141,1,\N,Missing
2009.mtsummit-papers.20,J03-1002,0,\N,Missing
2011.mtsummit-papers.14,J96-1002,0,0.0526965,"Missing"
2011.mtsummit-papers.14,W09-0436,0,0.0226187,"Missing"
2011.mtsummit-papers.14,J07-2003,0,0.0233923,"reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where all the transla"
2011.mtsummit-papers.14,D09-1024,0,0.0211603,"tion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a s"
2011.mtsummit-papers.14,W04-3250,0,0.0218324,"el data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligned (i.e., aligned to NULL) in word alignment. Table 5 shows their statistical information. For convenience, each setting of TFWIM is de144 Table 5:"
2011.mtsummit-papers.14,N03-1017,0,0.017962,"odel for SMT, which can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase transl"
2011.mtsummit-papers.14,W08-0301,1,0.923541,"everaging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function"
2011.mtsummit-papers.14,D08-1077,0,0.0845194,"t improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntactic-based SMT system that allows structural word insertion and deletion. The effectiveness of these methods motivates us to address the generation of target function words in the phrasebased SMT which is a popular system in both academic and industrial areas. 141 3 Our Method Our method focuses on the processing of target function words, including the deletion and the insertion. The deletion takes place during the model training, where the target function words are removed from the training data before conducting translation modeling. The insertio"
2011.mtsummit-papers.14,P03-1021,0,0.0381847,"Missing"
2011.mtsummit-papers.14,J04-4002,0,0.0445212,"can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where"
2011.mtsummit-papers.14,P02-1040,0,0.0820753,"alized reordering model comes from LDC2003E14, which contains 128K sentence pairs. A 5-gram language model (LM) is trained over the English portion of parallel data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligne"
2011.mtsummit-papers.14,P07-1090,0,0.0177966,"anwhile, the second step is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English tr"
2011.mtsummit-papers.14,P09-1037,0,0.0521647,"p is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang e"
2011.mtsummit-papers.14,J97-3002,0,0.242662,"Missing"
2011.mtsummit-papers.14,P06-1066,0,0.0601275,"Missing"
2011.mtsummit-papers.14,P08-1011,1,0.912537,", 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntact"
2020.acl-main.321,W05-0909,0,0.192678,"-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer (Zhang et al., 2018) +BERT 24.58 24.62"
2020.acl-main.321,N18-1118,0,0.310147,"ncoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this c"
2020.acl-main.321,N19-1423,0,0.0488099,"sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately. Moreover, it cannot be directly adapted to the recent pre-training models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Dong et al., 2019; Song et al., 2019; Lample and Conneau, 2019), which encodes multiple sentences with a single encoder. Different from the dual-encoder structure, the uni-encoder structure takes the concatenation of contexts and source sentences as the input (as shown in Figure 1b). Therefore, when modeling the contexts, it can make full use of the interaction between the source sentences and the contexts, while the dual-encoder model fails to exploit this information. Moreover, the uni-encoder structure is identical to the recent pre-training mode"
2020.acl-main.321,W19-5321,0,0.0606372,"the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully mo"
2020.acl-main.321,D18-1512,0,0.378057,"models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the context sentences and the source sentences. Therefore, we introduce the segment embedding to identify the"
2020.acl-main.321,P18-1118,0,0.155313,"Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is"
2020.acl-main.321,N19-1313,0,0.747779,"velopment of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside ea"
2020.acl-main.321,W18-6307,0,0.0517618,"Missing"
2020.acl-main.321,P02-1040,0,0.106916,"e denote the proposed model as Flat-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer"
2020.acl-main.321,N18-1202,0,0.0428805,"cument-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately. Moreover, it cannot be directly adapted to the recent pre-training models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Dong et al., 2019; Song et al., 2019; Lample and Conneau, 2019), which encodes multiple sentences with a single encoder. Different from the dual-encoder structure, the uni-encoder structure takes the concatenation of contexts and source sentences as the input (as shown in Figure 1b). Therefore, when modeling the contexts, it can make full use of the interaction between the source sentences and the contexts, while the dual-encoder model fails to exploit this information. Moreover, the uni-encoder structure is identical to the recent pre-training models (e.g., 3505 Procee"
2020.acl-main.321,P16-1162,0,0.110385,"l stateof-the-art models on three document-level machine translation benchmarks. We denote the proposed model as Flat-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN ("
2020.acl-main.321,W11-4624,0,0.0203562,"document-level machine translation datasets. Experiments show that it can achieve better performance than the baseline models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the cont"
2020.acl-main.321,W17-4811,0,0.420201,"Encoder Transformer Decoder + Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder"
2020.acl-main.321,D19-1081,0,0.742702,"Missing"
2020.acl-main.321,P19-1116,0,0.540392,"Missing"
2020.acl-main.321,P18-1117,0,0.125667,"ving the architectures of the document machine translation models. Tiedemann and Scherrer (2017) and Wang et al. (2017) explore possible solutions to exploit the crosssentence contexts for neural machine translation. Zhang et al. (2018) extends the Transformer model with a new context encoder to represent documentlevel context. Werlen et al. (2018) and (Maruf et al., 2019) propose two different hierarchical attention models to model the contexts. Yang et al. (2019) introduces a capsule network to improve these hierarchical structures. There are also some works analyzing the contextual errors (Voita et al., 2018, 2019b; Bawden et al., 2018) and providing the test suites (M¨uller et al., 2018). More recently, Voita et al. (2019a) explores the approaches to incorporate the mono-lingual data to augment the document-level bi-lingual dataset. Different from these works, this paper mainly discusses the comparison between dual-encoder models and uniencoder models and proposes a novel method to improve the uni-encoder structure. 5 Conclusions In this work, we explore the solutions to improve the uni-encoder structures for document-level machine translation. We propose a Flat-Transformer model with a unified"
2020.acl-main.321,D17-1301,0,0.43505,"slation Transformer Encoder Transformer Decoder + Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new co"
2020.acl-main.321,D18-1325,0,0.505586,"p learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately."
2020.acl-main.321,D19-1164,0,0.7981,"re BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer (Zhang et al., 2018) +BERT 24.58 24.62 25.19 24.01 23.19 45.48 45.32 45.91 45.30 45.25 25.03 24.84 22.37 22.42 22.06 44.02 44.27 41.88 42.30 42.25 29.58 29.90 29.82 29.93 30.72 46.91 47.11 47.86 48.16 48.62 Uni RNN (Bahdanau et al., 2015) Transformer (Vaswani et al., 2017) Our Flat-Transformer +BERT 19.24 23.28 24.87 26.61 40.81 44.17 47.05 48.53 16.51 22.78 23.55 24.52 36.79 42.19 43.97 45.40 26.26 28.72 30.09 31.99 44.14 46.22 48.56 49.76 Model Table 2: Results on three document-level machine translation benchmarks (“Dual” denotes dual-encoder, while “Uni” means uni-encoder). TE"
2020.acl-main.321,D18-1049,0,0.699692,"models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the context sentences and the source sentences. Therefore, we introduce the segment embedding to identify the"
2020.acl-main.531,P17-2021,0,0.0218203,"oleft (R2L) and left-to-right (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templa"
2020.acl-main.531,P19-1122,0,0.0955751,"outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman"
2020.acl-main.531,Q18-1031,0,0.0459095,"ion Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are soft because no explicit paradigms are inaugurated to build new translation from them, and the target tokens could be modified. In order to effectively use the templates, we introduce soft template-based neural machine translation (ST-NMT), which can use source text and soft templates to predict the final translation. Our approach can be split"
2020.acl-main.531,I17-1013,0,0.0171418,"ces, we adopt the constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding auth"
2020.acl-main.531,P18-1015,0,0.04749,"Missing"
2020.acl-main.531,P07-2045,0,0.0108457,"validation set is devtest2014, and the test set is newstest2014. ASPEC Japanese-Chinese We use 0.67M sentence pairs from ASPEC Japanese-Chinese corpus (Nakazawa et al., 2016) 2 . We use the devtest as the development data, which contains 2090 sentences, and the test data contains 2107 sentences with a single reference per source sentence. 3.2 Preprocessing and Training Details LDC Chinese-English The base Transformer model is used for this task, which includes 6 layers, each layer of which has the hidden dimensions of 512, feedforward dimensions of 2048 , and 8 attention heads. We use Moses (Koehn et al., 2007) to tokenize English sentences and our in-house tool to tokenize Chinese sentences. We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to encode 1 LDC2002E17, LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E17, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T06, LDC2004T08, LDC2005T10 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ sentences using a shared vocabulary of 40K symbols. IWSLT14 German-English We adopt the small setup of the Transformer model. The model has 6 layers with the embedding size of 512, a feedforward size of 1024, and 4 attention he"
2020.acl-main.531,N16-1046,0,0.0389191,"Missing"
2020.acl-main.531,P18-1008,0,0.301326,"as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt the constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 20"
2020.acl-main.531,P14-5010,0,0.00243224,"et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chinese-English and GermanEnglish translation tasks, we use case-insensitive tokenized BLEU scores. For the English-German translation task, we use case-sensitive tokenized BLEU scores for evaluation. All the experiments last for 150 epochs and use Stanford parser to generate templates (Manning et al., 2014). For all translation tasks, we use the checkpoint, which has the best valid performance on the valid set. For different test sets, we adapt the beam size and the length penalty to get better performance. In order to avoid the difference of the tokenizer for Chinese translation result evaluation, we adopt the character-level BLEU for testing. Checkpoint averaging is not used, except notification. 5983 Zh → En MT06 MT03 MT05 MT08 MT12 Avg. ConvS2S (Gehring et al., 2017) GNMT (Wu et al., 2016) 39.98 40.53 42.25 42.88 41.22 42.73 33.43 33.97 32.21 32.55 37.28 38.03 Transformer (our implementation"
2020.acl-main.531,P18-1068,0,0.115432,"t al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituency"
2020.acl-main.531,D17-1090,1,0.803851,"nced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are"
2020.acl-main.531,P16-1078,0,0.0162146,"019b,a) use the right-toleft (R2L) and left-to-right (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source"
2020.acl-main.531,D18-1048,0,0.0726954,"se tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing ba"
2020.acl-main.531,C16-1172,0,0.0577942,"Missing"
2020.acl-main.531,W18-6301,0,0.0136486,"a label smoothing of 0.1. We use BPE to encode sentences with a shared vocabulary of 10K symbols. WMT14 English-German We use the big setting of Transformer (Vaswani et al., 2017), in which both the encoder and the decoder have 6 layers, with the embedding size of 1024, feedforward size of 4096, and 16 attention heads. The dropout rate is fixed as 0.3. We adopt Adam (Kingma and Ba, 2015) optimizer with a learning rate 0.1 of the similar learning rate schedule as Transformer (Vaswani et al., 2017). We set the batch size as 6000 and the update frequency as 16 on 8 GPUs for updating parameters (Ott et al., 2018) to imitate 128 GPUs. The datasets are encoded by BPE with a shared vocabulary (Sennrich et al., 2016) of 40K symbols. ASPEC Japanese-Chinese We use the base setting of Transformer the same to the ChineseEnglish translation task. Following the similar learning rate schedule (Vaswani et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chine"
2020.acl-main.531,P18-1123,0,0.0676718,"19), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are soft because no explicit paradigms are inaugurated to build new translation from them, and the target tokens could be modified. In order to effectively use the templates, we introduce soft template-based neural machine translation (ST-NMT), which can use source text and"
2020.acl-main.531,P02-1040,0,0.10654,"pdating parameters (Ott et al., 2018) to imitate 128 GPUs. The datasets are encoded by BPE with a shared vocabulary (Sennrich et al., 2016) of 40K symbols. ASPEC Japanese-Chinese We use the base setting of Transformer the same to the ChineseEnglish translation task. Following the similar learning rate schedule (Vaswani et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chinese-English and GermanEnglish translation tasks, we use case-insensitive tokenized BLEU scores. For the English-German translation task, we use case-sensitive tokenized BLEU scores for evaluation. All the experiments last for 150 epochs and use Stanford parser to generate templates (Manning et al., 2014). For all translation tasks, we use the checkpoint, which has the best valid performance on the valid set. For different test sets, we adapt the beam size and the length penalty to get better performance. In order to avoid the difference of the tokenizer for Chinese translation re"
2020.acl-main.531,P16-1162,0,0.428682,"-Chinese corpus (Nakazawa et al., 2016) 2 . We use the devtest as the development data, which contains 2090 sentences, and the test data contains 2107 sentences with a single reference per source sentence. 3.2 Preprocessing and Training Details LDC Chinese-English The base Transformer model is used for this task, which includes 6 layers, each layer of which has the hidden dimensions of 512, feedforward dimensions of 2048 , and 8 attention heads. We use Moses (Koehn et al., 2007) to tokenize English sentences and our in-house tool to tokenize Chinese sentences. We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to encode 1 LDC2002E17, LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E17, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T06, LDC2004T08, LDC2005T10 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ sentences using a shared vocabulary of 40K symbols. IWSLT14 German-English We adopt the small setup of the Transformer model. The model has 6 layers with the embedding size of 512, a feedforward size of 1024, and 4 attention heads. In order to prevent overfitting, we use a dropout of 0.3, a l2 weight decay of 10−4 , and a label smoothing of 0.1. We use BPE to enco"
2020.acl-main.531,D19-1633,0,0.0843568,"tes and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pan"
2020.acl-main.531,D18-1037,0,0.0372152,"Missing"
2020.acl-main.531,P19-1207,0,0.347103,"achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of"
2020.acl-main.531,D18-1509,0,0.018984,"e the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templates and source text, we incorporate"
2020.acl-main.531,D18-1356,0,0.0718155,"Missing"
2020.acl-main.531,P17-1065,1,0.691077,"t (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templates and source te"
2020.findings-emnlp.332,N19-1408,0,0.0656674,") or diagnosis codes (Du et al., 2019). The dataset’s shared focus on the COVID-19 pandemic also sets it apart from open-domain datasets and academic paper classification datasets such as IMDB or the arXiv Academic Paper Dataset (AAPD) (Yang et al., 2018) in which no shared topic can be found in most of the documents, and it poses unique challenges for document classification models. We evaluate a number of models on the LitCovid dataset and find that fine-tuning pre-trained language models yields higher performance than traditional machine learning approaches and neural models such as LSTMs (Adhikari et al., 2019b; Kim, 2014; Liu et al., 2017). We also notice that BioBERT (Lee et al., 2019), a BERT model pretrained on the original corpus for BERT plus a large set of PubMed articles, performed slightly better 3715 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3715–3722 c November 16 - 20, 2020. 2020 Association for Computational Linguistics # of Classes # of Articles Avg. sentences Avg. tokens Total # of tokens LitCovid 8 23,038 74 1,399 32,239,601 CORD-19 Test 8 100 109 2,861 286,065 Class Prevention Treatment Diagnosis Mechanism Case Report Transmission Forecasting Gene"
2020.findings-emnlp.332,N19-1423,0,0.0181372,"ted bag-of-words as input. 3.2 Conventional Neural Models Using Hedwig2 , a document classification toolkit, we evaluate the following models: KimCNN (Kim, 2014), XML-CNN (Liu et al., 2017) as well as an unregularized and a regularized LSTM (Adhikari et al., 2019b). We notice that they all perform similarly and slightly better traditional methods. 3.3 Pre-Trained Language Models Using the same Hedwig document classification toolkit, we evaluate the performance of DocBERT (Adhikari et al., 2019a) on this task with a few different pre-trained language models. We fine-tune BERT base, BERT large (Devlin et al., 2019) and BioBERT (Lee et al., 2019), a version of BERT base which was further pre-trained on a collection of PubMed articles. We find all BERT models achieve their best performance with their highest possible sequence length of 512 subwords. Additionally, we fine-tune the pre-trained Longformer (Beltagy et al., 2020) in the same way and find that it performs best when a maximum sequence length of 1024 is used. As in the original Longformer paper, we use global attention on the [CLS] token for document 2 https://github.com/castorini/hedwig classification but find that performance improves by around"
2020.findings-emnlp.332,D14-1181,0,0.0291766,"et al., 2019). The dataset’s shared focus on the COVID-19 pandemic also sets it apart from open-domain datasets and academic paper classification datasets such as IMDB or the arXiv Academic Paper Dataset (AAPD) (Yang et al., 2018) in which no shared topic can be found in most of the documents, and it poses unique challenges for document classification models. We evaluate a number of models on the LitCovid dataset and find that fine-tuning pre-trained language models yields higher performance than traditional machine learning approaches and neural models such as LSTMs (Adhikari et al., 2019b; Kim, 2014; Liu et al., 2017). We also notice that BioBERT (Lee et al., 2019), a BERT model pretrained on the original corpus for BERT plus a large set of PubMed articles, performed slightly better 3715 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3715–3722 c November 16 - 20, 2020. 2020 Association for Computational Linguistics # of Classes # of Articles Avg. sentences Avg. tokens Total # of tokens LitCovid 8 23,038 74 1,399 32,239,601 CORD-19 Test 8 100 109 2,861 286,065 Class Prevention Treatment Diagnosis Mechanism Case Report Transmission Forecasting General Table 1:"
2020.findings-emnlp.332,2020.nlpcovid19-acl.1,0,0.0393853,"Missing"
2020.nlpcovid19-acl.3,N19-1408,0,0.0352734,"Missing"
2020.nlpcovid19-acl.3,N19-1423,0,0.0406422,"Missing"
2020.nlpcovid19-acl.3,D14-1181,0,0.0140029,"Missing"
2020.nlpcovid19-acl.3,2020.nlpcovid19-acl.1,0,0.135415,"Missing"
2020.nlpcovid19-acl.3,C18-1330,0,0.0364603,"Missing"
2021.acl-short.31,N19-1388,0,0.0333583,"nces among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContr"
2021.acl-short.31,N19-1121,0,0.0224927,"Missing"
2021.acl-short.31,C18-1263,0,0.0176046,"an Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directio"
2021.acl-short.31,2020.eamt-1.49,0,0.0187435,"Machine Translation Previous works (Zoph et al., 2016; Firat et al., 2016b; Johnson et al., 2017) have explored different settings of the multilingual neural machine translation (MNMT). Recent studies show that MNMT (Blackwood et al., 2018; Platanios et al., 2018; Gu et al., 2018) helps improve the performance of the lowresource or zero-shot translation. Some researchers Agreement-based Learning Many works try to use the agreement-based method (Liang et al., 2007, 2006; Al-Shedivat and Parikh, 2019) to encourage agreement among different translation orders and directions (Liang et al., 2006; Castilho, 2020; Yang et al., 2020a; Cheng et al., 2016; Zhang et al., 2019). Besides, the agreement-based method is also used to minimize the difference between the representation of source and target sentence (Yang et al., 2019). Our method further explores the approach of the multilingual agreement. 7 Conclusion We propose a novel agreement-based framework to encourage multilingual agreement across different translation directions by the agreement term. Experimental results on the multilingual translation task demonstrate that our method effectively minimizes the gaps among different translation direction"
2021.acl-short.31,N13-1073,0,0.221871,"wstest19 Table 3: The statistics of the training, valid, and test sets on WMT datasets of 10 language pairs. Experiment Setup Multilingual Data 10.00M 10.00M 4.60M 4.80M 1.40M 0.70M 0.50M 0.26M 0.18M 0.08M Valid Baselines and Evaluation We compare our method against the following baselines. Bilingual baseline is trained on each language pair separately. One-to-Many and Manyto-One are trained on the En→X and X→En directions respectively. We collect all English sentences (33M) of the bilingual corpora described above and translate them into other languages sentences. We extract alignment pairs (Dyer et al., 2013) across different languages for our method. One-to-Many + Pseudo and Many-to-One + Pseudo are trained on multilingual data combined with the pseudo data. We average the last 5 checkpoints and employ the beam search strategy with a beam size of 5 for evaluation. The evaluation metric is case-sensitive detokenized sacreBLEU2 (Post, 2018). 1 https://github.com/google/ sentencepiece 2 BLEU+case.mixed+lang.{src}{tgt}+numrefs.1+smooth.exp+tok.13a+version.1.4.14 235 3.3 ▁m We adopt the Transformer big architecture as the backbone model for all our experiments, which has 6 layers with an embedding siz"
2021.acl-short.31,N16-1101,0,0.0163994,". To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based meth"
2021.acl-short.31,D16-1026,0,0.0553303,"Missing"
2021.acl-short.31,N18-1032,0,0.0474607,"Missing"
2021.acl-short.31,2020.emnlp-main.204,0,0.0284964,"multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to enc"
2021.acl-short.31,Q17-1024,0,0.0520573,"Missing"
2021.acl-short.31,D18-2012,0,0.0136573,"use the same training, valid, and test sets as the previous work (Wang et al., 2020) to evaluate multilingual models by parallel data from multiple WMT datasets with various languages, including English (En), French (Fr), Czech (Cs), German (De), Finnish (Fi), Latvian (Lv), Estonian (Et), Romanian (Ro), Hindi (Hi), Turkish (Tr), and Gujarati (Gu). For each language, we concatenate the WMT data of the latest available year and get at most 10M sentences by randomly sampling. Detailed statistics of datasets are listed in Table 3. All sentences in our experiments are tokenized by SentencePiece1 (Kudo and Richardson, 2018). Test newstest13 newstest16 newstest16 newstest16 newsdev17 newsdev18 newsdev16 newsdev14 newstest16 newsdev19 newstest15 newstest18 newstest18 newstest18 newstest17 newstest18 newstest16 newstest14 newstest18 newstest19 Table 3: The statistics of the training, valid, and test sets on WMT datasets of 10 language pairs. Experiment Setup Multilingual Data 10.00M 10.00M 4.60M 4.80M 1.40M 0.70M 0.50M 0.26M 0.18M 0.08M Valid Baselines and Evaluation We compare our method against the following baselines. Bilingual baseline is trained on each language pair separately. One-to-Many and Manyto-One are"
2021.acl-short.31,N06-1014,0,0.3185,"Missing"
2021.acl-short.31,D18-1039,0,0.0355016,"Missing"
2021.acl-short.31,W18-6319,0,0.0120737,"separately. One-to-Many and Manyto-One are trained on the En→X and X→En directions respectively. We collect all English sentences (33M) of the bilingual corpora described above and translate them into other languages sentences. We extract alignment pairs (Dyer et al., 2013) across different languages for our method. One-to-Many + Pseudo and Many-to-One + Pseudo are trained on multilingual data combined with the pseudo data. We average the last 5 checkpoints and employ the beam search strategy with a beam size of 5 for evaluation. The evaluation metric is case-sensitive detokenized sacreBLEU2 (Post, 2018). 1 https://github.com/google/ sentencepiece 2 BLEU+case.mixed+lang.{src}{tgt}+numrefs.1+smooth.exp+tok.13a+version.1.4.14 235 3.3 ▁m We adopt the Transformer big architecture as the backbone model for all our experiments, which has 6 layers with an embedding size of 1024, a dropout of 0.1, the feed-forward network size of 4096, and 16 attention heads. We train multilingual models with Adam (Kingma and Ba, 2015) (β1 = 0.9, β2 = 0.98). The learning rate is set as 5e4 with a warm-up step of 4,000. The models are trained with the label smoothing cross-entropy with a smoothing ratio of 0.1. The ba"
2021.acl-short.31,P19-1297,0,0.0256514,"thod, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignmen"
2021.acl-short.31,N19-1044,0,0.0373592,"Missing"
2021.acl-short.31,2020.acl-main.324,0,0.0247877,"(MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directions and cannot explicitly promote each"
2021.acl-short.31,2020.emnlp-main.75,0,0.123872,"achine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directions and cannot expli"
2021.acl-short.31,P19-1296,0,0.0208365,"MNMT (Blackwood et al., 2018; Platanios et al., 2018; Gu et al., 2018) helps improve the performance of the lowresource or zero-shot translation. Some researchers Agreement-based Learning Many works try to use the agreement-based method (Liang et al., 2007, 2006; Al-Shedivat and Parikh, 2019) to encourage agreement among different translation orders and directions (Liang et al., 2006; Castilho, 2020; Yang et al., 2020a; Cheng et al., 2016; Zhang et al., 2019). Besides, the agreement-based method is also used to minimize the difference between the representation of source and target sentence (Yang et al., 2019). Our method further explores the approach of the multilingual agreement. 7 Conclusion We propose a novel agreement-based framework to encourage multilingual agreement across different translation directions by the agreement term. Experimental results on the multilingual translation task demonstrate that our method effectively minimizes the gaps among different translation directions and significantly outperforms the multilingual baselines. The analytic experiment about the crosslingual representation shows the effectiveness of our multilingual agreement in minimizing the differences among dif"
2021.acl-short.31,2020.acl-main.148,0,0.0223165,"inimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives"
2021.acl-short.31,D16-1163,0,0.024088,"ctiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ ("
2021.emnlp-main.2,N19-1388,0,0.0155195,"directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1"
2021.emnlp-main.2,P17-1176,1,0.597432,"seen languages. SixT significantly outperforms mBART with an average improvement of 7.1 BLEU on zeroshot any-to-English translation across 14 source languages. Furthermore, with much less training computation cost and training data, the SixT model gets better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.1 2 NMT model is directly tested between the unseen language pairs lzi -to-lt in a zero-shot manner. Different from multilingual NMT (Johnson et al., 2017), unsupervised NMT (Lample et al., 2018) or zero-resource NMT through pivoting (Chen et al., 2017, 2018), neither the parallel nor monolingual data in the language lzi is directly accessible in the ZeXT task. The model has to rely on the offthe-shelf MPE to translate from language lzi . The challenge to this task is how to leverage an MPE for machine translation while preserving its crosslingual transferability. In this paper, we utilize XLM-R, which is jointly trained on 100 languages, as the off-the-shelf MPE. The ZeXT task calls for approaches to efficiently build a many-to-one NMT model that can translate from 100 languages supported by XLM-R with parallel dataset of only one language"
2021.emnlp-main.2,2020.acl-main.747,0,0.236641,"th the baselines. SixT gets 18.3 average BLEU and improves over the best baseline by 5.4 average BLEU, showing that SixT successfully learns to translate while preserving the cross-lingual transferability of XLM-R. For all language pairs, SixT obtains better transferring scores. In contrast, vanilla Transformer can hardly transfer and the other baselines do not well transfer to the distant languages. In addition to zero-shot performance, SixT also achieves the best result on De-En test set. Note that the best checkpoint is selected with zero-shot validation set for all methods. Previous work (Conneau et al., 2020; Hu et al., 2020) mainly uses XLM-R for cross-lingual transfer on NLU tasks. The experiments demonstrate that XLM-R can be also utilized for zero-shot neural machine translation if it is fine-tuned properly. We leave the exploration of cross-lingual transfer using XLM-R for other NLG tasks as the future work. Baselines We compare our model with vanilla Transformer and five conventional methods to apply pretrained Transformer encoder on NMT task. The pretrained encoders in these methods are replaced with XLM-R base for fair comparison. • Vanilla Transformer. The encoder is with the same size o"
2021.emnlp-main.2,2020.tacl-1.47,0,0.373676,")) while degrades with Resdrop (see results of (2)→(5) and (4)→(6)). This is expected since Resdrop helps to build a more language-agnostic encoder. Although Resdrop degrades supervised performance, it improves zero-shot translation. The zero-shot performance is related with both supervised performance and model transferability. By either enhancing the supervised performance (with TwoStage and BigDec) or the model transferability (with Resdrop), the overall performance of zero-shot translation can be improved. Analysis Comparison with multilingual NMT In this part, we compare SixT with mBART (Liu et al., 2020), CRISS (Tran et al., 2020) and m2m-100 (Fan et al., 2020) on any-to-English test sets. mBART is a strong pretrained multilingual encoderdecoder based Transformer explicitly designed for NMT. We follow their setting and directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 tra"
2021.emnlp-main.2,D18-1398,1,0.602418,"is the layer normalization. Liu et al. (2021) aim at training a language-agnostic encoder for NMT using parallel corpus from scratch. Compared with them, our method shows that it’s possible to make a pretrained multilingual encoder more language-agnostic by relaxing the position constraint during fine-tuning. Capacity-enhanced decoder Some previous work (Zhu et al., 2020; Yang et al., 2020) incorporates BERT into NMT and configures the decoder size as Vaswani et al. (2017). For example, to train an NMT on Europarl De-En training dataset, the default decoder configuration is Transformer base (Gu et al., 2018; Currey et al., 2020). However, our model relies more on the decoder to learn from the labeled data, as the encoder is mainly responsible for cross-lingual transfer. This is also reflected in our training strategy: at the first stage only the decoder parameters are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention"
2021.emnlp-main.2,N19-4009,0,0.02752,"are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention heads at both the first and second training stages. The improvement brought by the big decoder is not simply because of more model parameters. More Model settings We use the XLM-R base model as the off-the-shelf MPE. The model is implemented on fairseq toolkit (Ott et al., 2019). We set Transformer encoder the same size as the XLMR base model. For the decoder, we use the same hyper-parameter setting as the encoder. We denote model with such configuration as SixT and use this configuration for our NMT models through the paper unless otherwise stated. The encoderdecoder attention modules are randomly initialized. We remove the residual connection at the 11-th (penultimate) encoder layer, which is selected on the validation dataset. For the empirical exploration in Table 1, we use two model configurations. For Strategy (1)–(7) where decoder layers are trained from scrat"
2021.emnlp-main.2,D19-5603,0,0.0395317,"rmer. The encoder is with the same size of XLM-R base, the decoder uses the size of BaseDec. All model parameters are randomly initialized. • +XLM-R fine-tune encoder (Conneau and Lample, 2019). The encoder is initialized with XLM-R. All parameters are trained. • +XLM-R fine-tune all (Conneau and Lample, 2019). All parameters except those of cross attention module are initialized with XLM-R and directly fine-tuned. • +XLM-R as encoder embedding (Zhu et al., 2020). The XLM-R output is leveraged as the encoder input of the NMT. The XLM-R model is fixed during training. • +Recycle XLM-R for NMT (Imamura and Sumita, 2019). The method initializes the encoder with XLM-R and only trains decoder at the first step. Then all are trained at the second step. • XLM-R fused model (Zhu et al., 2020). The XLM-R output is fused into encoder and decoder separately with attention mechanism. The encoder embedding is initialized from XLM-R to facilitate 4.3 Ablation Study We conduct an ablation study with the proposed SixT on the Europarl De-En training set, as shown 5 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.5.0 6 19 We use De-En validation dataset this time. Model De Nl Es It Ro Fi Lv Et Hi Ne Zh Avg. Vanilla"
2021.emnlp-main.2,P18-1007,0,0.0646372,"Missing"
2021.emnlp-main.2,D19-1077,0,0.0708843,"even do not have labeled data. Encoder Decoder Given that MPE has achieved great success in crossbody body lingual NLU tasks, a question worthy of research is how to perform zero-shot cross-lingual transfer Encoder Decoder in the NMT task by embed embed leveraging the MPE. Some work (Zhu et al., 2020; Yang et al., 2020; Weng (2) Training the second stageand Sumita, 2019) explores (1) Training in the first stage et al., in 2020; Imamura 1 Introduction approaches to improve NMT performance by inMultilingual pretrained encoders (MPE) such as corporating monolingual pretrained Transformer mBERT (Wu and Dredze, 2019), XLM (Con- encoder such as BERT (Devlin et al., 2019). Howneau and Lample, 2019), and XLM-R (Conneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cr"
2021.emnlp-main.2,2020.emnlp-main.210,0,0.103365,"nneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cross-lingual transfer of NMT (Liu et al., ence (NLI). These methods jointly train a Trans- 2020; Lin et al., 2020). It is still unclear how to former (Vaswani et al., 2017) encoder to perform conduct cross-lingual transfer for NMT model with ∗ existing multilingual pretrained encoders such as Contribution during internship at Microsoft Research. † Corresponding author. XLM-R. 15 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15–26 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper, we focus on a Zero-shot crosslingual(X) NMT Transfer task (ZeXT, see Figure 1), which aims at translating multiple unseen languages by leveraging a"
2021.emnlp-main.2,2021.acl-long.101,0,0.37879,"her improve the translation performance by jointly fine-tuning all parameters except encoder embedding of the NMT.2 Since the decoder has been well adapted to the encoder at the first stage, we expect the model can be slightly fine-tuned to improve the translation capacity without losing the Position disentangled encoder The representations from XLM-R initialized encoder have a strong positional correspondence to the source sentence. The word order information inside is language-specific and may hinder the cross-lingual transfer from supervised source language to unseen languages. Inspired by Liu et al. (2021), we propose to relax this structural constraint and make the encoder outputs less position- and languagespecific. More specifically, at the second stage, we remove the residual connection after the selfattention sublayer in one of the encoder layers i 2 According to our preliminary experiment, the average BLEU is 0.2 lower when the encoder embedding is also learned at the second stage. Besides, freezing encoder embedding leads to higher computational efficiency. 17 Encoder body Decoder body Encoder body Decoder body Encoder embed Decoder embed Encoder embed Decoder embed (2) Training at the s"
2021.emnlp-main.2,2020.acl-main.148,0,0.0168613,"model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1 24.8 32.7 18.4 25.0"
2021.findings-acl.385,W14-3346,0,0.0205456,"eps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All word alignments in this paper are generated automatically using fastalign (Dyer et al., 2013).5 score(ˆ y |x, y) = λ sim(ˆ y , y) + (1 − λ) cxty(ˆ y , x) 4 where the similarity sim(ˆ y , y) measures how faithful the hypothesis y ˆ is to the original reference y and the complexity cxty(ˆ y , x) captures the relationship between the target sequence y ˆ and source sequence x. The similarity function is the smoothed sentence-level BLEU (Chen and Cherry, 2014) w.r.t the original reference. We use three different complexity functions: 1) FRS, 2) wordalignment score2 that measures complexity on a 1 This is inspired by sequence-level interpolation (Kim and Rush, 2016), but they select hypothesis using BLEU while we use more diverse criteria. We use beam search with k = 32. 2 Sum of the log probabilities of each target word conditioned on its aligned source words given by fast-align. Preliminary: SLKD Helps NAR Learn Word Alignment Our work is motivated by the hypothesis that SLKD helps NAR models learn (implicit) alignment between source and target wo"
2021.findings-acl.385,P11-2031,0,0.0603864,"w/ SLKD LevT w/o SLKD LevT w/ SLKD Conf ECE 63.9 63.7 65.1 66.8 65.9 72.3 74.2 86.5 53.3 71.3 10.34 10.49 21.41 20.26 15.17 Table 3: Average token-level accuracy (Acc), confidence (Conf ), and inference ECE (ECE) of AR and the two NAR models trained with and without SLKD. Table 2: Translation quality on WMT14 De-En. In the bottom two groups, models are trained on distilled data with similar faithfulness (Faith) but varying degree of reordering (FRS) and lexical diversity (LexDiv). ↓ marks significant drops compared to the first row in each group based on the paired bootstrap test at p &lt; 0.05 (Clark et al., 2011). this hypothesis by evaluating the effect of SLKD on two datasets: a) En-De train/dev/test sets from WMT14, and b) a synthetic version of the same task, where word alignment information is embedded by pre-reordering the source words so that they are monotonically aligned with target words (in train/dev/test sets). While SLKD improves BLEU by +2.4 on the original En-De task, it has no benefit on the synthetic task (Table 1). This supports our hypothesis and is consistent with other findings on real data: Ghazvininejad et al. (2019) and Gu et al. (2019) showed that SLKD improves the quality of"
2021.findings-acl.385,N19-1423,0,0.00578692,"EU scores on the original WMT14 En-De and the synthetic reordered version. For each task, we compare LevT models trained on real vs. distilled data. word level, and 3) NMT score3 that measures complexity on a sentence level. 3 Experimental Settings Set-Up We use En-De and De-En datasets from WMT14 (Bojar et al., 2014) with the same preprocessing steps as Gu et al. (2019). We evaluate translation quality with case-sensitive tokenized BLEU,4 using the Moses tokenizer. Models We use two state-of-the-art NAR models: • Mask-Predict (MaskT) (Ghazvininejad et al., 2019) uses a masked language model (Devlin et al., 2019) to generate the target sequence by iteratively masking out and regenerating the subset of tokens that the model is least confident about. • Levenshtein Transformer (LevT) (Gu et al., 2019) generates the target sequence through iterative insertion and deletion steps. All AR and NAR models adopt the base Transformer architecture (Vaswani et al., 2017). We train all models using a batch size of 64, 800 tokens for maximum 300, 000 steps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All wor"
2021.findings-acl.385,N13-1073,0,0.0418738,"ting the subset of tokens that the model is least confident about. • Levenshtein Transformer (LevT) (Gu et al., 2019) generates the target sequence through iterative insertion and deletion steps. All AR and NAR models adopt the base Transformer architecture (Vaswani et al., 2017). We train all models using a batch size of 64, 800 tokens for maximum 300, 000 steps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All word alignments in this paper are generated automatically using fastalign (Dyer et al., 2013).5 score(ˆ y |x, y) = λ sim(ˆ y , y) + (1 − λ) cxty(ˆ y , x) 4 where the similarity sim(ˆ y , y) measures how faithful the hypothesis y ˆ is to the original reference y and the complexity cxty(ˆ y , x) captures the relationship between the target sequence y ˆ and source sequence x. The similarity function is the smoothed sentence-level BLEU (Chen and Cherry, 2014) w.r.t the original reference. We use three different complexity functions: 1) FRS, 2) wordalignment score2 that measures complexity on a 1 This is inspired by sequence-level interpolation (Kim and Rush, 2016), but they select hypothe"
2021.findings-acl.385,D19-1633,0,0.124244,"e different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of all possible target sequences, ˆ is the output from running beam search with and y the teacher model q. ∗ Work done during internship"
2021.findings-acl.385,D16-1139,0,0.127027,"rmer and the Mask-Predict NAR models on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of a"
2021.findings-acl.385,D18-1149,0,0.0128127,"of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of all possible target sequences, ˆ is the output from running beam search with and y the teacher model q. ∗"
2021.findings-acl.385,2020.acl-main.15,0,0.0237838,"Missing"
2021.findings-acl.385,P16-1162,0,0.0344491,"Missing"
2021.findings-acl.385,W11-2102,0,0.0312409,"en source and target and thus improve translation quality. Further analysis shows that knowledge distillation lowers model uncertainty by reducing lexical diversity, which affects the calibration of Mask-Predict and Levenshtein Transformer models in opposite directions. 4392 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4392–4400 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Generating Diverse Distilled References We measure distilled corpus complexity with: • Word Reordering Degree computed by the average fuzzy reordering score (FRS) (Talbot et al., 2011) over all sentence pairs. FRS is an MT evaluation metric introduced to distinguish significant changes in reordering rules of MT systems on syntactically distant language pairs. A higher FRS indicates that the hypothesis is more monotonically aligned to the source. Zhou et al. (2019) show that distilled data has a higher FRS than the real data which may benefit NAR models. • Lexical Diversity which captures the diversity of target word choices given a source word. We compute the lexical diversity LD(d) of the distilled corpus d by averaging the entropy of target words y conditioned on a source"
2021.findings-acl.385,P19-1580,0,0.0166303,"ity while controlling for faithfulness (2nd and 3rd group of rows in Table 2). While the absolute BLEU deltas are small, BLEU decreases significantly as the lexical diversity increases despite reduced degree of reordering. This indicates that increased lexical diversity prevails over the effect of lower degree of reordering in decreasing BLEU scores. 6 Increases Confidence of Source-Target Attention SLKD To better understand how SLKD helps NAR learn the alignment between source and target, we measure how the confidence of the source-target attention changes over decoding iterations. Following Voita et al. (2019), we define the confidence of attention heads as the average of the maximum attention weights over source tokens, where the average is taken over target tokens. Higher confidence scores indicate that the model is more certain about which parts of the source sequence to attend to when predicting the target tokens. As seen in Figure 2, SLKD increases the confidence of source-target attention on both MaskT and LevT. The increase is larger for MaskT than for LevT. For LevT, SLKD increases the attention confidence the most at early decoding iterations. At later iterations, as the model becomes more"
2021.findings-acl.385,2020.acl-main.278,0,0.0156818,"that higher-capacity NAR models require more complex distilled data to achieve better translation quality. They further show that generating distilled references with mixture of experts (Shen et al., 2019) improves NAR translation quality. However, training samples can be complex in different ways, and it remains unclear how different types of data complexity alter the internal working of NAR models and their translation quality. We also anticipate that data complexity may impact the uncertainty and calibration of NAR models – an understudied question, unlike for AR models (Ott et al., 2018; Wang et al., 2020). This paper focuses on two types of data complexity – lexical diversity and degree of word reordering. We expose two state-of-the-art NAR models (Mask-Predict (Ghazvininejad et al., 2019) and Levenshtein Transformer (Gu et al., 2019)) to distilled references of varying complexity on the WMT14 German-English task. Experiments show that decreasing reordering complexity and reducing lexical diversity via distillation both help NAR models learn better alignment between source and target and thus improve translation quality. Further analysis shows that knowledge distillation lowers model uncertain"
2021.findings-emnlp.260,N19-1388,0,0.0326825,"age pair separately. 2) Multilingual baseline: multilingual NMT model trained on ZhJa and En-Ja data for Zh/En→Ja, and all Englishcentric data for En→X. 3) Multilingual + pseudo: multilingual NMT model trained on  the concatena tion of the original parallel data (xls , y lt ) and  la l pseudo data (˜ x , y t ) . 4) Multilingual + pivot: multilingual NMT model with pivot decoding (by first translating the source to the auxiliary language and then translating from the auxiliary to the target language). For all multilingual models, we add the target language tag and temperature-based sampling (Aharoni et al., 2019) with temperature τ = 5. We evaluate translation quality using sacreBLEU (Post, 2018).9 For Japanese, we use MeCab tokenizer before computing BLEU. 3.5 Zh/En→Ja Results As shown in Table 3, Multilingual baseline outperforms Bilingual baseline by 0.4–1.5 BLEU on average, while Multilingual + pivot underperforms Bilingual baseline, as it is prone to translation errors in the pivot sentence. Multilingual + pseudo fails to bring further improvements over Multilingual baseline in either direction: it improves BLEU by 0.5 on En→Ja query test set but degrades performance on science and news test sets"
2021.findings-emnlp.260,W19-5301,0,0.031477,"Missing"
2021.findings-emnlp.260,L18-1144,0,0.0323401,"on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually translated into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time, thus the approach is better suited to the zero-resource setting. More recently, Taitelbaum et al. (2019) shows that translating the source word to auxiliary languages improves word translation. Our work is also related to multi-t"
2021.findings-emnlp.260,P11-2031,0,0.130257,"Missing"
2021.findings-emnlp.260,D19-1146,0,0.016447,"achieves higher coverage scores over its counterpart with single-source inference and Multilingual baseline on both En→Cs and En→De. This confirms our hypothesis that adding an auxiliary language input during inference helps disambiguate word senses. ing a shared encoder-decoder model for many-tomany translation allows translation between unseen language pairs. More advanced techniques to further improve the translation quality include optimizing the parameter sharing strategies (Gu et al., 2018; Sachan and Neubig, 2018) and multi-stage fine-tuning to better improve low-resource translation (Dabre et al., 2019). Although we only focus on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually translated into multiple languages. Most studies assume access to multisource inputs during both"
2021.findings-emnlp.260,P15-1166,0,0.0154575,"ges improves word translation. Our work is also related to multi-task learning for machine translation. Tu et al. (2017) propose multi-task learning with an auxiliary reconstruction objective that reconstructs the source sentence 4 Related Work from decoder hidden states. Niu et al. (2019) Since the recent success of the end-to-end NMT further show that adding a reconstruction objective models (Sutskever et al., 2014; Bahdanau et al., by back-translating the target sentences to the 2015), multilingual NMT has become a promis- source helps low-resource translation. Zhou ing research direction. Dong et al. (2015) propose et al. (2019) propose multi-task training with a to perform one-to-many translation using a dedi- denoising objective to improve the robustness cated decoder for each target language. Firat et al. of NMT models. Wang et al. (2020) show that (2016a) further extend it to support many-to-many multi-task learning with two additional denoising translation using language-specific encoders and tasks on the monolingual data can effectively decoders with a shared attention module. Ha et al. improve translation quality. Our training strategy (2016) and Johnson et al. (2017) show that train- can"
2021.findings-emnlp.260,Q17-1024,0,0.0281268,"ing research direction. Dong et al. (2015) propose et al. (2019) propose multi-task training with a to perform one-to-many translation using a dedi- denoising objective to improve the robustness cated decoder for each target language. Firat et al. of NMT models. Wang et al. (2020) show that (2016a) further extend it to support many-to-many multi-task learning with two additional denoising translation using language-specific encoders and tasks on the monolingual data can effectively decoders with a shared attention module. Ha et al. improve translation quality. Our training strategy (2016) and Johnson et al. (2017) show that train- can also be viewed as multi-task learning as we 12 train our multilingual model on single-source and We assume that the correlation is weak if the absolute correlation score is below 0.4. bi-source inputs jointly. 3036 5 Conclusion We introduced a novel bi-source multilingual translation model that exploits an additional source input from an auxiliary language to improve translation quality. Our model can flexibly perform single-source and bi-source inference, in which it takes both the original source and a synthetic source sentence from an auxiliary language as inputs. Expe"
2021.findings-emnlp.260,N16-1101,0,0.0173065,"the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually translated into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time, thus the approach is better suited to the zero-resource setting. More recently, Taitelbaum et al. (2019) shows that translating the source word to auxiliary languages improves word translation. Our work is also related to multi-task learning for machine translation. Tu et al. (2017) propose multi-task learning with an auxiliary reconstruction objective that reco"
2021.findings-emnlp.260,P07-2045,0,0.00708396,"ng corpora (21.2M) from WMT18 (Bojar et al., 2018), newstest2017 as development set, and newstest2018 as test set. En→X We set the source language set S = {En}, the auxiliary and target language sets A = T = {Fr, Cs, De, Fi, Lv, Et, Ro, Hi, Tr, Gu}. The training data are from the WMT corpus (Bojar et al., 2013, 2014, 2016, 2017, 2018; Barrault et al., 2019).3 We use all the available parallel data except for the WikiTitles released by WMT19. For French and Czech, we randomly sample 10M sentence pairs from the full data. 3.2 Preprocessing Zh/En→Ja We tokenize the English sentences using Moses (Koehn et al., 2007) and segment 3 Data can be downloaded from http://www. statmt.org/wmt13/translation-task.html, http://statmt.org/wmt14/translation-task. html, http://www.statmt.org/wmt16/ translation-task.html, http://www.statmt. org/wmt17/translation-task.html, http: //www.statmt.org/wmt18/translation-task. html, and http://www.statmt.org/wmt19/ translation-task.html En→X We follow the preprocessing steps in Wang et al. (2020): we remove duplicated sentence pairs and the pairs with the same source and target sequences from the training corpora and then tokenize all data using SentencePiece (Kudo and Richards"
2021.findings-emnlp.260,D16-1026,0,0.0374453,"Missing"
2021.findings-emnlp.260,D18-2012,0,0.0203967,"ehn et al., 2007) and segment 3 Data can be downloaded from http://www. statmt.org/wmt13/translation-task.html, http://statmt.org/wmt14/translation-task. html, http://www.statmt.org/wmt16/ translation-task.html, http://www.statmt. org/wmt17/translation-task.html, http: //www.statmt.org/wmt18/translation-task. html, and http://www.statmt.org/wmt19/ translation-task.html En→X We follow the preprocessing steps in Wang et al. (2020): we remove duplicated sentence pairs and the pairs with the same source and target sequences from the training corpora and then tokenize all data using SentencePiece (Kudo and Richardson, 2018) with a shared vocabulary size of 64K tokens. Table 2 shows the training data size after preprocessing and the test set for each language pair. 3.3 Training We use the Transformer models (Vaswani et al., 2017) implemented in fairseq.8 Zh/En→Ja We use the Transformer base architecture with dmodel = 512, dhidden = 2048, nheads = 8, nlayers = 6, and pdropout = 0.1. We apply label smoothing of 0.1. We adopt Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 0.0005, batch size of 48,000 tokens, and 4,000 warm-up updates for maximum 500,000 steps or 50 epochs. We select the best c"
2021.findings-emnlp.260,C16-1133,0,0.0131505,"the translation quality include optimizing the parameter sharing strategies (Gu et al., 2018; Sachan and Neubig, 2018) and multi-stage fine-tuning to better improve low-resource translation (Dabre et al., 2019). Although we only focus on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually translated into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time,"
2021.findings-emnlp.260,E17-2002,0,0.0674425,"Missing"
2021.findings-emnlp.260,N18-1032,0,0.020724,"anato et al., 2019), a word sense disambiguation test suite. Table 6 shows that our model with bi-source inference achieves higher coverage scores over its counterpart with single-source inference and Multilingual baseline on both En→Cs and En→De. This confirms our hypothesis that adding an auxiliary language input during inference helps disambiguate word senses. ing a shared encoder-decoder model for many-tomany translation allows translation between unseen language pairs. More advanced techniques to further improve the translation quality include optimizing the parameter sharing strategies (Gu et al., 2018; Sachan and Neubig, 2018) and multi-stage fine-tuning to better improve low-resource translation (Dabre et al., 2019). Although we only focus on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the sourc"
2021.findings-emnlp.260,N19-4007,0,0.0123947,"ngual baseline by 0.8–1.8 BLEU on En→Ja query and news (out-of-domain) test sets, while achieving on par performance on Zh→Ja and En→Ja science (in-domain) test set. This is probably because English and Japanese are more distant, thus adding a high-quality synthetic Chinese source sentence helps translate the domain-specific English words and phrases that are infrequent in the training data. To better understand the improvements in BLEU, we conduct the following analysis: Our model improves accuracy on lowfrequency words. We compute the target word F1 binned by frequency in the training data (Neubig et al., 2019) on the three out-of-domain test sets. As shown in Figure 3, on En→Ja where our model obtains the largest BLEU improvements, the largest improvements over the baseline models are on lowfrequency words – in the news domain, the largest improvements are on words with frequency between 10 and 50, while in the query domain, it improves more on words with frequency between 50 and 100. It also improves F1 on rare words with frequency below 10, but not as much as for words with frequency above 10. In addition, bi-source 10 All mentions of significance are based on the paired bootstrap test (Clark et"
2021.findings-emnlp.260,W18-2711,0,0.354359,"includes 10 language pairs from WMT. Results show that our method is simple yet effective – it improves English→Japanese translation on out-of-domain test sets and outperforms strong baselines by an average of +1.9 BLEU on the English-to-many translation benchmark. The largest improvements are on low-resource languages, where it brings up to +4.0 BLEU improvements. Further analysis confirms our hypothesis that bi-source inference helps the model disambiguate word senses during translation. 2 Bi-Source Multilingual NMT Inspired by prior work on multi-source translation (Zoph and Knight, 2016; Nishimura et al., 2018), we hypothesize that multilingual translation models can benefit from additional synthetic source sentences that are automatically translated from the original source. 2.1 Model Formally, the model computes the probability of target sentence y lt in language lt given the original source sentences xls from language ls and a synthetic source sentence x ˜la translated from xls into an auxiliary language la (la 6= ls , la 6= lt ) by an MT model: ˜la ) = p(y lt |f (xls , x ˜la ; Θenc ); Θdec ) p(y lt |xls , x (1) Our encoder-decoder model is based on the Transformer architecture (Vaswani et al., 2"
2021.findings-emnlp.260,N19-1043,1,0.834354,"the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time, thus the approach is better suited to the zero-resource setting. More recently, Taitelbaum et al. (2019) shows that translating the source word to auxiliary languages improves word translation. Our work is also related to multi-task learning for machine translation. Tu et al. (2017) propose multi-task learning with an auxiliary reconstruction objective that reconstructs the source sentence 4 Related Work from decoder hidden states. Niu et al. (2019) Since the recent success of the end-to-end NMT further show that adding a reconstruction objective models (Sutskever et al., 2014; Bahdanau et al., by back-translating the target sentences to the 2015), multilingual NMT has become a promis- source helps low-resource translation. Zhou ing research direction. Dong et al. (2015) propose et al. (2019) propose multi-task training with a to perform one-to-many translation using a dedi- denoising objective to improve the robustness cated decoder for each target language. Firat et al. of NMT models. Wang et al. (2020) show that (2016a) further extend"
2021.findings-emnlp.260,2001.mtsummit-papers.46,0,0.234312,"ore advanced techniques to further improve the translation quality include optimizing the parameter sharing strategies (Gu et al., 2018; Sachan and Neubig, 2018) and multi-stage fine-tuning to better improve low-resource translation (Dabre et al., 2019). Although we only focus on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually translated into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the"
2021.findings-emnlp.260,W18-6319,0,0.0145922,"data for Zh/En→Ja, and all Englishcentric data for En→X. 3) Multilingual + pseudo: multilingual NMT model trained on  the concatena tion of the original parallel data (xls , y lt ) and  la l pseudo data (˜ x , y t ) . 4) Multilingual + pivot: multilingual NMT model with pivot decoding (by first translating the source to the auxiliary language and then translating from the auxiliary to the target language). For all multilingual models, we add the target language tag and temperature-based sampling (Aharoni et al., 2019) with temperature τ = 5. We evaluate translation quality using sacreBLEU (Post, 2018).9 For Japanese, we use MeCab tokenizer before computing BLEU. 3.5 Zh/En→Ja Results As shown in Table 3, Multilingual baseline outperforms Bilingual baseline by 0.4–1.5 BLEU on average, while Multilingual + pivot underperforms Bilingual baseline, as it is prone to translation errors in the pivot sentence. Multilingual + pseudo fails to bring further improvements over Multilingual baseline in either direction: it improves BLEU by 0.5 on En→Ja query test set but degrades performance on science and news test sets. By contrast, our single-encoder bi-source model using single-source inference signi"
2021.findings-emnlp.260,W19-5354,0,0.0221861,"ed into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time, thus the approach is better suited to the zero-resource setting. More recently, Taitelbaum et al. (2019) shows that translating the source word to auxiliary languages improves word translation. Our work is also related to multi-task learning for machine translation. Tu et al. (2017) propose multi-task learning with an auxiliary reconstruction objective that reconstructs the source sentence 4 Related Work from decoder hidden states. Niu et al. (2019) Since the recent success of the end-to-end NMT further show that adding a reconstruction objective models (Sutskever et al., 2014; Bahdanau et al., by back-translating the target sentences to the 2015), multilingual NMT has become a promis- source he"
2021.findings-emnlp.260,W18-6327,0,0.0174574,"9), a word sense disambiguation test suite. Table 6 shows that our model with bi-source inference achieves higher coverage scores over its counterpart with single-source inference and Multilingual baseline on both En→Cs and En→De. This confirms our hypothesis that adding an auxiliary language input during inference helps disambiguate word senses. ing a shared encoder-decoder model for many-tomany translation allows translation between unseen language pairs. More advanced techniques to further improve the translation quality include optimizing the parameter sharing strategies (Gu et al., 2018; Sachan and Neubig, 2018) and multi-stage fine-tuning to better improve low-resource translation (Dabre et al., 2019). Although we only focus on improving the overall translation quality of a shared multilingual NMT model in this paper, our approach can also be combined with the aforementioned techniques to build better language-specific NMT models via fine-tuning, which we will explore in future work. Orthogonal to these techniques, multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016) has been shown to improve translation quality by exploiting the source sentences manually trans"
2021.findings-emnlp.260,P16-1009,0,0.0916872,"ask ) log p(y lt |xls , x ˜ la ) At each training iteration, we randomly pick a triplet of mutually distinct source, auxiliary, and target languages (ls , la , lt ). Next, we randomly sam ple a batch of training examples (xls , x ˜la , y lt ) from Dls ×la ×lt and maximize the log probability of the target sentence y lt given source sentence xls and auxiliary sentence x ˜la . To enable more flexible decoding and to improve model robustness, we randomly mask out the auxiliary sentences with probability pmask during training.2 Creating Pseudo Training Data We adopt data augmentation techniques (Sennrich et al., 2016a; Nishimura et al., 2018) to construct the bi-source data using parallel data from multiple language pairs. More specifically, we first train a multilingual NMT model MS→A to translate between source and auxiliary languages. Next, we extend  each parallel dataset (xls , y lt ) to pseudo bi source datasets (xls , x ˜la , y lt ) by translating xls into auxiliary languages la using MS→A . Finally, we combine all pseudo bi-source datasets into the training data D to train our bi-source model. 2.3 Inference We set pmask = 0.5 in all our experiments. Zh-En Prov. #Sent train dev test Science Scien"
2021.findings-emnlp.260,P16-1162,0,0.296328,"ask ) log p(y lt |xls , x ˜ la ) At each training iteration, we randomly pick a triplet of mutually distinct source, auxiliary, and target languages (ls , la , lt ). Next, we randomly sam ple a batch of training examples (xls , x ˜la , y lt ) from Dls ×la ×lt and maximize the log probability of the target sentence y lt given source sentence xls and auxiliary sentence x ˜la . To enable more flexible decoding and to improve model robustness, we randomly mask out the auxiliary sentences with probability pmask during training.2 Creating Pseudo Training Data We adopt data augmentation techniques (Sennrich et al., 2016a; Nishimura et al., 2018) to construct the bi-source data using parallel data from multiple language pairs. More specifically, we first train a multilingual NMT model MS→A to translate between source and auxiliary languages. Next, we extend  each parallel dataset (xls , y lt ) to pseudo bi source datasets (xls , x ˜la , y lt ) by translating xls into auxiliary languages la using MS→A . Finally, we combine all pseudo bi-source datasets into the training data D to train our bi-source model. 2.3 Inference We set pmask = 0.5 in all our experiments. Zh-En Prov. #Sent train dev test Science Scien"
2021.findings-emnlp.260,D19-1134,0,0.0227545,"manually translated into multiple languages. Most studies assume access to multisource inputs during both training and inference. Choi et al. (2018) and Nishimura et al. (2018) introduce data augmentation methods to fill in the missing source in the training data. Firat et al. (2016b) explore translating the source into a pivot language and feeding both the original source and pivot sentences to a multilingual model to improve zero-resource translation. However, the pivot sentence is added only at inference time, thus the approach is better suited to the zero-resource setting. More recently, Taitelbaum et al. (2019) shows that translating the source word to auxiliary languages improves word translation. Our work is also related to multi-task learning for machine translation. Tu et al. (2017) propose multi-task learning with an auxiliary reconstruction objective that reconstructs the source sentence 4 Related Work from decoder hidden states. Niu et al. (2019) Since the recent success of the end-to-end NMT further show that adding a reconstruction objective models (Sutskever et al., 2014; Bahdanau et al., by back-translating the target sentences to the 2015), multilingual NMT has become a promis- source he"
2021.findings-emnlp.260,W17-4811,0,0.0164852,"al., 2018) to obtain the hidden representations f (xls ; Θenc ) = H s and f (˜ xla ; Θenc ) = H a . Then, the decoder can attend to H s and H a separately and apply a gating mechanism to obtain the fusion vector hi : hsi = Attn(H s , htgt i ) hai = Attn(H a , htgt i ) g i = σ(W g [hsi ; hai ] + bg ) (2) hi = g i hsi + (1 − g i ) hai where htgt i represents the hidden state of the i-th target token, Wg and bg are model parameters, and σ represents the logistic sigmoid function. Single-Encoder Approach encodes the source sentences by concatenating them into a long sequence (Dabre et al., 2017; Tiedemann and Scherrer, 2017), which is then fed to an embedding layer1 and a stack of self-attention and position-wise feedforward layers to produce a sequence of hidden representations f ([xls ; x ˜la ]; Θenc ) = H. Then, we apply the encoder-decoder attention to the full sequence of encoder representations H: hi = Attn(H, htgt i ) (3) The single-encoder approach is simpler than the multi-encoder one and can be easily adapted to multiple auxiliary languages as inputs. 2.2 Training Our bi-source multilingual model is trained on a combination of datasets D = S ls ×la ×lt , where S is the set ls ∈S,la ∈A,lt ∈T D of source"
2021.findings-emnlp.260,P18-1117,0,0.021023,"tic source sentences that are automatically translated from the original source. 2.1 Model Formally, the model computes the probability of target sentence y lt in language lt given the original source sentences xls from language ls and a synthetic source sentence x ˜la translated from xls into an auxiliary language la (la 6= ls , la 6= lt ) by an MT model: ˜la ) = p(y lt |f (xls , x ˜la ; Θenc ); Θdec ) p(y lt |xls , x (1) Our encoder-decoder model is based on the Transformer architecture (Vaswani et al., 2017). As shown in Figure 2, we adopt techniques from context-aware machine translation (Voita et al., 2018) to integrate the additional source input into the model: Multi-Encoder Approach encodes the source sentences using separate encoders (Voita et al., 2018) to obtain the hidden representations f (xls ; Θenc ) = H s and f (˜ xla ; Θenc ) = H a . Then, the decoder can attend to H s and H a separately and apply a gating mechanism to obtain the fusion vector hi : hsi = Attn(H s , htgt i ) hai = Attn(H a , htgt i ) g i = σ(W g [hsi ; hai ] + bg ) (2) hi = g i hsi + (1 − g i ) hai where htgt i represents the hidden state of the i-th target token, Wg and bg are model parameters, and σ represents the l"
2021.findings-emnlp.260,2020.emnlp-main.75,0,0.137532,"WikiTitles released by WMT19. For French and Czech, we randomly sample 10M sentence pairs from the full data. 3.2 Preprocessing Zh/En→Ja We tokenize the English sentences using Moses (Koehn et al., 2007) and segment 3 Data can be downloaded from http://www. statmt.org/wmt13/translation-task.html, http://statmt.org/wmt14/translation-task. html, http://www.statmt.org/wmt16/ translation-task.html, http://www.statmt. org/wmt17/translation-task.html, http: //www.statmt.org/wmt18/translation-task. html, and http://www.statmt.org/wmt19/ translation-task.html En→X We follow the preprocessing steps in Wang et al. (2020): we remove duplicated sentence pairs and the pairs with the same source and target sequences from the training corpora and then tokenize all data using SentencePiece (Kudo and Richardson, 2018) with a shared vocabulary size of 64K tokens. Table 2 shows the training data size after preprocessing and the test set for each language pair. 3.3 Training We use the Transformer models (Vaswani et al., 2017) implemented in fairseq.8 Zh/En→Ja We use the Transformer base architecture with dmodel = 512, dhidden = 2048, nheads = 8, nlayers = 6, and pdropout = 0.1. We apply label smoothing of 0.1. We adopt"
2021.findings-emnlp.260,W19-5368,0,0.0447308,"Missing"
2021.findings-emnlp.260,N16-1004,0,0.259132,"nslation benchmark that includes 10 language pairs from WMT. Results show that our method is simple yet effective – it improves English→Japanese translation on out-of-domain test sets and outperforms strong baselines by an average of +1.9 BLEU on the English-to-many translation benchmark. The largest improvements are on low-resource languages, where it brings up to +4.0 BLEU improvements. Further analysis confirms our hypothesis that bi-source inference helps the model disambiguate word senses during translation. 2 Bi-Source Multilingual NMT Inspired by prior work on multi-source translation (Zoph and Knight, 2016; Nishimura et al., 2018), we hypothesize that multilingual translation models can benefit from additional synthetic source sentences that are automatically translated from the original source. 2.1 Model Formally, the model computes the probability of target sentence y lt in language lt given the original source sentences xls from language ls and a synthetic source sentence x ˜la translated from xls into an auxiliary language la (la 6= ls , la 6= lt ) by an MT model: ˜la ) = p(y lt |f (xls , x ˜la ; Θenc ); Θdec ) p(y lt |xls , x (1) Our encoder-decoder model is based on the Transformer archit"
2021.naacl-main.312,R13-1027,0,0.0105799,"tence X of length m and target sentence Y of length n, we can construct n intermediate sentences Zk = (yn−k+1 , . . . , yn , [m], y1 , . . . , yn−k )(k ∈ [1, n]). Because the target sentence length n can be too long, we randomly sample S intermediate sentences from n intermediate sentences to construct the subset SY , where S is the number of sampled start positions. We apply scores calculated by the hard or soft Smart-Start methods to the loss of different intermediate samples to teach model which start position is better. This procedure can be described by the weighted log-likelihood (WML) (Dimitroff et al., 2013) reward function L over the dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the"
2021.naacl-main.312,Q19-1042,0,0.033751,"Missing"
2021.naacl-main.312,W18-6301,0,0.0135741,"4000 warming-up steps. We set the number of sampled start positions S = 8 described as Equation 2. For the LDC Zh→En translation task, we use the Transformer_base setting with the embedding size as 512 and feed-forward network (FFN) size as 2048. For the IWSLT14 De→En translation task, we use the Transformer_small setting with embedding size as 512 and FFN size as 1024. The dropout is set as 0.3 and weight decay as 0.0001 to prevent overﬁtting. For the WMT14 En→De translation task, we use the Transformer_big setting with embedding size as 1024 and FFN size as 4096. Following the previos work (Ott et al., 2018), we accumulate the gradient for 16 iterations to simulate a 128-GPU environment. 3.2 3.3 In this section, we evaluate our method on three popular benchmarks. 3.1 Dataset Training Details Baselines and Results We conduct experiments on 8 NVIDIA 32G V100 We compare our method with the other baseGPUs and set batch size as 1024 tokens. In lines, including Transformer (Vaswani et al., 2017), RP Transformer (Shaw et al., 2018), Lightthe training stage, we adopt the Adam optimizer 3984 Zh → En MT06 MT02 MT03 MT05 MT08 MT12 Avg LightConv (Wu et al., 2019) DynamicConv (Wu et al., 2019) 43.41 43.65 42."
2021.naacl-main.312,P02-1040,0,0.109436,"Missing"
2021.naacl-main.312,W16-2323,0,0.114321,"he dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the traditional L2R and R2L (Sennrich et al., 2016a), our Smart-Start method wk = δLk ≥Lmed (3) predicts median word yn−k+1 over the source sentence. Furthermore, we predict the right part of tarwhere δLk ≥Lmed equals to 1 if Lk ≥ Lmed else 0. get sentence (yn−k+1 , . . . , yn ) sequentially which Lmed is the median loss of the sample in SY . For is on the right part of this word. Finally, we gener- each intermediate sentence Zk ∈ SY , the objective ate the rest words (y1 , . . . , yn−k ) on the left part of of Zk is denoted as Lk = log Pθ (Zk |X). 3983 Final Translation ܻ : ݕଵ ݕ୬ି୩ାଵ 濷 濷 ݕ୬ି୩ Left-to-Right ܼ ՜ ܻ Intermediate Translati"
2021.naacl-main.312,P16-1162,0,0.430013,"he dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the traditional L2R and R2L (Sennrich et al., 2016a), our Smart-Start method wk = δLk ≥Lmed (3) predicts median word yn−k+1 over the source sentence. Furthermore, we predict the right part of tarwhere δLk ≥Lmed equals to 1 if Lk ≥ Lmed else 0. get sentence (yn−k+1 , . . . , yn ) sequentially which Lmed is the median loss of the sample in SY . For is on the right part of this word. Finally, we gener- each intermediate sentence Zk ∈ SY , the objective ate the rest words (y1 , . . . , yn−k ) on the left part of of Zk is denoted as Lk = log Pθ (Zk |X). 3983 Final Translation ܻ : ݕଵ ݕ୬ି୩ାଵ 濷 濷 ݕ୬ି୩ Left-to-Right ܼ ՜ ܻ Intermediate Translati"
2021.naacl-main.312,N18-2074,0,0.0279932,"Missing"
2021.naacl-main.312,P18-2054,0,0.0189744,"models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), image caption (Mehri and Sigal, 2018), and dependency parsing (Kiperwasser and Goldberg, 2016; Li et al., 2019). Similarly, insertion-based method (Gu et al., 2019; Stern et al., 2019) predicts the next token and its position to be inserted. 5 Conclusion In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. Our method predicts a median word and then generates the words on the right part. Finally, it generates words on the left. Experimental results show that our Smart-Start me"
2021.naacl-main.312,Q16-1032,0,0.0174688,"al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), image caption (Mehri and Sigal, 2018), and dependency parsing (Kiperwasser and Goldberg, 2016; Li et al., 2019). Similarly, insertion-based method (Gu et al., 2019; Stern et al., 2019) predicts the next token and its position to be inserted. 5 Conclusion In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. Our method predicts a median word and then generates the words on the right part. Finally, it generates words on the left. Experimental results show that our Smart-Start method signiﬁcantly improves the quality of translation. Training Time The Transformer baseline costs Acknowledgments nearly 0.9 hours and our"
2021.naacl-main.312,P07-2045,0,0.00927966,"riments Number of Sampled Start Positions Figure 3: Results of different values of the number of sampled start positions on IWSLT14 De→En test set. IWSLT14 De-En corpus contains 16K training sequence pairs. The valid and test set both contain 7K sentence pairs. LDC Zh-En corpus is from the LDC corpus. The training data contains 1.4M sentence pairs. NIST 2006 is used as the valid set. NIST 2002, 2003, 2005, 2008, and 2012 are used as test sets. WMT14 En-De corpus has 4.5M sentence pairs. The newstest2013 and the newstest2014 are used as valid the test set. All languages are tokenized by Moses (Koehn et al., 2007) and our Chinese tokenizer, and then encoded using byte pair encoding (BPE) (Sennrich et al., 2016b) with 40K merge operations. The evaluation metric is BLEU (Papineni et al., 2002). (β1 = 0.9, β2 = 0.98) (Kingma and Ba, 2015) using the inverse sqrt learning rate schedule (Vaswani et al., 2017) with a learning rate of 0.1 and 4000 warming-up steps. We set the number of sampled start positions S = 8 described as Equation 2. For the LDC Zh→En translation task, we use the Transformer_base setting with the embedding size as 512 and feed-forward network (FFN) size as 2048. For the IWSLT14 De→En tra"
2021.naacl-main.312,D15-1166,0,0.0539853,"affecting the training time is the number of sampled start positions. We also investigate the proper value of the number of sampled start positions. In practice, smaller value such as 4 or 6 can also bring signiﬁcant improvements. Therefore, we choose a smaller value of the sampled start positions and use multiple GPUs to keep the training time in a reasonable range. 4 Related Work Neural Machine Translation (NMT) has attracted a lot of attention recently. The architecture of NMT models has evolved quickly so that there are many different models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), im"
2021.naacl-main.312,W19-3620,0,0.0789923,"ersonality 晝ⴷ ⷿ㘖 . very 濿濼瀉濸濿瀌 Happy to talk with people , Yang Sen has a lively personality (a) Left-to-Right Translation: Le talks with people , Yang Sen is very lively . Smart-Start: Yang Sen has a lively personality . [m] Chatting with people , (b) Translation: Chatting with people , Yang Sen has a lively personality . Figure 1: Example of baseline method (a) and our Smart-Start method (b). “[m]” is designed to indicate the termination of the right part generation. “[m]” is an abbreviation of “[middle]”. There are some related works on non-monotonic text generation (Mehri and Sigal, 2018; Welleck et al., 2019; Gu et al., 2019; Zhou et al., 2019b,a). 1 Introduction Inspired by these works, we are extremely interNeural machine translation (NMT) has made re- ested in considering choosing one proper position to start decoding instead of L2R or R2L order. We markable progress in recent years. There has been much progress in encoder-decoder framework, in- propose a novel method called the Smart-Start decluding recurrent neural models (Wu et al., 2016), coding method. Speciﬁcally, our method starts the generation of target words from the right part of convolutional models (Gehring et al., 2017) and self-"
C08-1141,E06-1032,0,0.273045,"phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems. 1 Introduction Automatic MT evaluation is a crucial issue for MT system developers. The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Papineni et al., 2002) and its variants. Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation. Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006). For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice. Furthermore, BL"
C08-1141,A00-2019,0,0.0155641,"Missing"
C08-1141,W04-3250,0,0.149378,"Missing"
C08-1141,P04-1077,0,0.0226721,"uding 3,200 pairs of sentences containing 6 classes of check-points. Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically. Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching. There are many recent work motivated by ngram based approach. (Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level. (Lin and Och, 2004) used longest common subsequence and skipbigram statistics. (Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses. (Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment. (Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings. (Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greate"
C08-1141,W05-0904,0,0.0285694,"Missing"
C08-1141,J03-1002,0,0.00842467,"Missing"
C08-1141,P02-1040,0,0.0750984,"Missing"
C08-1141,W00-1212,1,0.735942,"Missing"
C08-1141,P03-1054,0,0.00524886,"Missing"
C08-1141,W07-0738,0,\N,Missing
C08-1141,W05-0909,0,\N,Missing
C10-1036,J07-2003,0,0.117997,"Missing"
C10-1036,P09-1107,0,0.113563,"Missing"
C10-1036,P08-1023,0,0.055569,"Missing"
C10-1036,W05-1506,0,0.103769,"Missing"
C10-1036,P08-1067,0,0.0669287,"Missing"
C10-1036,W04-3250,0,0.0749687,"Missing"
C10-1036,N04-1022,0,0.722962,"Missing"
C10-1036,P09-1019,0,0.462334,"Missing"
C10-1036,P03-1021,0,0.202425,"Missing"
C10-1036,J04-4002,0,0.151503,"Missing"
C10-1036,P07-1040,0,0.0636909,"Missing"
C10-1036,D08-1065,0,0.787903,"n model’s distribution. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Their work has shown that MBR decoding performs better than Maximum a Posteriori (MAP) decoding for different evaluation criteria. After that, many dedi1 This work has been done while the author was visiting Microsoft Research Asia. Mu Li, Dongdong Zhang, Ming Zhou Microsoft Research Asia muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com cated efforts have been made to improve the performances of SMT systems by utilizing MBRinspired methods. Tromble et al. (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al. (2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al.’s work. DeNero et al. (2009) proposed a fast consensus decoding algorithm for MBR for both linear and non-linear similarity measures. All work mentioned above share a common setting: an MBR decoder is built based on one and only one MAP decoder. On the other hand, recent researc"
C10-1036,W02-1021,0,0.0878161,"Missing"
C10-1036,P09-1065,0,0.171644,"Missing"
C10-1036,D08-1022,0,\N,Missing
C10-1036,D07-1105,0,\N,Missing
C10-1036,P09-1066,1,\N,Missing
C10-1036,P06-1066,0,\N,Missing
C10-1036,P09-1064,0,\N,Missing
C10-1036,J97-3002,0,\N,Missing
C10-1036,2008.amta-srw.3,0,\N,Missing
C10-1075,P05-1033,0,0.132848,"llel corpora available to the constrained track of NIST 2008 Chinese-English MT evaluation task were used for translation model training, which consist of around 5.1M bilingual sentence pairs. GIZA++ was used for word alignment in both directions, which was further refined with the intersec-diaggrow heuristics. We used a 5-gram language model which was trained from the Xinhua portion of English Gigaword corpus version 3.0 from LDC and the English part of parallel corpora. 4.2 Machine Translation System We used an in-house implementation of the hierarchical phrase-based decoder as described in Chiang (2005). In addtion to the standard features used in Chiang (2005), we also used a lexicon feature indicating how many word paris in the translation found in a conventional Chinese-English lexicon. Phrasal rules were extracted from all the parallel data, but hierarchical rules were only extracted from the FBIS part of the parallel data which contains around 128,000 sentence pairs. For all the development data, feature weights of the decoder were tuned using the MERT algorithm (Och, 2003). 4.3 Results of Development Data Pre-construction In the following we first present some overall results using the"
C10-1075,C04-1059,0,0.0924552,"olingual and bilingual) to the existing training corpora has been shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes"
C10-1075,W07-0717,0,0.0553139,"n shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possi"
C10-1075,N04-1035,0,0.0759513,"Missing"
C10-1075,N03-1017,0,0.00907053,"Missing"
C10-1075,2007.mtsummit-tutorials.1,0,0.030423,"Missing"
C10-1075,D07-1036,0,0.303405,"Missing"
C10-1075,D09-1074,0,0.0531116,"ctive in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possible to make further impro"
C10-1075,P02-1038,0,0.0329115,"l methods that use a fixed model parameter setting across different data sets. 1 e∗ = argmax Pr(e|f ) e and the posterior probability distribution Pr(e|f ) is directly approximated by a log-linear formulation: Pr(e|f ) = pλ (e|f ) P exp( M m=1 λm hm (e, f )) =P PM 0 e0 exp( m=1 λm hm (e , f )) (1) in which hm ’s are feature functions and λ = (λ1 , . . . , λM ) are model parameters (feature weights). For a successful practical log-linear SMT model, it is usually a combined result of the several efforts: • Construction of well-motivated SMT models Introduction In recent years, log-linear model (Och and Ney, 2002) has been a mainstream method to formulate statistical models for machine translation. Using this formulation, various kinds of relevant properties and data statistics used in the translation process, either on the monolingual-side or on the bilingual-side, are encoded and used as realvalued feature functions, thus it provides an effective mathematical framework to accommodate a large variety of SMT formalisms with different computational linguistic motivations. ∗ This work was done while the author was visiting Microsoft Research Asia. • Accurate estimation of feature functions • Appropriate"
C10-1075,P03-1021,0,0.403672,"such λ∗ that is optimal for multiple data sets at the same time. Table 1 shows some empirical evidences when two data sets are mutually used as development and test data. In this setting, we used a hierarchical phrase based decoder and 2 years’ evaluation data of NIST Chineseto-English machine translation task (for the year 2008 only the newswire subset was used because we want to limit both data sets within the same domain to show that data mismatch also exists even if there is no domain difference), and report results using BLEU scores. Model parameters were tuned using the MERT algorithm (Och, 2003) optimized for BLEU metric. Dev data MT05 MT08-nw MT05 0.402 0.372 MT08-nw 0.306 0.343 Table 1: Translation performance of cross development/test on two NIST evaluation data sets. In our work, we present a solution to this problem by using test data dependent model parameters for test data translation. As discussed above, since model parameters are solely determined by development data D, selection of log-linear model parameters is basically equivalent to selecting a set of development data D. However, automatic development data selection in current SMT research remains a relatively open issue"
C10-1075,P07-1004,0,0.0590762,"e task. To our knowledge, there is no dedicated discussion on principled methods to perform development data selection in previous research. In L¨u et al. (2007), log-linear model parameters can also be adjusted at decoding time. But in their approach, the adjustment was based on heuristic rules and re-weighted training data distribution. In addition, compared with training data selection, the computational cost of development data selection is much smaller. From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al., 2007). But our methods do not rely on surface similarities between training and training/development sentences, and development/test sentences are not used to re-train SMT sub-models. 6 Conclusions and Future Work In this paper, we addressed the data mismatch issue between training and decoding time of loglinear SMT models, and presented principled methods for dynamically inferring test data dependent model parameters with development set selection. We describe two algorithms for this task, development set pre-construction and dynamic construction, and evaluated our method on the NIST data sets for"
C10-1075,P06-1077,0,\N,Missing
C10-1075,2005.eamt-1.19,0,\N,Missing
C10-2025,C08-1005,0,0.0879333,"rne, 2004) decoding over n-best list finds a translation that has lowest expected loss with all the other hypotheses, and it shows that improvement over the Maximum a Posteriori (MAP) decoding. Several word-based methods (Rosti et al., 2007a; Sim et al., 2007) have also been proposed. Usually, these methods take n-best list from different SMT systems as inputs, and construct a confusion network for second-pass decoding. There are also a lot of research work to advance the confusion network construction by finding better alignment between the skeleton and the other hypotheses (He et al., 2008; Ayan et al., 2008). Typically, all the approaches above only use full hypotheses but have no access to the PHS information. In this paper, we present hybrid decoding — a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems. In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses. Experimental results on"
C10-2025,P06-1121,0,0.0535726,"es word alignment information. However, in hybrid decoding, it is quite time-consuming and impractical to conduct word alignment like GIZA++ for each span. Fortunately, unit hypotheses word alignment can be obtained from the model training process, which is shown in Figure 2. We devise a heuristic approach for PHS alignment that leverages the translation derivations from the sub-phrases. The derivation information ultimately comes from the phrase table in phrase-based systems (Koehn et al., 2003; Xiong et al., 2006) or the rule table in syntactic-based systems (Chiang, 2007; Liu et al., 2007; Galley et al., 2006). The derivation is built in a phrase-based system as follows. For example, we have two phrase where string “m-n”means the mth word in the source phrase is aligned to the nth word in the target phrase. When combining the two phrases for generating “我们 的 经济 利益”, we obtain the translation hypothesis as “our economic interests”and also integrate the alignment fragment to get “0-0 1-0 2-1 3-2”. The case is similar in syntactic-based system for non-terminal substitution, which we will not discuss further here. Next, we introduce the skeleton-to-hypothesis word alignment algorithm in detail. With th"
C10-2025,N03-1017,0,0.0544342,"Missing"
C10-2025,P02-1040,0,0.0893694,"Suppose we have n individual decoders. The ranking function Fn of the nth decoder can be written as: Fn (e) = m X λn,i hn,i (f, e) (2) i=1 Figure 3: Algorithm for skeleton-to-hypothesis alignment Subroutines UNION(A,B) GETALIGN(S,align) tial in confusion network construction. The simplest way is to use the top-1 PHS from any individual decoder with the best performance under some criteria. However, this cannot always lead to better performance on some evaluation metrics (Rosti et al., 2007a). An alternative would be MBR method with some loss function such as TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). We show the experimental results of two skeleton selection methods for PHS combination in Section 3. Description the union of set A and set B get the words aligned to S based on align similarity between w1 and w2 , we use edit distance here align w1 with w2 Table 1: Description for subroutines Due to the variety of the word order in nbest outputs, skeleton selection becomes essenwhere each hn,i (f, e) is a feature function of the nth decoder, and λn,i is the corresponding feature weight. m is the number of features in each decoder. The final result of hybrid decoder is the top1 translation f"
C10-2025,P00-1056,0,\N,Missing
C10-2025,E06-1005,0,\N,Missing
C10-2025,P07-1089,0,\N,Missing
C10-2025,P09-1066,1,\N,Missing
C10-2025,P06-1066,0,\N,Missing
C10-2025,P07-1040,0,\N,Missing
C10-2025,N04-1022,0,\N,Missing
C10-2025,D07-1056,1,\N,Missing
C10-2025,P09-1065,0,\N,Missing
C10-2025,P03-1021,0,\N,Missing
C10-2025,P05-1033,0,\N,Missing
C10-2025,N07-1029,0,\N,Missing
C10-2025,J97-3002,0,\N,Missing
C10-2025,D08-1011,0,\N,Missing
C10-2025,W04-3250,0,\N,Missing
C10-2025,J07-2003,0,\N,Missing
C14-1108,P06-1067,0,0.0317284,"problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both te"
C14-1108,J07-2003,0,0.649224,"first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1144 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example"
C14-1108,D08-1089,0,0.57593,"ic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the"
C14-1108,C10-1050,0,0.44024,"nsistency in Moses between training and decoding. Here we would like to note that phrase based orientation depends on phrase segmentation. For example, in Figure 1, the orientation of phrase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model whic"
C14-1108,C08-1041,0,0.126231,"29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reorderi"
C14-1108,W05-1507,0,0.0354703,"ine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous g"
C14-1108,D10-1014,0,0.0157829,"hould be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a str"
C14-1108,W13-2258,0,0.237941,"r internal structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation probability of X1 will condition on “zhe, xiayou, this, river”. In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather than using “xiayou” as a feature to determine the orientation of X1 with respect to the next phrase, we think the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used"
C14-1108,P07-2045,0,0.00826616,"central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonter"
C14-1108,P06-1090,0,0.0287036,"anguage into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and n"
C14-1108,P13-1156,0,0.275902,"the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used to find the corresponding phrase-based path of a HPB derivation and calculate the phrase-based reordering features. Figure 2. The phrase-based path of the derivation in Figure 1. 1145 A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences and whose target sides generated the target sentences from left to right. For example, the phrase-based"
C14-1108,P00-1056,0,0.144249,"ectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #wor"
C14-1108,J04-4002,0,0.219684,"rectly on synchronous rules. For each target phrase contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model on both small and large scale data. On NIST machine translation test sets, our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline hierarchical phrase-based system. 1 Introduction In statistical machine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reord"
C14-1108,P03-1021,0,0.111575,"ner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua"
C14-1108,P02-1040,0,0.0916478,"s used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #words 3016570 3922816 Table 1. The statistics of FBIS corpus Table 2 summarizes the translation performance. The first row shows the results of baseline HPB system, and the second row shows the results when we integrated our lexicalized reordering model (LRM). We get 1.2, 0.8 and 0.7 BLEU point improv"
C14-1108,N04-4026,0,0.184029,"Missing"
C14-1108,D09-1105,0,0.113438,"ase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003)"
C14-1108,2011.mtsummit-papers.43,0,0.115615,"has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB baseline system. 2 Related work In this section, we briefly review two types of related work which are a nonterminal-based lexicalized reordering models and a path-based lexicalized reordering model. Both of them calculate the orientation for HPB translation. 2.1 Nonterminal-based lexicalized reordering models Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a derivation is calculated as the product of orientation probabilities of all nonterminals except the root. In order to define the relative orders of nonterminals and their adjacent phrase, they expand the alignment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They significantly outperformed the HPB system on both Chinese-English and German-English translation. Xiao et al"
C14-1108,P06-1066,0,0.0289271,"order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phr"
C14-1108,W06-3108,0,0.131425,"language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of"
C14-1108,W06-3119,0,0.0423035,"al Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based"
C14-1108,W12-3125,0,\N,Missing
C14-1210,W09-2307,0,0.162431,"gaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the preprocessing. 2233 Chinese English #sentences 128,832 128,832 #words 3,016,570 3,922,816 Table 2. The statistics of FBIS corpus The evaluation results over FBIS corpus are reported in Table 3. The first row shows the results of baseline, the next three rows show the effect of three features respectively and the last row gives the result when all features are"
C14-1210,P08-1009,0,0.0373898,"Missing"
C14-1210,J07-2003,0,0.881897,"translate a sentence, the dependency knowledge is used to compute the syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize the structure consistency by three features and integrate them into the standard SMT log-linear model to guide the translation process. Our method is evaluated on multiple Chinese-to-English machine translation test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points improvements over a strong baseline of an in-house implemented HPB translation system. 1 Introduction HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical trans"
C14-1210,D11-1079,0,0.0167224,"and that between the rule and its context. Both above related work and our work need parse the source sentence to get syntactic context before decoding. There are also some methods incorporating syntax information without the need of online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). They parse the training data to label the non-terminals with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextual information around translation rules. In this sense, it is similar to He et al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they leverage lexical context for rule selection while we focus on the syntactic contextual information. 3 Hierarchical Phrase"
C14-1210,C08-1041,0,0.017156,"with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextual information around translation rules. In this sense, it is similar to He et al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they leverage lexical context for rule selection while we focus on the syntactic contextual information. 3 Hierarchical Phrase based Machine Translation Our model proposed in this paper is an extension of the HPB model (Chiang, 2007). Formally, HPB model is a weighted synchronous context free grammar. It employs a generalization of the standard plain phrase extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text. Rules have the form of: where X is a"
C14-1210,2006.amta-papers.8,0,0.0356325,"phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been muc"
C14-1210,D10-1014,0,0.688855,"Missing"
C14-1210,D13-1053,0,0.358716,"ecial efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1> (6) <借了 NP, lent X> (8) <NP 借了 NP, X1 lent X2> Although augmenting the non-terminals with syntactic tags in these methods achieved better results for HPB model, they have limitations that the syntax information on the non-terminals are not discrimThis work is licenced under a Creative Commons Attrib"
C14-1210,P00-1056,0,0.147529,"PB rule is extended into the form of and the score is calculated by: ∑ where the additional three features are defined in Section 4.3, , and are corresponding feature weights. We test our soft dependency matching model on a Chinese-English translation task. The NIST06 evaluation data was used as our development set to tune the feature weights, and NIST04, NIST05 and NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel corpus, and then further test the performance of our method on a large scale training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations"
C14-1210,P03-1021,0,0.120898,"rd-aligned parallel text. Rules have the form of: where X is a nonterminal, and are both strings of terminals and non-terminals from source and target side respectively, and ∼ is a one-to-one correspondence between nonterminal occurrences in and . Associated with each rule is a set of feature functions with the form . These feature functions are combined into a log-linear model. When a rule is applied during SMT decoding, its score is calculated as: ∑ where is the weight associated with feature function . The feature weights are typically optimized using minimum error rate training algorithm (Och, 2003). 4 Soft Dependency Matching Model In order to incorporate syntactic knowledge to refine both the word ordering and word sense disambiguation for HPB model, we propose a soft dependency matching model (SDMM). It extends HPB rule into a form which is named as SDMM rule: where RDT(rule’s dependency triples) is a set of dependency triples defined on source string . Each element in RDT is a triple representing dependency knowledge in the form: {m-h-l} where m and h are the dependent and head respectively, l is the label of the dependency relation type. m and h could be any of terminals, non-termin"
C14-1210,N03-1017,0,0.0567749,"dency tree of the sentence. We characterize the structure consistency by three features and integrate them into the standard SMT log-linear model to guide the translation process. Our method is evaluated on multiple Chinese-to-English machine translation test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points improvements over a strong baseline of an in-house implemented HPB translation system. 1 Introduction HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008),"
C14-1210,W12-3128,0,0.0242719,"Missing"
C14-1210,P06-1077,0,0.057462,"model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010"
C14-1210,P08-1114,0,0.0791107,"(2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1>"
C14-1210,D08-1022,0,0.0554861,"Missing"
C14-1210,P02-1040,0,0.0917171,"nd NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel corpus, and then further test the performance of our method on a large scale training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus af"
C14-1210,P08-1066,0,0.026789,"by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by"
C14-1210,D09-1008,0,0.366391,"resent the syntactic variation of translation rules in the form of distribution. The main difference is that they annotate non-terminals with head POS tags while we use dependency triples (over both terminals and non-terminals) to explicitly represent both the dependency relations inside the rule, and that between the rule and its context. Both above related work and our work need parse the source sentence to get syntactic context before decoding. There are also some methods incorporating syntax information without the need of online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). They parse the training data to label the non-terminals with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextu"
C14-1210,2010.amta-papers.8,0,0.029154,"Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borr"
C14-1210,J97-3002,0,0.019268,"eved promising results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our method will distinguish the inconsistency degree for different rules. While, for such rules, Xiong et al. (2009) will give a sam"
C14-1210,D11-1020,0,0.0180229,"synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating synta"
C14-1210,P06-1066,0,0.0356965,"sing results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our method will distinguish the inconsistency degree for different rules. While, for such rules, Xiong et al. (2009) will give a same score which will be"
C14-1210,P09-1036,0,0.0164889,"credit if it respects the parse tree but may incur a cost if it violates a constituent boundary. The soft constrain based methods achieved promising results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our meth"
C14-1210,W06-3119,0,0.609439,"ference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1> (6) <借了 NP, lent X> (8) <NP 借了 NP, X1 lent X2> Although augmenting the non-terminals with syntactic tags in these methods achieved better results for HPB model, they have limitations that the syntax information on the non-terminals are not discrimT"
C14-1210,P08-1064,0,0.0171028,"r to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton"
C14-1210,P11-2033,0,0.0160391,"corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the preprocessing. 2233 Chinese English #sentences 128,832 128,832 #words 3,016,570 3,922,816 Table 2. The statistics of FBIS corpus The evaluation results over FBIS corpus are report"
D07-1056,J04-4002,0,0.328585,"ordering and the latter with the longer distance reordering as global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to"
D07-1056,N03-1017,0,0.22019,"s can compensate for the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation res"
D07-1056,koen-2004-pharaoh,0,0.243773,"the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation results be closer"
D07-1056,H05-1021,0,0.313844,"Missing"
D07-1056,P06-1077,0,0.143706,"T systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to handle global phrase reordering. Our method is different from previous syntax-based SMT systems in which the translation process was modeled based on specific syntactic structures, either phrase structures or dependency relatio"
D07-1056,P05-1034,0,0.375787,"e between the root node of T and the root node of the maximum sub-tree which exactly covers fi. For example, in Figure 1 the phrase “大幅” has the maximum sub-tree rooting at ADJP and its height is 3. The height of phrase “ 的 ” is 4 since its maximum sub-tree roots at 535 ADBP instead of AD. If two adjacent phrases have the same height, we regard them as peer phrases. In our model, we make use of bilingual phrases as well, which refer to source-target aligned phrase pairs extracted using the same criterion as most phrase-based systems (Och and Ney, 2004). 3.2 Model Similar to the work in Chiang (2005), our translation model can be formulated as a weighted synchronous context free grammar derivation process. Let D be a derivation that generates a bilingual sentence pair f, e, in which f is the given source sentence, the statistical model that is used to predict the translation probability p(e|f) is defined over Ds as follows: ? ? ? ∝ ? ? ∝ ??? ? ? ?? ?? ? → ?, ? × ? ?→?,?∈? ?? where plm(e) is the language model, i(X ,) is a feature function defined over the derivation rule X,, and i is its weight. Although theoretically it is ideal for translation reorder modeling by const"
D07-1056,J03-1002,0,0.0108535,"Missing"
D07-1056,P96-1021,0,0.649154,"global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we pr"
D07-1056,P06-1066,0,0.799337,"used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to hand"
D07-1056,P01-1067,0,0.4941,"Missing"
D07-1056,P05-1033,0,0.853395,"rd to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relati"
D07-1056,C04-1030,0,0.0617,"eign linguistic other models such as language model. In our system, we combine the parse trees gen- phrases can also be formed into two valid adjacent erated respectively by Stanford parser (Klein, 2003) phrase according to constraints proposed in the and a dependency parser developed by (Zhou, phrase extraction algorithm by Och (2003a), they 2000). Compared with the Stanford parser, the de- will be extracted as a reordering training sample. pendency parser only conducts shallow syntactic Finally, the ME modeling toolkit developed by analysis. It is powerful to identify the base NPs and Zhang (2004) is used to train the reordering model base VPs and their dependencies. Additionally, over the extracted samples. dependency parser runs much faster. For example, it took about three minutes for the dependency parser to parse one thousand sentences with aver4.2 Combination of Parse Trees 538 7 Experimental Results and Analysis We conducted our experiments on Chinese-toEnglish translation task of NIST MT-05 on a 3.0GHz system with 4G RAM memory. The bilingual training data comes from the FBIS corpus. The Xinhua news in GIGAWORD corpus is used to train a four-gram language model. The development"
D07-1056,P03-1021,0,0.134599,"tally twelve categories of features used to train the ME model. In fact, the probability of Rule (1) is just equal to the supplementary probability of Rule (2), and vice versa. For Rule (3)~(9), according to the syntactic structures, their application is determined since there is only one choice to complete reordering, which is similar to the “glue rules” in Chiang (2005). Due to the appearance of non-linguistic phrases, non-monotone phrase reordering is not allowed in these rules. We just assign these rules a constant score trained using our implementation of 537 Minimum Error Rate Training (Och, 2003b), which is 0.7 in our system. For Rule (10)~(12), they are also determined rules since there is no other optional rules competing with them. Constant score is simply assigned to them as well, which is 1.0 in our system. Fea. Description LS1 First word of first foreign phrase LS2 First word of second foreign phrase RS1 Last word of first foreign phrase RS2 Last word of second foreign phrase LT1 First word of first target phrase LT2 First word of second target phrase RT1 Last word of first target phrase RT2 Last word of second target phrase LPos POS of the node covering first foreign phrase RP"
D07-1056,W00-1212,1,\N,Missing
D07-1056,P03-1054,0,\N,Missing
D09-1038,W98-1115,0,0.0439035,"or the SCFG learnt automatically from the training corpus. It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization. We apply our method into a state-of-the-art string-to-tree SMT system. The experimental results show that our method outperforms the synchronous binarization method (Zhang et al., 2006) with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tr"
D09-1038,P05-1033,0,0.0773653,"owever, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtai"
D09-1038,J07-2003,0,0.862926,"method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004"
D09-1038,P06-1121,0,0.465965,"SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using th"
D09-1038,W07-0405,0,0.0851011,"oblem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP"
D09-1038,W04-3250,0,0.186026,"Missing"
D09-1038,W06-1606,0,0.331083,"2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP will be JJR R2 : S → NP 会 VP , NP will VP binarization G’ (R1) v11 : VP → V12 JJR , V12 JJR v12 : V12 → VB V13 , VB V13 v13 : V13 → NP 会 , NP will be (R2) v21 : S → V22 VP , v22 : V22 → NP 会 , rule bucket v11 3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed (left-heavy) way (Zhang et al., 2006) may lead to a large n"
D09-1038,H05-1101,0,0.0456339,"h it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using the synchronous binarization method (Zhang et al., 2006) as follows: VP → V1 JJR , V1 → VB V2 , V2 → NP 会 , V1 JJR VB V2 NP will be This binarization is shown with the solid lines as binarization (a) in Figure 1."
D09-1038,D08-1018,0,0.0770688,"NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2"
D09-1038,D07-1078,0,0.038172,"r development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speed up the Minimum Error Rate Training (MERT). The test data sets are the NIST evaluation sets of 2005 and 2008. Our string-to-tree SMT system is built based on the work of (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Before the rule extraction, we also binarize the parse trees on the English side using Wang et al. (2007) „s method to increase the coverage of GHKM and SPMT rules. There are totally 4.26M rules after the low frequency rules are filtered out. The pruning strategy is similar to the cube pruning described in (Chiang, 2007). To achieve acceptable translation speed, the beam size is set to 50 by default. The baseline system is based on the synchronous binarization (Zhang et al., 2006). 4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison. In this paper, the"
D09-1038,N06-1033,0,0.250451,"ing polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decod"
D09-1038,N04-1035,0,\N,Missing
D09-1038,N06-1022,0,\N,Missing
D13-1107,D11-1033,0,0.0801174,"Missing"
D13-1107,W06-1615,0,0.0632491,"ation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each 1063 task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking o"
D13-1107,J07-2003,0,0.0972138,"nces for testing in each domain. The details are shown in Table 2. Domain Business Ent. Health Sci&Tech Sports Politics Train En Ch 30M 28M 25M 22M 23M 20M 28M 26M 19M 16M 28M 24M Dev En Ch 36K 35K 21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation"
D13-1107,P13-2061,1,0.817113,"Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). ”M” refers to million and"
D13-1107,D08-1072,0,0.0445467,"omain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods"
D13-1107,W10-1757,0,0.0555944,"e Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general"
D13-1107,eck-etal-2004-language,0,0.18163,"g domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply the"
D13-1107,W07-0717,0,0.372942,"dels may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To levera"
D13-1107,W06-1607,0,0.0717034,"efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 R"
D13-1107,D10-1044,0,0.132836,"Missing"
D13-1107,P09-1098,1,0.83532,"addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled mono"
D13-1107,W07-0733,0,0.520345,"riety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we"
D13-1107,W04-3250,0,0.0374174,"++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison."
D13-1107,P06-1096,0,0.264664,"Missing"
D13-1107,D07-1036,0,0.13075,"Missing"
D13-1107,P10-2041,0,0.30299,"hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult t"
D13-1107,J03-1002,0,0.00587202,"21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We h"
D13-1107,P03-1021,0,0.0335982,"ser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to G"
D13-1107,P02-1040,0,0.0869864,"rithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-t"
D13-1107,P12-1099,0,0.144964,"s often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptati"
D13-1107,E12-1055,0,0.187281,"lation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise"
D13-1107,P12-1002,0,0.446041,"ased on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual 1056 adaptation approach towards each domain. The rest of the paper is organized as follows: The proposed approach is explain"
D13-1107,P07-1004,0,0.348402,"nd second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adapt"
P07-1091,koen-2004-pharaoh,0,0.0571564,"ssible reorderings of its children. The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases. The form of reordering rules, and the calculation of reordering probability for a particular node, can also be generalized easily.6 The only problem for the generalized reordering knowledge is that, as there are more classes, data sparseness becomes more severe. 6 The Decoder The last three sections explain how the S → n×S 0 part of formula 2 is done. The S 0 → T part is simply done by our re-implementation of PHARAOH (Koehn, 2004). Note that nonmonotonous translation is used here since the distance-based model is needed for local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All t"
P07-1091,P03-1021,0,0.0573639,"r local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All the feature weights (λs) were trained using our implementation of Minimum Error Rate Training (Och, 2003). The final translation Tˆ is the T with the highest total score. 5 Namely, N1 N2 N3 , N1 N3 N2 , N2 N1 N3 , N2 N3 N1 , N3 N1 N2 , and N3 N2 N1 , if the child nodes in the original order are N1 , N2 , and N3 . 6 For example, the reordering probability of a phrase p = p1 p2 p3 generated by a 3-ary node N is Pr (r)×Pr (pi1 )×Pr (pj2 )×Pr (pk3 ) where r is one of the six reordering patterns for 3-ary nodes. 724 It is observed in pilot experiments that, for a lot of long sentences containing several clauses, only one of the clauses is reordered. That is, our greedy reordering algorithm (c.f. secti"
P07-1091,P00-1056,0,0.055284,"set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible. The 3ary nodes occupy a certain proportion of the distribution,"
P07-1091,P02-1040,0,0.107922,") is trivial, but there is a subtle point about the calculation of language model score: the language model score of a translated clause is not independent from other clauses; it should take into account the last few words of the previous translated 0 clause. The best translated clause Tˆ(Ci ) is selected in step 3(a)(iii) by equation 3. In step 4 the best translation Tˆj is X arg max exp(λr logPr (S → Sj )+ Tj 0 score(T (Ci ))). i 7 Experiments 7.1 Corpora Our experiments are about Chinese-to-English translation. The NIST MT-2005 test data set is used for evaluation. (Case-sensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. The 7 IP stands for inflectional phrase and CP for complementizer phrase. These two types of phrases are clauses in terms of the Government and Binding Theory. Branching Factor Count Percentage 2 12294 73.41 3 3173 18.95 &gt;3 1280 7.64 Table 1: Distribution of Parse Tree Nodes with Different Branching Factors Note that nodes with only one child are excluded from the survey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translatio"
P07-1091,P05-1034,0,0.0665473,"ot monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notab"
P07-1091,P06-1067,0,0.209315,"spect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a method in learning patterns of rewriting SL sentences. This method parses training data and uses some heuristics to align SL phrases with TL ones. From such alignment it can extract rewriting patterns, of which the units are words and POSs. The learned rewriting rules are then applied to rewrite SL sentences before monotonous translation. Despite the encouraging results reported in these papers, the two attempts share the same shortcoming that their reordering is deterministic. As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. That is, the choice of reordering is independent from other translation factors, and once a reordering mistake is made, it cannot be corrected by the subsequent decoding. To overcome this weakness, we suggest a method to ‘soften’ the hard decisions in preprocessing. The essence is that our preprocessing module generates n-best S 0 s rather than merely one S 0 . A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reorder"
P07-1091,P05-1066,0,0.78591,"op half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the translation process S 0 →"
P07-1091,P03-1035,1,0.537454,"Missing"
P07-1091,P03-1054,0,0.0115851,"ey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible"
P07-1091,N03-1017,0,0.0335242,"Missing"
P07-1091,N04-4026,0,0.135932,"his model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notable ones are (Xia and McCord, 2004) and (Collins et al., 2005), both of which make use of linguistic syntax in the preprocessing stage. (Collins et al., 2005) analyze German clause structure and propose six types of rules for transforming German parse trees with respect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a"
P07-1091,C04-1073,0,0.804395,"lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the tr"
P07-1091,P01-1067,0,0.351717,"anslation process S 0 → T is not monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to r"
P07-1091,W06-1609,0,\N,Missing
P07-1091,W02-1039,0,\N,Missing
P07-1091,2005.iwslt-1.8,0,\N,Missing
P08-1011,C02-1126,0,0.0131409,"ed in a similar way as does with target features. The source head word feature is defined to be a function f4 to indicate whether a word ei is the source head word in English according to a parse tree of the source sentence. Similar to the definition of lexical features, we also use a set of features based on POS tags of source language. 3 3.1 Model Training and Application Training We parsed English and Chinese sentences to get training samples for measure word generation model. Based on the source syntax parse tree, for each measure word, we identified its head word by using a toolkit from (Chiang and Bikel, 2002) which can heuristically identify head words for sub-trees. For the bilingual corpus, we also perform word alignment to get correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we ne"
P08-1011,P05-1033,0,0.134325,"ls and indefinite articles are directly followed by countable nouns to denote the quantity of objects. Therefore, in the English-to-Chinese machine translation task we need to take additional efforts to generate the missing measure words in Chinese. For example, when translating the English phrase three books into the Chinese phrases “三本书”, where three corresponds to the numeral “三” and books corresponds to the noun “书”, the Chinese measure word “本” should be generated between the numeral and the noun. In most statistical machine translation (SMT) models (Och et al., 2004; Koehn et al., 2003; Chiang, 2005), some of measure words can be generated without modification or additional processing. For example, in above translation, the phrase translation table may suggest the word three be translated into “三”, “三本”, “三只”, etc, and the word books into “书”, “书本”, “名册” (scroll), etc. Then the SMT model selects the most likely combination “三本书” as the final translation result. In this example, a measure word candidate set consisting of “本” and “只” can be generated by bilingual phrases (or synchronous translation rules), and the best measure word “本” from the measure 2 1 There are some exceptional cases,"
P08-1011,N03-1017,0,0.0412262,"Missing"
P08-1011,P07-1017,0,0.0485184,"Missing"
P08-1011,P00-1056,0,0.0252377,"s and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in previous sections, we apply our measure word generation module into SMT output as a post-processing ste"
P08-1011,J04-4002,0,0.307392,"Missing"
P08-1011,P02-1040,0,0.0737285,"Missing"
P08-1011,N07-1051,0,0.0125043,"et correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in"
P09-1066,C08-1005,0,0.0599862,"ngalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Stati"
P09-1066,P05-1033,0,0.476399,"e rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a loglinear model aims to"
P09-1066,D08-1011,0,0.382928,"sensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire sea"
P09-1066,2008.amta-srw.3,0,0.391125,"y, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Statistical Machine Translation (SMT) model while a majority of the space, within which there are many potentially good translations, is pruned away in decoding."
P09-1066,N04-1022,0,0.0804613,"explored in decoding. Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination. 1 Introduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated"
P09-1066,P06-1077,0,0.0243484,"m step 2 to step 4 until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search sp"
P09-1066,E06-1005,0,0.0485259,"oduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better a"
P09-1066,P02-1038,0,0.124274,"Missing"
P09-1066,P03-1021,0,0.0111584,"egation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences in ℋ? ? of all n-grams in e: ? −?+1 ?? + ?, ?′ = ?=1 ?? − ?, ? ′ is the n-gram disagreement measure function which is complementary to ?? + ?, ? ′ : ? −?+1 ?=1 Model Training We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Let ?? be the feature weight vector for member decoder ?? , the training procedure proceeds as follows: 1. Choose initial values for ?1 , … , ?? 2. Perform co-decoding using all member decoders on a development set D with ?1 , … , ?? . For each decoder ?? , find a new feature weight vector ?′? which optimizes the specified evaluation criterion L on D using the MERT algorithm based on the n-best list ℋ? generated by ?? : ?′? = argmax? ? (?|?, ℋ? , ?)) where T denotes the translations selected by re-ranking the translations"
P09-1066,J04-4002,0,0.162557,"forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline model, the only change is hypothesis scoring. By rerunning a complete decoding process, member model can be applied to re-score all hypotheses explored by a decoder. Therefore step 3 can be viewed as full-scale hypothesis re-ranking because the re-ranking scope is beyond the limited n-best hypotheses currently cached in ℋ?"
P09-1066,W04-3250,0,0.67335,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,P08-1066,0,0.047141,"cal phrase-based decoder. Phrasal rules are extracted from all bilingual sentence pairs, while rules with variables are extracted only from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a BTG decoder with lexicalized reordering model based on maximum entropy principle as proposed by Xiong et al. (2006). We use all the bilingual data to extract phrases up to length 3. The third one (SYS3) is a string-to-dependency tree –based decoder as proposed by Shen et al. (2008). For rule extraction we use the same setting as in SYS1. We parsed the language model training data with Berkeley parser, and then trained a dependency language model based on the parsing output. All baseline decoders are extended with n-gram consensus –based co-decoding features to construct member decoders. By default, the beam size of 20 is used for all decoders in the experiments. We run two iterations of decoding for each member decoder, and hold the value of ? in Equation 5 as a constant 0.05, which is tuned on the test data of NIST 2004 Chinese-toEnglish machine translation task. 3.3 D"
P09-1066,koen-2004-pharaoh,0,0.617964,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,D08-1065,0,0.341537,"?′ ?? ?? (?, ?′) (4) ?′ ∈ℋ? ? where e is a translation of f by decoder ?? (? ≠ ?), ? ′ is a translation in ℋ? ? and ? ?′ ?? is the posterior probability of translation ? ′ determined by decoder ?? given source sentence f. ?? (?, ?′) is a consensus measure defined on e and ?′, by varying which different feature functions can be obtained. 587 Referring to the log-linear model formulation, the translation posterior ? ?′ ?? can be computed as: ? ?′ ?? = exp ??? ?′ ?′′ ∈ℋ? ? exp ??? ?′′ (5) 2.5 where ?? (∙) is the score function given in Equation 2, and ? is a scaling factor following the work of Tromble et al. (2008) To compute the consensus measures, we further decompose each ?? ?, ?′ into n-gram matching statistics between e and ?′. Here we do not discriminate among different lexical n-grams and are only concerned with statistics aggregation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences"
P09-1066,P07-2045,0,0.00862222,"Missing"
P09-1066,P06-1066,0,0.614687,"until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline"
P09-1066,W08-0329,0,\N,Missing
P09-1066,N07-1029,0,\N,Missing
P10-2002,C08-1041,0,0.496023,"selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structu"
P10-2002,N03-1017,0,0.0859175,"ngzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic infor"
P10-2002,D08-1010,0,0.286273,"othesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish"
P10-2002,P07-1089,0,0.0312973,"Missing"
P10-2002,P06-1077,0,0.112862,"ctical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model"
P10-2002,W06-1606,0,0.0274999,"with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into fou"
P10-2002,P08-1114,0,0.322519,"del in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the"
P10-2002,P08-1023,0,0.023425,"model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that"
P10-2002,P02-1040,0,0.0820023,"e instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 NIST 2006 0.3025 0.3061 0.3089 0.3141 As shown in Table 1, all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation. Apparently, our method also performs better than the other two approaches, indicating that our method is more effective in the hierarchical rule selection as both source-side and target-side rules are selected together. 4.3 Effect of sub-models Due to the space"
P10-2002,N07-1051,0,0.0342946,"d 5. Length features, which are the length of sub-phrases covered by source nonterminals. 4 4.1 Experiments NIST 2008 0.2200 0.2254 0.2253 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p < 0.01) Experiment setting We implement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evalu"
P10-2002,J96-1002,0,0.0681035,"e the system performance significantly. 2 the monolingual source side of the training corpus. CFSM is used to capture how likely the sourceside rule is linguistically motivated or has the corresponding target-side counterpart. For CBSM, it can be naturally viewed as a classification problem where each distinct source-side rule is a single class. However, considering the huge number of classes may cause serious data sparseness problem and thereby degrade the classification accuracy, we approximate CBSM by a binary classification problem which can be solved by the maximum entropy (ME) approach (Berger et al., 1996) as follows: Ps (α|C) ≈ Ps (υ|α, C) P exp[ i λi hi (υ, α, C)] P =P 0 υ 0 exp[ i λi hi (υ , α, C)] Hierarchical Rule Selection Model Following (Chiang, 2005), hα, γi is used to represent a synchronous context free grammar (SCFG) rule extracted from the training corpus, where α and γ are the source-side and target-side rule respectively. Let C be the context of hα, γi. Formally, our joint probability model of hierarchical rule selection is described as follows: P (α, γ|C) = P (α|C)P (γ|α, C) where υ ∈ {0, 1} is the indicator whether the source-side rule is applied during decoding, υ = 1 when the"
P10-2002,P05-1034,0,0.0418459,"d can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods"
P10-2002,P05-1033,0,0.339463,"stitute of Technology, Harbin, China {cuilei,tjzhao}@mtlab.hit.edu.cn ‡ Microsoft Research Asia, Beijing, China {dozhang,muli,mingzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more r"
P10-2002,N09-1025,0,0.0381803,"ilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table"
P10-2002,P08-1066,0,0.0351936,"The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that can be further clas"
P10-2002,P06-1121,0,0.115919,"rporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint"
P10-2002,D09-1008,0,0.0917229,"ume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marto"
P10-2002,W08-0302,0,0.0163984,"ed from multiple distinct sentence pairs in the bilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baselin"
P10-2002,P09-1037,0,0.121053,"didate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens"
P10-2002,J97-3002,0,0.020514,"edicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts between source and target components since it sh"
P10-2002,P06-1066,0,0.0365012,"e selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research A"
P10-2002,P09-1036,0,0.177282,"-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger sp"
P10-2002,P01-1067,0,0.112783,"s is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Lingu"
P10-2002,W04-3250,0,\N,Missing
P12-1096,P05-1033,0,0.211131,"Missing"
P12-1096,P05-1066,0,0.264094,"lysis, we find that the parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reord"
P12-1096,P06-1121,0,0.0208488,"the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data. Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree 919 levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006)) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to remember the source side context of the rules, the model size would still be much smaller than a full-fledged tree-to-string system because we do not need to explicitly store the target variants for each rule. 7 Conclusion and Future Work In this paper we present a ranking based reordering method to reorder source language to match the word order of target language given the source side parse tree. Reordering is formulated as"
P12-1096,D08-1089,0,0.0432048,"Missing"
P12-1096,C10-1043,0,0.846525,"king-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-based word reor"
P12-1096,N03-1017,0,0.0172438,"es in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. 1 Introduction Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi∗ This work has been done while the first author was visiting Microsoft Research Asia. Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally vi"
P12-1096,W02-2016,0,0.0316159,"ugh for reordering task, and can be searched through efficiently using a CKY decoder. After finding the best reordered tree Te00 , we can extract one reorder example from every node with more than one child. 3.2 Features Features for the ranking model are extracted from source syntax trees. For English-to-Japanese task, we extract features from Stanford English Dependency Tree (Marneffe et al., 2006), including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. For Japanese-to-English task, we use a chunkbased Japanese dependency tree (Kudo and Matsumoto, 2002). Different from features for English, we do not use dependency labels because they are not available from the Japanese parser. Additionally, Japanese function words are also included as features because they are important grammatical clues. The detailed feature templates are shown in Table 1. CLN (c) child c of t 3.3 Learning Method Take the original tree in Figure 1 for example. At the root node trying, CLN(trying) is 6 because there are six crossing-links under its sub-span: (e1 j4 , e2 j3 ), (e1 j4 , e4 j2 ), (e1 j4 , e5 j1 ), (e2 j3 , e4 j2 ), (e2 j3 , e5 j1 ) and (e4 j2 , e5 j1 ). On the"
P12-1096,C10-1071,0,0.290577,"and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entrop"
P12-1096,P07-1091,1,0.94429,"but effective ranking-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-"
P12-1096,P06-1077,0,0.33379,"ring between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word reordering based on s"
P12-1096,de-marneffe-etal-2006-generating,0,0.00844633,"Missing"
P12-1096,J03-1002,0,0.00483443,"Missing"
P12-1096,P05-1034,0,0.348162,"distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word re"
P12-1096,I08-1067,0,0.0231109,"errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using ei"
P12-1096,C10-1126,0,0.340886,"air. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1 , c2 , . . . , cn }, we rearrange the children to target-language-like order {cπ(i1 ) , cπ(i2 ) , . . . , cπ(in ) }. If we treat the reordered position π(i) of child ci as its “rank”, the reordering problem is naturally translated into a ranking problem: to reorder, we determine a “rank” for each child, then the children are sorted according to their “ranks”. As it i"
P12-1096,D07-1077,0,0.102202,"parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from"
P12-1096,J97-3002,0,0.140444,"clex · dst ctf · hf · dst ctf · hlex cl · dst cl · lcl cl · rcl · dst cl · clex · dst cl · hlex cl · clex · pct cl · hlex · pct ctf ctf ctf ctf ctf ctf · dst · lct · dst · clex · hf · hf · dst · hlex · dst cl · pct cl · rcl cl · clex cl · clex · dst cl · hlex · dst cl · clex · pct ctf · lct cl · rct · dst ctf · clex · dst ctf · hf ctf · hlex ctf · hlex · dst erence for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e0 . In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A → e/f A → [A1 , A2 ] A → hA1 , A2 i Table 1: Feature templates for ranking function. All templates are implicitly conjuncted with the pos tag of head node. c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f : Ja"
P12-1096,P06-1066,0,0.016997,"ctf · hf · dst ctf · hlex cl · dst cl · lcl cl · rcl · dst cl · clex · dst cl · hlex cl · clex · pct cl · hlex · pct ctf ctf ctf ctf ctf ctf · dst · lct · dst · clex · hf · hf · dst · hlex · dst cl · pct cl · rcl cl · clex cl · clex · dst cl · hlex · dst cl · clex · pct ctf · lct cl · rct · dst ctf · clex · dst ctf · hf ctf · hlex ctf · hlex · dst erence for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e0 . In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A → e/f A → [A1 , A2 ] A → hA1 , A2 i Table 1: Feature templates for ranking function. All templates are implicitly conjuncted with the pos tag of head node. c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f : Japanese function word"
P12-1096,N09-1028,0,0.863406,"panese sentence pair. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1 , c2 , . . . , cn }, we rearrange the children to target-language-like order {cπ(i1 ) , cπ(i2 ) , . . . , cπ(in ) }. If we treat the reordered position π(i) of child ci as its “rank”, the reordering problem is naturally translated into a ranking problem: to reorder, we determine a “rank” for each child, then the children are sorted accordi"
P12-1096,W06-3108,0,0.0291277,"Missing"
P12-1096,C04-1010,0,\N,Missing
P12-1096,2005.iwslt-1.8,0,\N,Missing
P12-1100,P08-1009,0,0.283488,"longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the me"
P12-1100,P05-1033,0,0.38994,"A derivation yields a pair of strings on the right-hand side which are translation of each other. In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f ) = max p(d, e|f , cˆ) d Y = max p(r, e|f , cˆ) d X (6) k We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decoding, we employ the same decoding algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. 2.1 A Loose Model In our model, we employ rules containing nont"
P12-1100,J07-2003,0,0.433856,"grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. X → hX 1 for X 2 , X 2 de X 1 i can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. 1 Introduction The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first"
P12-1100,P05-1066,0,0.0777595,"ed decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed is reported by seconds per sentence. The speed for the tree-tostring decoder includes the parsing time (0.23s) and the speed for the tight and loose models includes the shallow parsing time, too. extraction as: the height up to 3 and the number of leaf nodes up to 5. We give the results in Table 2. From the results, we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al. (2005) (p < 0.01). Specifically, the loose model has a better performance while the tight model has a faster speed. Compared with the hierarchical phrase-based model, the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space, so it decoders faster. We can conclude that linguistic syntax can indeed improve the translation performance; syntactic cohesion for nonterminals can explain linguistic phenomena well; noncohesive rules are useful, too. The extra time consumption against hierar"
P12-1100,P03-2041,0,0.0452022,"ntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hier"
P12-1100,C10-2033,1,0.84756,"reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two mode"
P12-1100,W02-1039,0,0.1364,"a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-t"
P12-1100,N04-1035,0,0.0742159,"tion We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsi"
P12-1100,N04-1014,0,0.119779,"e parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring t"
P12-1100,2006.amta-papers.8,0,0.0819158,"he syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997;"
P12-1100,N03-1017,0,0.0974779,"oduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -"
P12-1100,C10-1080,1,0.843744,"al phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as"
P12-1100,P06-1077,1,0.904774,"need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some langu"
P12-1100,J93-2004,0,0.0396526,"tially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignm"
P12-1100,P08-1114,0,0.0508566,"model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder"
P12-1100,P08-1023,1,0.860438,"edge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB"
P12-1100,P00-1056,0,0.604931,"sk and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, the"
P12-1100,P02-1038,0,0.20294,"ploy rules containing nonterminals to handle long-distance reordering where boundary words play an important role. So for the subphrases which cover more than one chunk, we just maintain boundary chunks: we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by “-” and next followed by the last chunk tag. Then, for the string pair <filed for bankruptcy, shenqing pochan>, we can get the rule r1 : X → hVBN 1 for NP 2 , VBN 1 NP 2 i while for the string pair <A request for a purchase of shares, goumai gufen de shenqing>, we can get r2 : Following Och and Ney (2002), we frame our model as a log-linear model: P exp k λk Hk (d, e, cˆ, f ) P (5) p(e|f ) = exp d′ ,e′ ,k λk Hk (d′ , e′ , cˆ, f ) Hk (d, e, ˆ c, f ) = λk Hk (d, e, cˆ, f ) X → hNP 1 for NP-NP 2 , NP-NP 2 de NP 1 i. (4) r∈d where X hk (f , cˆ, r) r 952 The rule matching “A request for a purchase of shares was” will be r3 : X → hNP-NP 1 VBD 2 , NP-NP 1 VBD 2 i. We can see that in contrast to the method of representing each chunk separately, this representation form can alleviate data sparseness and the influence of parsing errors. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hX 4 X 3 , X 4 X 3 i ⇒ hNP-NP"
P12-1100,J04-4002,0,0.217737,"kier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -Yv k β2 i. We evalu"
P12-1100,P03-1021,0,0.171403,"Missing"
P12-1100,P02-1040,0,0.0852269,"e first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks P = 2 The so"
P12-1100,P05-1034,0,0.106643,"ordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to"
P12-1100,N03-1028,0,0.0397363,"gnized as a chunk, we skip its children. In this way, we can get a sole chunk sequence given a parse tree. Then we label each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heurist"
P12-1100,P03-1039,0,0.0253069,"e way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003) presented a chunk-to-string translation model where the decoder generates a translation by first translating the words in each chunk, then reordering the translation of chunks. Our model distinguishes from their model mainly in reordering model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree tran"
P12-1100,J97-3002,0,0.132427,"al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse e"
P12-1100,P01-1067,0,0.0718737,"introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we"
P12-1100,W08-2119,0,\N,Missing
P12-1100,J08-3004,0,\N,Missing
P12-2057,P10-1088,0,0.0457426,"Missing"
P12-2057,J93-2003,0,0.0390093,"Missing"
P12-2057,P05-1033,0,0.0608879,"phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods a"
P12-2057,J07-2003,0,0.0340551,"nd hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examine"
P12-2057,C10-2025,1,0.846579,"duction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction. Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules. This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa. 3 Experiment We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task. The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC. NIST 2003 set is used as a development set. NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose. For word alignment, we use GIZA++1 , an implementation of IBM models (Brown et al., 1993). We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005). The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003). Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for sou"
P12-2057,C10-1056,0,0.258016,"del (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores"
P12-2057,D07-1103,0,0.114556,"rs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2"
P12-2057,N03-1017,0,0.0239505,"the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010)."
P12-2057,N03-2016,0,0.031157,"ranslation model, T M , our goal is to find the optimally reduced translation model, T M ∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is"
P12-2057,P03-1021,0,0.0219527,". The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both. Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely. Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously. In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003). One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding. The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction. Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule. Similar to the decoder, the scoring scheme is based on the log-linear framework: X P S(p) = λi hi (p) (3) i 2 Proposed Model Given an original translation model, T M , our"
P12-2057,P02-1040,0,0.0852558,"∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is its weight. As the conventional hierarchical phrase-based SMT model, our features are co"
P12-2057,2009.mtsummit-papers.17,0,0.505698,"various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phra"
P12-2057,P09-2060,0,0.449057,"s mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2 &gt; where si and ti are"
P12-2057,C08-1144,0,0.100555,"amework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estima"
P13-1074,W02-1001,0,0.511051,"Missing"
P13-1074,I11-1136,0,0.0400969,"unity, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways: 1). The former usually works on a well-formed single sentence while the latter needs to process multiple sentences that are very lengthy. 2). POS tags are must-have features to parsing while punctuations are not. The parsing quality in the former is more"
P13-1074,P05-1056,0,0.268622,"998; Nakamura, 2009). This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs. The punctuation prediction problem has attracted research interest in both the speech processing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorpor"
P13-1074,D10-1018,0,0.442686,"odels such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorporate syntactic features to broaden the view of hypotheses in the punctuation prediction models (Roark et al., 2006; Favre et al., 2008). In their methods, the punctuation prediction is treated as a separated post-procedure of parsing, which may suffer from the problem of error propagation. In addition, these approaches are not able to incrementally process inputs and are not efficient for very long inputs, especially in the cases of long transcribed speech texts from presentations where the number of streaming words c"
P13-1074,de-marneffe-etal-2006-generating,0,0.150074,"Missing"
P13-1074,2006.iwslt-papers.1,0,0.0606999,"9). This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs. The punctuation prediction problem has attracted research interest in both the speech processing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorporate syntactic features"
P13-1074,W03-3017,0,0.142399,"Missing"
P13-1074,C04-1010,0,0.0570201,"Missing"
P13-1074,P11-2033,0,0.548841,"r short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations. Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways"
P13-1074,D08-1059,0,0.271651,"speech text is exponentially complex, their approaches are only feasible for short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations. Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these"
P13-1074,D12-1133,0,\N,Missing
P13-1074,2009.iwslt-evaluation.1,0,\N,Missing
P13-2061,W06-1607,0,0.246555,"methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance s"
P13-2061,2012.amta-papers.7,0,0.0835577,"Missing"
P13-2061,P09-1098,1,0.833027,"test web data web data web data Table 1: Development and testing data used in the experiments. are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation. 2.5 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. based on the iterative computation in the Section 2.3. Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , r"
P13-2061,N03-1017,0,0.122322,"ften lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Senten"
P13-2061,W04-3250,0,0.270408,"Missing"
P13-2061,J05-4003,0,0.126612,"Missing"
P13-2061,J04-4002,0,0.0798821,"phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Sentence Pair Vertices Gra"
P13-2061,P03-1021,0,0.159372,"Missing"
P13-2061,P02-1040,0,0.0949147,"Missing"
P13-2061,J03-3002,0,0.192071,"Missing"
P13-2061,P06-1062,1,0.888295,"Missing"
P13-2061,P07-1070,0,0.162098,"s are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. The quality of bilingual data is a key factor in"
P13-2061,J97-3002,0,0.279256,"utlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , rij i and hpj , rij i. The pairs with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generr ated in the format hsi , P ij rkj · v(pj )i and hpj , P Experiments A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: Integration into translation modeling After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(si )) are obtained. Instead of simple filtering, we use the 1 Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 baseline (Wuebker et al., 2010) -0.25M -"
P13-2061,P10-1049,0,0.0134939,"me low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence p"
P13-2061,P06-1066,0,0.0731442,"Missing"
P13-2061,W04-3252,0,\N,Missing
P13-2061,J03-1002,0,\N,Missing
P14-1013,W06-1607,0,0.0295023,"The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. We i"
P14-1013,P05-1048,0,0.0187263,"vel. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Balt"
P14-1013,C08-1041,0,0.130559,"ent level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the"
P14-1013,2007.mtsummit-papers.11,0,0.0175661,"uation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, J"
P14-1013,P13-1126,0,0.0442344,"Missing"
P14-1013,W04-3250,0,0.0266605,"language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method"
P14-1013,J07-2003,0,0.732936,"tributions while topic-specific rules have sharper distributions. A standard entropy metric is used to measure the sensitivity of the source-side of hα, γi as: The model minimizes the pairwise ranking loss across all training instances: X L(f, e) (6) L= Sen(α) = − hf,ei We incorporate the learned topic similarity scores into the standard log-linear framework for SMT. When a synchronous rule hα, γi is extracted from a sentence pair hf, ei, a triple instance I = (hα, γi, hf, ei, c) is collected for inferring the topic representation of hα, γi, where c is the count of rule occurrence. Following (Chiang, 2007), we give a count of one for each phrase pair occurrence and a fractional count for each hierarchical phrase pair. The topic representation of hα, γi is then calculated as the weighted average: P (hα,γi,hf,ei,c)∈T {c × zf } zα = P (7) (hα,γi,hf,ei,c)∈T {c} zγ = (hα,γi,hf,ei,c)∈T P {c × ze } (hα,γi,hf,ei,c)∈T {c} eˆ = arg max P (e|f ) where the translation probability is given by: X P (e|f ) ∝ wi · log φi (f, e) i = Sim(zs , zγ ) = cos(zs , zγ ) (10) X | wj · log φj (f, e) + j {z Standard } X | k wk · log φk (f, e) {z Topic related (13) where φj (f, e) is the standard feature function and wj is"
P14-1013,D13-1107,1,0.837725,"hieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Related Work Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topic140 Sim(Src) Sim(Trg) Sim(Src+Trg) Sim(Src+Trg)+Sen(Src) Sim(Src+Trg)+Sen(Trg) Sim(Src+Trg)+Sen(Src+Trg) 42.51 42.43 42"
P14-1013,D08-1010,0,0.211625,"the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This ma"
P14-1013,P08-1114,0,0.0702404,"though we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches i"
P14-1013,P03-1021,0,0.0503972,"tions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the"
P14-1013,P12-1079,0,0.66379,"s are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Mar"
P14-1013,P02-1040,0,0.0916175,"News Weblog Total Chinese Docs Words 5.7M 5.4B 2.1M 8B 7.8M 13.4B using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method i"
P14-1013,P09-1036,0,0.0149151,"y LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches inefficient when appli"
P14-1013,P05-1044,0,0.0214738,"ss consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜ x) to a V -dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜ x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜ x)), x) = kh(g(˜ x)) − xk2 Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence. Inspired by the contrastive estimation method (Smith and Eisner, 2005), for each parallel sentence pair hf, ei as a positive instance, we select another sentence pair hf 0 , e0 i from the training data and treat hf, e0 i as a negative instance. To make the similarity of the positive instance larger than the negative instance by some margin η, we utilize the following pairwise ranking loss: (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function is calculated and back-propagated to the previous layer to update its parameters. Training neural networks involves many factors"
P14-1013,P06-2124,0,0.0322586,"a. By associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling t"
P14-1013,D11-1014,0,0.0446933,"Missing"
P14-1013,P12-1048,0,0.225478,"T Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of pa"
P15-1048,J92-4003,0,0.16594,"inear model for both models (6) and (7) to score a parsing tree as: X Score(T ) = φ(action) · ~λ action Where φ(action) is the feature vector extracted from partial hypothesis T for a certain action and ~λ is the weight vector. φ(action) · ~λ calculates the score of a certain transition action. The score of a parsing tree T is the sum of action scores. In addition to the basic features introduced in (Zhang and Nivre, 2011) that are defined over bag of words and POS-tags as well as tree-based context, our models also integrate three classes of new features combined with Brown cluster features (Brown et al., 1992) that relate to the rightto-left transition-based parsing procedure as detailed below. Simple repetition function • δI (a, b): A logic function which indicates whether a and b are identical. great X Syntax-based repetition function Figure 3: An example of UT model, where ‘N’ means the word is a fluent word and ‘X’ means it is disfluent. Words with italic font are Reparandums. 3.3 Training and decoding • δL (a, b): A logic function which indicates whether a is a left child of b. • δR (a, b): A logic function which indicates whether a is a right child of b. A binary classifier transition-based m"
P15-1048,N01-1016,0,0.830252,"Missing"
P15-1048,W02-1001,0,0.136275,", p0 ); unigrams δL (w0 , ws );δL (p0 , ps ); δR (w0 , ws );δR (p0 , ps ); NI (w0 , ws );NI (p0 , ps ); N# (w0..2 , ws );N# (p0..2 , ps ); Function δI (ws , w0 )δI (ps , p0 ); bigrams δL (w0 , ws )δL (p0 , ps ); δR (w0 , ws )δR (p0 , ps ); NI (w0 , ws )NI (p0 , ps ); N# (w0..2 , ws )N# (p0..2 , ps ); δI (ws , w0 )ws c; δI (ws , w0 )w0 c; Function ws w0 δI (ws , w0 ); trigrams ws w0 δI (ps , p0 ); Table 1: Feature templates designed for disfluency detection and dependency parsing. Similar to the work in (Zhang and Clark, 2008; Zhang and Nivre, 2011), we train our models by averaged perceptron (Collins, 2002). In decoding, beam search is performed to get the optimal parsing tree as well as the tag sequence. 4 4.1 Experiments Experimental setup Our training data is the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) corpus, which consists of telephone conversations about assigned topics. As not all the Switchboard data has syntactic bracketing, we only use the subcorpus of PAESED/MRG/SWBD. Following the experiment settings in (Charniak and Johnson, 2001), the training subcorpus contains directories 2 and 3 in PAESED/MRG/SWBD and directory 4 is split into test and development"
P15-1048,de-marneffe-etal-2006-generating,0,0.019972,"Missing"
P15-1048,P08-2027,0,0.0244163,"am-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. Miller and Schuler (2008) used a right corner transform of syntax trees to produce a syntactic tree with speech repairs. But their performance was not as good as labeling models. There exist two methods published recently which are similar to ours. Rasooli and Tetreault (2013) designed a joint model for both disfluency detection and dependency parsing. They regarded the two tasks as a two step classifications. Honnibal and Johnson (2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. They achieved the state-of-theart performance on both disfluency detection and p"
P15-1048,N09-2028,0,0.381295,"“you know”) and repairs. To identify and remove disfluencies, straightforward rules can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguis"
P15-1048,N13-1102,0,0.883224,"repairs. To identify and remove disfluencies, straightforward rules can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal"
P15-1048,Q14-1011,0,0.390168,"u, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods can jointly perform dependency parsing and disfluency detection. But in these methods, great efforts are made to distinguish normal words from disfluent words as decisions cannot be made imminently from left to right, leading to inefficient implementation as well as performance loss. In this paper, we propose detecting disfluencies using a right-to-left transition-based dependency parsing (R2L parsing), where the words are consumed from right to left to build the parsing tree based on which the current word is predicted to be either disfluent or no"
P15-1048,D13-1013,0,0.553101,"Missing"
P15-1048,C14-1138,0,0.0180684,"ure work, we will try to add new classes of features to further improve performance by capturing the property of disfluencies. We would also like to make an end-to-end MT test over transcribed speech texts with disfluencies removed based on the method proposed in this paper. Recently, the max-margin markov networks (M3 N) based model has achieved great improvement in this task. Qian and Liu (2013) presented a multi-step learning method using weighted M3 N model for disfluency detection. They showed that M3 N model outperformed many other labeling models such as CRF model. Following this work, Wang et al. (2014) used a beam-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disf"
P15-1048,D08-1059,0,0.215322,"RightArc : Links the front of the queue to the top of the stack and, removes the front of the queue and pushes it to the stack. The choice of each transition action during parsing is scored by a generalized perceptron (Collins, 496 2002) which can be trained over a rich set of nonlocal features. In decoding, beam search is performed to search the optimal sequence of transition actions. As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2 ∗ N , where N is the length of the sentence. Transition-based dependency parsing (Zhang and Clark, 2008) can be performed in either a leftto-right or a right-to-left way, both of which have a performance that is comparable as illustrated in Section 4. However, when they are applied to disfluency detection, their behaviors are very different due to the disfluency structure constraint. We prove that right-to-left transition-based parsing is more efficient than left-to-right transition-based parsing for disfluency detection. 3 As both the dependency tree and the disfluency tags are generated word by word, we decompose formula (3) into: (D∗ , T ∗ ) = argmax(D,T ) i=1 × P (di |W1i , T1i ) (5) n Y (D∗"
P15-1048,H05-1030,0,0.0216575,"es can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods"
P15-1048,P11-2033,0,0.292641,"ord “did” is obviously disfluent. Unlike UT model, the BCT will not link the word “did” to any word. Instead only a virtual link will add it to the virtual root. 3.4 In practice, we use the same linear model for both models (6) and (7) to score a parsing tree as: X Score(T ) = φ(action) · ~λ action Where φ(action) is the feature vector extracted from partial hypothesis T for a certain action and ~λ is the weight vector. φ(action) · ~λ calculates the score of a certain transition action. The score of a parsing tree T is the sum of action scores. In addition to the basic features introduced in (Zhang and Nivre, 2011) that are defined over bag of words and POS-tags as well as tree-based context, our models also integrate three classes of new features combined with Brown cluster features (Brown et al., 1992) that relate to the rightto-left transition-based parsing procedure as detailed below. Simple repetition function • δI (a, b): A logic function which indicates whether a and b are identical. great X Syntax-based repetition function Figure 3: An example of UT model, where ‘N’ means the word is a fluent word and ‘X’ means it is disfluent. Words with italic font are Reparandums. 3.3 Training and decoding •"
P15-1048,N06-2019,0,0.843419,"d that M3 N model outperformed many other labeling models such as CRF model. Following this work, Wang et al. (2014) used a beam-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. Miller and Schuler (2008) used a right corner transform of syntax trees to produce a syntactic tree with speech repairs. But their performance was not as good as labeling models. There exist two methods published recently which are similar to ours. Rasooli and Tetreault (2013) designed a joint model for both disfluency detection and dependency parsing. They regarded the two tasks as a two step classifications. Honnibal and Johnson (2014) presented a new joint model by extending the original tr"
P15-1048,P06-1071,0,0.023487,"to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods can jointly perform d"
P15-1048,P13-1074,1,0.851961,"i is the partial tree after word wi is consumed, di is the disfluency tag of wi . We simplify the joint optimization in a cascaded way with two different forms (5) and (6). n Y ∗ ∗ P (T1i |W1i , T1i−1 ) (D , T ) = argmax(D,T ) Model ∗ P (di , T1i |W1i , T1i−1 ) i=1 Our method 3.1 n Y argmaxD P (D1n |W1n ) Here, P (T1i |.) is the parsing model, and P (di |.) is the disfluency model used to predict the disluency tags on condition of the contexts of partial trees that have been built. In (5), the parsing model is calculated first, followed by the calculation of the disfluency model. Inspired by (Zhang et al., 2013), we associate the disfluency tags to the transition actions so that the calculation of P (di |W1i , T1i ) can be omitted as di can be inferred from the partial tree T1i . We then get ∗ ∗ (D , T ) = argmax(D,T ) n Y P (di , T1i |W1i , T1i−1 ) i=1 (1) The dependency parsing tree is introduced into model (1) to guide detection. The rewritten formula is shown below: X D∗ = argmaxD P (D1n , T |W1n ) (2) (7) Where the parsing and disfluency detection are unified into one model. We refer to this model as the Unified Transition(UT) model. While in (6), the disfluency model is calculated first, follow"
P15-1048,P11-1071,0,0.224498,"ork In practice, disfluency detection has been extensively studied in both speech processing field and natural language processing field. Noisy channel models have been widely used in the past to detect 501 previous work but also achieve significantly higher performance on disfluency detection as well as dependency parsing. disfluencies. Johnson and Charniak (2004) proposed a TAG-based noisy channel model where the TAG model was used to find rough copies. Thereafter, a language model and MaxEnt reranker were added to the noisy channel model by Johnson et al. (2004). Following their framework, Zwarts and Johnson (2011) extended this model using minimal expected f-loss oriented nbest reranking with additional corpus for language model training. 6 Conclusion and Future Work In this paper, we propose a novel approach for disfluency detection. Our models jointly perform parsing and disfluency detection from right to left by integrating a rich set of disfluency features which can yield parsing structure and difluency tags at the same time with linear complexity. The algorithm is easy to implement without complicated backtrack operations. Experiential results show that our approach outperforms the baselines on th"
P15-1048,J93-2004,0,0.0501048,"(p0 , ps ); NI (w0 , ws )NI (p0 , ps ); N# (w0..2 , ws )N# (p0..2 , ps ); δI (ws , w0 )ws c; δI (ws , w0 )w0 c; Function ws w0 δI (ws , w0 ); trigrams ws w0 δI (ps , p0 ); Table 1: Feature templates designed for disfluency detection and dependency parsing. Similar to the work in (Zhang and Clark, 2008; Zhang and Nivre, 2011), we train our models by averaged perceptron (Collins, 2002). In decoding, beam search is performed to get the optimal parsing tree as well as the tag sequence. 4 4.1 Experiments Experimental setup Our training data is the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) corpus, which consists of telephone conversations about assigned topics. As not all the Switchboard data has syntactic bracketing, we only use the subcorpus of PAESED/MRG/SWBD. Following the experiment settings in (Charniak and Johnson, 2001), the training subcorpus contains directories 2 and 3 in PAESED/MRG/SWBD and directory 4 is split into test and development sets. We use the Stanford dependency converter (De Marneffe 4.2 4.2.1 Experimental results Performance of disfluency detection on English Swtichboard corpus The evaluation results of both disfluency detection and parsing accuracy are"
P15-1048,P05-1012,0,0.235242,"Missing"
P15-1048,P04-1005,0,\N,Missing
P17-1065,P05-1033,0,0.559021,"Missing"
P17-1065,C16-2064,0,0.0395317,"Missing"
P17-1065,W16-4616,0,0.0282882,"Ensemble) RNNsearch SD-NMT BLEU 18.72 18.45 20.36 22.86 24.71 26.22 23.50 25.93 RIBES 0.6511 0.6451 0.6782 0.7508 0.7566 0.7459 0.7540 System Description Moses’ Hierarchical Phrase-based SMT Moses’ Phrase-based SMT Moses’ String-to-Tree Syntax-based SMT Single-layer NMT model without ensemble Self-ensemble of 2-layer NMT model Ensemble of 4 single-layer NMT models Single-layer NMT model Single-layer SD-NMT model Table 2: Evaluation results on Japanese-English translation task. model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL. Cromieres et al., 2016) that are the competitive NMT results on WAT 2016. According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results. The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016). Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016). We believe SD"
P17-1065,P15-1033,0,0.0124,"Missing"
P17-1065,N16-1024,0,0.224787,"ve two RNNs in our model, the termination condition is also different from a conventional NMT model. In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced. Figure 3 (a) gives an overview of our SD-NMT model. Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model 3.1 (10) Syntactic Context for Target Word Prediction Syntax has been proven useful for sentence generation task (Dyer et al., 2016). We propose to leverage target syntax to help translation generation. In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current 701 stamp for each training instance. Thus it is easy for the model to process multiple trees in one batch. In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the"
P17-1065,W16-4610,0,0.0122509,"n has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testsets. SMT and RNNsearch refer to the translation resu"
P17-1065,P16-1078,0,0.228746,", Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The translation fragment in italic is locally fluent around the word is, but from a"
P17-1065,W04-0308,0,0.0351929,"Machine Translation (SD-NMT) model in our paper. An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees. The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004). P (Y |X) = n Y j=1 P (yj |y<j , H) (1) Typically, for the jth target word, the probability P (yj |y<j , H) is computed as P (yj |y<j , H) = g(sj , yj−1 , cj ) (2) where g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state. The context cj is calculated at each timestamp j based on H by the attention network cj = m X ajk hk (3) k=1 ajk = exp(ejk ) Pm i=1 exp(eji ) ejk = vaT tanh(Wa sj−1 + Ua hk ) (4) (5) where va , Wa , Ua are the weight matrices. The attention mechanism is effective to model the correspondences between source and target. 699 2.2 Depend"
P17-1065,P06-1121,0,0.0266136,"Missing"
P17-1065,P02-1040,0,0.128454,"Missing"
P17-1065,P08-1066,0,0.0170142,"slation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby. Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees. In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is. This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008). On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models: Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation"
P17-1065,D10-1092,0,0.0354514,"Missing"
P17-1065,P16-1159,0,0.0688833,"e-to-Dependency Neural Machine Translation Shuangzhi Wu†∗, Dongdong Zhang‡ , Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The"
P17-1065,P15-1001,0,0.0266306,"Missing"
P17-1065,P16-1008,0,0.0188276,"the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada, July 30 - August 4"
P17-1065,1983.tc-1.13,0,0.455856,"Missing"
P17-1065,W04-3250,0,0.020876,"Missing"
P17-1065,P06-1077,0,0.341955,"Missing"
P17-1065,D15-1166,0,0.116367,"NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada,"
P17-1065,D16-1050,0,0.0111325,"corporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testse"
P17-1065,P15-1002,0,0.0265242,"Missing"
P17-1065,D08-1059,0,0.0126196,"uence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree, w0r and w1r their rightmost modifiers respectively. We define two unigram features and four bigram features. The unigram features are w0 and w1 which are represented by the word embedding vectors. The bigram features are w0 w0l , w0 w0r , w1 w1l and w1 w1r . Each of them is computed by bhc = tanh(Wb Ewh + Ub Ewhc ), h ∈ {0, 1}, c ∈ {l, r}. These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008). Based on these features, the syntactic context vector Kj is computed as l score = j=1 l 1X δ(SH, aj ) log P (ˆ yj |ˆ y<j , X, A≤j ) n where n is word sequence length and l is action sequence length. 4 where Wk , Uk , Wb , Ub are the weight matrices, E stands for the embedding matrix. Figure 2 (b) gives an overview of the construction of Kj . Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation. Adding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and"
P17-1065,P11-2033,0,\N,Missing
P17-1065,P16-5005,0,\N,Missing
W08-0301,W05-0909,0,0.023308,"Missing"
W08-0301,J07-2003,0,0.506228,"s have, in general, lower scores, and 2 Literature Review Research work in SMT seldom treats SWD as a problem separated from other factors in translation. However, it can be found in different SMT paradigms the mechanism of handling SWD. As to the pioneering IBM word-based SMT models (Brown et al., 1990), IBM models 3, 4 and 5 handle spurious source words by considering them as corresponding to a particular EMPTY word token on the English side, and by the fertility model which allows the English EMPTY to generate a certain number of foreign words. As to the hierarchical phrase-based approach (Chiang, 2007), its hierarchical rules are more powerful in SWD than the phrase pairs 2 Model 1 Model 2 Model 3 ˜ >, its probability is < F˜ , E P () P (|f ) PCRF (|F~ (f ) ( ˜ F˜ ) = P (E| Table 1: Summary of the Three SWD Models ˜ = () P () if E ˜ ˜ F˜ ) otherwise P (¯ )|F |PT (E| ˜ F˜ ) is the probability of the phrase where PT (E| pair as registered in the translation table, and |F˜ | is the length of the phrase F˜ . The estimation of P () is done by MLE: therefore the decoder has a bias towards shorter translations. Word penalty (in fact, it should be renamed as word reward) is used to neutraliz"
W08-0301,P03-1054,0,0.00813748,"d for training language model. The development/test corpora are based on the test sets for NIST MT-2005/6. The alignment matrices of the training data are produced by the GIZA++ (Och and Ney, 2000b) word alignment package with its default settings. The subsequent construction of translation table was done in exactly the same way as explained 1 Maximum Entropy was also tried in our experiments but its performance is not as good as CRF. 4 Data FBIS BFT NIST in (Koehn et al., 2003). For SWD model 2, the phrase enumeration step is modified as described in section 3.2. We used the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar for its POS-tagging as well as finding the head/dependent words of all source words. The CRF toolkit used for model 3 is CRF ++ 2 . The training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) f"
W08-0301,N03-1017,0,0.0738606,"Missing"
W08-0301,C00-2163,0,0.0999851,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P00-1056,0,0.184679,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P02-1040,0,0.0759968,"performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 as test variation is to test each model when medium size of data are available.4 NIST All the sections of the NIST training set are used. The purpose of this variation is to test each model when a large amount of data are available. (Case-insensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. In each test in our experiments, maximum BLEU training were run 10 times, and thus there are 10 BLEU scores for the test set. In the following we will report the mean scores only. 4.2 Experiment Results and Analysis Table 2 shows the results of the first experiment, which uses the NIST MT-2005 test set as development data and the NIST MT-2006 test set as test data. The most obvious observation is that any SWD model achieves much higher BLEU score than the baseline, as there is at least 1.6 BLEU point improvement in each case, and in some case the improvement"
W08-0301,N03-1028,0,0.0112237,"he source language sentence: each source word is tagged either as “spurious” or “non-spurious”. Under such a perspective, SWD 3 model 2 is merely a unigram tagging model, and it uses only one feature template, viz. the lexical form of the source word in hand. Such a model can by no means encode any contextual information, and therefore it cannot handle the “ACCORDING - TO /dd NP EXPRESS /dd” example in section 1. An obvious solution to this limitation is a more powerful tagging model augmented with contextsensitive feature templates. Inspired by research work like (Lafferty et al., 2001) and (Sha and Pereira, 2003), our SWD model 3 uses first-order Conditional Random Field (CRF) to tackle the tagging task.1 The CRF model uses the following feature templates: The training data for the CRF model comprises the alignment matrices of the bilingual training data for the MT system. A source word (token) in the training data is tagged as “non-spurious” if it is aligned to some target word(s), otherwise it is tagged as “spurious”. The sentences in the training data are also POS-tagged and parsed by some dependency parser, so that each word can be assigned values for the POS-based feature templates as well as the"
W08-0301,D07-1056,1,0.849556,"training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) for CRF training. The decoder in the experiments is our reimplementation of HIERO (Chiang, 2007), augmented with a 5-gram language model and a reordering model based on (Zhang et al., 2007). Note that no hierarchical rule is used with the decoder; the phrase pairs used are still those used in conventional phrase-based SMT. Note also that the decoder does not translate OOV at all even in the baseline case, and thus the SWD models do not improve performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 a"
W08-0301,J90-2002,0,\N,Missing
