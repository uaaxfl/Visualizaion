2011.mtsummit-papers.31,W09-0432,0,0.376054,"he target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific TM. Our work is close to the monolingual scenario. Provided with a monolingual in-domain corpus, we adapt our background MT into the one suitable for translating medical summaries. There are some major differences among our work and those proposed previously. First, we identify and translate significant patterns from in-domain corpus and introduce them into our SMT system. Instead, the related works exploited the entire in-domain training data to adapt the existing LM or TM by model mixture and parameter"
2011.mtsummit-papers.31,P05-1033,0,0.0448033,"Missing"
2011.mtsummit-papers.31,W07-0722,0,0.141891,"been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific"
2011.mtsummit-papers.31,W07-0717,0,0.0317757,"nguistic patterns. In recent years, training SMT system from large bilingual data has been a common practice. The parallel corpus used to train an MT system mainly comes from fixed domains such as parliamentary and news articles. The bilingual resources for a specific language pair or a specific domain usually come in small size, even unavailable. One of the challenging issues in a cross-domain MT applica278 tion is to realize an in-domain MT model in such a resource-poor environment. Depending on what kinds of in-domain resources are at hand, various adaptation techniques have been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work,"
2011.mtsummit-papers.31,D10-1044,0,0.0601468,"ng issues in a cross-domain MT applica278 tion is to realize an in-domain MT model in such a resource-poor environment. Depending on what kinds of in-domain resources are at hand, various adaptation techniques have been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-"
2011.mtsummit-papers.31,1985.tmi-1.10,0,0.676381,"Missing"
2011.mtsummit-papers.31,P03-1054,0,0.00713775,"rd in favor of our general purpose parser. For instance, we replace a complicated diagnosis ""primary biliary cirrhosis"" with the simpler one ""disease"". In this way, we reduce not only the OOV words, but also the length of sentences, and thereby facilitate the parsing procedure. For each extracted pattern, we select m distinct sentences in which it occurs. These sentences are then analyzed by a parser and m parsing trees are produced. The pattern is considered as a significant candidate, if it is a syntactic constituent in any one of these parsing trees. In this paper we apply Stanford Parser (Klein and Manning, 2003) and set m to 10 in consideration of the parsing speed. 4.4 Pattern Coverage Finding and Filtering Coverage Relation 1+4 2+3 3+2 4+1 Examples she is admitted for SURGERY she is admitted for SURGERY Lab data showed no DIAGNOSIS Lab data showed no DIAGNOSIS TEST on DATE showed DIAGNOSIS TEST on DATE showed DIAGNOSIS Past Surgical History : SURGERY Past Surgical History : SURGERY Table 1. Four kinds of coverage relations for 5-grams. most representative pattern may imply the translations of the others in the same cluster. An example of a cluster of similar patterns is illustrated below: The invol"
2011.mtsummit-papers.31,N03-1017,0,0.0165064,"e examine each sentence from left to right and adopt a longest-first strategy to replace medical terms with classes. In this way, a set of medical summaries are transformed into a new corpus. 4.2 Highly Frequent Pattern Identification To address the domain adaptation problem in MT, we extract patterns from an in-domain corpus to capture domain specific writing styles. These patterns are translated and will be applied in the runtime translation. Accordingly, we prefer the 280 format of patterns that is easy to be integrated into an SMT system for our target application. The phrase-based model (Koehn et al., 2003; Koehn, 2004) is one of the state of the art TMs, in terms of both accuracy and speed. The phrasebased system translates source phrases into target ones with phrase table, which consists of bilingual phrases and feature functions. Since a phrase (i.e., a string of consecutive words) is served as the basic unit of translation, integrating n-gram based patterns into the background phrase-based SMT system is a natural choice. We enumerate all n-grams from sentences of our in-domain corpus that contains words and medical classes. In this way, two kinds of patterns are extracted: (1) class pattern"
2011.mtsummit-papers.31,koen-2004-pharaoh,0,0.148043,"nce from left to right and adopt a longest-first strategy to replace medical terms with classes. In this way, a set of medical summaries are transformed into a new corpus. 4.2 Highly Frequent Pattern Identification To address the domain adaptation problem in MT, we extract patterns from an in-domain corpus to capture domain specific writing styles. These patterns are translated and will be applied in the runtime translation. Accordingly, we prefer the 280 format of patterns that is easy to be integrated into an SMT system for our target application. The phrase-based model (Koehn et al., 2003; Koehn, 2004) is one of the state of the art TMs, in terms of both accuracy and speed. The phrasebased system translates source phrases into target ones with phrase table, which consists of bilingual phrases and feature functions. Since a phrase (i.e., a string of consecutive words) is served as the basic unit of translation, integrating n-gram based patterns into the background phrase-based SMT system is a natural choice. We enumerate all n-grams from sentences of our in-domain corpus that contains words and medical classes. In this way, two kinds of patterns are extracted: (1) class patterns that contain"
2011.mtsummit-papers.31,P07-2045,0,0.00788532,"cal summaries. Google Translate reports its Chinese translation as ""ཽՑ ਢԫଡཬԵՂച۩ 2009  ڣ10 ִ 9 ֲ"". Compared to the Chinese reference translation "" ڇ2009  ڣ10 ִ 9 ֲച۩Գՠۨጥᆜ"", there are several translation and reordering errors. In this paper, we develop an English-Chinese medical summary translation system to tackle this problem. In an SMT model, an English-Chinese parallel corpus is indispensable for training the translation model (TM). However, only English medical summary corpus is available in this domain. Here, we first develop a general EnglishChinese SMT system with Moses toolkit (Koehn et al., 2007) and a general English-Chinese parallel corpus. Next, we adapt this SMT system with patterns learned from an English medical summary corpus. The problem is that these patterns are still monolingual. It is necessary to have domain expert involved in setting up bilingual patterns. The cost of domain experts is the major concerns. Therefore, to identify significant patterns from a monolingual in-domain corpus, to find their coverage relationships, to decide which patterns should be translated by experts, and to introduce these patterns to the general MT system are research issues in this paper. S"
2011.mtsummit-papers.31,P06-1066,0,0.248102,"Missing"
2011.mtsummit-papers.31,C04-1059,0,0.055875,"in development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific TM. Our work is close to the monolingual scenario. Provided with a monolingual in-domain corpus, we adapt our background MT into the one suitable for translating medical summaries. There are some major differences among our work and those proposed previously. First, we identify and translate significant patterns"
2020.acl-main.13,P16-1139,0,0.0275195,"Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhetorical relation recognition. Given a paragraph, our parser extracts all EDUs, builds the tree structure, identifies the nucleuses, and recognizes the rhetorical relations of all internal nodes. We propose a revised dynamic-oracle procedure (Yu et al., 2018) for trai"
2020.acl-main.13,W01-1605,0,0.740536,"ion Discourse parsing is one of the fundamental tasks in natural language processing (NLP). Typical types of discourse parsing include hierarchical discourse parsing and shallow discourse parsing. The former is aimed at finding the relationships among a series of neighboring elementary discourse units (EDUs) and further building up a hierarchical tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discours"
2020.acl-main.13,D14-1224,0,0.0188203,"ring elementary discourse units (EDUs) and further building up a hierarchical tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-inter"
2020.acl-main.13,C18-2016,1,0.917828,"in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhet"
2020.acl-main.13,J18-2001,0,0.0310017,"Missing"
2020.acl-main.13,N18-1202,0,0.0148654,"2020. 2020 Association for Computational Linguistics 3. We release the pre-trained, standalone, readyto-use parser as a resource for the research community.1 2 Converter Methodology Sense , Center Figure 1 gives an overview of our parser. Five stages are performed to transform a raw document into a parse tree: EDU segmentation, tree structure construction, rhetorical relation and nuclearity classification, binary tree conversion, and beam search. Reduce Classifier queue stack 2.1 Elementary Discourse Unit Segmentation Typically, EDU segmentation is a sequence labeling task (Wang et al., 2018; Peters et al., 2018). We propose a model for labeling each Chinese character in a raw document. The Begin-Inside scheme is employed that the word beginning with a new EDU will be labeled as B, and the rest of the words will be labeled as I. Our model is based on the pretrained text encoder BERT (Devlin et al., 2018). More specifically, we adopt the version BERT-base, Chinese since this is the only pre-trained BERT dedicated to Chinese so far. As the BERT for Chinese is character-based, we feed each Chinese character into a BERT layer to obtain its contextual embedding. Then, we fine tune the representation with a"
2020.acl-main.13,P16-1009,0,0.0247011,"Given a span of text x, our main model P (·) predicts the rhetorical relation yc . Eq. 2 shows the additional consistency loss to enforce the smoothness of our main model, and x ˆ stands for the augmented unlabeled sentence pair. L and U stand for labeled data and unlabeled data, respectively. As shown in Eq. 3, we train both objectives at the same time with a weight λ to adjust the effect of UDA. N M 1 XX H=− yc log (P (yc |x)) (1) N x∈L c=1   N 1 X P (y|x) DKL = − P (y|x) log (2) N P (y|ˆ x) proaches to paraphrasing can be employed. In this work, we utilize the back-translation strategy (Sennrich et al., 2016), where we translate the Chinese sentence pair to English and then translate back to Chinese. This is equivalent to add noises to the original inputs. As the original and the backtranslated sentence pairs express the same meaning, our model is expected to predict the same label for both pairs. By minimizing the consistency loss, our model can behave consistently no matter whether an original instance or its paraphrases are given. In this way, the model can be more generalized and robust. Besides, when our model is able to predict the same label for both sentence pairs, it means that our model"
2020.acl-main.13,D18-1116,0,0.0346168,"Missing"
2020.acl-main.13,D16-1137,0,0.032053,"Missing"
2020.acl-main.13,C18-1047,0,0.368036,"NN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhetorical relation recognition. Given a paragraph, our parser extracts all EDUs, builds the tree structure, identifies the nucleuses, and recognizes the rhetorical relations of all internal nodes. We propose a revised dynamic-oracle procedure (Yu et al., 2018) for training the shift-reduce parser. Because of the limited training instances in CDTB-14, we also address the data sparsity issue by introducing unsupervised data augmentation (Xie et al., 2019). Experimental results show that our methodology is effective, and our model outperforms all the previous models. The contributions of this work are three-fold shown as follows. 1. We explore the task of Chinese discourse parsing with a variety of strategies, and our parser achieves the state-of-the-art performance. Our robust dynamic-oracle procedure can be applied to other shift-reduce parsers. 2."
2020.acl-main.13,P12-1008,0,0.0270232,"l tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, help"
2020.coling-main.199,P17-1074,0,0.158633,"ng in a missing punctuation mark. Each family of model is adept at correcting different kinds of errors. By integrating these two kinds of models using recycle generation, a wider range of errors can be effectively corrected for each round of correction. In this work, we also discuss the performance metrics of Chinese GEC. The Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) that has been extensively used for English GEC, and also for the NLPCC 2018 Chinese GEC task can only report overall model performance. This problem has been solved in English GEC, with the introduction of the ERRANT scorer (Bryant et al., 2017). The ERRANT scorer can provide model performance in terms of edit-level operation as well as specific English grammatical error types. We extend the idea of the ERRANT scorer to deal with Chinese sentences. This will allow Chinese GEC researchers to be able to get more detailed analysis of model performance. In summary, our contributions are threefold as follows. 1. We use a heterogeneous system composed of multiple kinds of models for Chinese GEC, beating the previous state-of-the-art results on the NLPCC 2018 task dataset. Combining multiple models that are designed to correct different kin"
2020.coling-main.199,W19-4406,0,0.0106568,"ntences. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatical and spelling errors that appear in a sentence. An example of Chinese GEC is correcting the word-choice error in the following sentence: 本人是在貴公司的一名實習。 (I am an internship at your company.) by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of thi"
2020.coling-main.199,I05-2023,0,0.0677592,"th edits. We then adapt the original sentence alignment code from ERRANT 8 to be able to align Chinese sentences. Edit Extraction The per-sentence edits (annotations) are extracted in three steps: tokenization, alignment, and merging. For tokenization, we use character-level tokenization. Alignment works by computing an alignment score for each pair of characters in the source and target sentence. Matching characters 8 https://github.com/chrisjbryant/errant 2198 give a score of zero. Insertion and deletion is each given a score of one. Substitution score is computed using a method similar to (Che et al., 2005), in which the Cilin (Mei et al., 1996) thesaurus is leveraged to give a similarity score between characters. Once the alignment score matrix is computed, the sequence with the lowest total score is returned. We use a simple merging strategy that merges consecutive sequences of edits of the same type. 5.2 Error-type Specific Performance The results of re-scoring our systems with our scorer is presented in Table 6. We can see some clear differences in the strengths and weaknesses of our models. The NMT-based Transformer is best at correcting general substitution and word-ordering errors. The se"
2020.coling-main.199,N12-1067,0,0.235333,"re capable of rewriting the entire sentence, making large scale corrections such as re-ordering or performing multi-word substitutions possible. In contrast, sequence editing models focus on smaller scale corrections, such as removing a word or adding in a missing punctuation mark. Each family of model is adept at correcting different kinds of errors. By integrating these two kinds of models using recycle generation, a wider range of errors can be effectively corrected for each round of correction. In this work, we also discuss the performance metrics of Chinese GEC. The Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) that has been extensively used for English GEC, and also for the NLPCC 2018 Chinese GEC task can only report overall model performance. This problem has been solved in English GEC, with the introduction of the ERRANT scorer (Bryant et al., 2017). The ERRANT scorer can provide model performance in terms of edit-level operation as well as specific English grammatical error types. We extend the idea of the ERRANT scorer to deal with Chinese sentences. This will allow Chinese GEC researchers to be able to get more detailed analysis of model performance. In summary, our contributions are threefold"
2020.coling-main.199,P19-1331,0,0.0287622,"h applying a fixed set of operations to the input. This can be formulated in the following way: given a fixed vocabulary of edit operations E and an input sequence x1:n = (x1 , · · · , xn ), a model learns to predict an edit operation ei ∈ E for each xi in our input sequence. A set of rules can then be applied to the output sequence e1:n to obtain the target output sequence y. 2192 Several sequence editing models have been proposed for text simplification tasks. The Levenshtein Transformer (Gu et al., 2019) performs text simplification by using a sequence of insertion and deletion operations. Dong et al. (2019) perform sequence editing through three primary edit operations, KEEP, ADD, and DELETE. Currently, the only sequence editing model to be applied to GEC is LaserTagger (Malmi et al., 2019). Similarly to the two previously cited works, LaserTagger learns to edit sentences by two different edit operations: KEEP and DELETE, along with pairing these operations with a limited phrase vocabulary consisting of tokens that are frequently changed between the source and target sequences. While LaserTagger performed well for English GEC considering the small number of training samples that it used, it was"
2020.coling-main.199,N15-1060,0,0.0175025,"ypes, we have to be able to calculate model performance specific to each of our four error types: redundant (R), missing (M), word selection (S), and word ordering (W). Unfortunately, the official Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) does not have this capability. It can only provide the overall score of the system. In order to rectify this situation, we develop a scorer that can report the precision, recall, and F0.5 score of our models with respect to our four error types. Another issue with the MaxMatch scorer worth mentioning is that it is known to overestimate model performance (Felice and Briscoe, 2015; Napoles et al., 2015). The problem of error-type specific performance evaluation has already been solved for English GEC. The ERRANT scorer (Bryant et al., 2017) provides very detailed performance results for English GEC systems, by reporting the precision, recall, and F0.5 scores with respect to over twenty-five different error categories. The ERRANT scorer works in two main steps, (1) edit extraction (annotation) and (2) scoring. The edit extraction step works in the following way. First, the source and target sentences are tokenized and the part-of-speech for each token is computed. Next,"
2020.coling-main.199,N18-2046,0,0.0151312,"the small number of training samples that it used, it was still very far from reaching state-of-the-art performance. In this work, we apply LaserTagger to Chinese GEC, and also explore combining it with NMT-based models. 2.3 Recycle Generation Recycle generation refers to the method of performing multiple rounds of correction on an input sentence. Recycle generation is also known as iterative decoding or multi-pass decoding. This has been attempted in English GEC in which one NMT-based model is used repeatedly (Lichtarge et al., 2018), or with a combination of a SMT-based and NMT-based model (Grundkiewicz and Junczys-Dowmunt, 2018). In Chinese GEC, only NMT-based recycle generation has been used (Qiu and Qu, 2019). In previous works, recycle generation has always been performed with models trained to do translation. In this work, we attempt to perform recycle generation with one model trained to do translation and another model trained to do sequence editing. 3 Methodology Our GEC system is composed of three separate components: a neural machine translation system, a sequence editing system, and a spell-checker. Each model performs one or several rounds of correction on the input sentence to produce the final corrected"
2020.coling-main.199,W19-4427,0,0.164942,"ion (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al., 2019; Kiyono et al., 2019). In this work, we aim to avoid this issue by combining several different models that perform corrections in different ways. Another challenge of GEC is that sentences can have multiple errors. Sometimes a model is not able to correct all of the errors present in a sentence in one pass, resulting in only a partial correction. One of the methods used to resolve this issue is recycle generation, also known as iterative decoding (Lichtarge et al., 2018). In this method, a system performs multiple iterations of correction on an erroneous sentence. This work is licensed under"
2020.coling-main.199,C16-1085,1,0.82935,") by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amoun"
2020.coling-main.199,D19-1119,0,0.121187,"recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al., 2019; Kiyono et al., 2019). In this work, we aim to avoid this issue by combining several different models that perform corrections in different ways. Another challenge of GEC is that sentences can have multiple errors. Sometimes a model is not able to correct all of the errors present in a sentence in one pass, resulting in only a partial correction. One of the methods used to resolve this issue is recycle generation, also known as iterative decoding (Lichtarge et al., 2018). In this method, a system performs multiple iterations of correction on an erroneous sentence. This work is licensed under a Creative Commons Att"
2020.coling-main.199,D19-1510,0,0.061409,"n ), a model learns to predict an edit operation ei ∈ E for each xi in our input sequence. A set of rules can then be applied to the output sequence e1:n to obtain the target output sequence y. 2192 Several sequence editing models have been proposed for text simplification tasks. The Levenshtein Transformer (Gu et al., 2019) performs text simplification by using a sequence of insertion and deletion operations. Dong et al. (2019) perform sequence editing through three primary edit operations, KEEP, ADD, and DELETE. Currently, the only sequence editing model to be applied to GEC is LaserTagger (Malmi et al., 2019). Similarly to the two previously cited works, LaserTagger learns to edit sentences by two different edit operations: KEEP and DELETE, along with pairing these operations with a limited phrase vocabulary consisting of tokens that are frequently changed between the source and target sequences. While LaserTagger performed well for English GEC considering the small number of training samples that it used, it was still very far from reaching state-of-the-art performance. In this work, we apply LaserTagger to Chinese GEC, and also explore combining it with NMT-based models. 2.3 Recycle Generation R"
2020.coling-main.199,P15-2097,0,0.0181601,"o calculate model performance specific to each of our four error types: redundant (R), missing (M), word selection (S), and word ordering (W). Unfortunately, the official Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) does not have this capability. It can only provide the overall score of the system. In order to rectify this situation, we develop a scorer that can report the precision, recall, and F0.5 score of our models with respect to our four error types. Another issue with the MaxMatch scorer worth mentioning is that it is known to overestimate model performance (Felice and Briscoe, 2015; Napoles et al., 2015). The problem of error-type specific performance evaluation has already been solved for English GEC. The ERRANT scorer (Bryant et al., 2017) provides very detailed performance results for English GEC systems, by reporting the precision, recall, and F0.5 scores with respect to over twenty-five different error categories. The ERRANT scorer works in two main steps, (1) edit extraction (annotation) and (2) scoring. The edit extraction step works in the following way. First, the source and target sentences are tokenized and the part-of-speech for each token is computed. Next, the resulting tokenize"
2020.coling-main.199,W14-1701,0,0.0269174,"he ERRANT scorer to be able to score Chinese sentences. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatical and spelling errors that appear in a sentence. An example of Chinese GEC is correcting the word-choice error in the following sentence: 本人是在貴公司的一名實習。 (I am an internship at your company.) by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected"
2020.coling-main.199,N19-4009,0,0.115516,"， 一@@ 男@@ 一@@ 女 Table 3: Example of preprocessing steps. 3.2 Neural Machine Translation Model Our NMT model, based on the Transformer architecture (Vaswani et al., 2017), is an encoder-decoder sequence to sequence model, where both the encoder and decoder are composed of six layers of selfattention modules. We use the “Transformer (big)” settings described in Vaswani et al. (2017). In general, we follow similar training steps as described in English state-of-the-art models (Kiyono et al., 2019; Grundkiewicz et al., 2019). Training Settings Our model is implemented using the Fairseq5 toolkit (Ott et al., 2019). Optimization is performed using the Adam (Kingma and Ba, 2014) optimizer, with criterion set to label-smoothed cross entropy (Szegedy et al., 2016). We use beta values of 0.9 and 0.98 for Adam, and a smoothing value of 0.1 for the criterion. We first set the learning rate to 10−7 and perform 4,000 warm-up updates. After the warm-up period, the learning rate is increased to 0.001. Thereafter we use an inverse square 2 https://github.com/fxsjy/jieba https://github.com/rsennrich/subword-nmt 4 https://github.com/BYVoid/OpenCC 5 https://github.com/pytorch/fairseq 3 2194 Figure 1: An example of La"
2020.coling-main.199,P16-1162,0,0.0310895,"n pairs. Since an official validation set is not provided, we randomly select 5,000 pairs from the training set to serve as a validation set. In addition to 1 https://lang-8.com/ 2193 samples from lang8, we also use monolingual WMT News data (Barrault et al., 2019) to form a training set for the language model that we use in our spell checker. All three models that we use require different preprocessing steps, due to several different reasons. The NMT-based model uses standard preprocessing steps: word-level tokenization followed by subword segmentation of rare words using byte pair encoding (Sennrich et al., 2016) to handle out of vocabulary words. Word-level segmentation is performed using Jieba.2 BPE is performed using subword-nmt,3 with the number of merge operations set to 35k and the vocabulary threshold set to 50. The language model cannot use subword units because the spelling check algorithm that we use requires looking up words in a dictionary. The sequence editing model has better results with character-level segmentation because the algorithm it uses to build its vocabulary is sensitive to any noise introduced by incorrect segmentation that often occurs in the erroneous source sentences. For"
2020.coling-main.199,W13-4406,0,0.0243482,"sulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al.,"
2020.coling-main.199,N19-1014,0,0.0190484,"gger’s small phrase vocabulary will not be able to be corrected by it. Currently, most word-order errors (W) are also not able to be corrected by LaserTagger. This is because LaserTagger lacks a method to re-arrange arbitrary subsequences of tokens inside of a sequence. The main advantage LaserTagger has over NMT models is that it is very easy for LaserTagger to copy or delete a token from the source sentence to the output sentence. This is very useful in GEC, as errors are usually limited to just a few words in the source sentence, so most words can be copied directly to the target sentence (Zhao et al., 2019). 6 Conclusion In this work, we propose a system for Chinese GEC that uses three different models: a NMT-based model, a sequence editing model, and a spell checker. We showed how these models can be composed using heterogeneous recycle generation into a system that achieves state of the art performance for Chinese GEC. Furthermore, we extended an automatic annotator and scorer for parallel error corpora to be able to handle Chinese sentences. We use this scorer to evaluate each models performance in terms of the four error types, and show how each model is adept at correcting different types o"
2020.fnp-1.11,D19-1238,0,0.046511,"Missing"
2020.fnp-1.11,W19-5510,0,0.0135367,"in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parser are not feasible. In order to deal with this issue, we formulate the cause-effect detection task as a sequence labeling problem and propose an approach using BIO scheme and Viterbi decoder. We find that the proposed approach has the ability to identify multiple ca"
2020.fnp-1.11,P00-1043,0,0.322526,"etection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parser are not feasible. In order to deal with this issue, we formulate the cause-effect detect"
2020.fnp-1.11,P14-5010,0,0.00244555,"ns are two-fold as follows. 1. We propose an approach to cause-effect detection for financial news that could better identify multiple causal events and event spans. 2. We release the code of the best-performing model for future research.1 2 Pre-processing We experiment on the FinCausal-2020 dataset (Mariko et al., 2020), which consists of two subtasks, including causal meanings detection (Task 1) and cause-effect detection (Task 2). The numbers of training instances are 22,058 and 1,750 for Task 1 and Task 2, respectively. This work only focuses on Task 2. We use the Stanford CoreNLP Stanza (Manning et al., 2014; Qi et al., 2020) toolkit2 to tokenize each sentence and generate the part-of-speech (POS) tag for each token. For the examples with multiple causal events, we recognize them by their indices and add a special number token before each example to treat them as different model inputs. As for causal relations tagging, we use “B, I, O” (Begin, Inside, and Outside) and “C, E” (Cause and Effect) labels to represent the positional information of the words and the semantic roles of the causal events. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
2020.fnp-1.11,2020.fnp-1.3,0,0.21222,"Missing"
2020.fnp-1.11,2020.acl-demos.14,0,0.0118515,"lows. 1. We propose an approach to cause-effect detection for financial news that could better identify multiple causal events and event spans. 2. We release the code of the best-performing model for future research.1 2 Pre-processing We experiment on the FinCausal-2020 dataset (Mariko et al., 2020), which consists of two subtasks, including causal meanings detection (Task 1) and cause-effect detection (Task 2). The numbers of training instances are 22,058 and 1,750 for Task 1 and Task 2, respectively. This work only focuses on Task 2. We use the Stanford CoreNLP Stanza (Manning et al., 2014; Qi et al., 2020) toolkit2 to tokenize each sentence and generate the part-of-speech (POS) tag for each token. For the examples with multiple causal events, we recognize them by their indices and add a special number token before each example to treat them as different model inputs. As for causal relations tagging, we use “B, I, O” (Begin, Inside, and Outside) and “C, E” (Cause and Effect) labels to represent the positional information of the words and the semantic roles of the causal events. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4"
2020.fnp-1.11,D16-1014,0,0.0570101,"the artificial intelligence research community. In this paper, we explore the causeeffect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parse"
2020.lrec-1.128,H91-1060,0,0.0230007,"ation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are always binary. Details of Chinese discourse parsing can be found in (Li et al., 2014b). The CDT annotates the hierarchical discourse structure of a given article, which is different from the PDTB-style scheme proposed by (Zhou and Xue, 2012). Although the annotation of the Chinese Discourse TreeBank (CDTB) is well-defined, the evaluation is divergent. Generally, PARSEVAL (Black et al., 1991) is used to evaluate the quality of a predicted parsing tree. For a golden standard discourse tree, we have a set of non-leaf nodes N = {n1 , n2 , ..., nk }. We also have a set of text spans T = {t1 , t2 , ..., tk } where ni dominates ti for all i (e.g., the node B in Figure 1 dominates the text spanning from EDU 1 to EDU 3, so if we use nj to represent node B, then tj should represent the text span from EDU 1 to 3). Similarly, for a predicted discourse parsing tree, we 0 0 0 0 have non-leaf nodes N = {n1 , n2 , ..., nh } and text spans E Coordination Equal B Causation Latter A C Coordination"
2020.lrec-1.128,N19-1423,0,0.0260207,"Missing"
2020.lrec-1.128,P14-1002,0,0.0235016,"y. For English, the most commonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse struct"
2020.lrec-1.128,P13-1048,0,0.0316548,"ructure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three"
2020.lrec-1.128,D14-1220,0,0.404605,"pproaches lead to significant differences in performance. Keywords: Discourse parsing, CKY, Shift-reduce, PARSEVAL 1. Introduction As mentioned by Rhetorical Structure Theory (Mann and Thompson, 1988), a discourse is composed of elementary discourse units (EDUs), which can be formed into a hierarchical structure by relating each other with discourse relations. This kind of discourse parsing tree provides a deep understanding of an article and benefits downstream NLP tasks. The majority of Chinese discourse relation is “implicit”, lacking obvious lexical cues to discriminate the relation type (Li et al., 2014b). It makes the task of Chinese discourse parsing more challenging as the parser has to catch the implicit meaning from the text instead of relying on lexical information. According to the Connective-Driven Dependency Tree (CDT) scheme (Li et al., 2014b), there are four subtasks in Chinese discourse parsing, including EDU segmentation, tree structure construction, relation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are alwa"
2020.lrec-1.128,D14-1224,0,0.042013,"Missing"
2020.lrec-1.128,D16-1035,0,0.01257,"ommonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabi"
2020.lrec-1.128,C18-2016,1,0.935447,"are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three proposed models construct binary parsing trees and thus require either a de-binarization step or binarizing ground truth for comparison. Morey et al. (2017) note that there is a discrepancy in evaluation among different works on RST-DT even though these works are also based on PARSEVAL. They thus reproduce these methods to make direct comparisons. For CDTB, there are two main branches of evaluation scenarios. Kong and Zhou (2"
2020.lrec-1.128,D17-1136,0,0.0124553,"truct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three proposed models construct binary parsing trees and thus require either a de-binarization step or binarizing ground truth for comparison. Morey et al. (2017) note that there is a discrepancy in evaluation among different works on RST-DT even though these works are also based on PARSEVAL. They thus reproduce these methods to make direct comparisons. For CDTB, there are two main branches of evaluation scenarios. Kong and Zhou (2017) and Lin et al. (2018) adopt micro F1 score, multiway gold parsing tree, and left-heavy binarization. In contrast, Sun and Kong (2018) use macro F1, binary gold tree, and right-heavy binarization. Recently, Devlin et al. (2018) introduce a neural language representation model called Bidirectional Encoder Representations f"
2020.lrec-1.128,C18-1047,0,0.0132771,"parsers for English and Chinese respectively. For English, the most commonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural networ"
2020.lrec-1.128,P12-1008,0,0.0328783,"Tree (CDT) scheme (Li et al., 2014b), there are four subtasks in Chinese discourse parsing, including EDU segmentation, tree structure construction, relation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are always binary. Details of Chinese discourse parsing can be found in (Li et al., 2014b). The CDT annotates the hierarchical discourse structure of a given article, which is different from the PDTB-style scheme proposed by (Zhou and Xue, 2012). Although the annotation of the Chinese Discourse TreeBank (CDTB) is well-defined, the evaluation is divergent. Generally, PARSEVAL (Black et al., 1991) is used to evaluate the quality of a predicted parsing tree. For a golden standard discourse tree, we have a set of non-leaf nodes N = {n1 , n2 , ..., nk }. We also have a set of text spans T = {t1 , t2 , ..., tk } where ni dominates ti for all i (e.g., the node B in Figure 1 dominates the text spanning from EDU 1 to EDU 3, so if we use nj to represent node B, then tj should represent the text span from EDU 1 to 3). Similarly, for a predicted"
2020.lrec-1.711,S17-2002,0,0.0611061,"Missing"
2020.lrec-1.711,N19-1423,0,0.0401024,"Missing"
2020.lrec-1.711,P12-1092,0,0.77437,"w that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet"
2020.lrec-1.711,N15-1070,0,0.0820792,"g models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or Ba"
2020.lrec-1.711,D17-1034,0,0.0783968,"d syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which"
2020.lrec-1.711,C18-1141,1,0.887704,"ion from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which can support operati"
2020.lrec-1.711,W13-3512,0,0.0437674,"how useful a model would be in practical applications, this kind of evaluation is usually time-consuming. Furthermore, unlike word embeddings, sense embeddings can not be directly adopted in an application without some sense selection process, so it would be hard to decouple the quality of sense embeddings and the performance of WSD. Thus, intrinsic evaluation benchmarks are still important for efficient model and parameter selection. Researchers commonly use word embedding benchmarks, including semantic similarity datasets (Bruni et al., 2014; Radinsky et al., 2011; Finkelstein et al., 2002; Luong et al., 2013), the contextual word similarity dataset (Huang et al., 2012), and synonym selection datasets (Turney, 2001; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004), to evaluate sense embeddings. In this work, we argue that these evaluation benchmarks do not provide a solid base for testing sense embeddings in a polysemy scenario. We examined eight datasets and found that every dataset contains a great number of single-sense words, where there is no ambiguity to resolve. Moreover, most of these benchmarks have a biased distribution of human-annotated scores, leading to concerns about reliabil"
2020.lrec-1.711,W16-1620,0,0.019761,"rformance gap also widens, showing that models that emphasize multisense information from the ontology are stronger. 6. 6.1 (pat, perfectly) (register, join) (descent, falling) MSD 78 72 47 MSD 78 767 118 MSD 72 89 79 |D| 837 814 812 |D| 785 752 744 |D| 903 860 854 Table 7: Word pairs with the largest differences |D| between the similarity rank of MSD-1030 and that given by the embedding models. Evaluating Sense Embedding Models on MSD-1030 Performance of Sense Embeddings We evaluate two knowledge-based sense embedding models: GenSense and SenseRetro, and three unsupervised models: sensegram (Pelevina et al., 2016), AdaGram (Bartunov et al., 2016), and MUSE (Lee and Chen, 2017). When training the sense embeddings, knowledge-based models take advantage of knowledge bases such as WordNet, Wikipedia, and Roget, while unsupervised models learn sense representations directly from text corpora. For all models, we downloaded the off-the-shelf 300-dimensional pre-trained vectors and reported their best results. Table 6 shows the Spearman and Pearson correlation coefficients of various embedding models on four datasets. While these embedding models yielded high performance on previous semantic similarity dataset"
2020.lrec-1.711,D14-1162,0,0.0785946,"(MSD-1030), which contains a high ratio of multi-sense word pairs. A series of analyses and experiments show that MSD-1030 serves as a more reliable benchmark for sense embeddings. The dataset is available at http://nlg.csie.ntu.edu.tw/nlpresource/MSD-1030/. Keywords: semantics, evaluation methodologies, crowdsourcing 1. Introduction Word embeddings, or distributed word representations, have attracted much attention in recent years. Previous studies show that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above"
2020.lrec-1.711,N18-1202,0,0.0458027,"ematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which can support operations such as queries and making inference over the ontology or any connected knowledge base. On the other hand, sense embeddings can be trained with smaller amounts of data and further enhanced by external ontologies, making them extremely useful in low-resource scenarios"
2020.lrec-1.711,N19-1128,0,0.035966,"Missing"
2020.lrec-1.711,D18-1169,0,0.0174815,"drops significantly on every dataset as the weight of the original word vectors is decreased. That is, these existing benchmarks may favor dominating senses in the training corpus, making it unnecessary for the sense embedding models to deal with different senses separately. As a result, we cannot properly evaluate a sense embedding model’s ability of handling multiple diverse senses using these datasets. 3.4 Skewed Score Distribution To understand the distribution of the similarity score in the semantic similarity datasets and the SCWS dataset, we perform an analysis similar to that used by Pilehvar et al. (2018). We divide each dataset’s score scale into four equal bins on the interval [1, 10] and assign the similarity scores and human annotators to select the closest pair of senses, making the two judgments more comparable. 5804 Figure 2: Annotation guidelines. to their corresponding bins. The results in Figure 1 show that except for MEN and the proposed MSD (to be introduced later), the score distributions are significantly imbalanced. The skewed distributions suggest that the datasets cannot support evaluations on word pairs with similarity across a variety of levels. 3.5 High Proportion of Multi-"
2020.lrec-1.711,N10-1013,0,0.0906338,"years. Previous studies show that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an onto"
2020.lrec-1.749,baccianella-etal-2010-sentiwordnet,0,0.0493137,"indicates that we cannot evaluate investor performance directly by extracting gain/loss information from the content, and must therefore infer performance from past tweets. For example, capturing the buying price and selling price of the same writer (Chen et al., 2019) can be used to evaluate the performance of this writer. 5. Comparison of Market Sentiment and General Sentiment In order to show the difference between market sentiment and general sentiment, we experiment on the expertannotated dataset (Cortis et al., 2017). We compare the performance of using general dictionary, SentiWordNet (Baccianella et al., 2010) with that of using market sentiment dictionary, NTUSD-Fin (Chen et al., 2018a). The experimental results echo the finding of our annotation results of writers’ sentiment and the market sentiment. 5.1. Dataset Semeval-2017 task 5 dataset (Cortis et al., 2017) is adopted in this paper. There are 2,030 tweets collected from Twitter and Stocktwits. This dataset was annotated by three independent experts. The details of the toolkit for annotation and the annotating process are described in Daudert et al. (2019). They selected piece(s) of tweets containing opinions for a certain cashtag as the key"
2020.lrec-1.749,W14-4012,0,0.0110919,"Missing"
2020.lrec-1.749,S17-2089,0,0.0583157,"Missing"
2020.lrec-1.749,W19-5506,0,0.0202141,"t al., 2017). We compare the performance of using general dictionary, SentiWordNet (Baccianella et al., 2010) with that of using market sentiment dictionary, NTUSD-Fin (Chen et al., 2018a). The experimental results echo the finding of our annotation results of writers’ sentiment and the market sentiment. 5.1. Dataset Semeval-2017 task 5 dataset (Cortis et al., 2017) is adopted in this paper. There are 2,030 tweets collected from Twitter and Stocktwits. This dataset was annotated by three independent experts. The details of the toolkit for annotation and the annotating process are described in Daudert et al. (2019). They selected piece(s) of tweets containing opinions for a certain cashtag as the key snippet. In our experiment, we test the performance of the models in extracting the key snippet as the experts with different sentiment dictionaries. (T6) is an example of the key snippets of the target cashtag, $VXX (iPath S&P 500 VIX Short-Term Futures ETN) and $SPX (S&P 500 Index). (T6) $VXX on the move up again should bring $SPX down into the close. we see. Annotated result of (T6): • $SPX: should bring $SPX down into the close. • $VXX: on the move up again 5.2. Approaches to Key Snippet Extraction 5.2."
2020.lrec-1.749,de-marneffe-etal-2006-generating,0,0.160531,"Missing"
2020.lrec-1.749,P18-2122,1,0.830294,"xample, which provides a forum for investors to discuss their ideas, trading strategies, and analyses of financial instruments. On StockTwits, a leading social trading platform, investors use cashtags to indicate financial instruments. For example, Apple Inc. and Microsoft Corporation are tagged as $AAPL and $MSFT, respectively. Furthermore, investors can also annotate their tweets with a bullish or bearish label. Financial tweets with the writer-labeled annotations have been used as training and test data (Li and Shah, 2017). However, the reliability of writer-labeled data is sometimes poor (Huang et al., 2018). For instance, tweet (T1) is labeled as bullish but lacks enough content. (T2), assigned a bullish label, is a description of the trading volume of $DPW. Note that both soaring and collapsing stocks are characterized by huge trading volumes. When the content of such tweets is associated with bullish or bearish labels and is used as training data, the performance of the resulting model may be greatly diminished. (T1) $AAPL today... (T2) $DPW 50 MILLION VOLUME!!! Specifically, although an investor’s label provides information about the mentioned cashtag, it does not always represent the content"
2020.lrec-1.749,K17-1031,0,0.138755,"social media platforms have been developed. The social trading platform is a typical example, which provides a forum for investors to discuss their ideas, trading strategies, and analyses of financial instruments. On StockTwits, a leading social trading platform, investors use cashtags to indicate financial instruments. For example, Apple Inc. and Microsoft Corporation are tagged as $AAPL and $MSFT, respectively. Furthermore, investors can also annotate their tweets with a bullish or bearish label. Financial tweets with the writer-labeled annotations have been used as training and test data (Li and Shah, 2017). However, the reliability of writer-labeled data is sometimes poor (Huang et al., 2018). For instance, tweet (T1) is labeled as bullish but lacks enough content. (T2), assigned a bullish label, is a description of the trading volume of $DPW. Note that both soaring and collapsing stocks are characterized by huge trading volumes. When the content of such tweets is associated with bullish or bearish labels and is used as training data, the performance of the resulting model may be greatly diminished. (T1) $AAPL today... (T2) $DPW 50 MILLION VOLUME!!! Specifically, although an investor’s label pr"
2020.lrec-1.749,R13-1054,0,0.0686687,"Missing"
2020.lrec-1.749,P13-2005,0,0.0268916,"r, we address critical problems when using financial social media data, and discuss directions for future research. The contributions of our work are three-fold as follows. 1. We conduct a careful study of financial social media data with an adequate annotated dataset. 2. We publish Fin-SoMe, a dataset for academic use with 10,000 tweets labeled from four aspects. 3. We present several perspectives toward research on financial social media data. 2. Related Work Sentiment analysis in financial social media data has been a research focus in the recent decade. Several works (Bollen et al., 2011; Si et al., 2013; Sul et al., 2017; Vanstone et al., 2018) show that financial social media data provides trading clues. However, to the best of our knowledge, little work takes a close look at the contents of individual investors’ tweets. Maks and Vossen (2013) show that evaluations may differ between the writer and readers of a given product review; 6106 they recommend to use product reviews based on readers’ ratings rather than the writer’s rating. However, most works on financial social media data use writer-labeled information to construct lexicons (Li and Shah, 2017) or train sentiment classifiers (Deng"
2020.lrec-1.749,P18-1183,0,0.0209054,"Missing"
2020.lrec-1.76,D18-1547,0,0.0675599,"Missing"
2020.lrec-1.76,L18-1252,0,0.0746388,"e former is built in the open domain. The social dialogue system aims to understand both users’ intents and their feelings, and creates an appropriate response to the users. To design a dialogue system that understands the users’ feelings, a dataset with the labels of the users’ mental state is indispensable. However, those researches about the emotion detection are usually based on the datasets crawled from comment (Feng et al, 2010) and social media (Sun et al., 2010). There is few publicly available Chinese dataset containing conversation content and emotion labels similar to EmotionLines (Chen et al., 2018). In addition to the users’ emotion, we aim to investigate other factors that may affect the conversation. Interpersonal relationship between a speaker and a listener plays an important role. When people talk to friends, the wording would be much different from that talking to a stranger. If a dialogue system can predict which relation type the wording of a user belongs to, the system will be able to generate a better response. In this paper, we release a dataset, Multi-Party Dialogue Dataset (MPDD),1 with both emotion and relation labels on each utterance. All the dialogues are collected from"
2020.lrec-1.76,D14-1181,0,0.00471923,": responded utterance selector, contextualized embedding encoder, feature connector, and the output layer. 4.1 Responded Utterance Selector In MPDD, every utterance has the labels that are the listeners of the utterance. We postulate that the closest utterance spoken by one of the listeners is the responded utterance that the current utterance responds to. For the utterances without responded utterances, we select the previous utterances as the alternatives. 4.2 Contextualized Embedding Encoder To convert an utterance to the contextualized embedding, we adopt two kinds of encoders, i.e., CNN (Kim, 2014) and BERT (Devlin et al., 2018). CNN: If there is a responded utterance, we concatenate the responded utterance and the current utterance first. Then we represent the concatenation with the word embedding, which is a 300-dimensional vector pre-trained on the Wikipedia dataset with word2vec. The word embedding matrix is fed into an 1D-Convolution layer that has 64 filters with the window size in the range of 1 to 5. After that, we fed the result into an 1D-max pooling layer, and flatten 612 Encoder CNN BERT Baseline .7169 .7259 w/ previous utterance .7207 .7494 w/ responded utterance .7207 .763"
2020.lrec-1.76,P18-2033,0,0.0608422,"Missing"
2020.lrec-1.76,I17-1099,0,0.0232711,"oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Twitter (Ritter et al., 2011). However, the former has no emotion labels, which are important for training, and the latter extracts dialogues from the post-reply pairs, which are quite different from human conversation. DailyDialog (Li et al., 2017) is a dataset in which the daily dialogues are crawled from various English learning websites and annotated with the emotion and intension type on every utterance. EmotionLines (Chen et al., 2018) is a dialogue dataset whose materials are crawled from Friends TV scripts and private Facebook messenger dialogues, and has the emotion labels on each utterance. Both DailyDialog and EmotionLines are in English. We cannot directly translate these datasets to Chinese for system development because of translation errors and the culture difference. In this paper, we aim to build a Chinese dialogue datas"
2020.lrec-1.76,petukhova-etal-2014-dbox,0,0.0268277,"this paper, we release a dataset, Multi-Party Dialogue Dataset (MPDD),1 with both emotion and relation labels on each utterance. All the dialogues are collected from TV series scripts. To the best of our knowledge, this dataset is very rare Chinese multi-party dialogue dataset with both emotion and relation labels. We also make some experiments to observe the effects of the context on the emotion and relation classification, and the correlation between emotion and relation types. 2. Related Work Several dialogue datasets have been released in recent years. Some focus on the specific domains (Petukhova et al., 2014; Feng et al., 2016; Wei et al., 2018), and some on 1 the multiple domains (Budzianowski et al., 2018). These datasets are annotated with the dialogue act information, and appropriate for the slot filling task when building tasked-oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Tw"
2020.lrec-1.76,D11-1054,0,0.0254358,"et al., 2016; Wei et al., 2018), and some on 1 the multiple domains (Budzianowski et al., 2018). These datasets are annotated with the dialogue act information, and appropriate for the slot filling task when building tasked-oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Twitter (Ritter et al., 2011). However, the former has no emotion labels, which are important for training, and the latter extracts dialogues from the post-reply pairs, which are quite different from human conversation. DailyDialog (Li et al., 2017) is a dataset in which the daily dialogues are crawled from various English learning websites and annotated with the emotion and intension type on every utterance. EmotionLines (Chen et al., 2018) is a dialogue dataset whose materials are crawled from Friends TV scripts and private Facebook messenger dialogues, and has the emotion labels on each utterance. Both DailyDialog and"
2020.lrec-1.76,O10-1013,0,0.0282181,"major types of dialogue systems. Unlike the latter, which needs to focus on confirming user’s purpose and usually targets a specific domain, the former is built in the open domain. The social dialogue system aims to understand both users’ intents and their feelings, and creates an appropriate response to the users. To design a dialogue system that understands the users’ feelings, a dataset with the labels of the users’ mental state is indispensable. However, those researches about the emotion detection are usually based on the datasets crawled from comment (Feng et al, 2010) and social media (Sun et al., 2010). There is few publicly available Chinese dataset containing conversation content and emotion labels similar to EmotionLines (Chen et al., 2018). In addition to the users’ emotion, we aim to investigate other factors that may affect the conversation. Interpersonal relationship between a speaker and a listener plays an important role. When people talk to friends, the wording would be much different from that talking to a stranger. If a dialogue system can predict which relation type the wording of a user belongs to, the system will be able to generate a better response. In this paper, we releas"
2020.semeval-1.279,N19-1423,0,0.0412291,"Missing"
2020.semeval-1.279,S19-2011,0,0.0218706,"Missing"
2020.semeval-1.279,S19-2123,0,0.0210275,"Missing"
2020.semeval-1.279,N19-1144,0,0.0410524,"Missing"
2020.semeval-1.279,S19-2010,0,0.0203437,"/licenses/by/4.0/. 2105 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2105–2110 Barcelona, Spain (Online), December 12, 2020. IU@LING team (Zhu et al., 2019) wins the third place in Sub-task A. They use the PyTorch framework to build the BERT model in Sub-task A, and use the SVM model in Sub-tasks B and C. Multi-task learning (MTL) is one of widely-used strategies for improving neural network models. Ruder (2017) introduces the two most commonly used methods of MTL in deep learning, namely hard parameter sharing and soft parameter sharing approaches. Sanh et al. (2019) propose a hierarchical multi-task (HMTL) model, by combining low-level simple tasks with high-level complex tasks to train. The model achieves the state-of-the-art results in named entity recognition, entity mention detection, and relation extraction. In this paper, we consider the hierarchical nature of the three tasks and introduces the HMTL framework to deal with the problems. 3 3.1 Data and Methodology Data The OLID 1.0 dataset used in the competition last year is available as a part of the OffensEval 2019 Shared Task Zampieri et al. (2019a). It is collected from Twitter API by searching"
2020.semeval-1.279,S19-2138,0,0.011562,"shows that the pre-trained BERT model has a good competition result in the task. The vradivchev_anikolov team (Radivchev and Nikolov, 2019) wins the first place and the second place in Sub-tasks C and A, respectively. They explore several models, among which BERT has the highest performance. The UMThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 2105 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2105–2110 Barcelona, Spain (Online), December 12, 2020. IU@LING team (Zhu et al., 2019) wins the third place in Sub-task A. They use the PyTorch framework to build the BERT model in Sub-task A, and use the SVM model in Sub-tasks B and C. Multi-task learning (MTL) is one of widely-used strategies for improving neural network models. Ruder (2017) introduces the two most commonly used methods of MTL in deep learning, namely hard parameter sharing and soft parameter sharing approaches. Sanh et al. (2019) propose a hierarchical multi-task (HMTL) model, by combining low-level simple tasks with high-level complex tasks to train. The model achieves the state-of-the-art results in named"
2021.eacl-main.122,C16-1284,0,0.0699899,"Missing"
2021.eacl-main.122,D19-1490,0,0.0560409,"Missing"
2021.eacl-main.122,D19-1495,0,0.0157294,"ph in1426 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1426–1431 April 19 - 23, 2021. ©2021 Association for Computational Linguistics formation. Our approach outperforms recent works in implicit tag recognition. • Our pre-training task, masked entity prediction, is helpful for predicting the implicit entities. The pre-trained model can be also applied in other information extraction tasks. 2 Related work Extracting and using the information in articles is one of the focuses in the NLP community. Some works (Hu et al., 2018; Ding et al., 2019; Ma et al., 2019) adopt the extracted information for stock market prediction. Some of them (Baker et al., 2016; Min and Zhao, 2019) use the information in news articles to construct socio-economic indicators. Most of previous works only focus on explicit information in the articles. In this way, the implicit entities in the articles may be under looked. In this paper, we aim to extract the non-mentioned but related entities from a document. Recently, GNN has become popular for modeling relationships among multiple entities. Kipf and Welling (2016) use the convolution neural network to learn"
2021.eacl-main.122,P19-1136,0,0.062595,"Missing"
2021.eacl-main.122,D19-1510,0,0.0252661,", two companies, Sprint and T-Mobile, are explicitly mentioned companies in the news article in Figure 1. However, the stock price of a non-mentioned company, SoftBank, may be influenced since Softbank owns shares of Sprint. Although this kind of inference is intuitive for professional analysts, few previous works take such implicit information into consideration. In this paper, we aim to increase the sense of machines toward this kind of implicit information. Transformer-based (Vaswani et al., 2017) neural networks achieve state-of-the-art performances in many NLP tasks (Devlin et al., 2018; Malmi et al., 2019). To model the relationships between entities, graph neural network (GNN) is a well-known architecture for representing the knowledge and additional information (Fu et al., 2019; You et al., 2020). Furthermore, the models blending these two architectures show their effectiveness (Lu et al., 2020). In this paper, we propose dynamic graph transformer (DGT), a novel blend of Transformer and GNN. In previous work, the weights of the GNN are pre-determined in the training stage and not affected by the given input. Our DGT adjusts the weights depending on the input on the fly. In this way, the repre"
2021.eacl-main.122,D19-1121,0,0.0140288,"431 April 19 - 23, 2021. ©2021 Association for Computational Linguistics formation. Our approach outperforms recent works in implicit tag recognition. • Our pre-training task, masked entity prediction, is helpful for predicting the implicit entities. The pre-trained model can be also applied in other information extraction tasks. 2 Related work Extracting and using the information in articles is one of the focuses in the NLP community. Some works (Hu et al., 2018; Ding et al., 2019; Ma et al., 2019) adopt the extracted information for stock market prediction. Some of them (Baker et al., 2016; Min and Zhao, 2019) use the information in news articles to construct socio-economic indicators. Most of previous works only focus on explicit information in the articles. In this way, the implicit entities in the articles may be under looked. In this paper, we aim to extract the non-mentioned but related entities from a document. Recently, GNN has become popular for modeling relationships among multiple entities. Kipf and Welling (2016) use the convolution neural network to learn the node representation by aggregating the features of neighboring nodes. Veliˇckovi´c et al. (2017) employ the attention mechanism t"
2021.eacl-main.122,N16-1041,0,0.0326594,"Missing"
2021.eacl-main.122,D19-1471,0,0.0217301,"Missing"
2021.eacl-main.122,P18-1183,0,0.0666733,"Missing"
2021.emnlp-main.362,N19-1423,0,0.0965982,", we propose a method that not only provides more data, but also retains the meanings of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmentation methods. 3 review. For each review, we concatenate on the auxiliary sentence and the review with a special token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S"
2021.emnlp-main.362,D18-1045,0,0.0144991,"(Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set in size. In ATE task, late corresponding auxiliary sentences, and experwe augment the reviews according to the number of iment on ABSA datasets (Pontiki et al., 2016) in 4419 Model Bertbase + BT + C-BERT + AS-SPM & AE + Senti-SPM & AE + AS-SPM & Seq2Seq + Senti-SPM & Seq2Seq AR 88.480.78 88.240.87 87.882.24 89.201.01 87.410.88 90.281.34 88.481.76 CH 93.790.87 94.581.40"
2021.emnlp-main.362,D19-1654,0,0.0201007,"ts on ACSC-Rest14 under different multiples of the training set size. The performances of models become better than that only using the original training set when the multiple is between 1 and 2. It also shows that using too much training data generated by the proposed method harms the performance of ABSA because it may contain too much noise. 5.4 Case Study Table 6 presents the augmented results of different approaches. It shows that previous approaches 5.2 Multi-Aspect Multi-Sentiment may change the aspect or sentiment of an instance. Experiment For example, EDA generates an unmeaningful senJiang et al. (2019) propose a multi-aspect multi- tence. Although “ugly” could be a hint for negative sentiment dataset, MAMS. In MAMS, each in- sentiment, the aspect of the generated instance is stance is annotated with different sentiments from changed. C-BERT shows the other worse case— at least two aspects. They claim that this is a more the sentiment of the generated review is changed. challenging dataset than that in the previous works. Both cases show the importance of the proposed We further experiment on this dataset with the pro- idea, i.e., controlling the aspect or sentiment word posed method, and re"
2021.emnlp-main.362,2020.findings-emnlp.372,0,0.0958615,"idered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant word with pre-trained"
2021.emnlp-main.362,N18-2072,0,0.115871,"enarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant word with pre-trained lanmethod can achieve better performances in guage m"
2021.emnlp-main.362,2020.acl-main.703,0,0.0178045,"61 80.662.27 70.341.65 81.001.68 69.211.14 82.861.50 70.681.15 81.511.07 69.270.87 - Rest16 72.422.38 74.230.64 75.190.57 75.620.64 75.240.58 - Table 2: Experimental results. Auto Encoding (AE): The AE model is initialized with the pre-trained weights of BERT (Devlin et al., 2019), which will predict the masked word. We use the predicted word to replace the unimportant term. aspect terms. Accuracy is adopted as the evaluation metric for ACSC, ATSC, and SC tasks. F1-score is used in ATE task. Sequence-to-Sequence (Seq2Seq): The Seq2Seq model is initialized with the pre-trained weights of BART (Lewis et al., 2020) which includes encoder and decoder. The predicted word from the decoder is adopted for replacing the unimportant term. We report the averaged results across five random seeds in Table 2, and the standard deviations are also shown in subscripts. We do not adopt EDA to the ATE task because the insertion and the deletion operations are not suitable for token-level tasks. Firstly, we find that the combinations of the SPM settings (AS-SPM and Senti-SPM) and token replacement strategies (AE and Seq2Seq) achieve better performances on all settings with stable results (lower standard deviations). Tha"
2021.emnlp-main.362,D13-1170,0,0.00673025,"the proposed SPM consistently outperforms the random masking strategy (C-BERT). Thirdly, the proposed token replacement strategy Seq2Seq performs well in ACSC and ATSC, and AE achieves the best results in ATE. 4 4.1 Experiments Experimental Setup 4.2 Experimental Results We experiment on four widely-used datasets in ABSA, including Lap14 (Pontiki et al., 2014), Rest14 (Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set i"
2021.emnlp-main.362,N19-1035,0,0.0415176,"Missing"
2021.emnlp-main.362,D16-1058,0,0.027143,"ormation and use the language model to randomly replacing single-word with more diverse substitutions. Xie et al. (2020) replace the uninformative words based on TF-IDF scores. Although previous works show that their methods can improve the performance of some NLP tasks, their data augmentation methods may change the meanings of the given instances. In this paper, we propose a method that not only provides more data, but also retains the meanings of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmen"
2021.emnlp-main.362,D19-1670,0,0.269046,"ve always been considered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant wo"
2021.emnlp-main.362,2020.acl-main.383,0,0.0147395,"ecial token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S) is thus formulated as follows: S = a a r ], [w1a , ..., waspect , ..., wsentiment , w[SEP] , w1r ..., wN a r where w and w denote words in auxiliary sentence and review, respectively. waspect and wsentiment are the words denoting aspect and sentiment, respectively, and N is the length of the review. 3.2 Selective Perturbed Masking (SPM) Wu et al. (2020) introduce perturbed masking (PM) to analyze syntactic information, and verify its effectiveness on syntactic parsing and discourse dependency parsing. In this work, we propose selective perturbed masking (SPM) to estimate the correlation between the tokens in reviews and sentiment words (aspect terms) in the auxiliary sentence. The following procedure is proposed to measure the impact wir (1 ≤ i ≤ N ) on predicting a a waspect and wsentiment , respectively. First, we replace waspect (wsentiment ) with the special token [MASK], and use this word sequence as BERT’s input for predicting the mask"
2021.emnlp-main.362,N19-1242,0,0.0266341,"of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmentation methods. 3 review. For each review, we concatenate on the auxiliary sentence and the review with a special token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S) is thus formulated as follows: S = a a r ], [w1a , ..., waspect , ..., wsentime"
2021.emnlp-main.362,P18-1183,0,0.112237,"asks to the given sentence “the staff was so kind” are (service, positive), (staff, positive), and “staff”, respectively. These tasks are commonly taken as examples for evaluating the performance of augmentation methods. In this paper, we explore both sentiment analysis and all subtasks in ABSA to show the usefulness of the proposed method in semantics preservation. In addition to probe on sentiment analysis and ABSA tasks, we also experiment on stock price and risk movement prediction tasks. These experiments have always been considered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce t"
2021.emnlp-main.362,P05-1015,0,0.208777,"he random masking strategy (C-BERT). Thirdly, the proposed token replacement strategy Seq2Seq performs well in ACSC and ATSC, and AE achieves the best results in ATE. 4 4.1 Experiments Experimental Setup 4.2 Experimental Results We experiment on four widely-used datasets in ABSA, including Lap14 (Pontiki et al., 2014), Rest14 (Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set in size. In ATE task, late corresponding aux"
2021.emnlp-main.362,S15-2082,0,0.067438,"Missing"
2021.emnlp-main.362,2021.naacl-main.185,0,0.0202721,"Missing"
2021.emnlp-tutorials.2,2020.lrec-1.749,1,0.850016,"nical details and the application scenarios will be introduced. The contrast of financial opinion mining with general opinion mining will also be discussed. The characteristics of different kinds of financial documents will be listed. 3.1 Coarse-grained Financial Opinion Mining The topic of the first session gives the overview of financial opinion mining, including the investor’s opinion and the customer’s opinion. We start with sentiment analysis in the financial domain. The comparison between the general sentiment and the market sentiment will also be discussed (Loughran and McDonald, 2011; Chen et al., 2020b). The lexicons for the sentiment analysis (Bodnaruk et al., 2015; Li and Shah, 2017; Sedinkina et al., 2019) in financial documents and the applications of adopting sentiment analysis results (Bollen et al., 2011; Du et al., 2019; Lin et al., 2020) will be included. This session also covers the sentiment analysis of financial narratives from different resources, including formal documents such as financial statements 7 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 7–10 Punta Cana and Online, November 10–11, 2021. ©20201Assoc"
2021.emnlp-tutorials.2,P19-1047,0,0.0939837,"issed if we only focus on the market sentiment analysis of financial documents. Thanks to the recent “CS + X” trend, more interdisciplinary cooperation exists between computer science and other domains. In the “NLP + Finance” community, lots of recent works pay their attention to in-depth analysis of different kinds of financial documents rather than market sentiment prediction. For example, our previous works (Chen et al., 2018, 2019a) find that the numeral information extracted from financial social media data is comparable to the price targets extracted from professional analysts’ reports. Keith and Stent (2019) analyze the pragmatic and semantic features in the earnings conference calls and discuss how these features influence the investor’s decision-making process. Zong et al. (2020) point out the difference between the textual factors and cognitive factors by comparing the accurate and inaccurate professional analysts’ reports. The abovementioned works conclude the necessity of capturing fine-grained opinions in the financial narratives. As the increasing interest of our community on this topic, recently, more and more related workshops spring up in the leading conferences, including FinWeb-2021 i"
2021.emnlp-tutorials.2,J19-4006,0,0.0308938,"ch competition (2018), and the 2nd prize in both the Jih Sun FinTech Possible Research Directions In the last session, we will discuss four possible research directions for future works (Chen et al., 2020a), including (1) argument mining in finance, (2) opinion quality evaluation, (3) implicit influence inference, and (4) opinion tracking in time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica,"
2021.emnlp-tutorials.2,K17-1031,0,0.0240112,"cial opinion mining with general opinion mining will also be discussed. The characteristics of different kinds of financial documents will be listed. 3.1 Coarse-grained Financial Opinion Mining The topic of the first session gives the overview of financial opinion mining, including the investor’s opinion and the customer’s opinion. We start with sentiment analysis in the financial domain. The comparison between the general sentiment and the market sentiment will also be discussed (Loughran and McDonald, 2011; Chen et al., 2020b). The lexicons for the sentiment analysis (Bodnaruk et al., 2015; Li and Shah, 2017; Sedinkina et al., 2019) in financial documents and the applications of adopting sentiment analysis results (Bollen et al., 2011; Du et al., 2019; Lin et al., 2020) will be included. This session also covers the sentiment analysis of financial narratives from different resources, including formal documents such as financial statements 7 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 7–10 Punta Cana and Online, November 10–11, 2021. ©20201Association for Computational Linguistics and professional analyst’s reports and informal"
2021.emnlp-tutorials.2,P16-2032,0,0.0291716,"(4) opinion tracking in time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica, Taiwan. His research interests include natural language processing and information retrieval. His work has been published in ACL, SIGIR, WWW, IJCAI, CIKM, COLING, and so on. Dr. Huang received the Honorable Mention of Doctoral Dissertation Award of ACLCLP in 2014 and the Honorable Mention of Master Thesis Awar"
2021.emnlp-tutorials.2,2020.acl-main.473,0,0.209943,"ce and other domains. In the “NLP + Finance” community, lots of recent works pay their attention to in-depth analysis of different kinds of financial documents rather than market sentiment prediction. For example, our previous works (Chen et al., 2018, 2019a) find that the numeral information extracted from financial social media data is comparable to the price targets extracted from professional analysts’ reports. Keith and Stent (2019) analyze the pragmatic and semantic features in the earnings conference calls and discuss how these features influence the investor’s decision-making process. Zong et al. (2020) point out the difference between the textual factors and cognitive factors by comparing the accurate and inaccurate professional analysts’ reports. The abovementioned works conclude the necessity of capturing fine-grained opinions in the financial narratives. As the increasing interest of our community on this topic, recently, more and more related workshops spring up in the leading conferences, including FinWeb-2021 in the Web Conference, FinNLP-2021 in IJCAI, FinIR-2020 in SIGIR, and FNP-2020 in COLING. In this tutorial, we will show where we are and where we will be to those researchers in"
2021.emnlp-tutorials.2,P19-1329,0,0.0161227,"pinions. The audience will be inspired by this tutorial and find an interesting research direction for their work. With the discussion on the possible research directions, many novel ideas will be figured out during this tutorial. Fine-grained Financial Opinion Mining 4 The second session will focus on the fine-grained financial opinion mining, which is the recent trend in this field and also the research interest of the presenters. This session will start by the discussion of the aspect analysis of financial narratives (Maia et al., 2018; Chen et al., 2019a). The numeral in the textual data (Naik et al., 2019; Wallace et al., 2019; Chen et al., 2018, 2019a, 2020c) and the numeracy of the neural network models (Spithourakis and Riedel, 2018; Chen et al., 2019b) attract lots of attentions recently. In the financial narrative, the proportion of numerals are higher than that of other domains’ documents. Without numerals, more important information will be missed. Thus, we summarize the related works for understanding the numerals in financial documents and provide a systematic analysis on these studies. The linguistic features of different kinds of financial documents will also be discussed (Keith and"
2021.emnlp-tutorials.2,P18-1065,0,0.0214254,"time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica, Taiwan. His research interests include natural language processing and information retrieval. His work has been published in ACL, SIGIR, WWW, IJCAI, CIKM, COLING, and so on. Dr. Huang received the Honorable Mention of Doctoral Dissertation Award of ACLCLP in 2014 and the Honorable Mention of Master Thesis Award of ACLCLP in 2008. He ser"
2021.emnlp-tutorials.2,P19-1034,0,0.0548762,"Missing"
2021.emnlp-tutorials.2,P18-1196,0,0.0136317,"he discussion on the possible research directions, many novel ideas will be figured out during this tutorial. Fine-grained Financial Opinion Mining 4 The second session will focus on the fine-grained financial opinion mining, which is the recent trend in this field and also the research interest of the presenters. This session will start by the discussion of the aspect analysis of financial narratives (Maia et al., 2018; Chen et al., 2019a). The numeral in the textual data (Naik et al., 2019; Wallace et al., 2019; Chen et al., 2018, 2019a, 2020c) and the numeracy of the neural network models (Spithourakis and Riedel, 2018; Chen et al., 2019b) attract lots of attentions recently. In the financial narrative, the proportion of numerals are higher than that of other domains’ documents. Without numerals, more important information will be missed. Thus, we summarize the related works for understanding the numerals in financial documents and provide a systematic analysis on these studies. The linguistic features of different kinds of financial documents will also be discussed (Keith and Stent, 2019; Zong et al., 2020), which can provide insights for the future works on feature engineering. The results of cross-docume"
C12-1034,W11-0207,0,0.0526315,"Missing"
C12-1034,2010.eamt-1.31,0,0.0131036,"h as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the original diagnosis term and produce the translation result. FIGURE conducted a pilot study of targeted paraphrasing in which monolingual speakers on both sides collaborate to improve SMT output by paraphrasing the critical segments of source text. Different f"
C12-1034,W09-0432,0,0.033412,"ty between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT system. In the medical domain, for example, a term may not even appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific te"
C12-1034,N06-1003,0,0.0218042,"as a pre-processing module of the out-domain SMT model which has poor in-domain knowledge. The simplification approach can be viewed as a variant of paraphrasing, which expresses the same meaning in different ways. Paraphrasing is employed for various NLP tasks, such as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the ori"
C12-1034,D07-1007,0,0.0769833,"Missing"
C12-1034,P07-1005,0,0.0186598,"ws the conclusion, indicates the potentials of our method, and shows some future work. 2 Related Work Building an SMT system from large scale bilingual data for a specific application has become a practical option today. On the other hand, SMT model heavily relies on the statistical evidences in the training corpus. As a result, it may learn a biased SMT model, and suffer from the data sparseness problem of the training corpus when dealing with the ambiguity nature of human language. This drawback results in some typical issues such as translation disambiguation problem (Carpuat et al., 2007; Chan et al., 2007), in which a word has several senses, but the corpus is biased towards a particular subset of the senses. The SMT model trained from such a corpus is therefore prone to give the wrong translation due to the wrong choice of sense. 547 When a cross-domain SMT application is concerned, data sparseness problem is worsened by limited in-domain bilingual corpus. Domain adaptation techniques therefore play a key role in building an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resour"
C12-1034,2011.mtsummit-papers.31,1,0.810228,"nowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers and modifiers. For these in-domain terms, we simplify them by finding"
C12-1034,P05-1033,0,0.0309072,"al in-domain segments are available. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we se"
C12-1034,W07-0722,0,0.0586024,"n resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT sys"
C12-1034,D10-1044,0,0.026738,"lding an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to"
C12-1034,P08-1088,0,0.011748,"a special domain, there are various ways to collect bilingual in-domain knowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers a"
C12-1034,P07-1019,0,0.0272359,"e. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn et al., 2007) SMT system"
C12-1034,P03-1054,0,0.00948156,"Missing"
C12-1034,N03-1017,0,0.00518571,"統的資料稀疏問題。我們取得翻譯結果後，根據提供的領域 內知識將這些重要片段還原。最後我們進行了醫療領域的英中翻譯的實驗，並且評估STR 架構內的每一步驟。翻譯結果顯示我們的方法顯著地優於領域外的SMT系統，以及簡易型 的領域內SMT系統。 KEYWORDS : Cross-Domain SMT, Domain Adaptation, Statistical Machine Translation 關鍵詞 : 跨領域統計式機器翻譯, 領域調適, 統計式機器翻譯 Proceedings of COLING 2012: Technical Papers, pages 545–560, COLING 2012, Mumbai, December 2012. 545 1 Introduction Over the past decades, the rapid growth of available parallel corpus makes SMT development feasible, and SMT system has gradually moved toward practical use because of its relatively acceptable translation speed and quality. A phrase-based SMT system (Koehn et al., 2003; Koehn, 2004), for example, trains a phrase table from a large bilingual corpus as its translation model, and decodes source language input in polynomial time with greedy algorithms such as beam search. It translates phrases as basic units, and thus captures short-range reordering phenomena between source and target languages. Generally phrase-based SMT models outperform word-based ones (Koehn et al., 2003). However, an SMT system fails to capture long-range contextual knowledge due to the limited horizon and the sparseness nature of lexical n-grams. These drawbacks reduce the translation qua"
C12-1034,koen-2004-pharaoh,0,0.0310985,"據提供的領域 內知識將這些重要片段還原。最後我們進行了醫療領域的英中翻譯的實驗，並且評估STR 架構內的每一步驟。翻譯結果顯示我們的方法顯著地優於領域外的SMT系統，以及簡易型 的領域內SMT系統。 KEYWORDS : Cross-Domain SMT, Domain Adaptation, Statistical Machine Translation 關鍵詞 : 跨領域統計式機器翻譯, 領域調適, 統計式機器翻譯 Proceedings of COLING 2012: Technical Papers, pages 545–560, COLING 2012, Mumbai, December 2012. 545 1 Introduction Over the past decades, the rapid growth of available parallel corpus makes SMT development feasible, and SMT system has gradually moved toward practical use because of its relatively acceptable translation speed and quality. A phrase-based SMT system (Koehn et al., 2003; Koehn, 2004), for example, trains a phrase table from a large bilingual corpus as its translation model, and decodes source language input in polynomial time with greedy algorithms such as beam search. It translates phrases as basic units, and thus captures short-range reordering phenomena between source and target languages. Generally phrase-based SMT models outperform word-based ones (Koehn et al., 2003). However, an SMT system fails to capture long-range contextual knowledge due to the limited horizon and the sparseness nature of lexical n-grams. These drawbacks reduce the translation quality in terms"
C12-1034,P07-2045,0,0.00491577,", 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn et al., 2007) SMT system and apply its advanced feature of specifying reordering constraint to each of the simplified phrases. Under the constraint, a simplified phrase is translated as a block and its translation is continuous on the target side. FIGURE 3.4 4 – The incorrect reordering of the in-domain segments ""bone lesion"". Restoration In the restoration step, we receive the translation result from the background SMT system and perform post-processing steps. We locate the simplified phrase pairs and replace them with their corresponding bilingual in-domain segments as illustrated in FIGURE 2(c). The res"
C12-1034,D09-1040,0,0.0142004,"can be viewed as a variant of paraphrasing, which expresses the same meaning in different ways. Paraphrasing is employed for various NLP tasks, such as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the original diagnosis term and produce the translation result. FIGURE conducted a pilot study of targeted paraphrasi"
C12-1034,D09-1074,0,0.0350734,"oss-domain SMT application is concerned, data sparseness problem is worsened by limited in-domain bilingual corpus. Domain adaptation techniques therefore play a key role in building an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synt"
C12-1034,P07-1084,0,0.0134218,"ilingual in-domain knowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers and modifiers. For these in-domain terms, we simpl"
C12-1034,W99-0604,0,0.104326,"ments and the thin lines are word alignments. FIGURE In some cases, a simplified phrase is translated together with its nearby context, and therefore we need to determine the translation of the simplified phrase before we can restore its original form, such as the term ""hypertension"" in FIGURE 2(b). Phrase level alignments are insufficient now and higher granularity is needed, such as word-level alignments, to separate a simplified phrase from 553 its contexts. With word alignment information, we may extract a simplified phrase pair without violating the consistency judgment of a phrase pair (Och et al., 1999). We then replace the simplified phrase pair with its original bilingual in-domain segment. Our word alignment method is illustrated in FIGURE 5(b). The hollow blocks denote the context f2f3, which is translated together with its nearby simplified term f1. The thin lines and the thick lines are word alignments and phrase alignments, respectively. Compliant with the consistency of phrase extraction, we separate the phrase pair f1-e1 from the phrase pair f1f2f3-e1e2e3, and perform the restoration. The method is exemplified in FIGURE 6(b). Based on the word alignments, the simplified term ""hypert"
C12-1034,P02-1040,0,0.0851785,"Missing"
C12-1034,D10-1013,0,0.100809,"and report the experimental results of the translation tasks on medical summaries in a hospital. The central issues in this framework include (1) collecting bilingual in-domain knowledge and identifying indomain segments in a source text, (2) replacing the in-domain segments with the proper simplified forms, (3) translating the modified text with a background SMT system and (4) restoring the original in-domain segments after receiving the translation results from the background SMT system. We are not the first to rephrase source language text in order to improve SMT output. As a pilot study, Resnik et al. (2010) proposed a targeted paraphrasing approach which identifies the critical source segments difficult for the background SMT system to translate. These segments are then manually paraphrased in many ways in order to provide the SMT system with more choices of decoding paths. Different from their work, we automatically identify the critical segments with in-domain knowledge, simplify them with linguistic information, and restore these critical segments after receiving the SMT results. This paper is organized as follows. In Section 2, we review the previous works on domain adaptation and related wo"
C12-1034,2006.amta-papers.25,0,0.0448089,"Missing"
C12-1034,D07-1077,0,0.0211107,"cular tachycardia and dyslipidemia"" to ""with diseases"". (d) S (Clause) We simplify a clause by simplifying its children recursively according to the above simplification rules. FIGURE 3 – A parse tree of a syntactic unit. A bracketed string at each non-terminal node is the head of the corresponding syntactic category. The rule-based simplification approach is straightforward, but can be effectively applied to most of the syntactic units as discussed in Section 4. Applying transformation or rewriting rules on source sentences based on their syntactic structures has been adopted in other works. Wang et al. (2007) listed a set of prominent syntactic reordering rules that systematically describe the word order difference between the source and target languages. Based on these rules, they parsed a source language input and reordered its structure to match the target language grammar for training a better translation model and improving a phrase-based SMT system. In their work, source side syntactic reordering also serves as a pre-processing module of the SMT system. Different from their work, we simplify a source language input in favour of the background SMT system instead of changing the order of its s"
C12-1034,D11-1038,0,0.148502,"en appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific terms such as diagnosis and surgical names in biomedical literature or medical records using a general purpose SMT system. Different from the previous works, we address cross-domain issues in SMT across two largely distinct domains by simplifying in-domain segments to the ones that can be recognized by the out-domain or the background SMT system, and restoring the in-domain segments after receiving the SMT results. Text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) itself has some straightforward NLP applications. For example, we produce a simpler version of a text by modifying the lexical contents and shortening the grammatical structures without changing the original text at the semantic level. Such simplified contents are beneficial for language learners and people with lower levels of literacy. One of the real world applications is Simple English Wikipedia (http://simple.wikipedia.org), which uses simple English words and grammar, and thus English language learners can benefit from it. In this paper we apply sentence simplifica"
C12-1034,P12-1107,0,0.189295,"Missing"
C12-1034,P06-1066,0,0.0222127,"egments are available. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn"
C12-1034,C04-1059,0,0.0184736,"Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT system. In the medical"
C12-1034,C10-1152,0,0.135047,"a term may not even appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific terms such as diagnosis and surgical names in biomedical literature or medical records using a general purpose SMT system. Different from the previous works, we address cross-domain issues in SMT across two largely distinct domains by simplifying in-domain segments to the ones that can be recognized by the out-domain or the background SMT system, and restoring the in-domain segments after receiving the SMT results. Text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) itself has some straightforward NLP applications. For example, we produce a simpler version of a text by modifying the lexical contents and shortening the grammatical structures without changing the original text at the semantic level. Such simplified contents are beneficial for language learners and people with lower levels of literacy. One of the real world applications is Simple English Wikipedia (http://simple.wikipedia.org), which uses simple English words and grammar, and thus English language learners can benefit from it. In this paper w"
C12-1034,2010.amta-workshop.3,0,\N,Missing
C12-3028,I11-1170,1,0.646173,"Missing"
C12-3028,miltsakaki-etal-2004-penn,0,0.0650152,"Missing"
C12-3028,W12-1636,1,\N,Missing
C12-3028,P04-1087,0,\N,Missing
C12-3028,prasad-etal-2008-penn,0,\N,Missing
C12-3028,P12-1008,0,\N,Missing
C12-3028,W00-1205,0,\N,Missing
C14-1060,O06-1001,0,0.0234419,"ink to the previous or the next discourse units. S5 and S6 have the similar behaviors. The singleword “但是” (dàn shì, but) in (S5) shows a backward-linking. In (S6), it is shifted to the first position and becomes ambiguous. It may be linked to the previous, or to the next discourse units. The correct interpretation depends on the context. These phenomena show a single-word connective may have different senses when it is not at its original position. Figure 1: Examples for forward linkging and backward linking. In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006), and Lu (2007). The discourse connective lexicon contains 319 single-word and 489 word-pair connectives. Initially, each connective is associated with only one discourse function manually by linguists. 634 For example, the word-pair connective, “雖然...但是” (suī rán…dàn shì, although…but), is assigned a Comparison function. The assignment is one-to-one mapping, thus it cannot capture the complete discourse functions of Chinese connectives. Table 1 shows an overview of the discourse connective lexicon. In this lexicon, Expansion is the majority, and Comparison is the minority. The percentages of"
C14-1060,P13-2114,0,0.0349182,"Missing"
C14-1060,I11-1170,1,0.868821,"e dataset. 3.1 A Semi-Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are consi"
C14-1060,W12-1636,1,0.859959,"Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The b"
C14-1060,C12-3028,1,0.796366,"Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The b"
C14-1060,W13-2309,1,0.906501,"rse relation labeling determines how two discourse units cohere to each other. A discourse unit may be a clause, a sentence, or a group of sentences. The labeled relation has many potential applications. Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al., 2011). Discourse relations are used to order sentences in an event in a summarization system (Derczynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the discourse relation labeling tasks. When two arguments are temporally related, they form a Temporal relation. The"
C14-1060,P04-1087,0,0.0967729,"Missing"
C14-1060,P11-1100,0,0.0746483,"Missing"
C14-1060,P09-2004,0,0.106921,"Missing"
C14-1060,prasad-etal-2008-penn,0,0.238062,"in an event in a summarization system (Derczynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the discourse relation labeling tasks. When two arguments are temporally related, they form a Temporal relation. The Contingency relation talks about the situation that the event in one argument casually affects the event in the other argument. Comparison is used to show the difference between two arguments. The last one relation, Expansion, is the most common. An Expansion relation either expands the information for one argument in the other one or continues the na"
C14-1060,C12-2128,0,0.0451216,"Missing"
C14-1060,J05-2005,0,0.225533,"Missing"
C14-1060,yu-etal-2012-development,1,0.576379,"595 0.6879 0.7334 0.7344 0.6600 0.6694 0.6731 0.7276 0.7299 0.6606 0.6644 0.6805 0.7305 0.7322 Table 3: Performance comparisons among models. (a) F-Score (b) Precision/Recall Figure 2: Effects of the number of test instances for each connective on relation labeling. 638 4 Further Analyses on a Big Dataset We further apply the best model (M4) to predict the probability distributions of discourse functions of connectives on a big dataset. For each discourse connective c, up to 500 sentences composed of two discourse units linked by c are randomly selected from the Chinese Web POS tagged corpus (Yu et al., 2012). The limitation of 500 is set to reduce the imbalance among the discourse connectives. Some connectives appear quite often in the dataset, e.g., the connective “也” (yě, also). Some connectives appear less than 500 times, e.g., “千萬…不然” (qiān wàn…bù rán, must...otherwise) occurs only 212 times. Finally, total 302,293 sentences are extracted and predicted. Because the dataset is very large, it is not easy to evaluate each pair of discourse units. We examine the linguistic phenomena instead. A lexicon of the probability distributions of connectives estimated by M4 is available at http://nlg.csie."
C14-1060,D11-1015,0,0.03034,"orithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The bags of hypernyms of"
C14-1060,C12-2138,0,0.338029,"r of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The bags of hypernyms of the words in ds1 an"
C14-1060,P12-1008,0,0.178731,"Missing"
C14-1060,afantenos-etal-2012-empirical,0,\N,Missing
C16-1085,J96-1002,0,0.0290129,"the performance gaps among the three models are more apparent. The information of Chinese word segmentation and POS tags not only speeds up training, but also improves the generalization. (a) L1 Testing (b) L2 Testing Figure 5: Accuracy versus order of n-gram in L1 and L2 testing. 895 Previous work suggests that the ensemble of RNN and traditional language model may improve the performance, especially for the RNN models with small hidden layer (Mikolov, 2012; Mikolov et al., 2011). We build an ensemble model GRU-ME by joint training the GRU model with a 4-gram maximum entropy language model (Berger et al., 1996). The input unit is word with POS tag (w/p), which has best performance in the experiments. The feature size of maximum entropy model is 1 billion. Figure 6 compares the performances between GRU and GRU-ME in L1 and L2. In L1 testing, ensembling the maximum entropy model with GRU significantly increases the performances of three GRU models, especially the ones with smaller hidden layer. In L2 testing, ensembling the maximum entropy model increases the performances of the GRU model with hidden sizes of 128. The results confirm that the RNN models with smaller hidden size gain from the combinati"
C16-1085,P15-1068,0,0.0461291,"Missing"
C16-1085,N13-1055,0,0.0200958,"ted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), H"
C16-1085,P15-1168,0,0.0177291,"lity of a preposition in a given sentence. Instead of the single sigmoid or tanh function used by the simple RNN, GRU model, which is simplified from LSTM, uses a structure namely gated recurrent unit in the hidden layer. ?? = (1 − ?)ℎ + ???−1 ℎ = tanh(?ℎ ?? + ?? (?ℎ ??−1 )) ? = σ(?? ?? + ?? ??−1 ) ? = σ(?? ?? + ?? ??−1 ) As shown in Figure 3, the update gate z decides how much the hidden state s is updated with the candidate hidden state ? , while the reset gate r decides how much the memory to be forgotten. GRU is reported to be better for long-term dependency modeling (Chung et al., 2014; Chen et al., 2015b) than the simple RNN. Compared to LSTM, GRU requires few parameters for the same size of hidden layer. Figure 3: Gated recurrent unit (GRU). In this work, we use the noise contrastive estimation (NCE) (Chen et al., 2015a) as the output layer for both simple RNN and GRU language models. The training performance of NCE is comparable to the class-based softmax, but NCE is faster in testing stage and can speed up together with GPU in training stage. The implementation of RNN models is based on faster-rnnlm, a toolkit for RNN language modelling5. 6 Experiments Three language models, n-gram, simpl"
C16-1085,C14-1028,1,0.858158,"l for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way that the wrong sentences"
C16-1085,W07-1604,0,0.110389,"d in this study. Section 5 presents our approach to Chinese preposition selection. Section 6 shows the experimental results. We further analyze the results in Section 7. Finally, Section 8 concludes this work. 2 Related Work English preposition error detection has attracted much attention for years. Felice and Pulman (2007) proposed a voted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) in"
C16-1085,W11-2838,0,0.0865719,"is task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-T"
C16-1085,W07-1607,0,0.0921105,"Missing"
C16-1085,C08-1022,0,0.195871,"Missing"
C16-1085,huang-etal-2008-quality,0,0.0781653,"Missing"
C16-1085,D15-1022,0,0.031023,"sk of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and cor"
C16-1085,L16-1033,1,0.590434,"s. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way that the wrong sentences are translated to correct ones. The preposition error detection is one of error cases in NLP-TEA shared tasks. However, the preposition corre"
C16-1085,P10-2065,0,0.0243199,"much attention for years. Felice and Pulman (2007) proposed a voted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered i"
C16-1085,W13-3616,0,0.0207347,"tection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error dete"
C16-1085,C12-1184,1,0.868922,"um entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way"
C16-1085,W14-1713,0,0.0278781,". Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese gr"
C16-1085,W15-4417,0,0.0306622,"Missing"
C16-1210,P15-2110,0,0.0172882,"th International Conference on Computational Linguistics: Technical Papers, pages 2227–2237, Osaka, Japan, December 11-17 2016. lation from CDTB, and manually label the tense for each argument as ground-truth for the two tasks. To automatically extract the tense features, a Chinese tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type classification and caus"
C16-1210,P14-1093,0,0.0146087,"tention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2"
C16-1210,D12-1057,0,0.0735927,"Missing"
C16-1210,D14-1181,0,0.00996357,"l. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information features to the sentence representation. In this work, we employ DCNN for Chinese tense classification under supervised, unsupervised, and semi-supervised learning. 3 Linguistic Resources Three types of corpora are used in this work. Section 3.1 describes the corpus for Chinese causal analysis. Section 3.2 and Section 3.3 int"
C16-1210,D14-1224,0,0.165688,"ish, the topic of causal analysis in Chinese is rarely touched. In this work, we explore the role of tense information in Chinese causal analysis. As pointed by Mirza (2014), the causal relation and temporal information is correlated. In a causal relation, the cause intuitively precedes its effect. In other words, the tense information could be useful features in the tasks of causal analysis. Two tasks of causal analysis are investigated in this study: causal type classification and causal directionality identification. The Chinese discourse relation corpus, Chinese Discourse Treebank (CDTB) (Li et al., 2014), is adopted as our dataset. In CDTB, six types of causality relations, Purpose, Background, Hypothetical, Inference, Condition, and Cause-Result, are defined. A discourse relation connects two arguments. In the case of causality, one of the two arguments (e.g., arg1) presents a situation, and it is causally affected by the other argument (e.g., arg2). For example, the first part of the sentence (S1) shows a reason, and the second part, which is underlined, is its effect. (S1) 由於產能不足，國內自給率不到四成，大部分要仰賴進口。 (Because of insufficient capacity, the domestic self-sufficiency rate is less than 40, most"
C16-1210,I11-1125,0,0.0171324,"include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014"
C16-1210,P15-2029,0,0.0150605,"supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information features to the sentence representation. In this work, we employ DCNN for Chinese tense classification under supervised, unsupervised, and semi-supervised learning. 3 Linguistic Resources Three types of corpora are used in this work. Section 3.1 describes the corpus for Chinese causal analysis. Section 3.2 and Section 3.3 introduce the corpora for developing our Chinese tense predictor. 3.1 Chinese Causality Corpus There are few resources for Chinese causal analysis. In this work, we extract instances labelled with ca"
C16-1210,P14-5010,0,0.00503366,"Missing"
C16-1210,P13-3006,0,0.0291371,"ion in causal type and causal directionality identification. Section 6 concludes this paper. 2 Related Work Causal analysis attracts much attention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised mod"
C16-1210,P14-3002,0,0.136386,"our semisupervised approach improves the dependency-based convolutional neural network (DCNN) models for Chinese tense labelling and thus the causal analysis. 1 Introduction Causal analysis plays a crucial role in the applications such as event extraction (Hashimoto et al., 2012; 2014), causality inference (Tanaka et al., 2012), question-answering (Oh et al., 2013), and motivation identification (Nguyen et al., 2015). Compared to English, the topic of causal analysis in Chinese is rarely touched. In this work, we explore the role of tense information in Chinese causal analysis. As pointed by Mirza (2014), the causal relation and temporal information is correlated. In a causal relation, the cause intuitively precedes its effect. In other words, the tense information could be useful features in the tasks of causal analysis. Two tasks of causal analysis are investigated in this study: causal type classification and causal directionality identification. The Chinese discourse relation corpus, Chinese Discourse Treebank (CDTB) (Li et al., 2014), is adopted as our dataset. In CDTB, six types of causality relations, Purpose, Background, Hypothetical, Inference, Condition, and Cause-Result, are define"
C16-1210,C14-1198,0,0.0306565,"a sentence. The rest of this paper is organized as follows. Section 2 surveys the related work. Section 3 describes the experimental materials. Section 4 shows our approach to Chinese tense labelling. Section 5 illustrates the use of tense information in causal type and causal directionality identification. Section 6 concludes this paper. 2 Related Work Causal analysis attracts much attention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to addre"
C16-1210,D15-1308,0,0.0437231,"Missing"
C16-1210,J03-1002,0,0.0114412,"Missing"
C16-1210,P13-1170,0,0.0619385,"Missing"
C16-1210,W14-4322,0,0.0683198,"Missing"
C16-1210,tian-etal-2014-um,0,0.123759,"se tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type classification and causal directionality identification. The experimental results are compared with the supervised approach and the ideal situation where human-labelled information is available. The contribution of this paper is three-fold: (1) we transfer the tense information from English sentence to i"
C16-1210,xue-etal-2008-annotating,0,0.0199489,"OLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2227–2237, Osaka, Japan, December 11-17 2016. lation from CDTB, and manually label the tense for each argument as ground-truth for the two tasks. To automatically extract the tense features, a Chinese tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type class"
C16-1210,xue-zhang-2014-buy,0,0.0204678,"h et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information feat"
C16-1210,D14-1204,0,0.0190837,"lity detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vect"
C16-2059,D11-1142,0,0.0605895,"ional patterns like PATTY (Nakashole et al., 2012) show efficacy on related applications (Dutta et al., 2015). In this work, we present a system for Chinese relation extraction and release a collection of human-verified Chinese relational patterns as a resource. We also demonstrate the applications of relational patterns on the demo website. This paper is organized as follows. Section 2 surveys the related work. Section 3 describes the methodology. Section 4 shows and discusses the results. Section 5 demonstrates the NL2KB system. 2 Related Work Information extraction (IE) models like ReVerb (Fader et al., 2011) automatically extract information from unstructured or semi-structured documents. Given an English sentence, ReVerb identifies two arguments and their relation in the form of (argument1, relation, argument2). PATTY (Nakashole et al., 2012) is a taxonomy system of relational patterns in English. From Wikipedia and the New York Times, 127,811 relational patterns are mined to describe 225 DBpedia properties, and 43,124 relational patterns are mined to describe 25 YAGO’s properties. However, the coverage is still an issue. Most open IE systems are developed for English, and few are for other lang"
C16-2059,D12-1104,0,0.0275646,"becomes a trend. In the sentence “蜜雪兒歐巴馬嫁給巴拉克奧巴馬” (Michelle Obama is married to Barack Obama), there are the two entities, i.e., 蜜雪兒歐巴馬 (Michelle Obama) and 巴拉克奧巴馬 (Barack Obama), and a relation 嫁給 (is married to) between them. In DBpedia, the relation 嫁給 (is married to) is represented as the property &lt;spouse&gt;. In other words, 嫁給 (is married to) in NL side is an NL relational pattern of the property &lt;spouse&gt; in KB side. The vocabulary gap not only affects knowledge base construction, but also knowledge retrieval applications such as question answering. English relational patterns like PATTY (Nakashole et al., 2012) show efficacy on related applications (Dutta et al., 2015). In this work, we present a system for Chinese relation extraction and release a collection of human-verified Chinese relational patterns as a resource. We also demonstrate the applications of relational patterns on the demo website. This paper is organized as follows. Section 2 surveys the related work. Section 3 describes the methodology. Section 4 shows and discusses the results. Section 5 demonstrates the NL2KB system. 2 Related Work Information extraction (IE) models like ReVerb (Fader et al., 2011) automatically extract informat"
C16-2059,D14-1201,0,0.0388192,"Missing"
C16-2059,P15-1128,0,0.02107,"lled NL2KB in this paper, users can browse which properties in KB side may be mapped to for a given relational pattern in NL side. Besides, they can retrieve the sets of relational patterns in NL side for a given property in KB side. We describe how the mapping is established in detail. Although the mined patterns are used for Chinese knowledge base applications, the methodology can be extended to other languages. 1 Introduction Knowledge bases (KBs) such as YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2014) are useful resources in various applications such as question answering (Yih et al., 2015). KBs contain rich information of entities and their properties. A fact in a KB is usually represented as the form (entity1, property, entity2). Most KBs rely on manpower for editing and maintenance, so it is challenging to keep them up-to-date. Frank et al. (2012) point out the latency issue in knowledge base update. How to construct and update the knowledge base automatically is indispensable. Mining facts from natural language (NL) statements and introducing them to knowledge base becomes a trend. In the sentence “蜜雪兒歐巴馬嫁給巴拉克奧巴馬” (Michelle Obama is married to Barack Obama), there are the tw"
C18-1141,N16-1163,0,0.0172016,"014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?, ?) be an ontology that contains the semantic relationship, where ? = {?1 , … , ?? } is a set of senses and |? |is total number of senses. The edge (?, ?) ∈ ? indicat"
C18-1141,N15-1184,0,0.0654751,"Missing"
C18-1141,P12-1092,0,0.2912,"gies. In Table 1, row 3 shows the number of words that are both listed in the datasets and the ontology. The word count in Roget is 63,942. MEN 3,000 751 707 Pair count Word count Roget MTurk 287 499 416 RW 2,034 2,951 2,371 WS353 353 437 412 Table 1. A summary of the semantic relatedness benchmark datasets. 4.3 Contextual Word Similarity Although the semantic relatedness datasets are used in many researches, one major disadvantage is that the words in those word pairs do not have contexts. Therefore, we also conduct experiments with the Stanford's Contextual Word Similarities (SCWS) dataset (Huang et al., 2012). SCWS consists of 2,003 word pairs together with human rated scores. A higher score value indicates higher semantic similarity. Different from the semantic relatedness datasets, the words in the SCWS have their contexts and partof-speech tags. That is, the human subjects can know the usage of the word when they rate the similarity. For each word pair, we compute its AvgSimC/MaxSimC scores from the learned sense embedding (Reisinger and Mooney, 2010): ? AvgSimC(?, ? ′) K 1 ≝ 2 ∑ ∑ ??,?,? ?? ′ ,?′ ,? ? (?? (?), ?? (? ′ )) ? (7) ?=1 ?=1 MaxSimC(?, ? ′ ) ≝ ?(?(?), ?(? ′ )) (8) where ??,?,? ≝ ?("
C18-1141,P15-1010,0,0.171877,"pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding learning framework that retrofits a pre-trained word embedding via incorporating with the semantic relations between the senses, the relation strength and the semantic strength. Although some parts of the idea are not new, it is the first time of putting all the parts into a generalized framework. Our proposed GenSense for generating low-dimensional sense embedding is inspired from sense retro (Jauhar et al., 2015), bu"
C18-1141,N15-1070,0,0.669654,"es propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding learning framework that retrofits a pre-trained word embedding via incorporating with the semantic relations between the senses, the relation strength and the semantic strength. Although some parts of the idea are not new, it is the first time of putting all the parts into a generalized framework. Our proposed GenSense for generating low-dimensional sense embedding is inspired from sense retro (Jauhar et al., 2015), but has three major differences. First,"
C18-1141,W16-2509,0,0.0302928,"′ )) (8) where ??,?,? ≝ ?(?(?), ?? (?)) is the likelihood of context ? belonging to cluster ?? , and ?(?) ≝ ?arg max ??,?,? (?), the maximum likelihood cluster for ? in context ?. We use a window size of 5 for 1≤?≤? the words in the word pairs (i.e., 5 words prior to ?/? ′ and 5 words after ?/? ′ ). Stop words are removed from the context. For measuring the performance, we compute the spearman correlation between the human rated scores and the AvgSimC/MaxSimC scores. 4.4 Semantic Difference This task is defined to answer if a word has a closer semantic feature to a concept than another word (Krebs and Paperno, 2016). In this dataset, there are 528 concepts, 24,963 word pairs, and 128,515 items. Each word pair comes with a feature. For example, in the test (????????, ℎ?????????): ?????, choosing the first word if and only if cos(????????, ?????) > ???(ℎ?????????, ?????), otherwise, choose the second word. As this dataset does not provide context for disambiguation, we use the similar strategies from the semantic relatedness task: ?? ??′ 1 AvgSimD(?, ? ′ ) ≝ ∑ ∑ cos (??? , ?? ′ ) ? ?? ??′ (9) ?=1 ?=1 MaxSimD(?, ? ′ ) ≝ max 1≤?≤?? ,1≤?≤??′ 1666 cos (??? , ?? ′ ) ? (10) In AvgSimD, we choose the first word i"
C18-1141,D17-1034,0,0.144594,"ntain lexical knowledge, such as WordNet (Fellbaum, 1998), Roget’s 21st Century Thesaurus (Kipfer and Institute, 1993) or the paraphrase database (Pavlick et al., 2015). As a result, many researches combine the word embedding with ontological resources, either in a joint training (Bian et al., 2014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learn"
C18-1141,D15-1200,0,0.0409704,"phrase database (Pavlick et al., 2015). As a result, many researches combine the word embedding with ontological resources, either in a joint training (Bian et al., 2014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?,"
C18-1141,W13-3512,0,0.202468,"ess specifically address, αs are set to 1 in the experiments. We set the convergence criteria for sense vectors to ϵ = 0.1 with the number of iterations of 10. With the capability of generalization, we run three types of the model: GenSense-syn (only considers the synonyms and positive contextual neighbors), GenSense-ant (only considers the antonyms and negative contextual neighbors) and GenSense-all (considers everything). 4.2 Semantic Relatedness We downloaded four semantic relatedness benchmark datasets from the web: MEN (Bruni et al., 2014), MTurk (Radinsky et al., 2011), Rare Words (RW) (Luong et al., 2013) and WordSim353 (WS353) (Finkelstein et al., 2001). In MEN dataset, there are two versions of the word pairs: lemma and natural form. We show the natural form in the experimental result, but the performances on two datasets are similar. In each dataset, there is a list of word pairs together with their corresponding human rated scores. A higher score value indicates higher semantic similarity. For example, the score of (journey, voyage) is 9.29 and the score of (king, cabbage) is 0.23 in WS353. For measuring the semantic similarity between a word pair (?, ? ′ ) in the datasets, we adopt the se"
C18-1141,P15-2070,0,0.0440337,"Missing"
C18-1141,D14-1162,0,0.103966,"om the existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiments, we show that the generalized model outperforms the previous approaches in three aspects: semantic relatedness, contextual word similarity and semantic difference. 1 Introduction The distributed representation of word model (word embedding) has drawn great interest in recent years due to its ability to acquire syntactic and semantic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). With the pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pr"
C18-1141,D16-1018,0,0.0210197,"Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?, ?) be an ontology that contains the semantic relationship, where ? = {?1 , … , ?? } is a set of senses and |? |is total number of senses. The edge (?, ?) ∈ ? indicates a semantic relat"
C18-1141,N10-1013,0,0.424163,"., 2001). In MEN dataset, there are two versions of the word pairs: lemma and natural form. We show the natural form in the experimental result, but the performances on two datasets are similar. In each dataset, there is a list of word pairs together with their corresponding human rated scores. A higher score value indicates higher semantic similarity. For example, the score of (journey, voyage) is 9.29 and the score of (king, cabbage) is 0.23 in WS353. For measuring the semantic similarity between a word pair (?, ? ′ ) in the datasets, we adopt the sense evaluation metrics AvgSim and MaxSim (Reisinger and Mooney, 2010): ?? ??′ 1 AvgSim(?, ? ′ ) ≝ ∑ ∑ cos (??? , ?? ′ ) ? ?? ?? ′ ?=1 ?=1 1665 (5) MaxSim(?, ? ′ ) ≝ max 1≤?≤?? ,1≤?≤??′ cos (??? , ?? ′ ) ? (6) where ?? and ?? ′ denote the number of senses of ? and ? ′ , respectively. The AvgSim can be seen as a soft metric as it averages all the similarity scores. Whereas the MaxSim can be seen as a hard metric as it only selects the senses with maximum similarity score. For measuring the performance of the sense embedding, we compute the spearman correlation between the human rated scores and the AvgSim/MaxSim scores. Table 1 shows a summary of the benchmark da"
C18-1141,P14-2089,0,0.115809,"eneralized model outperforms the previous approaches in three aspects: semantic relatedness, contextual word similarity and semantic difference. 1 Introduction The distributed representation of word model (word embedding) has drawn great interest in recent years due to its ability to acquire syntactic and semantic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). With the pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding l"
C18-1204,C14-1028,1,0.844615,"check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs a"
C18-1204,W17-5037,0,0.0123316,"rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary research in Chinese WUE correction, we impose restrictions"
C18-1204,D16-1195,0,0.0134594,"nal human annotations. 2 Related Work English GEC is a rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary rese"
C18-1204,D11-1010,0,0.0247116,"WUE dataset with additional human annotations. 2 Related Work English GEC is a rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Theref"
C18-1204,W11-2838,0,0.0564404,"Missing"
C18-1204,W12-2006,0,0.0686296,"Missing"
C18-1204,P13-2121,0,0.0327239,"Missing"
C18-1204,C16-1085,1,0.859745,"et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs and nouns. Correcting errors of such open-set types could be much more difficult since the set of candidates can be huge. To the best of our knowledge, this is the first research dealing with general-type Chinese WUE correction. 3 Neural Network-based Correctio"
C18-1204,W15-4401,0,0.0132183,"is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection"
C18-1204,W16-4906,0,0.0153392,"ent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 20"
C18-1204,P14-5010,0,0.00249225,"llowing weighted harmonic mean to obtain a new “rank” for the candidate. 1 rcom = α (4) 1−α rLM + rM LP 2415 where α is a parameter that can be tuned with the validation set (actual values will be given in Section 8.2). Preliminary experiments show that harmonic mean performs better than arithmetic and geometric mean. Though rcom may not be an integer, it can be interpreted as a rank. The correction with smaller rcom is considered better. 7 Experimental Settings We adopt the HSK WUE dataset released by Shiue et al. (2017)2 and follow their train/validation/test split. We use Stanford CoreNLP (Manning et al., 2014) for Chinese word segmentation and POS tagging. For each split, we filter the instances where the correction is not within the top 50,000 frequent words in the Chinese part of the ClueWeb corpus3 (Yu et al., 2012). This decision is made based on the fact that the vocabulary used by non-native language learners is limited. After filtering, the number of instances in the train, validation, and test sets are 8,205, 1,026, and 1,025, respectively. With punctuation marks and English words eliminated, the candidate vocabulary size |D |is 48,394. Our MLP model has two hidden layers of size 1,024. The"
C18-1204,K16-1006,0,0.199464,"cope with this problem, we experimented with the CWE variant that only keeps one vector for each Chinese character regardless of its position, but the performance is not as good as the model with 2413 CWE+P features. Alternatively, we design a separate set of character embedding features CWEc which is the sum of the character embedding of all positions divided by the number of characters in the word. 1P For instance, CWEc (決解) = 2 p=s,m,e [CE(決, p) + CE(解, p)]. As can be seen, CWEc (決解) will contain CE(解, s) and CE(決, e), which are the terms of CWEw (解決). 5.3 Context2vec Features Context2vec (Melamud et al., 2016) is a bidirectional LSTM-based model that can encode a “context” into a real-valued vector. A context is a sequence of words with a certain position blanked out. For instance, (E3-1) is a context: (E3-1) 可是 每 個 人 的 [ ] 都 千差萬別 ( but everyone’s [ ] is different ) The representation of a context is a combination of the sequence of words before and after the blank: C2Vctx (w1 ...wp−1 [ ]wp+1 ...wL ) = LSTM(w1 ...wp−1 ) ⊕ LSTM(wp+1 ...wL ) (2) where each wi is a token, L is the number of tokens, p is the index of the blank, and ⊕ is the vector concatenation operation. Context2vec also keeps the emb"
C18-1204,W13-3601,0,0.0213176,"he model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary research in Chinese WUE correction, we impose restrictions on our setting that there is exactly one error in a sentence segment, the error position is known, and the error can be corrected by replacing the erroneous token with an appropriate word. The distribution of errors of non-native Chinese differ a lot from that of non-native English. In the CoNLL 2013 shared task (Ng et al., 2013), the most frequent error types are article, preposition, noun number, verb form, and subject-verb agreement. These error types are mostly in violation of English 2411 Input Sentence 生活方式已經 猛烈 地改變了 The way of living has been fiercely Character-enhanced Word Embedding Target Feature changed Candidate Vocabulary Candidate Embedding Context2vec LM Re-Ranking Candidate Vector Context Feature Multilayer Perceptron Correction Candidate Correction Vector Cosine Similarity Candidate Score Figure 1: Overview of our correction generation model. The score of a correction candidate d is predicted for repl"
C18-1204,W14-1701,0,0.0551719,"Missing"
C18-1204,I17-4001,0,0.0188285,"error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. ("
C18-1204,L16-1033,1,0.877351,"tools can help language learners revise their writing. Chinese GEC tools are in high demand since Chinese has become an increasingly popular second language worldwide. Despite the increasing need, most of the existing studies on GEC are based on English learner data. The method of correcting sentences in Chinese, a language which differs substantially from English in major aspects such as the morphological structure and the distribution of learner errors, has not yet been fully developed. This paper focuses on the correction of Chinese word usage errors (WUEs). According to the definition of Shiue and Chen (2016), a WUE refers to an incorrect token that involves morphological, syntactical, or semantical problems. The token is either an incorrect word form, or a correct existent word that is improper for its context. Given a token in a sentence segment that is known to be erroneous, we aim to generate a suitable correction for it. The criteria for a suitable correction are: • Correctness: After substituting the erroneous token with the correction token, the result is a syntactically and semantically correct Chinese sentence segment. This work is licensed under a Creative Commons Attribution 4.0 Interna"
C18-1204,P17-2064,1,0.798642,"Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs and nouns. Correcting errors of such open-set types could be much more difficult since the set of candidates can be huge. To the best of our knowledge, this is the first research dealing with general-type Chinese WUE correction. 3 Neural Ne"
C18-1204,W15-3106,1,0.863638,", in the HSK dynamic composition corpus built by Beijing Language and Culture University, which is the largest available Chinese learner corpus at the time of this study, WUE is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction ca"
C18-1204,W13-4406,0,0.0323545,"patterns of correction. In contrast, in the HSK dynamic composition corpus built by Beijing Language and Culture University, which is the largest available Chinese learner corpus at the time of this study, WUE is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further"
C18-1204,C12-1184,1,0.756229,"ects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating ho"
C18-1204,yu-etal-2012-development,1,0.824212,". Preliminary experiments show that harmonic mean performs better than arithmetic and geometric mean. Though rcom may not be an integer, it can be interpreted as a rank. The correction with smaller rcom is considered better. 7 Experimental Settings We adopt the HSK WUE dataset released by Shiue et al. (2017)2 and follow their train/validation/test split. We use Stanford CoreNLP (Manning et al., 2014) for Chinese word segmentation and POS tagging. For each split, we filter the instances where the correction is not within the top 50,000 frequent words in the Chinese part of the ClueWeb corpus3 (Yu et al., 2012). This decision is made based on the fact that the vocabulary used by non-native language learners is limited. After filtering, the number of instances in the train, validation, and test sets are 8,205, 1,026, and 1,025, respectively. With punctuation marks and English words eliminated, the candidate vocabulary size |D |is 48,394. Our MLP model has two hidden layers of size 1,024. The activation function is Rectified Linear Unit (ReLU) and the dropout rate is set to 0.2. The parameters are optimized with Adagrad (Duchi et al., 2011) under a cosine proximity objective function. CWE (embedding s"
C18-1204,W14-6820,1,0.909066,"Missing"
C18-2016,P16-1139,0,0.0455085,"eft input right input Figure 2: Tree-LSTM unit used in the recursive neural network. 2.1 Recursive Neural Network Figure 2 illustrates the unit in our RvNN based on the Tree-LSTM unit (Tai et al., 2015). Given the left and the right inputs (i.e. two text segments or two DUs), the Tree-LSTM composition function produces a representation for the new tree node. The Tree-LSTM unit generalizes the LSTM unit to the treebased input. Similar to LSTM, Tree-LSTM makes use of intermediate states as a pair of an active state representation ~h and a memory representation ~c. We use the version similar to (Bowman et al., 2016) as the formula:     ~i σ  ~  ! "" #  fl   σ  ~h1  ~   s (1) fr  =  σ  Wcomp ~ 2 + ~bcomp     hs  ~o   σ  ~g tanh (2) ~c = f~l ~cs2 + f~r ~cs1 + ~i ~g (3) ~h = ~o tanh(~c) where σ is the sigmoid activation function, is the element-wise product, and the pairs h~h1s , ~cs1 i and h~h2s , ~cs2 i are input from its two children tree nodes. The output of Tree-LSTM is the pair h~h, ~ci. Note that the Tree-LSTM unit is designed for binary tree. We handle the multinuclear with the same scheme as Kang et al. (2016). The representations ~h and ~c produced by Tree-LSTM are taken"
C18-2016,W01-1605,0,0.626667,"urse parse tree based on the predictions made by TreeLSTM. The Cocke–Younger–Kasami (CKY) algorithm (Younger, 1967) is employed to maximize the probability of the whole parse tree. The dynamic programing algorithm simulates the recursive parsing procedure, considering local and global information jointly. 3 Experiments We compare our model LSTM-RvNN with the baseline model proposed by Kang et al. (2016). To the best of our knowledge, it is the only existing Chinese discourse parser at the paragraph level. We also evaluate our model given the golden EDUs. The standard evaluation tool PARSEVAL (Carlson et al., 2001) is performed to measure the F-score of the tree structure prediction. Table 1 shows the experimental results. The F-scores of EDU segmentation, parse tree construction (Structure), parse tree construction with sense labeling (+Sense), parse tree construction with center labeling (+Center), and parse tree construction with both sense and center labeling (Overall) are reported. In general, our model outperforms the baseline model in every aspect except EDU segmentation. Even so, the final discourse parse trees constructed and labeled by our model are more accurate. Model Baseline with golden ED"
C18-2016,W12-1636,1,0.826807,"ing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external"
C18-2016,P17-1092,0,0.0252636,"迴類神經網路為基 礎，同時對四項子任務進行學習，在中文語篇樹庫（CDTB）資料集上，達到最先進的 效能。我們釋出了這個剖析器的原始碼與預先訓練完成的模型，立即可用。據我們所 知，這是第一個開放原始碼的中文剖析工具集，而且這套獨立的工具集不須依賴外部資 源（如句法剖析器），便於下游應用的整合。 1 Introduction Discourse parsing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition,"
C18-2016,K16-2003,0,0.321901,"solated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external third party packages. This is an important issue especially for a pipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informa"
C18-2016,D14-1220,0,0.335996,"ipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informations to do the above tasks. For a toolkit targeting real-world applications, a standalone system is more robust and easy to This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 73 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 73–77 Santa Fe, New Mexico, USA, August 20-26, 2018. deploy. Inspired by Li et al. (2014a), in this work we propose an end-to-end Chinese discourse parser that performs EDU segmentation, discourse tree construction, and discourse relation labeling in a unified framework based on recursive neural network (RvNN) proposed by Goller and Kuchler (1996). The RvNN model learns to construct the structured output through merging children nodes to parent nodes in the bottom-up fashion. Within the RvNN paradigm, recurrent neural network (RNN) is employed to model the representations from word segments, discourse units, to the whole paragraph. RNN like long short-term memory (LSTM) neural ne"
C18-2016,D14-1224,0,0.318033,"ipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informations to do the above tasks. For a toolkit targeting real-world applications, a standalone system is more robust and easy to This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 73 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 73–77 Santa Fe, New Mexico, USA, August 20-26, 2018. deploy. Inspired by Li et al. (2014a), in this work we propose an end-to-end Chinese discourse parser that performs EDU segmentation, discourse tree construction, and discourse relation labeling in a unified framework based on recursive neural network (RvNN) proposed by Goller and Kuchler (1996). The RvNN model learns to construct the structured output through merging children nodes to parent nodes in the bottom-up fashion. Within the RvNN paradigm, recurrent neural network (RNN) is employed to model the representations from word segments, discourse units, to the whole paragraph. RNN like long short-term memory (LSTM) neural ne"
C18-2016,W10-4327,0,0.021974,"篇單元分割、剖析樹建立、主次關係識別、語 篇關係辨識等。本文展示一個點對點中文語篇剖析器，並提出一套統一架構，可以對輸 入之中文篇章直接產生完整的中文語篇剖析結果。我們的剖析器以遞迴類神經網路為基 礎，同時對四項子任務進行學習，在中文語篇樹庫（CDTB）資料集上，達到最先進的 效能。我們釋出了這個剖析器的原始碼與預先訓練完成的模型，立即可用。據我們所 知，這是第一個開放原始碼的中文剖析工具集，而且這套獨立的工具集不須依賴外部資 源（如句法剖析器），便於下游應用的整合。 1 Introduction Discourse parsing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of err"
C18-2016,P15-1150,0,0.0479636,"e11 s3 s2 s1 w13 …… w21 w22 w23 …… e31 e32 e33 …… =Embedding Layer w31 w32 w33 …… Figure 1: Architecture of our unified RvNN discourse parser. 1 Merge Score Sense Classifier Center Classifier http://nlg.csie.ntu.edu.tw/nlpresource/cdp/ to parent unit 74 Tree-LSTM Unit left input right input w11 w12 w13 …… Merge Score w21 w22 Sense Classifier w23 …… w31 Center Classifier to parent unit w32 w33 …… Tree-LSTM Unit left input right input Figure 2: Tree-LSTM unit used in the recursive neural network. 2.1 Recursive Neural Network Figure 2 illustrates the unit in our RvNN based on the Tree-LSTM unit (Tai et al., 2015). Given the left and the right inputs (i.e. two text segments or two DUs), the Tree-LSTM composition function produces a representation for the new tree node. The Tree-LSTM unit generalizes the LSTM unit to the treebased input. Similar to LSTM, Tree-LSTM makes use of intermediate states as a pair of an active state representation ~h and a memory representation ~c. We use the version similar to (Bowman et al., 2016) as the formula:     ~i σ  ~  ! "" #  fl   σ  ~h1  ~   s (1) fr  =  σ  Wcomp ~ 2 + ~bcomp     hs  ~o   σ  ~g tanh (2) ~c = f~l ~cs2 + f~r ~cs1 + ~i ~g (3)"
C18-2016,K16-2001,0,0.0256856,"r, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external third party packages. This is an important issue especially for a pipeline"
C18-2030,C14-1028,1,0.818261,"istical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained"
C18-2030,W17-5037,0,0.0159644,"trast, there is far fewer readily usable writing correction tools for Chinese. Chinese has become a popular foreign language to learn worldwide, motivating the development of Chinese writing correction system targeting second language (L2) learners. Unlike the classification approach, the translation approach to English GEC does not require exact recognition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al."
C18-2030,C16-1085,1,0.861265,"and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text. To our knowledge, we are the first to ap"
C18-2030,P17-4012,0,0.0426378,"it to the correction of Chinese. A typical NMT model is composed of an encoder and a decoder. The encoder transforms the input sequence into a sequence of hidden states, each of which is calculated with the hidden state of the previous time step and the input of the current time step. The decoder predicts the distribution of words for each time step conditioned on the encoder hidden states and the output of all previous time steps. The encoder-decoder network is trained to maximize the likelihood of the ground-truth translations in the training data. Our system is built on the top of OpenNMT (Klein et al., 2017). We adopt a bidirectional Long-short Term Memory (LSTM) encoder and a two-layer LSTM decoder. Global attention over the sequence of hidden states at the source side is applied. The model generates one to five corrections according to the n-best decoding result. Several design choices will be discussed in Section 2.2. 2.1 Datasets and Evaluation To train the NMT correction model, we utilize the publicly available datasets of the NLPTEA 14-17 CGED shared tasks1 . As a whole, there are more simplified Chinese sentences than traditional Chinese ones, so we convert all sentences to simplified Chin"
C18-2030,N15-1142,0,0.0159923,"ce “* 我 覺得 他 是 一個 很 好人” (* I think he is a very good-person) is corrected to “我 覺得 他 是 一個 很 好 的 人” (I think he is a very good person). Based on these results, we decide to use the word-based NMT model in our system. Pre-trained Word Embeddings Initializing word representations in NMT models with pre-trained word vectors can be useful when the training data is insufficient. In addition to the standard Word2vec continuous bag-of-words (CBOW) and Skip-gram (SG) embeddings (Mikolov et al., 2013), we also experiment with the continuous window (CWIN) and structured skip-gram (Struct-SG) embeddings (Ling et al., 2015), which consider the relative order of context words during training and are shown to be useful for Chinese error detection (Shiue et al., 2017). We segment the Chinese part of ClueWeb4 with the THULAC toolkit and train the embeddings with it. The embedding size is fixed to 500 and the context window size is 5 for all kinds of embeddings. The results are summarized in Table 1. All pre-trained word embeddings bring improvement over random embeddings. Generally, the NMT correction model with pre-trained Struct-SG embeddings achieves the best performance. Thus, we use Struct-SG embeddings in our"
C18-2030,P15-2097,0,0.0273702,"use the test data of NLPTEA 16 and 17 since there are only error type labels but no correction in the datasets. The correction performance can be evaluated by judging whether a correction is exactly the same as the ground-truth. We report the accuracy as well as hit rates of top candidates. However, hit rates can still be 1 https://sites.google.com/view/nlptea2018/shared-task 138 somehow strict since a model will not get any scores even if the top candidate it proposes is only slightly different from the answer. Thus, we also report the General Language Evaluation Understanding (GLEU) metric (Napoles et al., 2015), which is a modification of BLEU that rewards correct modifications while penalizing unnecessary changes. We use the publicly released toolkit2 to calculate GLEU of n-gram order 4. GLEU is calculated only for the top candidate. 2.2 Design Choices There are several design choices for building the NMT-based correction system. We discuss the reasons for each decision and show experimental results when necessary. In the experiments, we choose the model with the highest validation GLEU and report the performance on the test set. The GLEU of an output that is completely the same as the source can b"
C18-2030,I17-4001,0,0.0247161,"gnition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of w"
C18-2030,L16-1033,1,0.749204,"lish GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text"
C18-2030,P17-2064,1,0.916497,"eased by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text. To our knowledge, w"
C18-2030,tian-etal-2014-um,0,0.0232397,"0.418 0.414 0.433 0.431 char. GLEU 0.552 0.625 0.650 0.655 0.657 0.658 0.668 Table 1: Performance of NMT-based correction models 2 https://github.com/cnap/gec-ranking http://thulac.thunlp.org/ 4 http://lemurproject.org/clueweb09.php 3 139 word GLEU 0.411 0.558 0.564 0.564 0.566 0.580 3 Example Sentence Retrieval Besides giving correction suggestions, our system also shows example sentences to demonstrate how to correctly use the words and grammar patterns in the user input. These example sentences also serve as additional evidence of the correctness of some usage patterns.We adopt UM-Corpus (Tian et al., 2014), a sentence-aligned English-Chinese corpus, as the database of example sentences. We only use sentences in the “Education” domain, which are extracted from online teaching materials. There are 450,000 English-Chinese sentence pairs. We exclude example sentence pairs in which the Chinese sentence is longer than 30 Chinese characters since they usually have complex syntactic structures. Upon user input, ten example sentences are retrieved. They are ranked by the overlaps of Chinese character bigrams. The more character bigrams an example sentence has in common with the input sentence, the highe"
C18-2030,N16-1042,0,0.0278049,"ting the development of Chinese writing correction system targeting second language (L2) learners. Unlike the classification approach, the translation approach to English GEC does not require exact recognition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the p"
huang-etal-2014-sentence,O98-3002,0,\N,Missing
huang-etal-2014-sentence,lin-chen-2006-constructing,1,\N,Missing
huang-etal-2014-sentence,nilsson-nivre-2008-malteval,0,\N,Missing
huang-etal-2014-sentence,D09-1060,0,\N,Missing
huang-etal-2014-sentence,W13-1101,0,\N,Missing
huang-etal-2014-sentence,N10-4004,0,\N,Missing
huang-etal-2014-sentence,C12-2088,0,\N,Missing
huang-etal-2014-sentence,C12-1034,1,\N,Missing
huang-etal-2014-sentence,sekine-2008-extended,0,\N,Missing
I11-1170,W01-1605,0,0.194435,"the relationship between arguments is often difficult to decide and inherently subjective. Thus, the annotation and the evaluation are problematic and laborintensive. In recent years, the study of discourse relation recognition is growing in the English domain rapidly. One of the reasons is the availability of English corpora with discourse annotations. The two most popular discourse corpora are the RheHsin-Hsi Chen Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan hhchen@csie.ntu.edu.tw torical Structure Theory Discourse Treebank (RSTDT) (Carlson et al., 2001) and PDTB-2.0. Both of them are based on the Wall Street Journal corpus with human-annotated discourse information. The PDTB-2.0 consists of 36,592 pairs of successive arguments and is tagged with three classes, including Implicit, Explicit, and AltLex, and with the relation types at three levels. Based on these corpora, a number of aspects on discourse relation are explored in these years. Compared to the English corpora, there is still no Chinese discourse corpus worldwide available. For this reason, the dataset is the first challenge encountered in the study of Chinese discourse relation re"
I11-1170,D10-1039,0,0.0972035,"Missing"
I11-1170,D09-1036,0,0.140058,"Missing"
I11-1170,P02-1047,0,0.142817,"Missing"
I11-1170,P09-1077,0,0.262286,"Missing"
I11-1170,P09-2004,0,0.114215,"Missing"
I11-1170,prasad-etal-2008-penn,0,0.536791,"Missing"
I17-1098,D13-1160,0,0.111093,"Section 2 introduces related works of QA system. Section 3 presents our three-step paradigm. Section 4 shows the experiments on the SimpleQuestions dataset (Bordes et al., 2015) and compares ours with previous works. We also discuss the importance of each component of our system. Section 5 analyzes the errors in the experiments. Section 6 concludes the remarks. 2 Related Works Previous simple QA models can be divided into two categories. The first one is based on semantic parsing, which maps a question to its logical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form."
I17-1098,P14-1133,0,0.019421,"ts. Section 6 concludes the remarks. 2 Related Works Previous simple QA models can be divided into two categories. The first one is based on semantic parsing, which maps a question to its logical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form. Semantic parsing is then equivalent to finding a sub-graph of the KB which can represent the question. The semantic parsing approach which often requires the human annotated logical form may increase the cost of obtaining training data. Although the use of rule-based method to generate logical forms can reduce the use of annotat"
I17-1098,P16-1076,0,0.633441,"which can access KB to get the answer. 976 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 976–985, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Previous simple QA model often adopts a twostep paradigm. Entity-linking step identifies the subject entities in questions, and forms the candidate entity set and relation set. The candidate relation set is formed by all the relations which have connections with any entity in candidate entity set. Relation-finding step further identifies a proper relation from the candidate relation set. Dai et al. (2016) propose a neural-network based two-step approach to simple QA over Freebase, and formulate the task into a probabilistic form. Given a question q, the first step is to find the candidate relation r with high probability ?(?|?). The second step is to find the subject s with high ?(?|?, ?). As a result, the object in the KB triple which contains the subject s and the relation r with the highest ?(?, ?|?) is the answer. The relation in KB triple has hierarchical structure: domain-type-property. For example, in “people.person.place_of_birth”, “place_of_birth” is the property used to present the b"
I17-1098,P15-1026,0,0.0760089,"the QA problem into a probabilistic form. Given a question, the answer is the triple in KB containing the subject and relation with the highest conditional probability. They first use a focused pruning method to tag the span in question which is most probable to be the subject entity and gets the candidate answer triples. Then they use a relation network and a subject network, both are a two-layer bidirectional-GRU model, to get the similarity scores between candidate triples and question. We modify their probabilistic form with the splitting of the relation part ? into type ? and property ?. Dong et al. (2015) introduce a multi-column CNNs (MCCNNs) to analyze the question in three different aspects: answer path, answer context and answer type. The system represents the question in three low-dimensional vectors, each of them then matches to one of the answer aspect to derive the scores of candidates. Yin et al. (2016) also use the CNN-based approach to implement the QA system. They use a word-level CNN with attentive max-pooling to model the relationship between KB relations and question pattern. They also add an active linker, which is similar to the focused pruning method in Dai et al. (2016), to"
I17-1098,C16-1164,0,0.678121,"triples. Then they use a relation network and a subject network, both are a two-layer bidirectional-GRU model, to get the similarity scores between candidate triples and question. We modify their probabilistic form with the splitting of the relation part ? into type ? and property ?. Dong et al. (2015) introduce a multi-column CNNs (MCCNNs) to analyze the question in three different aspects: answer path, answer context and answer type. The system represents the question in three low-dimensional vectors, each of them then matches to one of the answer aspect to derive the scores of candidates. Yin et al. (2016) also use the CNN-based approach to implement the QA system. They use a word-level CNN with attentive max-pooling to model the relationship between KB relations and question pattern. They also add an active linker, which is similar to the focused pruning method in Dai et al. (2016), to reduce the number of candidates and improve the performance significantly. 3 Figure 1: System Overview. the relations “wine.wine.color” and “roses.roses.color” are used to describe the color of things, but they are in type “wine” and “roses”, respectively. We separate a relation into type and property parts to u"
I17-1098,P13-1158,0,0.120291,"e “(Harry Potter, author, J.K. Rowling)”. Although this category is “simple” question, retrieving a triple from the KB is not a trivial task due to the billions of facts in the KB. Complex questions contain more restrictions. These questions may involve two or more triples in the KB, or have other semantic constraints to restrict the answers to a smaller set. For example, “the first” in the question “What is the name of the first Harry Potter novel?” restricts that there is only one answer. Previous researches showed that simple questions are the more common category in community QA websites (Fader et al., 2013). This paper focuses on factoid simple question-answering over Freebase. Simple question can be answered with the object of one KB triple. Thus, the systems only need to find the subject and relation of the triple which can describe the question properly. The issues of simple QA are the identification of the subject entity in a question, and the resolution of the gap between the natural language expression in the question and the relation description in the KB. After a QA system receives users’ questions, it needs to transform a question into a KB query, e.g. SPARQL. The question can then be t"
I17-1098,D16-1166,0,0.0123487,"with the pre-trained GloVe (Pennington et al., 2014) with the dimension of 300. All the networks are optimized by mini-batch and Adam (Kingma et al., 2014) with the learning rate 0.001. The TYPE_LSTM in entity identification step has a drop rate of 0.2. The hidden size of LSTM is 2 Overall Results Table 1 shows the performances of our model compared with the other four methods. Bordes et al. (2015) use a memory network. Dai et al. (2016) employ a conditional focused neural-network based approach. Yin et al. (2016) apply attentive convolutional neural network with the passive or active linker. Golub and He (2016) use the character-level encoder-decoder framework. In our approach, “probability” means the outcome is the triple with the highest probability computed by Equation (1). We find that the subject entity and the property are more important than the type. The approach “sum” combines the scores ?1 (?, ?) , ?2 (?, ?) , and ?3 (?, ?) by weighted summation, and selects the triple with the highest weighted sum. The weights are entity:type:property = 4:1:3, which are tuned on the validation data. Our “probability” approach outperforms all previous models on FB5M, including the previous best model by Yi"
I17-1098,N16-1030,0,0.0385724,"verbs exist, this feature is set to 0 Out-degree: The number of out-going links of the entity in the KB is taken as Out-degree feature. The more the number of links the entity has, the higher the feature value is and the entity in the KB is more informative. The direction of links from subject to object represents the impact of the entity. IDF: We take each question in SimpleQuesions training set as a document, and compute the inverse document frequency of the entity. The higher the value is, the more specific the entity is. NER_LCS: We use the LSTM-CRF named entity recognition (NER) tagger1 (Lample et al., 2016) to find the span from the question that is most 1 Figure 2: Structure of Type_LSTM in entity identification. likely to be a subject entity, and compute the length of the longest common subsequence (in characters) between the candidate entity and the words in the tagged span. The tagger consists of an embedding layer, a bidirectional-LSTM layer, and a conditional random field layer to predict the label for each word. The higher the NER_LCS value is, the more possible the candidate is a subject entity. Type_LSTM: Each entity has entity types in Freebase to describe its characteristics, e.g., en"
I17-1098,D14-1162,0,0.080656,"7 test data, is adopted in the experiments. The evaluation is the same as in Bordes et al. (2015). The predicted answer is correct when the subject-relation pair is the same as the correct answer. We train our model on the training set. The validation set is used for early stop and parameter tuning. The test set is used for evaluation. 4.1 Experimental Setup The number of negative samples used in SVMrank is set to 5. Other parameters for SVMrank are C = 0.1, epsilon = 0.01, and loss function option = 2. The word embeddings used in each neural network is initialized with the pre-trained GloVe (Pennington et al., 2014) with the dimension of 300. All the networks are optimized by mini-batch and Adam (Kingma et al., 2014) with the learning rate 0.001. The TYPE_LSTM in entity identification step has a drop rate of 0.2. The hidden size of LSTM is 2 Overall Results Table 1 shows the performances of our model compared with the other four methods. Bordes et al. (2015) use a memory network. Dai et al. (2016) employ a conditional focused neural-network based approach. Yin et al. (2016) apply attentive convolutional neural network with the passive or active linker. Golub and He (2016) use the character-level encoder-"
I17-1098,P15-1128,0,0.0709812,"gical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form. Semantic parsing is then equivalent to finding a sub-graph of the KB which can represent the question. The semantic parsing approach which often requires the human annotated logical form may increase the cost of obtaining training data. Although the use of rule-based method to generate logical forms can reduce the use of annotated data, it limits the application domain. The second approach is information extraction, which needs only questionanswer pairs for training. This method retrieves some candidate answe"
L16-1164,D10-1039,0,0.020122,"bank (RST-DT) (Carlson et al., 2002) and the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) are two important resources to facilitate the researches of discourse relation recognition. Because most Chinese discourse corpora follow the PDTB scheme, this section surveys the related work from this direction. Pitler and Nenkova (2009) deal with two types of ambiguity of discourse connectives in English: non-discourse vs. discourse usage and unique vs. more discourse functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (201"
L16-1164,I11-1170,1,0.82219,"composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0.6288 on the 4-way inter-sentential relation classification. Zhou et al. (2012) report the F-score of 0.7481 on 4-way classification at the intra-sentential level. Li, Carpuat et al. (2014) show F-scores for binary classification for Temporal, Contingency, Comparison and Expansion are 0.4865, 0.4194, 0.5970 and 0.6920, respectively. 3. HIT-CDTB In HIT-CDTB 1.0 (http://ir.hit.edu.cn/hit-cdtb/), discourse relations at intra-sentential, inter-sentential, and passage levels are annotated. It follows the PDTB relation scheme with some modification. The top level consists of 4 di"
L16-1164,C14-1060,1,0.799083,"ation on 11 relation types is 0.40. Recently, five Chinese discourse corpora (Huang et al., 2013; Zhang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use c"
L16-1164,W13-2309,1,0.824976,"the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison vs. Rest, and Expansion vs. Rest are 0.2657, 0.4982, 0.3132, and 0.7922, respectively. Lin et al. (2009) report the micro-averaged F-score of a fine-grained classification on 11 relation types is 0.40. Recently, five Chinese discourse corpora (Huang et al., 2013; Zhang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 datase"
L16-1164,C14-1055,0,0.193887,"Missing"
L16-1164,D09-1036,0,0.025327,"functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-res"
L16-1164,W10-4310,0,0.0172922,"arlson et al., 2002) and the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) are two important resources to facilitate the researches of discourse relation recognition. Because most Chinese discourse corpora follow the PDTB scheme, this section surveys the related work from this direction. Pitler and Nenkova (2009) deal with two types of ambiguity of discourse connectives in English: non-discourse vs. discourse usage and unique vs. more discourse functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) pred"
L16-1164,W12-1614,0,0.0172378,"h, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison vs. Rest, and Expansion vs. Rest are 0.2657, 0.4982, 0.3132, and 0.7922, respectively. Lin et al. (2009) re"
L16-1164,P09-1077,0,0.0760211,"Missing"
L16-1164,prasad-etal-2008-penn,0,0.658426,"d to evaluate the proposed models. The 25-way classifier achieves 0.57 micro-averaged F-score. Keywords: Discourse analysis; Discourse relation labelling; HIT Chinese discourse relation treebank. 1. Introduction Discourse relation labelling aims at predicting the most proper discourse relation between two discourse units such as clauses, sentences, and groups of sentences. The labelling task can be done at the intra-sentential and the inter-sentential levels depending on the analysis units. Several schemes have been proposed to define discourse relations to be analyzed. In the PDTB framework (Prasad et al., 2008), three levels of sense hierarchy are utilized. The four classes on the top level are Temporal, Contingency, Comparison, and Expansion. In this paper, we present Chinese discourse analysis on fine-grained discourse relations at intra-sentential level. HIT Chinese discourse relation treebank (Zhang et al., 2014) is used to investigate some specific issues. A discourse marker in Chinese may belong to various lexical categories including conjunction, adverb, noun, preposition, and verb. It may be a single word such as “但” (but) or a word pair such as “雖然...但” (although … but). A discourse marker"
L16-1164,W05-0312,0,0.0266318,"HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0."
L16-1164,yu-etal-2012-development,1,0.584019,"hang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) prop"
L16-1164,zhou-etal-2014-cuhk,0,0.0315231,"Missing"
L16-1164,C12-2138,0,0.243937,"Missing"
L16-1164,C10-2172,0,0.0216632,"es. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison"
L16-1164,P12-1008,0,0.165896,"nese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0.6288 on the 4-way int"
L18-1139,fillmore-etal-2002-framenet,0,0.0612943,"Missing"
L18-1139,P14-1136,0,0.0356,"Missing"
L18-1139,C16-2037,0,0.278861,"LUs, 1,221 semantic frames, and 200,000 annotated sentences are provided, Chinese FrameNet is considerably smaller. In other words, Chinese FrameNet has a much low coverage of frames and lexical units, and results in limited applications. Efforts have been made to construct FrameNet resources in other languages. Most of which construct their resources with human annotation one by one laboriously, such as Japanese FrameNet (Ohara et al., 2003). Park et al. (2014) conduct the construction of Korean FrameNet by hiring trained translators to import 4,025 sentences selected from English FrameNet. Kim et al. (2016) further import additional 1,795 sentences to Korean FrameNet from Japanese FrameNet based on the similarities between these two languages. However, the high construction cost of such resources sometimes hinders it from growing to the proper scale that is applicable for real NLP tasks. Tonelli et al. (2008) propose an algorithm that projects English frames onto Italian ones, so that FrameNet in Italian could be constructed more easily. In this work, we propose a novel approach to automate FrameNet construction. Based on a large-scale bilingual corpus, we transfer the machine-annotated FEs from"
L18-1139,kingsbury-palmer-2002-treebank,0,0.364983,"Missing"
L18-1139,P15-1122,0,0.0606553,"Missing"
L18-1139,J03-1002,0,0.00770966,"osed to augment LUs based on the seeds. 3.1 3.2 Projection of Bilingual Frame Elements Word alignment tools align words in a sentence in the source language to the corresponding words in the sentence in the target language. In our case, we regard English as the source language and Chinese as the target language since we will utilize the alignment information as our basis of finding the projection of semantic relations from English to Chinese between bilingual sentences. In this paper, we employ TsinghuaAligner (Liu and Sun, 2015), which takes the translation probabilities derived from GIZA++ (Och and Ney, 2003) as the central feature in the word alignment process. The alignment procedure consists of three major steps. Figures 2, 3, and 4 show examples for the tasks respectively. First, we label 2.2M English sentences in the bilingual UMCorpus with frame semantics using SEMAFOR. As shown in Figure 2, SEMAFOR generates 14M predicate-argument structures from these 2.2M sentences. Figure 3 shows the second step, where TsinghuaAligner is performed to derive English-Chinese word pairs as our basis for the step. Finally, as illustrated in Figure 4, we utilize both the parsed result and the alignment inform"
L18-1139,tian-etal-2014-um,0,0.0185606,"for other languages. In the rest of this paper, we first introduce the linguistic resources that support our construction in Section 2. Section 3 presents our methodology and discusses the filtering strategies that are used for quality improvement. Section 4 demonstrates an application of our resource that would improve the annotation process in terms of annotation time consumption. Finally, we conclude our contributions and discuss future work in Section 5. 868 Figure 1: Overview of our approach to Chinese FrameNet construction. 2. Resources We construct Chinese FrameNet based on UM-Corpus (Tian et al., 2014), which is a large-scale, balanced EnglishChinese corpus consisting of 2.2 million parallel sentences from eight genres in a reasonable proportion, including News, Spoken, Laws, Thesis, Education, Science, Subtitle, and Microblog. They are parsed and extracted from online journals (national and international), official websites, online language learning resources (e.g. online dictionary and translation portals), TED, and Microblogs. Tian et al. (2014) apply some well-designed algorithms and tools to speed up the building process, such as document alignment, sentence boundary detection, and sen"
L18-1139,tonelli-pianta-2008-frame,0,0.0203702,"Missing"
L18-1541,D12-1104,0,0.104317,"not be mapped by string matching directly. In addition, a KB predicate may be described in multiple NL statements. A number of ReVerb patterns such as “is the hometown of”, “was raised in”, and “grew up in” are related to the predicate “hometown” in DBpedia. That makes the mapping between KB and NL even more challenging. Recently, more and more works show their interests in the issue of KB construction. Knowledge graph embedding models (Bordes et al., 2013; Yang et al., 2015; Xie et al., 2016a; Xie et al., 2016b) focus on learning the vector representation on the KB side only. Previous works (Nakashole et al., 2012; Riedel et al., 2013; Dutta et al., 2015) aim to solve similar problems as ours. Nakashole et al. (2012) in their PATTY approach try to learn paraphrases to define the predicates of DBpedia, and Dutta et al. (2015) propose clustering-based approaches to transform the knowledge extracted by an open information extraction system into DBpedia paradigm. Riedel et al. (2013) propose universal schemas, which are the mapping between NL surface forms to the KB predicates, by using matrix factorization. However, all of them suffer from low coverage on relational phrases. In this work, we aim to propos"
L18-1541,D11-1142,0,0.0986968,"Missing"
O14-4003,P06-4018,0,0.0467152,"Missing"
O14-4003,W08-0336,0,0.0291509,"Missing"
O14-4003,D09-1060,0,0.0765189,"Missing"
O14-4003,de-marneffe-etal-2006-generating,0,0.0073215,"Missing"
O14-4003,P03-1056,0,0.056901,"Missing"
O14-4003,P10-1122,0,0.0552529,"Missing"
O14-4003,D09-1082,0,0.0475942,"Missing"
P13-2079,P10-1122,0,0.31449,"nd hypothesis (H). If human would agree that the meaning of H can be inferred from the meaning of T, we say that T entails H (Dagan et al., 2006). The researches on textual entailment have attracted much attention in recent years due to its potential applications (Androutsopoulos and Malakasiotis, 2010). Recognizing Textual Entailment (RTE) (Bentivogli, et al., 2011), a series of evaluations on the developments of English TE recognition technologies, have been held seven times up to 2011. In the meanwhile, TE recognition technologies in other languages are also underway (Shima, et al., 2011). Sammons, et al., (2010) propose an evaluation metric to examine the characteristics of a TE recognition system. They annotate texthypothesis pairs selected from the RTE-5 test set with a series of linguistic phenomena required in the human inference process. The RTE systems are evaluated by the new indicators, such as how many T-H pairs annotated with a particular phenomenon can be correctly recognized. The indicators can tell developers which systems are better to deal with T-H pairs with the appearance of which phenomenon. That would give developers a direction to enhance their RTE systems. Such linguistic phenome"
P13-2079,D09-1082,0,0.0206814,"n (cnn) in H}. Then, we extract 2 sets of relations, including {relation in H} and {relation in T}, where each relation in the sets is in a form of Predicate(Argument1, Argument2). Some typical examples of relations are verb(subject, object) for verb phrases, neg(A, B) for negations, num(Noun, number) for numeric modifier, and tmod(C, temporal argument) for temporal modifier. A predicate has only 2 arguments in this representation. Thus, a di-transitive verb is in terms of two relations. Instead of measuring the relatedness of T-H pairs by comparing T and H on the predicateargument structure (Wang and Zhang, 2009), our method tries to find the five negative entailment phenomena based on the similar representation. Each of the five negative entailment phenomena is extracted as follows according to their definitions. To reduce the error propagation which may be arisen from the parsing errors, we directly match those nouns and named entities appearing in H to the text T. Furthermore, we introduce WordNet to align arguments in H to T. (a) Disconnected Relation. If (1) for each a  {noun in H}{nnp in H}{cnn in H}, we can find a  T too, and (2) for each r1=h(a1,a2)  {relation in H}, we can find a relatio"
P13-2079,W07-1401,0,0.150159,"Missing"
P13-2079,P06-4018,0,0.00528895,"5, 7, 8) setting does not have much performance difference. In the above experiments, we do all the analyses on the corpus annotated with linguistic phenomena by human. We aim at knowing the ulti447 mate performance of TE recognition systems embodying human knowledge in the inference. The human knowledge in the inference cannot be captured by TE recognition systems fully correctly. In the later experiments, we explore the five critical features, (3, 4, 5, 7, 8), and examine how the performance is affected if they are extracted automatically. 3 Levy and Manning, 2003), and stemming with NLTK (Bird, 2006). 3.1 Negative Entailment Phenomena Extraction The experimental results in Section 2.2 show that disconnected relation, exclusive argument, exclusive relation, missing argument, and missing relation are significant. We follow the definitions of Sammons et al. (2010) and show them as follows. (a) Disconnected Relation. The arguments and the relations in Hypothesis (H) are all matched by counterparts in Text (T). None of the arguments in T is connected to the matching relation. (b) Exclusive Argument. There is a relation common to both the hypothesis and the text, but one argument is matched in"
P13-2079,P03-1056,0,0.0137712,"n the one hand, adding more phenomena into (3, 4, 5, 7, 8) setting does not have much performance difference. In the above experiments, we do all the analyses on the corpus annotated with linguistic phenomena by human. We aim at knowing the ulti447 mate performance of TE recognition systems embodying human knowledge in the inference. The human knowledge in the inference cannot be captured by TE recognition systems fully correctly. In the later experiments, we explore the five critical features, (3, 4, 5, 7, 8), and examine how the performance is affected if they are extracted automatically. 3 Levy and Manning, 2003), and stemming with NLTK (Bird, 2006). 3.1 Negative Entailment Phenomena Extraction The experimental results in Section 2.2 show that disconnected relation, exclusive argument, exclusive relation, missing argument, and missing relation are significant. We follow the definitions of Sammons et al. (2010) and show them as follows. (a) Disconnected Relation. The arguments and the relations in Hypothesis (H) are all matched by counterparts in Text (T). None of the arguments in T is connected to the matching relation. (b) Exclusive Argument. There is a relation common to both the hypothesis and the"
P13-2079,de-marneffe-etal-2006-generating,0,0.0124486,"Missing"
P13-2079,W08-0336,0,\N,Missing
P13-2079,D09-1060,0,\N,Missing
P17-2064,N15-1142,0,0.0272701,". (2012). CBOW/Skip-gram Word Embeddings We trained word vectors with the two architectures included in the word2vec software (Mikolov et al., 2013a). The continuous bag-of-words model (CBOW) uses the words in a context window to predict the target word, while the skipgram model (SG) uses the target word to predict every word in the context window. CWINDOW/Structured Skip-gram Word Embeddings Taking the order of the context words into consideration, we also employ the continuous window model (CWIN) and the structured skip-gram 1 405 http://lemurproject.org/clueweb09.php 5.2 model (Struct-SG) (Ling et al., 2015). The former replaces the summation of context word vectors in CBOW with a concatenation operation, and the latter applies different projection matrices for predicting context words in different relative position with the target word. 3.2 Accuracy We use the detection accuracy as our main evaluation metric. A test instance is regarded as correct only if our system gives the highest score of incorrectness for the ground-truth position. This metric is relatively strict as the average length of the sentence segments in our dataset is 9.24. The McNemar’s test is adopted to perform statistical sign"
P17-2064,W12-2006,0,0.0862204,"Missing"
P17-2064,P14-5010,0,0.00572964,"Missing"
P17-2064,W11-2838,0,0.0851315,"Missing"
P17-2064,C16-1085,1,0.711149,"Missing"
P17-2064,N13-1090,0,0.185316,"on, their model only determines whether a sentence segment contains WUEs. Huang et al. (2016) used the HSK corpus to study the preposition selection problem. They proposed gated recurrent unit (GRU)-based models to select the most suitable one from a closed set of Chinese prepositions given the sentential context. Although their approach can be utilized to detect and correct preposition errors, it is still worth investigating how to recognize WUEs involving other types of words such as verbs and nouns. In the past few years, distributed word representations derived from neural network models (Mikolov et al., 2013a; Pennington et al., 2014) have become popular among various studies in natural language processing. Beyond surface forms, these low-dimensional vector representations can encode syntactic and semantic information implicitly (Mikolov et al., 2013b). Because WUEs involve syntactic or semantic problems, vector representations could be promising for finding the erroneous tokens. One challenging aspect of dealing with grammatical errors is that the errors usually do not stand on their own, but are dependent on the context (Chollampatt et al., 2016). Therefore, we need a model that considers the s"
P17-2064,W16-4919,0,0.0626137,"ational Linguistics https://doi.org/10.18653/v1/P17-2064 our system can output a ranked list of candidate error positions. The positions with the highest incorrectness scores will be marked as incorrect. In (E2) we show an example labeling result of our system. The tokens 差 (bad) and 知識 (knowledge), with the highest scores, are most likely to be incorrect. LSTM, to detect errors in English learner writing. However, they mainly focused on comparing different composition architectures under the same word representation, so it remained unclear to what extent pre-trained word embeddings can help. Huang and Wang (2016) used LSTM for Chinese grammatical error diagnosis, but their models are trained only on learner data, without external well-formed text. That means the performance might be limited by the relatively small amount of annotated sentences written by foreign learners. This paper utilizes LSTM and its extension (Bidirectional LSTM) along with the information derived from external resources to deal with Chinese WUE detection. Several types of pre-trained word embeddings and additional token-level features are considered. Each token in a sentence will be labeled correct or incorrect. Experimental res"
P17-2064,W14-1701,0,0.0950208,"Missing"
P17-2064,W13-3601,0,0.0867299,"Missing"
P17-2064,W16-4906,0,0.47638,"Missing"
P17-2064,W15-4401,0,0.191677,"Missing"
P17-2064,P16-1112,0,0.0378883,"Missing"
P17-2064,L16-1033,1,0.717535,"Missing"
P17-2064,yu-etal-2012-development,1,0.907131,"Missing"
P18-2122,S17-2089,0,0.123494,"Missing"
P18-2122,P14-5010,0,0.00468192,"Missing"
P18-2122,W10-2914,0,0.202584,"1 Chiao-Chen Chen,1 and Hsin-Hsi Chen12 1 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan 2 MOST Joint Research Center for AI Technology and All Vista Healthcare, Taipei, Taiwan hhhuang@nlg.csie.ntu.edu.tw, {b04902055,hhchen}@ntu.edu.tw Abstract nancial opinion mining (Cortis et al., 2017), and irony detection (Ghosh et al., 2015; Peled and Reichart, 2017; Hee et al., 2018). In the case of irony detection, it is impractical to manually annotate the ironic sentences from randomly sampled data due to the relatively low occurrences of irony (Davidov et al., 2010). Collecting the tweets with the hashtags like #sarcasm, #irony, and #not becomes the mainstream approach to dataset construction (Sulis et al., 2016). As shown in (S1), the tweet with the hashtag #not is treated as a positive (ironic) instance by removing #not from the text. The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural ne"
P18-2122,S16-1003,0,0.0691604,"Missing"
P18-2122,C04-1125,0,0.076951,"edia to check my social media. When the hashtag plays as a content word in a tweet, the tweet is not a good candidate of selflabeled ironic instances because the sentence will be incomplete once the hashtag is removed. In this work, both kinds of unreliable data, the tweets with a misused hashtag and the tweets in which the hashtag serves as a content word, are our targets to remove from the training data. Manual data cleaning is labor-intensive and inefficient (Van Hee et al., 2016a). Compared to general training data cleaning approaches (Malik and Bhardwaj, 2011; Esuli and Sebastiani, 2013; Fukumoto and Suzuki, 2004) such as boostingbased learning, this work leverages the characteristics of hashtag usages in tweets. With small amount of golden labeled data, we propose a neural network classifier for pruning the self-labeled tweets, and train an ironic detector on the less but cleaner instances. This approach is easily to apply to other NLP tasks that rely on self-labeled data. The contributions of this work are three-fold: (1) We make an empirically study on an issue that is potentially inherited in a number of research topics based on self-labeled data. (2) We propose a model for hashtag disambiguation."
P18-2122,D15-1116,0,0.0503357,"Missing"
P18-2122,P17-1155,0,0.0260925,"Missing"
P18-2122,D14-1127,0,0.174493,"are considered as labels for training and testing learning-based models, which usually benefit from large amount of data. One of the sources of self-labeled data widely used in the research community is Twitter, where the short-text messages tweets written by the crowd are publicly shared. In a tweet, the author can tag the short text with some hashtags such as #excited, #happy, #UnbornLivesMatter, and #Hillary4President to express their emotion or opinion. The tweets with a certain types of hashtags are collected as self-label data in a variety of research works including sentiment analysis (Qadir and Riloff, 2014), stance detection (Mohammad et al., 2016; Sobhani et al., 2017), fi(S2) BestProAdvice @Anonymous More clean OR cleaner, never more cleaner. #irony When the false-alarm instances like (S2) are collected and mixed in the training and test data, the models that learn from the unreliable data may be misled, and the evaluation is also suspicious. The other kind of unreliable data comes from the hashtags not only functioning as metadata. That is, a hashtag in a tweet may also function as a content word in its word form. For example, the hashtag #irony in (S3) is a part of the sentence “the irony of"
P18-2122,S18-1005,0,0.0449867,"Missing"
P18-2122,E17-2088,0,0.0137974,"odels, which usually benefit from large amount of data. One of the sources of self-labeled data widely used in the research community is Twitter, where the short-text messages tweets written by the crowd are publicly shared. In a tweet, the author can tag the short text with some hashtags such as #excited, #happy, #UnbornLivesMatter, and #Hillary4President to express their emotion or opinion. The tweets with a certain types of hashtags are collected as self-label data in a variety of research works including sentiment analysis (Qadir and Riloff, 2014), stance detection (Mohammad et al., 2016; Sobhani et al., 2017), fi(S2) BestProAdvice @Anonymous More clean OR cleaner, never more cleaner. #irony When the false-alarm instances like (S2) are collected and mixed in the training and test data, the models that learn from the unreliable data may be misled, and the evaluation is also suspicious. The other kind of unreliable data comes from the hashtags not only functioning as metadata. That is, a hashtag in a tweet may also function as a content word in its word form. For example, the hashtag #irony in (S3) is a part of the sentence “the irony of taking a break...”, in contrast to the hashtag #not in (S1), wh"
P18-2122,D14-1181,0,0.00245713,"tag sequences. Finally, the sigmoid activation function decides whether the instance is real ironic or false-alarm. The details of each component will be presented in the rest of this section. Word Sequences: The word sequences of the context preceding and following the targeting hashtag are separately encoded by neural network sentence encoders. The Penn Treebank Tokenizer provided by NLTK (Bird et al., 2009) is used for tokenization. As a result, each of the left and the right word sequences is encoded as a embedding with a length of 50. We experiments with convolution neural network (CNN) (Kim, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and attentive-GRU for sentence encoding. CNN for sentence classification has been shown effective in NLP applications such as sentiment analysis (Kim, 2014). Classifiers based on recurrent neural network (RNN) 2 773 https://code.google.com/archive/p/word2vec/ Model LR CNN GRU Att.GRU Our Method Our Method Our Method w/o LM ble of modeling the longer dependencies among the sequential tokens (Mikolov et al., 2011). Two millions of English tweets that are entirely different from those in the training and test data described in Section 2 are collect"
P18-2122,L16-1283,0,0.0531197,"Missing"
P18-2122,C16-1257,0,0.0528051,"Missing"
P19-1635,W14-4012,0,0.0568898,"Missing"
P19-1635,D14-1181,0,0.00509517,"onsidered the magnitude be6308 Magnitude Decimal 1 2 3 4 5 6 &gt;6 Range 0≤m&lt;1 1 ≤ m &lt; 10 10 ≤ m &lt; 102 102 ≤ m &lt; 103 103 ≤ m &lt; 104 104 ≤ m &lt; 105 105 ≤ m &lt; 106 106 ≤ m Ratio 23.24 37.53 25.36 12.21 1.12 0.29 0.23 0.01 Table 3: Distribution of numerals in the dataset. Model LR CNN GRU BiGRU CRNN CNN-capsule GRU-capsule BiGRU-capsule fore the decimal point, i.e., 10.08 was classified as a 2nd magnitude. Finally, we separate the dataset into training set and test set of sizes 500k and 100k, respectively. 4 4.1 Empirical Study Models We adopt seven different architectures for our task, including CNN (Kim, 2014), GRU (Cho et al., 2014), BiGRU, CRNN (Choi et al., 2017), CNNcapsule (Sabour et al., 2017), GRU-capsule, and BiGRU-capsule (Wang et al., 2018b). In our models, each word in the input sentence is represented as a d-dimensional vector with word embeddings, and all the words are concatenated in as a d × l matrix, where l denotes the sentence length. Some preprocessing was performed on the data. We transformed all characters to lowercase. The sentence representation was padded to the maximum length of an instance. The target numeral to be inferred is replaced with a special token &lt;TRT&gt;. Appendice"
P19-1635,P18-1196,0,0.112945,"Missing"
R11-1021,M95-1012,0,0.0176606,"Missing"
R11-1021,W08-0336,0,0.0454333,"Missing"
R11-1021,N09-2061,0,0.0365927,"Missing"
R11-1021,W10-4103,1,0.826054,"heme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of the k-tag set tagging scheme is shown in Figure 1. For example, the fragments in the"
R11-1021,W04-1101,0,0.391748,"Missing"
R11-1021,P03-1056,0,0.0270029,"e two datasets is 2.49 characters. For this reason, all the characters in most fragments are able to be captured within the leftmost and the rightmost trigrams. Part-of-Speech Level (POS): The features include the leftmost and the rightmost POS unigrams, bigrams, and trigrams in a fragment. Besides, the presences or absences of certain POS tags in a fragment are also checked. These tags include noun, pronoun, verb, conjunction, particle, adverb, adjective, and their combinations. Figure 2. Extracting the top-level structure from the syntax tree We perform POS tagging with the Stanford parser (Levy and Manning, 2003). Syntactic Level (S): We get the syntactic tree of a fragment by the Stanford parser, and extract the structure of the upper three levels, which forms the fundamental composition of the fragment. In addition, the leftmost path and the rightmost path of the tree are also extracted. Figure 2 shows the upper three levels of the parsing tree, the leftmost path, and the rightmost path of the sample fragment in the bold edges. For instance, the structure of the upper three levels in Figure 2 formed in preorder format is IP(IP(ADVP NP VP) VP(VV AS NP)), the leftmost path is IP(IP(ADVP(AD))), and the"
R11-1021,W03-1703,0,0.0486509,"Missing"
R11-1021,2005.eamt-1.37,0,0.0750046,"Missing"
R11-1021,O03-4002,0,0.0261339,"s larger than that in Labeling Method Tagging Scheme The typical tagging scheme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of t"
R11-1021,Y06-1012,0,0.0252148,"an that in Labeling Method Tagging Scheme The typical tagging scheme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of the k-tag set tagging"
R11-1021,J02-3002,0,\N,Missing
R11-1021,Y96-1018,0,\N,Missing
S17-2144,D14-1108,0,0.031547,"ependency-based approach (2.1) Stanford Parser (ED-S) A tweet is parsed by Stanford dependency parser (Marneffe, 2006). To reduce the effects of out of vocabulary (OOV) words in parsing, cashtags and company names are replaced by common names like “Bob”. A dependency tree for n-word tweet is composed of n triples in the form of dep(wordi,wordj), where wordi and wordj has a dependency dep, wordi is a parent of wordj, and wordj is a child of wordi. We take the ancestors and the decedents of a target as its span. (2.2) TweeboParser (ED-T) TweeboParser is a dependency parser, designed for tweets (Kong et al., 2014). It tries to deal with the following challenges: token selection, multiword expressions, multiple roots, and structure within noun phrases. The multiple roots property Figure 1. Structure of a financial tweet 848 tends to provide shorter span than ED-S with the same extracted algorithm. The average length of the tweets, manual spans, EP spans, ED-S and ED-T spans are 17.61, 6.26, 12.17, 10.27, and 7.78 words, respectively. Compared with 140-character word limit in twitter, a financial tweet is very short. In particular, manual spans are much shorter. Besides text span in tweets, tweets may co"
S17-2144,C16-1251,0,0.0310049,"Missing"
S17-2144,S17-2089,0,0.0517844,"Missing"
S18-1171,Q17-1010,0,0.0213967,"” word embedding features do not work, we turn to more abstract features. Let sim1 and sim2 be the cosine similarity of the vector of a 1028 1 https://code.google.com/archive/p/word2vec/ to the vector of w1 and w2 respectively. We compare the values sim1 and sim2 . The rationale is that if a word w has an attribute a, then it tends to, though not necessarily, be more similar to a than other words without a. The following six embedding models are experimented with. The embedding size is fixed to 300. 1. W2V(GNews): The standard Word2vec model as described in Section 2.1. 2. fastText: fastText (Bojanowski et al., 2017) is a modification of Word2vec that takes subword information into account. We adopt the pretrained vectors trained on 6B tokens2 . 3. Numberbatch: Numberbatch embeddings are built upon several corpus-based word embeddings and improved by retrofitting on ConceptNet, a large semantic network containing an abundance of general knowledge (Speer et al., 2017). We use the pre-trained embeddings of English concepts3 . 4. GloVe(Common Crawl): The GloVe model (Pennington et al., 2014) obtains word representation according to global co-occurrence statistics. We use the pre-trained vectors trained on 84"
S18-1171,S18-1117,0,0.0470901,"mpetence. Our knowledge about what is a “subway”, for example, may contain “it is a kind of train that runs underground”. Also, discriminating things is an important mechanism for teaching and learning. For example, if we would like to explain how a “plate” is different from a “bowl”, we may use expressions like “a plate is flatter” or “a bowl is deeper”. All these examples show that one form of semantic difference is a discriminative attribute which applies to one of the two concepts being compared but does not apply to the other. In the SemEval-2018 Capturing Discriminative Attributes task (Krebs et al., 2018), participants need to put forward semantic models that are aware of semantic differences. A data instance consists of a triple and a label. In this paper, we denote a triple with < w1 , w2 , a &gt;, in which w1 and w2 are the two words (concepts) to be compared, and a is an attribute. The label is either positive (1) or negative (0). In a positive example, a is an attribute of w1 but not an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The"
S18-1171,P16-2035,0,0.0188441,"ot an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The evaluation metric is the macro-averaged F1 score of the positive and the negative classes. Visual attribute learning has been investigated by past researchers. Silberer et al. (2013) build a dataset of concept-level attribute annotations based on images in ImageNet (Deng et al., 2009). For each attribute, they train a classifier to predict its presence or absence in the input image. Lazaridou et al. (2016) propose a model that does not learn visual attributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical difference is that in this task, the set of attributes is open. The dataset is partitioned so that all the attributes in the test set are unseen in the training set, which makes this task more challenging. The use of word embeddings for"
S18-1171,N15-1098,0,0.030501,"However, according to the results shown in Table 1, the models considering solely a part of every triple can still “learn” some information from the training set (majority-class baseline accuracy on the training set: 0.6383). Some models with partial information even achieve better validation scores than that with complete information. This indicates that the models overfit to the vocabulary of the training set. At the test time, all the attributes are unknown, so the model cannot make effective predictions. In fact, these results are similar to the lexical memorization phenomenon reported by Levy et al. (2015) on the hypernym detection task. 2.2 Embeddings Similarity Difference Because “raw” word embedding features do not work, we turn to more abstract features. Let sim1 and sim2 be the cosine similarity of the vector of a 1028 1 https://code.google.com/archive/p/word2vec/ to the vector of w1 and w2 respectively. We compare the values sim1 and sim2 . The rationale is that if a word w has an attribute a, then it tends to, though not necessarily, be more similar to a than other words without a. The following six embedding models are experimented with. The embedding size is fixed to 300. 1. W2V(GNews)"
S18-1171,D14-1162,0,0.0813083,"g size is fixed to 300. 1. W2V(GNews): The standard Word2vec model as described in Section 2.1. 2. fastText: fastText (Bojanowski et al., 2017) is a modification of Word2vec that takes subword information into account. We adopt the pretrained vectors trained on 6B tokens2 . 3. Numberbatch: Numberbatch embeddings are built upon several corpus-based word embeddings and improved by retrofitting on ConceptNet, a large semantic network containing an abundance of general knowledge (Speer et al., 2017). We use the pre-trained embeddings of English concepts3 . 4. GloVe(Common Crawl): The GloVe model (Pennington et al., 2014) obtains word representation according to global co-occurrence statistics. We use the pre-trained vectors trained on 840B tokens of Common Crawl4 . 5. Sense(enwiki)-c: Sense vectors may encode more fine-grained semantic information than word vectors do, so we also experimented with sense vectors. We perform word sense disambiguation (WSD) on the English Wikipedia corpus to get a sense-annotated corpus, using the Adapted Lesk algorithm implemented in pywsd5 . The sense inventory is based on synsets in WordNet. We train a Word2vec Skip-gram (SG) model with this corpus to obtain sense vectors. To"
S18-1171,P15-2119,0,0.0196331,"ttributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical difference is that in this task, the set of attributes is open. The dataset is partitioned so that all the attributes in the test set are unseen in the training set, which makes this task more challenging. The use of word embeddings for detecting semantic properties is studied by Rubinstein et al. (2015). They focus on a fixed set of properties and train a binary classifier for each property. Their 1027 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1027–1033 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics results indicate that word embeddings capture taxonomic properties (e.g. “an animal”) better than attributive properties (e.g. “is fast”), possibly because attributive signal is weak in text. In this task, most visual attributes are attributive properties. The signal of “visual” attributes can be even weake"
S18-1171,P13-1056,0,0.0237999,"enote a triple with < w1 , w2 , a &gt;, in which w1 and w2 are the two words (concepts) to be compared, and a is an attribute. The label is either positive (1) or negative (0). In a positive example, a is an attribute of w1 but not an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The evaluation metric is the macro-averaged F1 score of the positive and the negative classes. Visual attribute learning has been investigated by past researchers. Silberer et al. (2013) build a dataset of concept-level attribute annotations based on images in ImageNet (Deng et al., 2009). For each attribute, they train a classifier to predict its presence or absence in the input image. Lazaridou et al. (2016) propose a model that does not learn visual attributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical differenc"
W10-4103,P03-1035,0,0.064771,"Missing"
W10-4103,W04-3209,0,0.0693666,"Missing"
W10-4103,P05-1056,0,0.045505,"Missing"
W10-4103,C04-1081,0,0.179925,"ndled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single character that forms"
W10-4103,O03-4002,0,0.496095,"s can be handled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single"
W12-1636,W00-1205,0,0.167476,"e tree of the sentence. Shown in Figure 2 is the structure of sample (S3) based on the syntactic tree generated by the Stanford parser. However, it is clear that the 262 (S3) 目前雖然還只能在圖片上讓女性露露臉 (“Although women only appear in the pictures”)， 但 未 來 女 性 的 貢 獻 (“The contribution of women”)，將是教科書另一個著墨的重點 (“Will be another major focus in textbooks in the future”)。 Figure 1: Relation structure of sample (S1). Figure 2: Structure of sample (S3) based on the syntactic tree generated by the Stanford parser. Figure 3: Correct structure of sample (S3) 2 Dataset The corpus is based on the Sinica Treebank (Huang et al., 2000). A Total of 81 articles are randomly selected from the Sino and Travel sets. All the sentences that consist of two, three, and four clauses are extracted for relation and structure labeling by native Chinese speakers. A web-based system is developed for annotation. The annotation scheme is designed as follows. An annotator first signs in to the annotation system, and a list of sentences that are assigned to the annotator are given. The annotator labels the sentences one by one in the system. A sentence is split into clauses along commas, and all of its feasible binary tree structures are show"
W12-1636,I11-1170,1,0.883314,"Missing"
W12-1636,miltsakaki-etal-2004-penn,0,0.0408712,"Missing"
W12-1636,D09-1036,0,0.0323717,"ency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al. (2010), the entire paragraph is parsed Relation Temporal Contingency Comparison Expansion Type Single Phrase # 41 Samples 目前 “now” 之後 “after” Intra-Sent Phrase Pair 80 接著...再 “Then...again” 當初...曾 “At first...ever” Inter-Sent Phrase Pair 30 當初...後來 “Initially...Later” 最早...緊接著 “At first...Then” Single Phrase 62 如此一來 “As a result” 假設 “If” Intra-Sent Phrase Pair 180 如果...則 “If ... then” 無論...都 “Whether ...” Inter-Sent Phrase Pair 14 既然...看來 “Since... It seems” 幸而"
W12-1636,P11-1100,0,0.111357,"Missing"
W12-1636,P09-2004,0,0.0838576,"Missing"
W12-1636,P09-1077,0,0.0637267,"ency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al. (2010), the entire paragraph is parsed Relation Temporal Contingency Comparison Expansion Type Single Phrase # 41 Samples 目前 “now” 之後 “after” Intra-Sent Phrase Pair 80 接著...再 “Then...again” 當初...曾 “At first...ever” Inter-Sent Phrase Pair 30 當初...後來 “Initially...Later” 最早...緊接著 “At first...Then” Single Phrase 62 如此一來 “As a result” 假設 “If” Intra-Sent Phrase Pair 180 如果...則 “If ... then” 無論...都 “Whether ...” Inter-Sent Phrase Pair 14 既然...看來 “Since... It seems” 幸而"
W12-1636,prasad-etal-2008-penn,0,0.435317,"Missing"
W12-1636,P10-1073,0,0.028022,"Missing"
W12-1636,W05-0312,0,\N,Missing
W13-2309,C04-1200,0,0.0862213,"s positive than that of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polarity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant discourse markers. We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependency between the PDTB classes of discourse markers and each aspect of sentiment transitions. The results show that no matter whether the sentiment polarity transitions are categorized into Polarity Tende"
W13-2309,O06-1001,0,0.163071,"blicly available at present. To construct a Chinese discourse corpus, we sample instances from a huge Chinese corpus (Yu et al., 2012). This corpus was developed based on the ClueWeb09 dataset, where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents. In this paper, only the explicit discourse relations are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is developed based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an overview of the discourse marker dictionary. It contains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intrasentential, and Inter-sentential, according to their grammatical usages. The Single word group contains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same sent"
W13-2309,prasad-etal-2008-penn,0,0.679222,"annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 Introduction A discourse relation indicates how two arguments (i.e., elementary discourse units) cohere to each other. Various discourse relations were defined according to different taxonomy (Carlson and Marcu, 2001; Carlson et al., 2002; Prasad et al., 2008). In the work of the Penn Discourse Treebank 2.0 annotation, Prasad et al. (2008) labeled four grammatical classes of connectives in English, including subordinating conjunctions, coordinating conjunctions, adverbial connectives, and implicit connectives. Besides, the sense of each connective was also tagged. They defined three levels of sense hierarchy for the connectives. The four classes on the top level are Temporal, Contingency, Comparison, and Expansion. There are explicit and implicit uses of discourse relations. An explicit discourse relation indicates the arguments are connected with"
W13-2309,P07-1124,0,0.0315168,"of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polarity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant discourse markers. We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependency between the PDTB classes of discourse markers and each aspect of sentiment transitions. The results show that no matter whether the sentiment polarity transitions are categorized into Polarity Tendency, Polarity Change, Dir"
W13-2309,N03-1030,0,0.0409575,"a Positive-Negative Comparison relation. As the PDTB 2.0 annotation manual suggests (Prasad, et al., 2007), a Comparison relation is established to emphasize the differences between two arguments. Therefore, it is expected that the two arguments of a Comparison relation are relatively likely to have the opposing polarity states (i.e., Positive-Negative or NegativePositive). On the other hand, the two arguments of an Expansion relation are relatively likely to belong to the same polarity states (e.g., PositivePositive or Neutral-Neutral). Discourse relation recognition (Hernault et al., 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an anno"
W13-2309,I11-1170,1,0.772965,"likely to belong to the same polarity states (e.g., PositivePositive or Neutral-Neutral). Discourse relation recognition (Hernault et al., 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTBstyle Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled f"
W13-2309,J05-2005,0,0.191128,"where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents. In this paper, only the explicit discourse relations are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is developed based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an overview of the discourse marker dictionary. It contains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intrasentential, and Inter-sentential, according to their grammatical usages. The Single word group contains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same sentence and denote a discourse relation. Here, a Chinese sentence is defined as a sequence of successive words that is ended by a period, a question mark, or an exclamation mark. The clauses of a sentence ar"
W13-2309,W12-1636,1,0.758787,"discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider (S1), which consists of three clauses and forms a nested discourse structure shown in Figure 1. Abstract Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Fina"
W13-2309,yu-etal-2012-development,1,0.546942,"Missing"
W13-2309,C12-3028,1,0.918716,"discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider (S1), which consists of three clauses and forms a nested discourse structure shown in Figure 1. Abstract Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Fina"
W13-2309,C10-2172,0,0.11225,"Human-Annotated Corpus Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To resolve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure i"
W13-2309,P04-1087,0,0.341908,"pus Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To resolve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider"
W13-2309,P12-1008,0,0.139091,"is is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTBstyle Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled from the ClueWeb09. We review the results of annotation and analyze some language phenomena found in the corpus. The rest of this paper is organized as follows. In Section 2, we introduce the ClueWeb corpus 71 PDTB Class Expansion Temporal Comparison Contingency Scope Single word Intrasentential Intersentential Single word Intrasentential In"
W13-2309,C04-1020,0,\N,Missing
W13-2309,D09-1036,0,\N,Missing
W13-2817,D11-1038,0,0.018242,"effing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restoration methodology (Chen et al."
W13-2817,P12-1107,0,0.0532218,"Missing"
W13-2817,C04-1059,0,0.0260446,"do bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Berto"
W13-2817,C10-1152,0,0.0259508,"are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restorati"
W13-2817,W09-0432,0,0.0608974,"2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text"
W13-2817,C12-1034,1,0.882217,"tlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restoration methodology (Chen et al., 2012a), but emphasizes on how to update bilingual translation rules, translation model and language model, which are two kernels of rulebased and stati"
W13-2817,W07-0722,0,0.0249594,"rom the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from t"
W13-2817,W07-0717,0,0.0337879,"domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results"
W13-2817,D10-1044,0,0.0218196,"ul, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data."
W13-2817,D09-1074,0,0.034019,"using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resour"
W13-2817,2008.iwslt-papers.6,0,\N,Missing
W13-2817,P07-1004,0,\N,Missing
W19-4508,N16-1165,0,0.0280976,"Missing"
W19-4508,C16-1260,0,0.013467,"dia. However, only 2,500 of them are labeled. It may not be sufficient for training a model, especially for neural network models. The definition of argumentative components differs from dataset to dataset. In the dataset used in this work, an argumentative component is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together. In the work of Laha and Raykar (2016), they present several bi-sequence classification models on different datasets. However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM. Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion"
W19-4508,W17-5104,0,0.0227656,"yntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information o"
W19-4508,C18-1176,0,0.656208,"mental results and concludes this work. 2 3.1 We conduct the experiments on the dataset released by Stab et al. (2018b).1 The dataset includes 25,492 sentences over eight topics that are randomly selected from an online list of controversial topics.2 The selected topics, which are considered as queries, are used to retrieve documents from heterogeneous sources via the Google search engine. Among these sentences, 4,944 of them are supporting arguments, 6,195 are opposing arguments, and 14,353 are non-argument sentences. This dataset is commonly used for sentential argument identification task. Levy et al. (2018) collect a dataset with around 1.5 million sentences over 150 topics from Wikipedia. However, only 2,500 of them are labeled. It may not be sufficient for training a model, especially for neural network models. The definition of argumentative components differs from dataset to dataset. In the dataset used in this work, an argumentative component is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use atte"
W19-4508,W17-5110,0,0.800857,"m into roles of support/opposition. Our model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks (Cho et al., 2014). With the help of the attention mechanism (Bahdanau et al., 2015), RNN can further attend on the key information. We propose a novel attention mechanism that is guided by argumentative lexicon information. Lexicon information is reported as one kind of the most frequently used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this wo"
W19-4508,D14-1179,0,0.00758891,"Missing"
W19-4508,P17-1002,0,0.0195477,"nt is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together. In the work of Laha and Raykar (2016), they present several bi-sequence classification models on different datasets. However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM. Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al."
W19-4508,W15-0503,0,0.022469,"port/opposition. Our model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks (Cho et al., 2014). With the help of the attention mechanism (Bahdanau et al., 2015), RNN can further attend on the key information. We propose a novel attention mechanism that is guided by argumentative lexicon information. Lexicon information is reported as one kind of the most frequently used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this work is two-fold: (1) We pr"
W19-4508,W13-2707,0,0.0695041,"Missing"
W19-4508,D14-1162,0,0.0817048,"Missing"
W19-4508,W16-0410,0,0.0145063,"ntly used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this work is two-fold: (1) We propose an attention mechanism to leverage lexicon information. (2) In the face of the scarcity of argument lexicon, we explore several different types of lexicons to verify whether outside resources are useful for AM tasks. The rest of this paper is organized as follows. Section 2 summarizes related works about AM. The dataset and linguistic resources used for experiments are shown in Section 3. We introduce Identification of argume"
W19-4508,P18-2095,0,0.0145672,"been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information of the lexicons. 3 Data 3.2 Lexicon resource To improve the baseline model, we consider several existing lexicons across different domains. We first explore the claim lexicon that is built for argument mining task (Levy et al., 2017). We also include the lexic"
W19-4508,C14-1142,0,0.0679932,"Missing"
W19-4508,D18-1402,0,0.197443,"Missing"
W19-4508,D16-1169,0,0.0576156,"Missing"
W19-4508,C18-1074,0,0.0256188,"al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end t"
