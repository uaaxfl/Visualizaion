2021.findings-emnlp.145,{TIAGE}: A Benchmark for Topic-Shift Aware Dialog Modeling,2021,-1,-1,5,0,6789,huiyuan xie,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, we introduce three tasks to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation. Experiments on these tasks show that the topic-shift signals in TIAGE are useful for topic-shift response generation. On the other hand, dialog systems still struggle to decide when to change topic. This indicates further research is needed in topic-shift aware dialog modeling."
2020.coling-main.256,Morphologically Aware Word-Level Translation,2020,-1,-1,4,1,14874,paula czarnowska,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a novel morphologically aware probability model for bilingual lexicon induction, which jointly models lexeme translation and inflectional morphology in a structured way. Our model exploits the basic linguistic intuition that the lexeme is the key lexical unit of meaning, while inflectional morphology provides additional syntactic information. This approach leads to substantial performance improvements{---}19{\%} average improvement in accuracy across 6 language pairs over the state of the art in the supervised setting and 16{\%} in the weakly supervised setting. As another contribution, we highlight issues associated with modern BLI that stem from ignoring inflectional morphology, and propose three suggestions for improving the task."
W19-4806,The Meaning of {``}Most{''} for Visual Question Answering Models,2019,0,0,2,0,24002,alexander kuhnle,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of {``}most{''}, we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber{'}s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system."
W19-0408,"Words are Vectors, Dependencies are Matrices: Learning Word Embeddings from Dependency Graphs",2019,0,3,3,1,14874,paula czarnowska,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a matrix, with each dependency-label. This allows it to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word."
D19-1090,Don{'}t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction,2019,0,6,5,1,14874,paula czarnowska,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Human translators routinely have to translate rare inflections of words {--} due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl{\'a}ramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology."
W18-1003,Deep learning evaluation using deep linguistic processing,2018,0,3,2,0,24002,alexander kuhnle,Proceedings of the Workshop on Generalization in the Age of Deep Learning,0,"We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing {`}deep{'} linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail, as compared to a single performance value on a static and monolithic dataset."
W17-6806,Semantic Composition via Probabilistic Model Theory,2017,50,1,2,1,10836,guy emerson,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,"Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models."
W17-3533,Realization of long sentences using chunking,2017,6,0,2,0,31783,ewa muszynska,Proceedings of the 10th International Conference on Natural Language Generation,0,"We propose sentence chunking as a way to reduce the time and memory costs of realization of long sentences. During chunking we divide the semantic representation of a sentence into smaller components which can be processed and recombined without loss of information. Our meaning representation of choice is the Dependency Minimal Recursion Semantics (DMRS). We show that realizing chunks of a sentence and combining the results of such realizations increases the coverage for long sentences, significantly reduces the resources required and does not affect the quality of the realization."
W16-1605,Functional Distributional Semantics,2016,48,0,2,1,10836,guy emerson,Proceedings of the 1st Workshop on Representation Learning for {NLP},0,"Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets."
L16-1197,Resources for building applications with Dependency {M}inimal {R}ecursion {S}emantics,2016,29,12,1,1,6790,ann copestake,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe resources aimed at increasing the usability of the semantic representations utilized within the DELPH-IN (Deep Linguistic Processing with HPSG) consortium. We concentrate in particular on the Dependency Minimal Recursion Semantics (DMRS) formalism, a graph-based representation designed for compositional semantic representation with deep grammars. Our main focus is on English, and specifically English Resource Semantics (ERS) as used in the English Resource Grammar. We first give an introduction to ERS and DMRS and a brief overview of some existing resources and then describe in detail a new repository which has been developed to simplify the use of ERS/DMRS. We explain a number of operations on DMRS graphs which our repository supports, with sketches of the algorithms, and illustrate how these operations can be exploited in application building. We believe that this work will aid researchers to exploit the rich and effective but complex DELPH-IN resources."
W15-0101,Leveraging a Semantically Annotated Corpus to Disambiguate Prepositional Phrase Attachment,2015,29,1,2,1,10836,guy emerson,Proceedings of the 11th International Conference on Computational Semantics,0,"Accurate parse ranking requires semantic information, since a sentence may have many candidate parses involving common syntactic constructions. In this paper, we propose a probabilistic framework for incorporating distributional semantic information into a maximum entropy parser. Furthermore, to better deal with sparse data, we use a modified version of Latent Dirichlet Allocation to smooth the probability estimates. This LDA model generates pairs of lemmas, representing the two arguments of a semantic relation, and can be trained, in an unsupervised manner, on a corpus annotated with semantic dependencies. To evaluate our framework in isolation from the rest of a parser, we consider the special case of prepositional phrase attachment ambiguity. The results show that our semantically-motivated feature is effective in this case, and moreover, the LDA smoothing both produces semantically interpretable topics, and also improves performance over raw co-occurrence frequencies, demonstrating that it can successfully generalise patterns in the training data."
W15-0116,Hierarchical Statistical Semantic Realization for {M}inimal {R}ecursion {S}emantics,2015,29,3,2,0,34802,matic horvat,Proceedings of the 11th International Conference on Computational Semantics,0,"We introduce a robust statistical approach to realization from Minimal Recursion Semantics representations. The approach treats realization as a translation problem, transforming the Dependency MRS graph representation to a surface string. Translation is based on a Synchronous Context-Free Grammar that is automatically extracted from a large corpus of parsed sentences. We have evaluated the new approach on the Wikiwoods corpus, where it shows promising results. 1"
W15-0128,Layers of Interpretation: On Grammar and Compositionality,2015,39,14,5,0,11448,emily bender,Proceedings of the 11th International Conference on Computational Semantics,0,"With the recent resurgence of interest in semantic annotation of corpora for improved semantic parsing, we observe a tendency which we view as ill-advised, to conflate sentence meaning and speaker meaning into a single mapping, whether done by annotators or by a parser. We argue instead for the more traditional hypothesis that sentence meaning, but not speaker meaning, is compositional, and accordingly that NLP systems would benefit from reusable, automatically derivable, taskindependent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method."
togia-copestake-2014-tagntext,{T}ag{NT}ext: A parallel corpus for the induction of resource-specific non-taxonomical relations from tagged images,2014,22,0,2,0,23942,theodosia togia,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"When producing textual descriptions, humans express propositions regarding an object; but what do they express when annotating a document with simple tags? To answer this question, we have studied what users of tagging systems would have said if they were to describe a resource with fully fledged text. In particular, our work attempts to answer the following questions: if users were to use full descriptions, would their current tags be words present in these hypothetical sentences? If yes, what kind of language would connect these words? Such questions, although central to the problem of extracting binary relations between tags, have been sidestepped in the existing literature, which has focused on a small subset of possible inter-tag relations, namely hierarchical ones (e.g. {``}car{''} --is-a-- {``}vehicle{''}), as opposed to non-taxonomical relations (e.g. {``}woman{''} --wears-- {``}hat{''}). TagNText is the first attempt to construct a parallel corpus of tags and textual descriptions with respect to particular resources. The corpus provides enough data for the researcher to gain an insight into the nature of underlying relations, as well as the tools and methodology for constructing larger-scale parallel corpora that can aid non-taxonomical relation extraction."
W13-0602,Can distributional approaches improve on Good Old-Fashioned Lexical Semantics?,2013,23,1,1,1,6790,ann copestake,Proceedings of the {IWCS} 2013 Workshop Towards a Formal Distributional Semantics,0,"In this position paper, I discuss some linguistic problems that computational work on lexical semantics has attempted to address in the past and the implications for alternative models which incorporate distributional information. I concentrate in particular on phenomena involving count/mass distinctions, where older approaches attempted to use lexical semantics in their models of syntax. I outline methods by which the earlier models allowed the transmission of information between lexical items (regular polysemy and inheritance) and address the possibility that similar techniques could usefully be incorporated into distributional models."
dayrell-etal-2012-rhetorical,Rhetorical Move Detection in {E}nglish Abstracts: Multi-label Sentence Classifiers and their Annotated Corpora,2012,20,6,5,0,35163,carmen dayrell,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The relevance of automatically identifying rhetorical moves in scientific texts has been widely acknowledged in the literature. This study focuses on abstracts of standard research papers written in English and aims to tackle a fundamental limitation of current machine-learning classifiers: they are mono-labeled, that is, a sentence can only be assigned one single label. However, such approach does not adequately reflect actual language use since a move can be realized by a clause, a sentence, or even several sentences. Here, we present MAZEA (Multi-label Argumentative Zoning for English Abstracts), a multi-label classifier which automatically identifies rhetorical moves in abstracts but allows for a given sentence to be assigned as many labels as appropriate. We have resorted to various other NLP tools and used two large training corpora: (i) one corpus consists of 645 abstracts from physical sciences and engineering (PE) and (ii) the other corpus is made up of 690 from life and health sciences (LH). This paper presents our preliminary results and also discusses the various challenges involved in multi-label tagging and works towards satisfactory solutions. In addition, we also make our two training corpora publicly available so that they may serve as benchmark for this new task."
W11-2707,Exciting and interesting: issues in the generation of binomials,2011,-1,-1,1,1,6790,ann copestake,Proceedings of the {UCNLG}+{E}val: Language Generation and Evaluation Workshop,0,None
W11-2315,Towards an on-demand Simple {P}ortuguese {W}ikipedia,2011,20,1,2,0,7636,arnaldo jr,Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,0,"The Simple English Wikipedia provides a simplified version of Wikipedia's English articles for readers with special needs. However, there are fewer efforts to make information in Wikipedia in other languages accessible to a large audience. This work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a Simple Portuguese Wikipedia on demand, based on user interactions with the main Portuguese Wikipedia. Our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2%."
W11-0118,Formalising and specifying underquantification,2011,22,5,2,1,2275,aurelie herbelot,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"This paper argues that all subject noun phrases can be given a quantified formalisation in terms of the intersection between their denotation set and the denotation set of their verbal predicate. The majority of subject noun phrases, however, are only implicitely quantified and the task of retrieving the most plausible quantifier for a given NP is non-trivial. We propose a formalisation which captures the underspecification of the quantifier in subject NPs and we show that this formalisation is widely applicable, including in statements involving kinds. We then present a baseline for a quantification resolution system using syntactic features as basis for classification. Although the syntactic baseline provides a respectable 78% precision, our error analysis shows that obtaining true performance on the task requires information beyond syntax."
W10-1809,Annotating Underquantification,2010,17,3,2,1,2275,aurelie herbelot,Proceedings of the Fourth Linguistic Annotation Workshop,0,"Many noun phrases in text are ambiguously quantified: syntax doesn't explicitly tell us whether they refer to a single entity or to several, and what portion of the set denoted by the Nbar actually takes part in the event expressed by the verb. We describe this ambiguity phenomenon in terms of underspecification, or rather underquantification. We attempt to validate the underquantification hypothesis by producing and testing an annotation scheme for quantification resolution, the aim of which is to associate a single quantifier with each noun phrase in our corpus."
W09-0623,Investigating Content Selection for Language Generation using Machine Learning,2009,6,15,2,0,42408,colin kelly,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"The content selection component of a natural language generation system decides which information should be communicated in its output. We use information from reports on the game of cricket. We first describe a simple factoid-to-text alignment algorithm then treat content selection as a collective classification problem and demonstrate that simple 'grouping' of statistics at various levels of granularity yields substantially improved results over a probabilistic baseline. We additionally show that holding back of specific types of input data, and linking database structures with commonality further increase performance."
E09-1001,"\\textbf{Invited Talk:} Slacker Semantics: Why Superficiality, Dependency and Avoidance of Commitment can be the Right Way to Go",2009,0,14,1,1,6790,ann copestake,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,None
E09-1071,Using Lexical and Relational Similarity to Classify Semantic Relations,2009,28,29,2,1,20639,diarmuid seaghdha,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Many methods are available for computing semantic similarity between individual words, but certain NLP tasks require the comparison of word pairs. This paper presents a kernel-based framework for application to relational reasoning tasks of this kind. The model presented here combines information about two distinct types of word pair similarity: lexical similarity and relational similarity. We present an efficient and flexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state-of-the-art results on a compound noun interpretation task."
W08-0608,Cascaded Classifiers for Confidence-Based Chemical Named Entity Recognition,2008,21,57,2,0,46344,peter corbett,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"Chemical named entities represent an important facet of biomedical text. We have developed a system to use character-based ngrams, Maximum Entropy Markov Models and rescoring to recognise chemical names and other such entities, and to make confidence estimates for the extracted entities. An adjustable threshold allows the system to be tuned to high precision or high recall. At a threshold set for balanced precision and recall, we were able to extract named entities at an F score of 80.7% from chemistry papers and 83.2% from PubMed abstracts. Furthermore, we were able to achieve 57.6% and 60.3% recall at 95% precision, and 58.9% and 49.1% precision at 90% recall. These results show that chemical named entities can be extracted with good performance, and that the properties of the extraction can be tuned to suit the demands of the task."
P08-4002,Generating Research Websites Using Summarisation Techniques,2008,6,0,2,1,10906,advaith siddharthan,Proceedings of the {ACL}-08: {HLT} Demo Session,0,"We describe an application that generates web pages for research institutions by summarising terms extracted from individual researchers' publication titles. Our online demo covers all researchers and research groups in the Computer Laboratory, University of Cambridge. We also present a novel visualisation interface for browsing collaborations."
rupp-etal-2008-language,Language Resources and Chemical Informatics,2008,10,1,2,0,48159,cj rupp,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Chemistry research papers are a primary source of information about chemistry, as in any scientific field. The presentation of the data is, predominantly, unstructured information, and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques. At one level, extracting the relevant information from research papers is a text mining task, requiring both extensive language resources and specialised knowledge of the subject domain. However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels. The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research. This relies on the cooperation of several journal publishers to provide papers in an appropriate form. The work is carried out as a collaboration involving the Computer Laboratory, Chemistry Department and eScience Centre at Cambridge University, and is funded under the UK eScience programme."
C08-1082,Semantic Classification with Distributional Kernels,2008,35,35,2,1,20639,diarmuid seaghdha,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Distributional measures of lexical similarity and kernel methods for classification are well-known tools in Natural Language Processing. We bring these two methods together by introducing distributional kernels that compare co-occurrence probability distributions. We demonstrate the effectiveness of these kernels by presenting state-of-the-art results on datasets for three semantic classification: compound noun interpretation, identification of semantic relations between nominals and semantic classification of verbs. Finally, we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations."
W07-1210,Semantic Composition with (Robust) {M}inimal {R}ecursion {S}emantics,2007,10,29,1,1,6790,ann copestake,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"We discuss semantic composition in Minimal Recursion Semantics (MRS) and Robust Minimal Recursion Semantics (RMRS). We demonstrate that a previously defined formal algebra applies to grammar engineering across a much greater range of frameworks than was originally envisaged. We show how this algebra can be adapted to composition in grammar frameworks where a lexicon is not assumed, and how this underlies a practical implementation of semantic construction for the RASP system."
W07-1108,Co-occurrence Contexts for Noun Compound Interpretation,2007,22,28,2,1,20639,diarmuid seaghdha,Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,0,"Contextual information extracted from corpora is frequently used to model semantic similarity. We discuss distinct classes of context types and compare their effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining word-similarity and relation-similarity kernels further improves SVM classification performance."
2007.mtsummit-ucnlg.18,"Evaluating an open-domain {GRE} algorithm on closed domains system {ID}s: {CAM}-{B}, {CAM}-{T}, {CAM}-{BU} and {CAM}-{TU}",2007,-1,-1,2,1,10906,advaith siddharthan,Proceedings of the Workshop on Using corpora for natural language generation,0,None
W06-2802,Errors in wikis,2006,-1,-1,1,1,6790,ann copestake,Proceedings of the Workshop on {NEW} {TEXT} Wikis and blogs and other dynamic text sources,0,None
W06-2718,A Standoff Annotation Interface between {DELPH}-{IN} Components,2006,5,2,2,0,48161,benjamin waldron,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"We present a standoff annotation framework for the integration of NLP components, currently implemented in the context of the DELPH-IN tools. This provides a flexible standoff pointer scheme suitable for various types of data, a lattice encodes structural ambiguity, intra-annotation relationships are encoded, and annotations are decorated with structured content. We provide an XML serialization for intercomponent communication."
waldron-etal-2006-preprocessing,Preprocessing and Tokenisation Standards in {DELPH}-{IN} Tools,2006,16,9,2,0,48161,benjamin waldron,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We discuss preprocessing and tokenisation standards within DELPH-IN, a large scale open-source collaboration providing multiple independent multilingual shallow and deep processors. We discuss (i) a component-specific XML interface format which has been used for some time to interface preprocessor results to the PET parser, and (ii) our implementation of a more generic XML interface format influenced heavily by the (ISO working draft) Morphosyntactic Annotation Framework (MAF). Our generic format encapsulates the information which may be passed from the preprocessing stage to a parser: it uses standoff-annotation, a lattice for the representation of structural ambiguity, intra-annotation dependencies and allows for highly structured annotation content. This work builds on the existing Heart of Gold middleware system, and previous work on Robust Minimal Recursion Semantics (RMRS) as part of an inter-component interface. We give examples of usage with a number of the DELPH-IN processing components and deep grammars."
2005.mtsummit-osmtw.3,Open Source Machine Translation with {DELPH}-{IN},2005,0,27,4,0,6126,francis bond,Workshop on open-source machine translation,0,The Deep Linguistic Processing with HPSG Initiative (DELPH-IN) provides the infrastructure needed to produce open-source semantic transfer-based...
W04-0411,Lexical Encoding of {MWE}s,2004,9,29,2,0,7141,aline villavicencio,Proceedings of the Workshop on Multiword Expressions: Integrating Processing,0,"Multiword Expressions present a challenge for language technology, given their flexible nature. Each type of multiword expression has its own characteristics, and providing a uniform lexical encoding for them is a difficult task to undertake. Nonetheless, in this paper we present an architecture for the lexical encoding of these expressions in a database, that takes into account their flexibility. This encoding extends in a straightforward manner the one required for simplex (single) words, and maximises the information contained for them in the description of multiwords."
P04-1052,Generating Referring Expressions in Open Domains,2004,11,43,2,0.666667,10906,advaith siddharthan,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classification for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on Word-Net synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the first algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank."
copestake-etal-2004-lexicon,A Lexicon Module for a Grammar Development Environment,2004,7,11,1,1,6790,ann copestake,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,Past approaches to developing an effective lexicon component in a grammar development environment have suffered from a number of usability and efficiency issues. We present a lexical database module currently in use by a number of grammar development projects. The database module presented addresses issues which have caused problems in the past and the power of a database architecture provides a number of practical advantages as well as a solid framework for future extension.
copestake-etal-2002-multiword,Multiword expressions: linguistic precision and reusability,2002,4,52,1,1,6790,ann copestake,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper discusses the approach to multiword expressions being adopted in the LinGO English Resource Grammar (http://lingo.stanford.edu), a broad-scale bidirectional grammar of English in the HPSG framework. We discuss how the lexicon of multiword expressions is encoded in a database and describe the implications for building a reusable lexical resource."
P01-1019,An Algebra for Semantic Construction in Constraint-based Grammars,2001,7,121,1,1,6790,ann copestake,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification-based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability."
W00-0708,Memory-Based Learning for Article Generation,2000,14,57,3,0,53749,guido minnen,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,"Article choice can pose difficult problems in applications such as machine translation and automated summarization. In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase. We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation."
copestake-flickinger-2000-open,An Open Source Grammar Development Environment and Broad-coverage {E}nglish Grammar Using {HPSG},2000,32,235,1,1,6790,ann copestake,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The LinGO (Linguistic Grammars Online) projectxe2x80x99s English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels."
J99-4002,Lexical rules in constraint based grammars,1999,59,54,2,0,21810,ted briscoe,Computational Linguistics,0,"Lexical rules have been used to cover a very diverse range of phenomena in constraint-based grammars. Examination of the full range of rules proposed shows that Carpenter's (1991) postulated upper bound on the length of list-valued attributes such as SUBCAT in the lexicon cannot be maintained, leading to unrestricted generative capacity in constraint-based formalisms utilizing HPSG-style lexical rules. We argue that it is preferable to subdivide such rules into a class of semiproductive lexically governed genuinely lexical rules, and a class of fully productive unary syntactic rules.We develop a restricted approach to lexical rules in a typed default feature structure (TDFS) framework (Lascarides et al. 1995; Lascarides and Copestake 1999), which has enough expressivity to state, for example, rules of verb diathesis alternation, but which does not allow arbitrary manipulation of list-valued features. An interpretation of such lexical rules within a probabilistic version of a TDFS-based linguistic (lexical and grammatical) theory allows us to capture the semiproductive nature of genuinely lexical rules, steering an intermediate course between fully generative or purely abbreviatory rules.We illustrate the utility of this approach with a treatment of dative constructions within a linguistic framework that borrows insights from the constraint-based theories: HPSG, UCG, (Zeevat, Klein, and Calder 1987) and construction grammar (Goldberg 1995). We end by outlining how our approach to lexical rules allows for a treatment of passive and recursive affixation, which are generally assumed to require unrestricted list manipulation operations."
J99-1002,Default Representation in Constraint-based Frameworks,1999,46,57,2,0,1045,alex lascarides,Computational Linguistics,0,"Default unification has been used in several linguistic applications. Most of them have utilized defaults at a metalevel, as part of an extended description language. We propose that allowing default unification to be a fully integrated part of a typed feature structure system requires default unification to be a binary, order independent function, so that it acquires the perspicuity and declarativity familiar from normal unification-based frameworks. Furthermore, in order to respect the behaviour of defaults, default unification should allow default reentrancies and values on more general types to be overridden by conflicting default information on more specific types. We define what we believe is the first version of default unification to fully satisfy these criteria, and argue that it can improve the representation of a range of phenomena in syntax, semantics and the lexico-pragmatic interface."
W97-0506,Augmented and alternative {NLP} techniques for augmentative and alternative communication,1997,-1,-1,1,1,6790,ann copestake,Natural Language Processing for Communication Aids,0,None
P97-1018,Intergrating Symbolic and Statistical Representations: The Lexicon Pragmatics Interface,1997,20,25,1,1,6790,ann copestake,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We describe a formal framework for interpretation of words and compounds in a discourse context which integrates a symbolic lexicon/grammar, word-sense probabilities, and a pragmatic component. The approach is motivated by the need to handle productive word use. In this paper, we concentrate on compound nominals. We discuss the inadequacies of approaches which consider compund interpretation as either wholly lexico-grammatical or wholly pragmatic, and provide an alternative integrated account."
W96-0303,Controlling the Application of Lexical Rules,1996,16,13,2,0.459923,21810,ted briscoe,Breadth and Depth of Semantic Lexicons,0,"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora."
1995.tmi-1.2,Translation using {M}inimal {R}ecursion {S}emantics,1995,-1,-1,1,1,6790,ann copestake,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
A92-1012,The {ACQUILEX} {LKB}: representation issues in semi-automatic acquisition of large lexicons,1992,13,39,1,1,6790,ann copestake,Third Conference on Applied Natural Language Processing,0,"We describe the lexical knowledge base system (LKB) which has been designed and implemented as part of the ACQUILEX project1 to allow the representation of multilingual syntactic and semantic information extracted from machine readable dictionaries (MRDs), in such a way that it is usable by natural language processing (NLP) systems. The LKB's lexical representation language (LRL) augments typed graph-based unification with default inheritance, formalised in terms of default unification of feature structures. We evaluate how well the LRL meets the practical requirements arising from the semi-automatic construction of a large scale, multilingual lexicon. The system as described is fully implemented and is being used to represent substantial amounts of information automatically extracted from MRDs."
1992.tmi-1.1,Translation equivalence and lexicalization in the {ACQUILEX} {LKB},1992,-1,-1,3,0,49115,antonio sanfilippo,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W91-0209,Lexical Operations in a Unification-based Framework,1991,0,82,1,1,6790,ann copestake,Lexical Semantics and Knowledge Representation,0,We consider lexical operations and their representation in a unification based lexicon and the role of lexical semantic information. We describe a unified treatment of the linguistic aspects of sense extension and derivational morphological processes which delimit the range of possible coercions between lexemes and give a preliminary account of how default interpretations may arise.
C90-2008,Enjoy the Paper: Lexicology,1990,7,0,2,0,21810,ted briscoe,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,None
