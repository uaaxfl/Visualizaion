2020.acl-main.287,D19-1290,1,0.813755,"almari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup work, Longpre et al. (2019) observed that style is more important for decided listeners."
2020.acl-main.287,D17-1141,1,0.932093,"n common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Count of emotions (e,g. sad, etc.) and polari"
2020.acl-main.287,C16-1324,1,0.870048,"Missing"
2020.acl-main.287,W09-3723,0,0.726797,"s editorials, we finally obtain common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Co"
2020.acl-main.287,bal-saint-dizier-2010-towards,0,0.142438,"con Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Count of emotions (e,g. sad, etc.) and polarity words Count of each evidence type (e.g., statistics) Count of 17 types of arguing (e.g., assessments) Count of subjective and objective sentences Pennebaker et al. (2015) Mohammad and Turney (2013) Al-Khatib et al. (2017) Somasundaran et al. (2007) Riloff and Wiebe (2003) Table 1: Summary of the style feature types in our dataset. Each feature is quantified at the level of the editorial. analyzing their properties (Bal and Dizier, 2010; Scheffler and Stede, 2016). While Al-Khatib et al. (2016) modeled the structure underlying editorial argumentation, we use the corpus of El Baff et al. (2018) meant to study the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (201"
2020.acl-main.287,N18-1094,0,0.372668,"the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTimes editorial corpus of El Baff et al."
2020.acl-main.287,K18-1044,1,0.884192,"Missing"
2020.acl-main.287,W19-4519,0,0.0137528,"impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup work, Longpre et al. (2019) observed that style is more important for decided listeners. Unlike them, we focus on the stylistic choices made in well-planned argumentative texts. The lead paragraphs and the ending of an editorial have special importance (Rich, 2015). Hynds (1990) analyzes how leads and endings changed over time, whereas Moznette and Rarick (1968) examined the readability of an editorial based on them. To our knowledge, however, no one investigated their importance computationally so far. In this paper, we close this gap by analyzing what style of leads and endings is particularly effective compared to th"
2020.acl-main.287,E17-1070,0,0.419734,"aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTim"
2020.acl-main.287,W19-8607,1,0.887444,"Missing"
2020.acl-main.287,P15-1053,0,0.144705,"tudy the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup w"
2020.acl-main.287,C18-1318,1,0.925384,"Missing"
2020.acl-main.287,I17-1060,0,0.0976805,"uistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativen"
2020.acl-main.287,Q17-1016,0,0.102694,"encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features c"
2020.acl-main.287,W03-1014,0,0.231148,"tative units in editorials that present evidence, we use the pre-trained evidence classifier of Al-Khatib et al. (2017). For each editorial, we identify the number of sentences that manifest anecdotal, statistical, and testimonial evidence respectively. MPQA Arguing Somasundaran et al. (2007) constructed a lexicon that includes various patterns of arguing such as assessments, doubt, authority, emphasis. For each lexicon, we have one feature that represents the count of the respective pattern in an editorial. MPQA Subjectivity We apply the subjectivity classifier provided in OpinionFinder 2.0 (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) on the editorials, in order to count the number of subjective and objective sentences there. 4 Data As the basis of our analysis, we use the WebisEditorial-Quality-18 corpus (El Baff et al., 2018). The corpus includes persuasive effect annotations of 1000 English news editorials from the liberal New York Times (NYTimes).2 The annotations capture whether a given editorial challenges the prior stance of readers (i.e., making them rethink it, but not necessarily change it), reinforces their stance (i.e., helping them argue better about the discussed topic), or is ineffec"
2020.acl-main.287,2007.sigdial-1.5,0,0.670249,"addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTimes editorial corpus of El Baff et al. (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented classifiers for persuasive effect (Section 5).1 While the general performance of effect prediction seems somewhat limited on the corpus, our experiments yield important results: Conservative readers seem largely unaffected by the style of the (liberal) NYTimes, matching the intuition that content is what dominates opposing ideologies. On the other ha"
2020.acl-main.287,W03-1017,0,0.19064,"t the specific structure of news editorials, we finally obtain common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in p"
2020.acl-main.287,N16-1017,0,0.166831,"nteraction between the author and the intended reader of an argumentative text is encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political"
2020.acl-main.287,C16-1158,1,0.916003,"he author and the intended reader of an argumentative text is encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative"
2020.acl-main.287,E17-1017,1,0.85022,"of the editorial. analyzing their properties (Bal and Dizier, 2010; Scheffler and Stede, 2016). While Al-Khatib et al. (2016) modeled the structure underlying editorial argumentation, we use the corpus of El Baff et al. (2018) meant to study the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work,"
2020.acl-main.511,P16-1150,0,0.415226,"ata on an interval scale. However, assessors rarely perceive labels as equidistant, thus producing only ordinal data. This leads to a misuse of statistical tests and results in low statistical power of subsequent analyses. (2) Absolute rating is difficult for assessors without prior domain knowledge, since they may be unsure which label to assign. This results in noisy, inconsistent, and unreliable data. As an alternative, preference rating (i.e., a relative comparison by showing two arguments to an assessor and letting them declare their preference towards one of them) has been considered by Habernal and Gurevych (2016), who compile an exhaustive set of pairwise comparisons to infer labels for argument convincingness. For 1,052 arguments on 32 issues, each of the over 16,000 total comparisons was annotated by five different crowd workers on MTurk. While no α statistics are provided, the authors do conclude that preference ratings in a crowdsourced setting are sufficiently accurate, since the best-ranked rater for each pair achieves 0.935 accuracy compared to a gold label. The indicated reliability of pairwise annotation for argument quality is further corroborated by Toledo et al. (2019), who compile a large"
2020.acl-main.511,Q18-1026,0,0.17038,"s promising based on the reported reliability, it creates the need for a model that infers score labels from the collected comparison data. Habernal and Gurevych propose the use of PageRank (Page et al., 1999). This is problematic, since cycles in the comparison graph may form rank sinks, distorting the latent rankings. Habernal and Gurevych deal with this problem by constructing a directed acyclic graph (DAG) from the collected data prior to applying PageRank, assuming that argument convincingness exhibits the property of total order. However, no prior evidence for this property is apparent. Simpson and Gurevych (2018) note further problems with PageRank and propose the use of Gaussian process preference learning instead, demonstrating a high scalability. However, for a practical approach, an effective strategy to minimize the number of needed comparisons is warranted, since, to build the DAG, exhaustive comparison data is required. This is inefficient;  n at worst 2 comparisons have to be obtained for n arguments. Also, no data was collected on how the PageRank method performs on incomplete or sparse comparison data. Chen et al. (2013) also propose an online sampling strategy based on the Bradley-Terry mo"
2020.acl-main.511,W15-4631,0,0.171934,"n issue in crowdsourced settings, where judgments can be collected in abundance for a comparatively cheap price. However, the problem of annotation quality is more severe here: argument quality might be even more difficult to judge without prior domain-specific knowledge, creating the need for annotation frameworks that can still maintain a sufficiently high data quality. Judging from the agreement scores given by Wachsmuth et al. and Potthast et al., obtaining reliable data using classic graded scales proves infeasible, an effect that should be even more pronounced in a crowdsourced setting. Swanson et al. (2015) measure an arguments’ quality as the amount of context or inference required for it to be understood, describing an annotation setup where assessors judge seven individual quality dimensions on a 0-1-slider. Recruiting assessors on Amazon Mechanical Turk (MTurk), they use intra-class correlation to estimate inter-rater agreement, with an average value of 0.42 over all topics, thus also indicating a poor reliability (Portney et al., 2009). They further observe a correlation with sentence length, prompting them to remove all sentences shorter than four words. All three studies indicate that abs"
2020.acl-main.511,D19-1564,0,0.0842744,"onsidered by Habernal and Gurevych (2016), who compile an exhaustive set of pairwise comparisons to infer labels for argument convincingness. For 1,052 arguments on 32 issues, each of the over 16,000 total comparisons was annotated by five different crowd workers on MTurk. While no α statistics are provided, the authors do conclude that preference ratings in a crowdsourced setting are sufficiently accurate, since the best-ranked rater for each pair achieves 0.935 accuracy compared to a gold label. The indicated reliability of pairwise annotation for argument quality is further corroborated by Toledo et al. (2019), who compile a large dataset of about 14,000 annotated argument pairs, and absolute ratings in the 0-1-range for about 6,300 arguments. Pairwise annotations were made in regard to the overall quality of arguments, operationalized as “Which of the two arguments would have been preferred by most people to support/contest the 5773 topic?” Using a strict quality control, they show that the annotated relations consistently reproduce the direction implied by absolute ratings. Yet, annotating quality as a single feature is problematic, since (1) it is hard to capture the multi-facet nature of argume"
2020.acl-main.511,E17-1017,1,0.66167,"l, argumentation generation, and question answering, compiling labeled data for argument quality remains an important prerequisite, yet, also a difficult problem. Most commonly, human assessors have been presented with one argument at a time and then asked to assign labels on a graded quality scale h0, 1, 2i with label descriptions such as (0) “low quality”, (1) “medium quality” and (2) “high quality” for guidance. In previous work, this was usually done concurrently for multiple orthogonal sub-dimensions of argument quality; judging the overall quality of an argument has been deemed complex (Wachsmuth et al., 2017). But on closer inspection, even the more specialized quality dimensions considered are difficult to be assessed as evidenced by the low reliability scores reported. Especially crowdsourcing suffers from assessors often having different reference frames to base their judgments on and task instructions being nondescript and therefore unhelpful in ensuring consistency. Employing experts, however, not only comes at a significantly higher cost per label; despite their expertise, even experts did not achieve more reliable judgments. We pursue an alternative approach: stochastic transitivity modelin"
2020.acl-main.632,N18-1094,0,0.312004,"iveness as well as of debaters’ resistance to persuasion. 1 Introduction Persuasion is a primary goal of argumentation (O’Keefe, 2006). It is often carried out in the form of a debate or discussion, where debaters argue to persuade others to take certain stances on controversial topics. Several studies have examined persuasiveness in debates by probing the main factors for establishing persuasion, particularly regarding the role of linguistic features of debaters’ arguments (Zhang et al., 2016), the interaction between debaters (Tan et al., 2016), and the personal characteristics of debaters (Durmus and Cardie, 2018). While the impact of debaters’ characteristics on persuasiveness has been observed in online debates, the exploitation of these characteristics for predicting persuasiveness has been done based on explicit characteristics-related information in users’ profiles or on questionnaires. For example, Lukin et al. (2017a) performed a personality trait test for selected people and asked them for their stances on specific topics to estimate their beliefs. Also, Durmus and Cardie (2018) used the information in users’ profiles in an online forum, where their stances on controversial topics are explicitl"
2020.acl-main.632,W19-8607,1,0.878105,"Missing"
2020.acl-main.632,N18-1036,1,0.805122,"ing demographics such as gender, ethnicity, and user’s beliefs. Our data source lacks this information, which increases the difficulty of modeling users. Recently, Guo et al. (2020) modeled the interplay of comments to study their cumulative influence on persuading the audience. They proposed a sequential model that captures the interplay as local and non-local dependencies and outperforms studies focusing only on lexical features. The “ChangeMyView” subreddit (CMV) has been exploited for argument persuasiveness in many studies. For example, (Tan et al., 2016), (Hidey and McKeown, 2018), and (Habernal et al., 2018) used CMV as a source of real-world persuasive discourse. 3 Persuasiveness Tasks and Data In this paper, we address the two persuasiveness tasks that have been proposed by Tan et al. (2016): 1. Predicting argument persuasiveness: given a debate topic and an argument regarding it, the task is to predict if the argument is persuasive, in terms of whether it is able to change the stance of an opponent. 2. Predicting resistance to persuasion: given a controversial topic (with a specific stance towards it) written by a debater, the task is to identify whether the debater’s stance is resistant. We u"
2020.acl-main.632,W19-4519,0,0.0812976,"change that results from social media dialogs to that from professionally curated monologic summaries. Participants were profiled for prior beliefs and personality types—neutral and balanced arguments were successful at changing the beliefs of all participants. In contrast, an entrenched audience was convinced by more emotional dialogs. (Durmus and Cardie, 2018) further explored the role of prior beliefs by predicting the success of debaters with explicitly stated religious and political ideologies, and found that readers were more likely to be convinced by a debater with the same ideology. (Longpre et al., 2019) examined linguistic features of debates together with audience features such as demographic information, prior beliefs, and debate platform behavior. They found that for a priori undecided users, audience features were prominent in predicting persuasiveness. For decided users, stylistic features of the argument were more effective. Closely related to our work, Durmus and Cardie (2019) explored the effects of debaters’ language, their prior beliefs and traits, and social interactions with other users on the DDO (debate.org) platform. The social interaction features were crucial in predicting t"
2020.acl-main.632,I17-1060,0,0.0711062,"approach that tackles two persuasiveness tasks with improved effectiveness over previous approaches.2 1 The corpus can be found at webis.de/data and https://zenodo.org/record/3778298 2 To reproduce our experiments, the code is found here: https://github.com/webis-de/ACL-20 7067 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7067–7072 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Related Work The prediction of argument persuasiveness has been investigated in several studies (e.g., (Tan et al., 2016), (Zhang et al., 2016), (Persing and Ng, 2017), and (Hidey and McKeown, 2018). To mitigate the lack of annotated data, Persing and Ng (2017) proposed a light supervision model for persuasiveness scoring by explicitly modeling errors that negatively impact the persuasiveness of an argument. Musi et al. (2018) built an annotated corpus of concessions in CMV discussions using expert annotations and automatic classification. They observed that concessions are equally distributed among persuasive and non-persuasive threads and that they do not play any significant role as a means of persuasion. Studying the effect of argument sequencing, Hidey"
2020.acl-main.632,N16-1017,0,0.163512,"/ChangeMyView, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness as well as of debaters’ resistance to persuasion. 1 Introduction Persuasion is a primary goal of argumentation (O’Keefe, 2006). It is often carried out in the form of a debate or discussion, where debaters argue to persuade others to take certain stances on controversial topics. Several studies have examined persuasiveness in debates by probing the main factors for establishing persuasion, particularly regarding the role of linguistic features of debaters’ arguments (Zhang et al., 2016), the interaction between debaters (Tan et al., 2016), and the personal characteristics of debaters (Durmus and Cardie, 2018). While the impact of debaters’ characteristics on persuasiveness has been observed in online debates, the exploitation of these characteristics for predicting persuasiveness has been done based on explicit characteristics-related information in users’ profiles or on questionnaires. For example, Lukin et al. (2017a) performed a personality trait test for selected people and asked them for their stances on specific topics to estimate their beliefs. Also, Durmus and Cardie"
2020.acl-main.632,E17-1070,0,0.303909,"suasiveness in debates by probing the main factors for establishing persuasion, particularly regarding the role of linguistic features of debaters’ arguments (Zhang et al., 2016), the interaction between debaters (Tan et al., 2016), and the personal characteristics of debaters (Durmus and Cardie, 2018). While the impact of debaters’ characteristics on persuasiveness has been observed in online debates, the exploitation of these characteristics for predicting persuasiveness has been done based on explicit characteristics-related information in users’ profiles or on questionnaires. For example, Lukin et al. (2017a) performed a personality trait test for selected people and asked them for their stances on specific topics to estimate their beliefs. Also, Durmus and Cardie (2018) used the information in users’ profiles in an online forum, where their stances on controversial topics are explicitly stated, as a proxy of their beliefs. Such a means of exploitation limits the applicability of predicting persuasiveness, as the characteristics of debaters are usually not explicitly available in online debates, and it is not practicable to survey every debater. The paper at hand studies how the characteristics"
2020.argmining-1.12,D17-1141,1,0.891754,"Missing"
2020.argmining-1.12,N12-1033,0,0.0156979,"eve a particular goal, such as persuading the readers (Burton, 2007). “Style” is an elusive concept which covers a wide range of techniques an author can follow, including justifying a conclusion by anecdotal evidence, using regular repetition of the same phrase, or raising questions and then answering them. In the literature on the subject, these techniques are called rhetorical devices (Johnson, 2016). The automatic analysis of style has been addressed mostly by developing a set of style features (aka style indicators) such as the percentage of function words (Ganjigunte Ashok et al., 2013; Bergsma et al., 2012). Those features have proven to be effective in various analysis tasks, such as genre classification and author recognition. However, they are not appropriate for typical text synthesis and writing assistance tasks, since they cannot reveal the “essence of a style” in an explicit and describable manner. By contrast, analyzing the writing style based on rhetorical devices provides a mechanism to describe where, what, and how specific techniques are used. This kind of analysis is not only important for exploring content in social science (Niculae and Danescu-Niculescu-Mizil, 2014), but it can al"
2020.argmining-1.12,P14-2084,0,0.0127052,"sidential debates from the American presidency project (Woolley and Gerhard, 2017). We consider the gained qualitative and quantitative insights about the usage of rhetorical devices as step forward to a new generation of semi-automated argumentative text generation and writing tools. All developed resources in this paper are made publicly available at www.webis.de 2 Related Work Recently, Investigating rhetorical devices for style analysis has been considered in computational linguistics. Various devices at the semantic and pragmatic levels have been addressed singly such as irony (e.g., (C. Wallace et al., 2014) ), sarcasm (e.g., (Ghosh et al., 2015)), evidence (e.g., (Rinott et al., 2015)), and means of persuasion (e.g., (Duthie et al., 2016)). In a notable work, Strommer (2011) work on identifying ‘epanaphora’. They try to distinguish between accidental and intentional use of this device. Other studies target identifying a mix of syntax, and semantic devices. Gawryjołek et al. (2009) addressed four rhetorical devices: ‘anaphora’, ‘isocolon’, ‘epizeuxis’, and ‘oxymorons’. These devices were utilized to recognize the author of a set of documents. Java (2015) identified the four devices mentioned abov"
2020.argmining-1.12,D13-1181,0,0.108945,"or who wants to achieve a particular goal, such as persuading the readers (Burton, 2007). “Style” is an elusive concept which covers a wide range of techniques an author can follow, including justifying a conclusion by anecdotal evidence, using regular repetition of the same phrase, or raising questions and then answering them. In the literature on the subject, these techniques are called rhetorical devices (Johnson, 2016). The automatic analysis of style has been addressed mostly by developing a set of style features (aka style indicators) such as the percentage of function words (Ganjigunte Ashok et al., 2013; Bergsma et al., 2012). Those features have proven to be effective in various analysis tasks, such as genre classification and author recognition. However, they are not appropriate for typical text synthesis and writing assistance tasks, since they cannot reveal the “essence of a style” in an explicit and describable manner. By contrast, analyzing the writing style based on rhetorical devices provides a mechanism to describe where, what, and how specific techniques are used. This kind of analysis is not only important for exploring content in social science (Niculae and Danescu-Niculescu-Mizi"
2020.argmining-1.12,D15-1116,0,0.0131815,"dency project (Woolley and Gerhard, 2017). We consider the gained qualitative and quantitative insights about the usage of rhetorical devices as step forward to a new generation of semi-automated argumentative text generation and writing tools. All developed resources in this paper are made publicly available at www.webis.de 2 Related Work Recently, Investigating rhetorical devices for style analysis has been considered in computational linguistics. Various devices at the semantic and pragmatic levels have been addressed singly such as irony (e.g., (C. Wallace et al., 2014) ), sarcasm (e.g., (Ghosh et al., 2015)), evidence (e.g., (Rinott et al., 2015)), and means of persuasion (e.g., (Duthie et al., 2016)). In a notable work, Strommer (2011) work on identifying ‘epanaphora’. They try to distinguish between accidental and intentional use of this device. Other studies target identifying a mix of syntax, and semantic devices. Gawryjołek et al. (2009) addressed four rhetorical devices: ‘anaphora’, ‘isocolon’, ‘epizeuxis’, and ‘oxymorons’. These devices were utilized to recognize the author of a set of documents. Java (2015) identified the four devices mentioned above in addition to nine new devices belon"
2020.argmining-1.12,D11-1067,0,0.0257956,"Missing"
2020.argmining-1.12,P14-5010,0,0.00423211,"to ‘other’. The distribution is shown in Table 2. This dataset, despite its relatively small size, is significantly larger than those that have been used for rhetorical devices in related work (Java, 2015). Experimental Settings: The implementation of our approach was carried out using Apache RutaTM (Rule-based Text Annotation) (Kluegl et al., 2016). This tool provides a flexible language for identifying patterns in text spans intuitively. Thus, it facilitates identifying sophisticated patterns with a few lines of code. The implementation is performed on top of the outputs of Stanford Parser (Manning et al., 2014), the version of 3.8.0. We evaluated our approach using the one-vs.-rest classification. That means we performed one classification experiment for each device; The instances of this device in the evaluation dataset is considered as the positive class, and the instances of the remaining devices as well as the ‘other’ as the negative class. The classifiers’ effectiveness is reported in terms of precision, recall, and F1 -score. Classification Results: Table 2 shows the results of our experiments. Overall, we manage to identify the 26 devices with an average of 0.70 F1 -score., which indicates a"
2020.argmining-1.12,J93-2004,0,0.07099,".g., “I will be the greatest jobs president that God ever created”. • The passive voice might be used to hide the subject of a negative action, or to stress the importance of an event, e.g., “many mistakes were made, but the future will be great”. Table 1 provides an overview of the conditionals, comparatives and superlatives, and passive voice devices. The overview is analog to the one of the figurative category. The formalization is based on definitions from the same set of resources used in the figurative category. The elements of formalization are taken from The Penn Treebank POS Tag Set (Marcus et al., 1993). 3.3 Experiments and Results Device Instances Prec. Recall F1 Device Instances Prec. Recall F1 (B1) Enumeration (B2) Isocolon∗ (B3) Pysma (O1) Asyndeton (O2) Hypozeugma (O3) Epizeugma (R1) Epanalepsis (R2) Mesarchia (R3) Epiphoza (R4) Mesodiplosis (R5) Anadiplosis (R6) Diacope (R7) Epizeuxis (R8) Polysyndeton 60 180 60 60 60 60 60 20 60 40 60 60 60 60 0.76 0.57 1 0.25 0.61 0.65 0.63 0.45 0.58 0.27 0.76 0.73 0.79 0.77 0.93 0.83 1 0.93 0.8 0.7 0.83 0.85 0.93 0.68 0.73 0.73 0.77 0.7 0.84 0.68 1.00 0.39 0.69 0.67 0.72 0.59 0.71 0.39 0.74 0.73 0.78 0.73 (C1) If-cond. Zero (C2) If-cond. One (C3) If"
2020.argmining-1.12,D14-1215,0,0.013073,"rds (Ganjigunte Ashok et al., 2013; Bergsma et al., 2012). Those features have proven to be effective in various analysis tasks, such as genre classification and author recognition. However, they are not appropriate for typical text synthesis and writing assistance tasks, since they cannot reveal the “essence of a style” in an explicit and describable manner. By contrast, analyzing the writing style based on rhetorical devices provides a mechanism to describe where, what, and how specific techniques are used. This kind of analysis is not only important for exploring content in social science (Niculae and Danescu-Niculescu-Mizil, 2014), but it can also serve text synthesis systems by improving the quality of automatically generated texts (Hu et al., 2017). Moreover, it can form the backbone of style suggestion tools. For example, when writing a text for which the desired specification (e.g., the genre) is given, adequate style techniques can be suggested to improve the text quality. In such a manner, new writers can learn to improve their texts and approach the quality of masterpieces written by top writers. Figure 1 illustrates the described connections. Rhetoric has been the subject of investigation amongst scholars since"
2020.argmining-1.12,D15-1050,0,0.0488896,"Missing"
2020.argmining-1.12,W15-4713,0,0.0170555,"n structure and the identified devices. Few resources for rhetorical devices are publicly free. Up to our knowledge, the code of the previous studies is not available anywhere on the web. Hence, researchers often have to write a new piece of code every time they need to analyze style based on rhetorical devices. This paper resolves this problem considerably by providing a tool for identifying 26 different rhetorical devices. Our developed resources, including the code, will be made freely available. PCFG outputs have been employed for different tasks including response generation in dialogue (Yuan et al., 2015), multiword expression identification (Green et al., 2011), and the task at hand: identifying 107 rhetorical devices (Gawryjołek et al., 2009; Java, 2015). However, we develop a set of original heuristic rules that map the devices’ definitions to PCFG grammars. As far as we know, many devices from the 26 we identified have not been considered in any other study. Writing style analysis has been studied widely. The authorship recognition has been tackled in a large number of papers (e.g., (Sundararajan and Woodard, 2018)). Besides, quality assessment research has involved applying several style"
2020.findings-emnlp.383,N19-1216,0,0.0715649,"sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a source. The relation between sentencelevel and article-level bias re"
2020.findings-emnlp.383,W18-6212,0,0.221679,"al. (2014) used RNNs to aggregate the polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a sou"
2020.findings-emnlp.383,W18-6509,1,0.479296,"utperform those without. 1 Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Lysol maker issues warning against injections of disinfectant after Trump comments — The Hill, center-oriented “This notion of injecting or ingesting any type of cleansing product into the body is irresponsible and it’s dangerous,” said Gupta. — NBC News, left-oriented Introduction Media bias is discussed and analyzed in journalism research (Groseclose and Milyo, 2005; DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009) and NLP research (Gerrish and Blei, 2011; Iyyer et al., 2014; Chen et al., 2018). According to the study of Groseclose and Milyo (2005), bias “has nothing to do with the honesty or accuracy”, but it means “taste or preference”. In fact, journalists may (1) report facts only in favor of one particular political side and thus (2) conclude with their own opinion. As an example, the following sentences from allsides.com reporting on the event “Trump asks if disinfectant, sunlight can treat coronavirus” From an NLP perspective, bias in the example sentences could be detected by capturing sentiment words, such as “falsely” or “irresponsible”. Without the background knowledge of"
2020.findings-emnlp.383,D19-1664,0,0.399279,"c) their sequential order. For each type, we model the bias distribution in a new way through a Gaussian Mixture Model (GMM), in order to then exploit it as features of an SVM (for frequency), Naïve Bayes (for positions), and a firstorder Markov model (for sequential order). The results show strong correlations between the two levels for frequency and position information, whereas sequential order seems less correlated. To study Q1–Q3, we employ the BASIL dataset, which includes manually annotated bias labels at article level as well as lexical and informational bias labels at sentence level (Fan et al., 2019). While the dataset contains only 300 articles, it provides the best basis for understanding the interaction of bias at both levels available so far. For Q3, finally, we propose a new approach applicable in realistic settings. In particular, we retrain the bias detectors from the Q1 experiments on the sentence level and then exploit the GMM as above to predict to article level bias. In our evaluation, the approach significantly outperforms the article-level approaches analyzed for Q1. CounterQ1. How effective are standard classification approaches in article-level bias detection, with and with"
2020.findings-emnlp.383,W19-4809,0,0.0845579,"from the used dataset. It becomes clear that the actual words in the biased sentences are not always indicative to distinguish biased from neutral articles, nor is the count of the biased sentences: Bias assessments on sentence level do not “add up”. In this regard, the position of biased sentences appears to be a better feature. The existing approaches to bias detection are transferred from other, less intricate text classification tasks. They largely model low-level lexical information, either explicitly, e.g. by using bag-ofwords (Gerrish and Blei, 2011), or implicitly via neural networks (Gangula et al., 2019). Such approaches tend to fail at the article level, particularly for articles on events not covered in the training data. The reason is that bias clues are subtle and rare in articles, especially event-independent clues. Altogether, modeling low-level information at the 4290 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4290–4300 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Facing Congress, Clinton Defends Her Actions Before and After Libya Attack Republicans challenge Clinton claims on budget cuts, Benghazi cable 5 Middle Beginning 1"
2020.findings-emnlp.383,C18-1232,0,0.0166763,"remains unstudied so far. The goal of this paper is not to discuss the difference between these levels. Rather, we examine how to aggregate the sentence-level bias to generate second-order features, and then use these features to predict article-level bias. The use of low-level information to generate second-order features was studied in the context of product reviews by modeling patterns in the reviews’ sentiment flow (Wachsmuth et al., 2015), by tuning neural network to capture important sentences (Xu et al., 2016), and by routing in aggregating sentence embeddings into document embedding (Gong et al., 2018). In particular, our usage of low-level information is inspired by Wachsmuth et al. (2015), where we hypothesize that such flows exist in media bias as well. However, we do not limit our approach to entire sequences of sentencelevel information, but we also consider frequency, position, or only two to three continuous sentences. 3 Standard Bias Detection Approaches Standard approaches for bias detection, on both article and sentence level, mainly exploit the lowlevel lexical features to classify the texts as biased or not, neglecting bias-specific features. The two main low-level lexical featu"
2020.findings-emnlp.383,N09-1057,0,0.0140725,"r bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-t"
2020.findings-emnlp.383,P14-1105,0,0.490258,"nformation clearly outperform those without. 1 Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Lysol maker issues warning against injections of disinfectant after Trump comments — The Hill, center-oriented “This notion of injecting or ingesting any type of cleansing product into the body is irresponsible and it’s dangerous,” said Gupta. — NBC News, left-oriented Introduction Media bias is discussed and analyzed in journalism research (Groseclose and Milyo, 2005; DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009) and NLP research (Gerrish and Blei, 2011; Iyyer et al., 2014; Chen et al., 2018). According to the study of Groseclose and Milyo (2005), bias “has nothing to do with the honesty or accuracy”, but it means “taste or preference”. In fact, journalists may (1) report facts only in favor of one particular political side and thus (2) conclude with their own opinion. As an example, the following sentences from allsides.com reporting on the event “Trump asks if disinfectant, sunlight can treat coronavirus” From an NLP perspective, bias in the example sentences could be detected by capturing sentiment words, such as “falsely” or “irresponsible”. Without the bac"
2020.findings-emnlp.383,S19-2145,1,0.885092,"Missing"
2020.findings-emnlp.383,D18-1388,0,0.101855,"polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a source. The relation between sentencele"
2020.findings-emnlp.383,P19-1247,0,0.014373,"have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-tank citations to estimate the bias. With the rise of deep learning, NLP researchers have also used neural-based approaches for bias detection. Iyyer et al. (2014) used RNNs to aggregate the polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps underst"
2020.findings-emnlp.383,W06-2915,0,0.352236,"the fact that the sentencelevel detector creates more deterministic sentence bias features, allowing our approach to learn from them in a more robust way. Altogether, the contribution of this paper is threefold: (1) We provide evidence that standard approaches fail in detecting article-level bias. (2) We develop a new approach utilizing second-order bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sente"
2020.findings-emnlp.383,P17-1068,0,0.0601338,"Missing"
2020.findings-emnlp.383,D17-1317,0,0.0564728,"er is threefold: (1) We provide evidence that standard approaches fail in detecting article-level bias. (2) We develop a new approach utilizing second-order bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the prefere"
2020.findings-emnlp.383,P13-1162,0,0.0248535,"rticle-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-tank citations to estimate the bias. With the rise of deep learning, NLP researchers have also used neural-based approaches for bias"
2020.findings-emnlp.383,D15-1072,1,0.848763,"All sentences labeled as having lexical or informational bias are highlighted; their position can be read from the numbers next to them. article level is insufficient to detect article-level bias, as we will later stress in experiments. We study article-level bias detection both with and without allowing to learn event-specific information. The latter scenario is more challenging, but it is closer to the real world, because we cannot expect that the information in future articles always relates to past events. Inspired by ideas from modeling local and global polarities in sentiment analysis (Wachsmuth et al., 2015), we hypothesize that using second-order bias information in terms of lexical and informational bias at the sentence level is key to detecting article-level bias. To the best of our knowledge, no bias detection approach so far uses such information. We investigate this hypothesis in light of three research questions: For Q1, we evaluate an n-gram-based SVM and a BERT-based neural network in article-level bias detection. To assess the impact of event-related information, we split the dataset in two ways, once with event overlap in the training set and test set, and once without. As expected, we"
2020.findings-emnlp.383,D16-1172,0,0.0247725,"as the feature vectors of a source. The relation between sentencelevel and article-level bias remains unstudied so far. The goal of this paper is not to discuss the difference between these levels. Rather, we examine how to aggregate the sentence-level bias to generate second-order features, and then use these features to predict article-level bias. The use of low-level information to generate second-order features was studied in the context of product reviews by modeling patterns in the reviews’ sentiment flow (Wachsmuth et al., 2015), by tuning neural network to capture important sentences (Xu et al., 2016), and by routing in aggregating sentence embeddings into document embedding (Gong et al., 2018). In particular, our usage of low-level information is inspired by Wachsmuth et al. (2015), where we hypothesize that such flows exist in media bias as well. However, we do not limit our approach to entire sequences of sentencelevel information, but we also consider frequency, position, or only two to three continuous sentences. 3 Standard Bias Detection Approaches Standard approaches for bias detection, on both article and sentence level, mainly exploit the lowlevel lexical features to classify the"
2020.nlpcss-1.16,D18-1389,0,0.0214508,"ntial media bias patterns. The results show that our model can utilize highlevel semantic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs"
2020.nlpcss-1.16,N19-1216,0,0.0171293,"C categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature importance) for each text segment (Bahdanau"
2020.nlpcss-1.16,W18-6212,0,0.0129239,"he paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output wei"
2020.nlpcss-1.16,D18-1388,0,0.0172569,"level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature impor"
2020.nlpcss-1.16,D14-1162,0,0.0864531,"Missing"
2020.nlpcss-1.16,D17-1317,0,0.0210276,"tic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularit"
2020.nlpcss-1.16,N16-1174,0,0.0222836,"t granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature importance) for each text segment (Bahdanau et al., 2014). Zhou et al. (2016), for instance, use word-level attention to focus on sentiment words, while Ji et al. (2017) use sentencelevel attention to select valid sentences for entity relation extraction. Related to media bias, Kulkarni et al. (2018) show that the learned attention can focus on the biased sentences when detecting political ideology. Regarding “attention” at multi-level granularity, Yang et al. (2016) propose a hierarchical attention network for document classification that is used at both word and sentence level. Yet, problems when using attention for such analyses are that the analysis unit (be it a word or a sentence) has to be defined before the training process, and that the set of classifiers which can output attention is limited. By contrast, our unsupervised reverse feature analysis can be used with any classifier and at an arbitrary semantic level, even after the training process. 3 Media Bias Corpus Although bias detection is viewed as an important task, we could not find any rea"
2020.nlpcss-1.16,D16-1024,0,0.0292956,"Missing"
2020.nlpcss-1.16,2020.findings-emnlp.383,1,0.87009,"Paderborn University Bauhaus-Universität Weimar Department of Computer Science Faculty of Media, Webis Group cwf@mail.upb.de khalid.alkhatib@uni-weimar.de Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Benno Stein Bauhaus-Universität Weimar Faculty of Media, Webis Group benno.stein@uni-weimar.de Abstract them) can allow media organizations to maintain credibility and can enable readers to choose what to consume and what not. In pursuit of this goal, we recently studied how sentence-level bias in an article affects the political bias of the whole article (Chen et al., 2020). Also other researchers have proposed approaches to automatic bias detection (see Section 2 for details). However, the existing approaches lack an analysis of what makes up bias, and how it exposes in different granularity levels, from single words, to sentences and paragraphs, to the entire discourse. To close this gap, we analyze political bias and unfairness in this paper within three steps: Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by repor"
2020.nlpcss-1.16,W18-6509,1,0.630631,"lems when using attention for such analyses are that the analysis unit (be it a word or a sentence) has to be defined before the training process, and that the set of classifiers which can output attention is limited. By contrast, our unsupervised reverse feature analysis can be used with any classifier and at an arbitrary semantic level, even after the training process. 3 Media Bias Corpus Although bias detection is viewed as an important task, we could not find any reasonably sized labeled corpus that suits our study, which is why we decided to built a new one. We started with the corpus of Chen et al. (2018) in order to obtain news articles with topics and political bias labels. We extended the corpus by crawling the latest articles from allsides.com and by adding the fairness labels provided by adfontesmedia.com. The new corpus is available at https://github.com/ webis-de/NLPCSS-20. Allsides.com This website is a news aggregator that collects news articles about American politics, starting from June 1st, 2012. Each event comes with articles representing one of the three political camps: left, center, and right. Chen et al. (2018) crawled the website to extract 6447 articles. For our study, we ex"
2020.nlpcss-1.16,P14-1105,0,0.204757,"how that our model can utilize highlevel semantic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actua"
2020.peoples-1.4,C16-1324,1,0.931256,"lity is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phrased way. This view was modeled by Wachsmuth et al. (2018) for argumentati"
2020.peoples-1.4,D17-1141,1,0.910616,"s, the average range is ≥ 33 and &lt; 67. 32 Feature Base Overview Linguistic inquiry and word count Psychological meaningfulness in percentile NRC emotional and sentiment lexicon Count of emotions (e,g. fear, etc.) and polarity words Webis Argumentative Discourse Units Count of each evidence type (anecdote and testimony) Count of 17 types of arguing (asMPQA Arguing Lexicon sessments, doubt, etc.) Count of subjective and objecMPQA Subjectivity Classifier tive sentences Lemma 1–3-grams TfIdf of lemma for 1–3-grams Reference Label Pennebaker et al. (2015) liwc Mohammad and Turney (2013) emotion Al Khatib et al. (2017) evidence Somasundaran et al. (2007) arguing Riloff and Wiebe (2003) subjectivity Miller (1998) lemma Table 3: Summary of the six feature types used. Each feature is quantified at both the level of the editorial. The labels (rightmost column) are used to refer to the respective feature. Gerlach et al. (2018) developed an approach to identify personality types, which they applied to more than 1.5 million participants. They found robust evidence for at least four distinct personality types and one of them is labeled as the “role model”, who is low in neuroticism and high in all the other traits."
2020.peoples-1.4,2020.acl-main.632,1,0.926615,"aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German newspaper: https://www.spiegel.de/international/"
2020.peoples-1.4,bal-saint-dizier-2010-towards,0,0.0279657,"e somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phr"
2020.peoples-1.4,W09-3723,0,0.0606116,"ow that people with extreme ideology are somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned"
2020.peoples-1.4,N18-1094,0,0.204693,"states that persuasive text aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German newspaper: https://www.s"
2020.peoples-1.4,K18-1044,1,0.708994,"Missing"
2020.peoples-1.4,2020.acl-main.287,1,0.375206,"Missing"
2020.peoples-1.4,P16-1150,0,0.01192,"liefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we"
2020.peoples-1.4,W17-5102,0,0.0116636,"a wellarranged and well-phrased way. This view was modeled by Wachsmuth et al. (2018) for argumentation synthesis. Instead, we here follow the communication-persuasion paradigm of O’Keefe (2015), stating that an argumentative text, and hence a news editorial, should comply with five factors to be persuasive, as already indicated in Section 1. Each of them is tackled in some way in related work: (1) Source refers to the prior beliefs and behaviors of the writer. Each news portal reflects its beliefs (van Dijk, 1995). (2) Message deals with the linguistic choices in the content. In this regard, Hidey et al. (2017) study the semantic types of argument components in an online forum, and El Baff et al. (2020) analyze the persuasive effect of linguistic style on readers. Also, Hidey and McKeown (2018) and Durmus et al. (2020), respectively, exploit the role of argument sequencing in detecting persuasive influence, and the role of pragmatics and discourse context in determining argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie"
2020.peoples-1.4,W15-0505,1,0.824942,"style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phrased way. This view w"
2020.peoples-1.4,E17-1070,0,0.318433,"and Virtanen (2005) states that persuasive text aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German"
2020.peoples-1.4,P15-1053,0,0.0188343,"se context in determining argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation"
2020.peoples-1.4,I17-1060,0,0.0117986,". (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we considered the beliefs"
2020.peoples-1.4,W03-1014,0,0.0556693,"Missing"
2020.peoples-1.4,2007.sigdial-1.5,0,0.282657,"nd &lt; 67. 32 Feature Base Overview Linguistic inquiry and word count Psychological meaningfulness in percentile NRC emotional and sentiment lexicon Count of emotions (e,g. fear, etc.) and polarity words Webis Argumentative Discourse Units Count of each evidence type (anecdote and testimony) Count of 17 types of arguing (asMPQA Arguing Lexicon sessments, doubt, etc.) Count of subjective and objecMPQA Subjectivity Classifier tive sentences Lemma 1–3-grams TfIdf of lemma for 1–3-grams Reference Label Pennebaker et al. (2015) liwc Mohammad and Turney (2013) emotion Al Khatib et al. (2017) evidence Somasundaran et al. (2007) arguing Riloff and Wiebe (2003) subjectivity Miller (1998) lemma Table 3: Summary of the six feature types used. Each feature is quantified at both the level of the editorial. The labels (rightmost column) are used to refer to the respective feature. Gerlach et al. (2018) developed an approach to identify personality types, which they applied to more than 1.5 million participants. They found robust evidence for at least four distinct personality types and one of them is labeled as the “role model”, who is low in neuroticism and high in all the other traits. Figure 2 shows that the upper clust"
2020.peoples-1.4,C16-1158,1,0.849608,"ing argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined"
2020.peoples-1.4,E17-1017,1,0.835541,"debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we considered the beliefs of readers by profiling annotators based on political ideology (liberals or conservatives). We also provides additional information about the annotators’ ideology 30 (a) Intensity Effect Lean Extreme (b) Personality Role Models Other Train Test Train Test Train Test Train Test Effect Challenging Ineffective Reinforcing 100 274 409 21 70 105 156 133 494 43 30 123 Challenging Ineffective Reinforcing 74 121 588 15 31 150 106 412 265 26 108 62 Overall 783 196 783 196 Overall 783 196 783 196 Table 1: Distribution of the majority persuasive effect"
2020.peoples-1.4,C18-1318,1,0.890881,"Missing"
2020.peoples-1.4,W03-1017,0,0.425214,"style features. Our results show that people with extreme ideology are somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logica"
2021.acl-long.366,2020.findings-emnlp.47,0,0.171872,"ng the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in different ways. For example, meaningful prompts on controversial topics can be constructed from an AKG"
2021.acl-long.366,2021.eacl-main.17,1,0.712023,"neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based claims, encoding the beliefs via conditional language models. Most similar to our work are the studies of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge"
2021.acl-long.366,N19-1174,0,0.0187107,"his section, we outline related studies on argument generation, argumentation knowledge graphs, and graph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based c"
2021.acl-long.366,P16-2085,0,0.0204118,"astly, since our approach is meant as a proof of concept, we used the small GPT-2 model with the parameters adopted from Gretz et al. (2020). Using a larger model and exploring different sampling methods and parameter settings will probably result in a higher quality of the arguments generated. 5 Related Work In this section, we outline related studies on argument generation, argumentation knowledge graphs, and graph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance."
2021.acl-long.366,P19-1255,0,0.0193515,"ph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based claims, encoding the beliefs via conditional language models. Most similar to our work are the st"
2021.acl-long.366,2020.findings-emnlp.428,0,0.0211573,"ith the type of effect relation between them as keyphrases separated by special tokens. We use ‘positive’ and ‘negative’ to represent the effect relations. For example, the paragraph “Animal studies suggests marijuana causes physical dependence, and serious problems” will be transformed into: 4747 “&lt;|startoftext|>’[’marijuana»positive»physicaldependence’, ’mariguana»positive»problems’] @ Animal studies suggests ...’&lt;|endoftext|>” While this way of matching and encoding has limitations, it has shown good results in practice when used with pre-trained neural models (Witteveen and Andrews, 2019; Cachola et al., 2020). 3.3 Neural Language Model Fine-tuning We use our text-knowledge encoding dataset to fine-tune the GPT-2 neural language model (Radford et al., 2019) for argument generation. Since GPT-2 cannot deal with graph structure as input directly, we fine-tune it on all paragraphs, including those with encoded relations as textual representations (i.e., keyphrases). We expect to thereby leverage the powerful generation capabilities of GPT-2 while biasing it to generate texts related to the encoded relations. It is worth noting that, in training, we encode multiple relations at once and the generated a"
2021.acl-long.366,P18-1021,0,0.0125581,", including argumentativeness and plausibility. The results demonstrate the positive impact of encoding the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in dif"
2021.acl-long.366,D14-1125,0,0.0475823,"Missing"
2021.acl-long.366,W19-8607,1,0.902254,"Missing"
2021.acl-long.366,P11-1099,0,0.0235406,"ch controlled argument generation, investigating for the first time the ability to generate high-quality and content-rich arguments by integrating knowledge from AKGs into standard neural-based generation models. To this end, we exploit multiple manually and automatically created knowledge graphs, devoting particular attention to causal knowledge (Al-Khatib et al., 2020; Heindorf et al., 2020). Causality plays a major role in argumentation due to its frequent usage in real-life discussions; argument from cause to effect and argument from consequences are frequently used argumentation schemes (Feng and Hirst, 2011; Reisert et al., 2018). To utilize AKGs for argument generation, we collect argumentative texts from diverse sources such as online debate portals. In these texts, we find arguments that contain instances of the knowledge covered in the graphs. We encode this knowledge as keyphrases in the arguments. Unlike Gretz et al. (2020) and Schiller et al. (2021), our keyphrases cover multiple aspects and stances related to the same topic. The resulting texts are used to finetune a transformer-based generation model, GPT-2 (Radford et al., 2019). The underlying hypothesis is 4744 Proceedings of the 59t"
2021.acl-long.366,P19-1049,0,0.0282992,"of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge graph of Al-Khatib et al. (2020), Toledo-Ronen et al. (2016) created an expert stance graph to support stance classification. Gemechu and Reed (2019) encoded the relations between segments of an argument into a graph and demonstrated the graph’s effectiveness for argument mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. G"
2021.acl-long.366,N19-1238,0,0.0570514,"Missing"
2021.acl-long.366,N19-1236,0,0.0645107,"Missing"
2021.acl-long.366,W18-5210,0,0.0233569,"generation, investigating for the first time the ability to generate high-quality and content-rich arguments by integrating knowledge from AKGs into standard neural-based generation models. To this end, we exploit multiple manually and automatically created knowledge graphs, devoting particular attention to causal knowledge (Al-Khatib et al., 2020; Heindorf et al., 2020). Causality plays a major role in argumentation due to its frequent usage in real-life discussions; argument from cause to effect and argument from consequences are frequently used argumentation schemes (Feng and Hirst, 2011; Reisert et al., 2018). To utilize AKGs for argument generation, we collect argumentative texts from diverse sources such as online debate portals. In these texts, we find arguments that contain instances of the knowledge covered in the graphs. We encode this knowledge as keyphrases in the arguments. Unlike Gretz et al. (2020) and Schiller et al. (2021), our keyphrases cover multiple aspects and stances related to the same topic. The resulting texts are used to finetune a transformer-based generation model, GPT-2 (Radford et al., 2019). The underlying hypothesis is 4744 Proceedings of the 59th Annual Meeting of the"
2021.acl-long.366,2021.naacl-main.34,0,0.454671,"edge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in different ways. For example, meaningful prompts on controversial topics can be constructed from an AKG with simple hand-defined"
2021.acl-long.366,2020.argmining-1.9,1,0.730117,"l use the keyphrases to constrain the generation of arguments. During application, we provide the model with knowledge (as keyphrases) to obtain new arguments that further elaborate the knowledge. Figure 1 gives an overview of the main steps of our approach. We evaluate the ability of our approach to generating new arguments for a variety of claim-like prompts: 400 generated arguments are manually assessed for their relevance to the prompt, argumentativeness, content richness, and plausibility. As a recent study indicates the adoption of bias from argumentative source data in word embeddings (Spliethöver and Wachsmuth, 2020), we also inspect potential social bias and abusive language in the generated arguments. Moreover, we evaluate the generated arguments automatically using recently developed argument mining techniques, in order to then examine correlations between manual and automatic evaluations. The results reveal an evident benefit of using the graphs’ knowledge in generating controlled arguments that are rich in content and plausible. However, we also observe the presence of social bias in the outputs of GPT-2, suggesting the need for careful postproceeing step in argument generation. Both the resources an"
2021.acl-long.366,N18-5005,0,0.0430293,"Missing"
2021.acl-long.366,W16-2814,0,0.0288407,"oding the beliefs via conditional language models. Most similar to our work are the studies of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge graph of Al-Khatib et al. (2020), Toledo-Ronen et al. (2016) created an expert stance graph to support stance classification. Gemechu and Reed (2019) encoded the relations between segments of an argument into a graph and demonstrated the graph’s effectiveness for argument mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utili"
2021.acl-long.366,C18-1318,1,0.901555,"Missing"
2021.acl-long.366,D19-5623,0,0.0177936,"he paragraph, encoding them with the type of effect relation between them as keyphrases separated by special tokens. We use ‘positive’ and ‘negative’ to represent the effect relations. For example, the paragraph “Animal studies suggests marijuana causes physical dependence, and serious problems” will be transformed into: 4747 “&lt;|startoftext|>’[’marijuana»positive»physicaldependence’, ’mariguana»positive»problems’] @ Animal studies suggests ...’&lt;|endoftext|>” While this way of matching and encoding has limitations, it has shown good results in practice when used with pre-trained neural models (Witteveen and Andrews, 2019; Cachola et al., 2020). 3.3 Neural Language Model Fine-tuning We use our text-knowledge encoding dataset to fine-tune the GPT-2 neural language model (Radford et al., 2019) for argument generation. Since GPT-2 cannot deal with graph structure as input directly, we fine-tune it on all paragraphs, including those with encoded relations as textual representations (i.e., keyphrases). We expect to thereby leverage the powerful generation capabilities of GPT-2 while biasing it to generate texts related to the encoded relations. It is worth noting that, in training, we encode multiple relations at o"
2021.acl-long.366,2020.emnlp-main.577,0,0.015936,"ble graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. Graph-to-Text Generation In the related area of neural graph-to-text generation, researchers have used various techniques (Song et al., 2018; KoncelKedziorski et al., 2019b; Schmitt et al., 2020). Within this area, the approaches most related to ours are those that exploit the usage of knowledge in graphs as input to sequence-to-sequence models (Moryossef et al., 2019) as well as those that make use of large pre-trained language models such as Liu et al. (2021), where the pretrained model BART is augmented by knowledge from a graph for generative commonsense reasoning. Overall, our work concentrates on the context of argumentation, with an approach to encoding different types of argumentation knowledge into the pretrained model GPT-2 in order to allow for more controlled argument gene"
2021.acl-long.366,P18-1150,0,0.0254763,"t mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. Graph-to-Text Generation In the related area of neural graph-to-text generation, researchers have used various techniques (Song et al., 2018; KoncelKedziorski et al., 2019b; Schmitt et al., 2020). Within this area, the approaches most related to ours are those that exploit the usage of knowledge in graphs as input to sequence-to-sequence models (Moryossef et al., 2019) as well as those that make use of large pre-trained language models such as Liu et al. (2021), where the pretrained model BART is augmented by knowledge from a graph for generative commonsense reasoning. Overall, our work concentrates on the context of argumentation, with an approach to encoding different types of argumentation knowledge into the pretrained model GP"
2021.argmining-1.4,W14-2109,0,0.0294538,"suggests listing the image both as a pro and a con. In the future, however, additional considerations of argumentative quality (especially in terms of clarity) might suggest to omit such images completely, or to show them separately. Topic relevance The image content is related to the query topic. This criterion corresponds to the notion of relevance in keyword-based image retrieval (Shanbehzadeh et al., 2000). Argumentativeness The image can be used to support a stance regarding the query topic. This criterion corresponds to the notion of a context-dependent claim in textual argument mining (Aharoni et al., 2014). Stance relevance The image can be used to support the predicted stance within the query topic. This criterion corresponds to the categorization into pros and cons in standard argument search (Wachsmuth et al., 2017b). Since stance relevance entails argumentativeness, which in turn entails topic relevance, we refer to these three as “levels” of relevance. Though previous work focused on stance relevance only (e.g., Stab et al., 2018), an analysis on all three levels provides more insight into the errors made and is especially warranted for “argumentative images.” Figure 2 illustrates the diff"
2021.argmining-1.4,goldhahn-etal-2012-building,0,0.00947894,"this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas “radiation” occurs more often in con arguments than in pro ones. Based on work in anomaly detection (Afgani et al., 2008), this method calculates the specificity of a term t to a stance s, δ(t, s), as their contribution to the Kullback-Leibler divergence of the term distributions between the two stances: (1) Good-Anti Conceivably the single most"
2021.argmining-1.4,P18-1023,1,0.763553,"st for a stance, then the second of each, and so on. We devise three methods: (1) appending always the same stance-indicating terms to the user’s query, (2) appending sentiment-indicating terms that co-occur with the query’s terms, and (3) appending topic-specific stance-indicating terms obtained from a text argument search engine. for some topics this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas"
2021.argmining-1.4,walker-etal-2012-corpus,0,0.0426073,"ence (Berger, 2010), or the recent departure of Western military forces from Afghanistan. Argument search Based on argument mining from texts (cf. Peldszus and Stede, 2013), argument search engines aim to support decisionmaking and persuasion. Conceptually, a query to an argument search engine may either name an issue without a stance (Stab et al., 2018) (e.g., nuclear energy), or represent a conclusion for which supporting and attacking premises are to be retrieved (e.g., nuclear energy mitigates climate change). The first collection of arguments from the web is the Internet Argument Corpus (Walker et al., 2012), containing 400,000 posts from an online debate portal. The first argument search engine, args.me, indexes a similar dataset of arguments (Wachsmuth et al., 2017b). Not relying on retrieval in collections of arguments, ArgumenText (Stab et al., 2018) first searches for documents relevant to a user’s query in generic web crawls, and then mines arguments on the fly within retrieved documents. Regarding the evaluation of argument search engines, judging the topic relevance of a retrieved text alone is insufficient, it must also be argumentative (Potthast et al., 2019; Bondarenko et al., 2021). R"
2021.argmining-1.4,H05-1044,0,0.0300411,"hat co-occur with the query’s terms, and (3) appending topic-specific stance-indicating terms obtained from a text argument search engine. for some topics this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas “radiation” occurs more often in con arguments than in pro ones. Based on work in anomaly detection (Afgani et al., 2008), this method calculates the specificity of a term t to a stance s, δ(t"
2021.argmining-1.4,N18-5005,0,0.0353157,"Missing"
2021.argmining-1.4,E17-1017,1,0.796036,"Missing"
2021.argmining-1.4,W17-5106,1,0.914024,"nformation and express, underline, or popularize an opinion (Dove, 2012), thereby taking the form of subjective statements (Dunaway, 2018). Some images express both a premise and a conclusion, making them full arguments (Roque, 2012; Grancea, 2017). Other images may provide contextual information only and need to be combined with a conclusion to form an argument. In this regard, a recent SemEval task distinguished a total of 22 persuasion techniques in memes alone (Dimitrov et al., 2021). Moreover, argument quality dimensions like acceptability, credibility, emotional appeal, and sufficiency (Wachsmuth et al., 2017a) all apply to arguments that include images as well. And as a kind of visual argumentation scheme (a “stereotypical pattern of human reasoning”; Walton et al., 2008), some images are frequently adapted to different topics (Heiskanen, 2017). Social groups even create their own symbolisms and use them to express opinions (e.g., fringe web communities; Zannettou et al., 2018). The potentially high emotional impact of images to a vast audience (Adler-Nissen et al., 2020) can cause changes in the social discourse and eventually politics (Woods and Hahner, 2019). Examples include photos of a drown"
2021.findings-acl.451,D16-1245,0,0.0451648,"Missing"
2021.findings-acl.451,L16-1586,0,0.0231091,"to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closely related to our work, software mentions in scientific documents can be mined using a Grobid library module,10 e.g., to give research software more credit. In 2019, a shared task focusing on the tasks of dataset review mining and extraction of scientific methods and fields w"
2021.findings-acl.451,I11-1001,0,0.0423591,"h as CKAN, integrate this functionality by default. In addition, improvements on tracking citations of data are being actively developed. Projects, like Semantic Scholar, Microsoft Academic Graph,9 and Google Dataset Search, along with Google Scholar, couple data and publications to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closel"
2021.findings-acl.451,2020.acl-main.670,0,0.0317915,"Missing"
2021.findings-acl.451,D18-1360,0,0.0117576,"Semantic Scholar, Microsoft Academic Graph,9 and Google Dataset Search, along with Google Scholar, couple data and publications to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closely related to our work, software mentions in scientific documents can be mined using a Grobid library module,10 e.g., to give research software mor"
2021.findings-emnlp.228,D19-1290,1,0.930048,"eform, though, the presence of what may be six million illegal aliens in this country exacts an economic and social toll. Table 1: (a) Sample text from the media frames corpus (Card et al., 2015). The bold sentence is labeled with the economic frame. Having reframed the sentence with the approach of this paper, the text remains largely coherent and topic-consistent while showing the legality frame (b) and crime frame (c), respectively. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle and Roberts, 2004). The media employ framing who may prefer to use “undocumented worker” into reorient how audiences think (Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke"
2021.findings-emnlp.228,P15-2072,0,0.134377,"sence of what may be six million illegal aliens in this country exacts an economic and social toll. (c) Crime Frame (reframed text) Key Congressional backers of the measure, sponsored by Senator Alan K. Simpson, Republican of Arizona, and Romano L. Mazzoli, Democrat of Kentucky, wanted a flexible spending limit. “Illegal aliens’ is a growing problem in the country,” says a spokesman for the measure’s sponsors. Without reform, though, the presence of what may be six million illegal aliens in this country exacts an economic and social toll. Table 1: (a) Sample text from the media frames corpus (Card et al., 2015). The bold sentence is labeled with the economic frame. Having reframed the sentence with the approach of this paper, the text remains largely coherent and topic-consistent while showing the legality frame (b) and crime frame (c), respectively. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle"
2021.findings-emnlp.228,2021.naacl-main.394,0,0.120217,"Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke a discussion of the economic impact of hiring them; the latter may quences regarding crime. raise issues of crime and possible deportation. Such Reframing means to change the perspective of low-level style reframing has been studied in recent an issue. It can be a strategy to communicate with work (Chakrabarty et al., 2021). opposing camps of audiences, and, sometimes, just 2683 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2683–2693 November 7–11, 2021. ©2021 Association for Computational Linguistics Usually, reframing requires rewriting entire sentences rather than single words or phrases. Table 1 illustrates the change of a sentence from the economic frame (a) to the legality frame (b) and the crime frame (c). While the original text emphasizes the cost of immigration reform, the legality-framed text quotes that “It’s time for Congress to take action,” and the crime-framed text"
2021.findings-emnlp.228,2020.findings-emnlp.383,1,0.753833,"that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal w"
2021.findings-emnlp.228,2020.nlpcss-1.16,1,0.783591,"that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal w"
2021.findings-emnlp.228,W18-6509,1,0.812941,"2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal with reframing at the sentence level,"
2021.findings-emnlp.228,N19-1142,0,0.0162792,"eneric. For example, the possible issue-specific frames for the topic of Internet may include online communication and online services, whereas the generic ones include economically optimistic and political criticism (Rössler, 2001). In this paper, we adopt the following narrow definition of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our tas"
2021.findings-emnlp.228,2021.naacl-main.174,0,0.0326384,"tion of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically le"
2021.findings-emnlp.228,2020.emnlp-main.491,0,0.0383988,"t of the reframed text is kept, and only a few words are replaced. In contrast, we deal with reframing at the sentence level, and we do not require parallel training pairs or a dictionary to correlate words and frames. In principle, reframing can be seen as a style transfer task (Shardlow, 2014; Shen et al., 2017; Chen et al., 2018). Research on text style transfer focus on the areas of sentiment transfer (e.g., replacing ‘gross’ by ‘awesome’) (Shen et al., 2017) and text simplification (e.g., replacing ‘perched’ by ‘sat’) (Shardlow, 2014). We applied recent style transfer models to our task (Mai et al., 2020; Shen et al., 2020), observing that these models perform very poorly (e.g., generating unreadable text). 3 Approach We now present our approach to sentence-level reframing. We discuss how we tackle the reframing problem as a fill-in-the-blank task, and we propose three training strategies to generate a sentence that is framed as desired and that fits to the surrounding text. Figure 1 illustrates our approach. 3.1 Reframing as a Fill-in-the-Blank Task Input s1 Key Congressional backers ... wanted a flexible spending limit. s2 MASK s3 ... illegal aliens in this country exacts an economic and so"
2021.findings-emnlp.228,naderi-hirst-2017-classifying,0,0.0278332,"his paper, we adopt the following narrow definition of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly m"
2021.findings-emnlp.228,2020.emnlp-main.335,0,0.0377094,"ely. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle and Roberts, 2004). The media employ framing who may prefer to use “undocumented worker” into reorient how audiences think (Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke a discussion of the economic impact of hiring them; the latter may quences regarding crime. raise issues of crime and possible deportation. Such Reframing means to change the perspective of low-level style reframing has been studied in recent an issue. It can be a strategy to communicate with work (Chakrabarty et al., 2021). opposing camps of audiences, and, sometimes, just 2683 Findings of the Association for Computational Linguis"
2021.findings-emnlp.228,2020.emnlp-demos.6,0,0.0253146,"Missing"
barron-cedeno-etal-2010-corpus,clough-etal-2002-building,0,\N,Missing
barron-cedeno-etal-2010-corpus,P02-1020,0,\N,Missing
C10-1127,J96-2004,0,0.0184661,"ime, a trend indicator and the author of a statement. Altogether, 2,075 statements are tagged in this way. As in Figure 3, only information that refers to a statement on revenue (typed in bold face) is annotated. These annotations may be spread across the text. The source documents were manually selected and prepared by our industrial partners, and two of their employees annotated the plain document text. With respect to the statement annotations, a preceding pilot study yielded substantial interannotator agreement, as indicated by the value κ = 0.79 of the conservative measure Cohen’s Kappa (Carletta, 1996). Additionally, we performed a manual correction process for each annotated document to improve consistency. Experimental Setup To find candidate sentences, we implemented a sentence splitter that can handle article elements such as subheadings, URLs, or bracketed sentences. We then constructed sophisticated, but efficient regular expressions for time and money. They do not represent correct language, in general, but model the structure of temporal and monetary entities, and use word lists provided by domain experts on the lowest level.4 For feature computation, we assumed that the closest pai"
C10-1127,C08-1060,0,0.031082,"like us, the authors put much emphasis on retrieval aspects and applied dependency grammar parsing to identify market statements. As a consequence their approach suffers from the limitation to a small number of predefined sentence structures. While we obtain market forecasts by extracting expert statements from the Web, related approaches derive them from past market behavior and quantitative news data. Koppel and Shtrimberg (2004) studied the effect of news on financial markets. Lavrenko et al. (2000) used timeseries analysis and language models to predict stock market prices and, similarly, Lerman et al. (2008) proposed a system for forecasting public opinion based on concurrent modeling of news articles and market history. Another related field is opinion mining in the sense that it relies on the aggregation of individual statements. Glance et al. (2005) inferred marketing intelligence from opinions in online discussions. Liu et al. (2007) examined the effect of Weblogs on box office revenues and combined time-series with sentiment analysis to predict the sales performance of movies. The mentioned approaches are intended to reflect or to predict present developments and, therefore, primarily help f"
C10-1127,P00-1010,0,0.081855,"Missing"
C10-1127,C02-1053,0,0.0847143,"Missing"
C10-1127,W09-1119,0,0.0204774,"ressions, which represent the complex but finite structures of such phrases, we can achieve nearly perfect recall in recognition (see Section 5). We apply named entity recognition (NER) of organizations and markets in this stage, too, so we can relate statements to the appropriate subjects, later on. Note that market names do not follow a unique naming scheme, but we observed that they often involve similar phrase patterns that can be exploited as features. NER is usually done by sequence labeling, and we use heuristic beam search due to our effort to design a highly efficient overall system. Ratinov and Roth (2009) have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time. 3.4 Identify Statements Based on time and money information, sentences that represent a statement Sχ can be identified. Such a sentence gives us valuable hints on which temporal and monetary entity stick together and how to interpret them in relation. Additionally, it serves as evidence for the statement’s correctness (or incorrectness). Every sentence with at least one temporal and one monetary en"
C10-2115,ambati-etal-2010-active,0,0.0315161,"Missing"
C10-2115,N03-1003,0,0.0248306,"Missing"
C10-2115,clough-etal-2002-building,0,0.519814,"Missing"
C12-2125,P10-1014,0,0.0341808,"ngly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are established for rule-based approaches by Chiticariu et al. (2010), we explain the determinants of efficiency for any set of extraction algorithms. To the best of our knowledge, we are the first to address scheduling in information extraction with dynamic programming, which relies on dividing a problem into smaller subproblems and solving recurring subproblems only once (Cormen et al., 2009). In our research we analyze the impact of text types on the efficiency of information extraction pipelines. Existing work on text types in information extraction mainly deals with the filtering of promising documents, such as (Agichtein and Gravano, 2003). Instead, we id"
C12-2125,P05-1045,0,0.0948357,"ormer contains 752 online business news articles with 21,586 sentences, whereas 553 mixed classic newspaper articles with 12,713 sentences refer to the latter. Task. The conjunctive filtering task that we consider emanates from the purpose of the Revenue corpus, namely, we define a text unit to be classified as relevant as a sentence that represents a forecast and that contains a money entity, a time entity, and an organization name. Algorithms. We employed four algorithms that filter text units: regex-based money and time entity recognizers A M and A T , the CRF-based STANFORD NER system AN (Finkel et al., 2005; Faruqui and Padó, 2010) for organization names, and the SVM-based forecast event detector A F from (Wachsmuth et al., 2011) that needs time entities as input. Further algorithms were used only as preprocessors. In all experiments we executed each preprocessing algorithm right before its output was needed. Hence, we simply speak of the algorithm set A1 = {A M , A T , AN , A F } in the following without loss of generality. All algorithms in A1 operate on sentence-level. Application of the Pipeline Viterbi Algorithm. On both corpora, we executed all applicable ( j) ( j) ( j) pipelines Πi for A1"
C12-2125,W06-1673,0,0.0285444,"ploys special index structures and fast extraction algorithms, but it is restricted to simple relations. In contrast, we target at template filling tasks that relate several entities to events (Cunningham, 2006). We approach such tasks with classic pipelines where each algorithm takes on one analysis, e.g. a certain type of entity recognition (Grishman, 1997). The decisions within a pipeline can be viewed as irreversible, which allows to perform filtering. Hence, an algorithm can never make up for false classifications of its predecessors, as in iterative or probabilistic pipeline approaches (Finkel et al., 2006; Hollingshead and Roark, 2007). Accordingly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are e"
C12-2125,P07-1120,0,0.0176629,"tructures and fast extraction algorithms, but it is restricted to simple relations. In contrast, we target at template filling tasks that relate several entities to events (Cunningham, 2006). We approach such tasks with classic pipelines where each algorithm takes on one analysis, e.g. a certain type of entity recognition (Grishman, 1997). The decisions within a pipeline can be viewed as irreversible, which allows to perform filtering. Hence, an algorithm can never make up for false classifications of its predecessors, as in iterative or probabilistic pipeline approaches (Finkel et al., 2006; Hollingshead and Roark, 2007). Accordingly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are established for rule-based appro"
C12-2125,C08-5001,0,0.0440676,"Missing"
C12-2125,W11-1802,0,0.0709323,"Missing"
C12-2125,C10-1127,1,0.322235,"ed to U, which leads to an overall upper bound of O(m3 · t ma x (U)).3 4 The Impact of Text Types on the Efficiency of Pipelines In this section, we first analyze the influence of text types on the optimality of schedules. Then, we reveal that the efficiency of an information extraction pipeline is governed by the distribution of relevant entities, relations, and events in the input texts. In all experiments, we evaluated the following set-up on a 2 GHz Intel Core 2 Duo MacBook with 4 GB memory: Data. We processed the training sets of two German text corpora: the Revenue corpus introduced in (Wachsmuth et al., 2010) and the corpus of the CoNLL’03 shared task (Tjong Kim Sang and De Meulder, 2003).4 The former contains 752 online business news articles with 21,586 sentences, whereas 553 mixed classic newspaper articles with 12,713 sentences refer to the latter. Task. The conjunctive filtering task that we consider emanates from the purpose of the Revenue corpus, namely, we define a text unit to be classified as relevant as a sentence that represents a forecast and that contains a money entity, a time entity, and an organization name. Algorithms. We employed four algorithms that filter text units: regex-bas"
C14-1053,C10-1103,0,\N,Missing
C14-1053,baccianella-etal-2010-sentiwordnet,0,\N,Missing
C14-1053,D08-1020,0,\N,Missing
C14-1053,P12-2041,0,\N,Missing
C14-1053,P04-1035,0,\N,Missing
C14-1053,D09-1155,0,\N,Missing
C14-1053,I11-1071,1,\N,Missing
C14-1053,P05-1015,0,\N,Missing
C14-1053,W02-1011,0,\N,Missing
C14-1053,P13-1160,0,\N,Missing
C14-1053,P10-1114,1,\N,Missing
C14-1053,P13-1119,1,\N,Missing
C14-1053,D13-1170,0,\N,Missing
C14-1053,C12-1113,0,\N,Missing
C14-1091,C08-3010,0,\N,Missing
C14-1091,W10-0404,0,\N,Missing
C14-1091,P05-3009,0,\N,Missing
C14-1091,P06-1032,0,\N,Missing
C14-1091,P13-4024,0,\N,Missing
C14-1190,I05-5002,0,\N,Missing
C14-1190,I05-1011,0,\N,Missing
C14-1190,W04-3219,0,\N,Missing
C14-1190,C10-2017,0,\N,Missing
C14-1190,P02-1020,0,\N,Missing
C14-1190,P09-1094,0,\N,Missing
C14-1190,P01-1008,0,\N,Missing
C14-1190,D08-1021,0,\N,Missing
C14-1190,N03-1024,0,\N,Missing
C14-1190,P09-2063,0,\N,Missing
C14-1190,N03-1003,0,\N,Missing
C14-1190,D11-1108,0,\N,Missing
C14-1190,N06-1058,0,\N,Missing
C14-1190,P05-1074,0,\N,Missing
C14-1190,P08-1116,0,\N,Missing
C14-1190,N13-1092,0,\N,Missing
C14-1190,P11-2096,0,\N,Missing
C16-1158,C16-1324,1,0.460322,"Missing"
C16-1158,W15-0514,0,0.0600879,"Missing"
C16-1158,W98-0303,0,0.625226,"dress a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikli, 2006), partly employing structural features like discourse markers (Burstein et al., 1998). In contrast, Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer"
C16-1158,P12-2041,0,0.0159023,"ustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adheren"
C16-1158,C12-1041,0,0.0473775,"texts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Simil"
C16-1158,C14-1089,0,0.0102423,"-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common f"
C16-1158,P16-2089,0,0.198839,"t1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) analyze argumentative discourse units found with a simple heuristic algorithm. And Ghosh et al. (2016) even derive features from argument mining, although they hardly exploit structure. Either way, all these approaches assign overall essay scores only, leaving unclear to what extent argumentation quality is captured. 3 Mining Argumentative Structure This paper does not aim at new approaches to argument mining. Still, the effectiveness of mining as well as the underlying argumentation model directly affect the analysis of argumentative structure. Therefore, we summarize our mining approach in the following.1 3.1 An Application-Oriented Model of Argumentative Structure We focus on the argumentat"
C16-1158,D15-1255,0,0.0555108,"common patterns in the argumentative structure of persuasive essays statistically. 3. We provide the new state of the art approach to two argumentation-related essay scoring tasks. 2 Related Work Several approaches to argument mining have been introduced, often grounded in argumentation theory: Matching the argumentation schemes of Walton et al. (2008), Mochales and Moens (2011) model each argument in legal cases as a conclusion with a set of premises. Based on (Freeman, 2011), Peldszus and Stede (2015) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus"
C16-1158,W14-2104,0,0.0755636,"Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) analyze argumentative discourse units found with a simple heuristic algorithm. And Ghosh et al. (2016) even derive features from argument mining, although they hardly exploit structure. Either way, all these approaches assign overall essay scores only, leaving unclear to what extent argumentation quality is captured. 3 Mining Argumentative Structure This paper does not aim at new approaches to argument mining. Still, the effectiveness of mining as well as the underlying argumentation model directly affect the analysis of argumentative structure. Therefore, we summarize our mining approach in"
C16-1158,D15-1110,0,0.15759,"earch: 1. We examine the use of argument mining for assessing argumentation quality for the first time. 2. We reveal common patterns in the argumentative structure of persuasive essays statistically. 3. We provide the new state of the art approach to two argumentation-related essay scoring tasks. 2 Related Work Several approaches to argument mining have been introduced, often grounded in argumentation theory: Matching the argumentation schemes of Walton et al. (2008), Mochales and Moens (2011) model each argument in legal cases as a conclusion with a set of premises. Based on (Freeman, 2011), Peldszus and Stede (2015) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teu"
C16-1158,P13-1026,0,0.484215,"d structure. So far, however, the benefit of this structure remains largely unexplored (see Section 2 for details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive"
C16-1158,P14-1144,0,0.516928,"lyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, sophisticated features are engineered to address a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often foc"
C16-1158,P15-1053,0,0.49707,"on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, sophisticated features are engineered to address a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikl"
C16-1158,D10-1023,0,0.392736,"sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentio"
C16-1158,D15-1050,0,0.0919594,"global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, soph"
C16-1158,D13-1170,0,0.00217968,"Missing"
C16-1158,W14-2110,0,0.069122,". The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikli, 2006), partly employing structural features like discourse markers (Burstein et al., 1998). In contrast, Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) a"
C16-1158,C14-1142,0,0.299109,"details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive novel solely structure-oriented features for machine learning (Section 4). Finally, we tackle essay scoring"
C16-1158,D14-1006,0,0.492205,"details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive novel solely structure-oriented features for machine learning (Section 4). Finally, we tackle essay scoring"
C16-1158,D09-1155,0,0.00791671,"15) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an e"
C16-1158,C14-1053,1,0.954964,"ture. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies t"
C16-1158,D15-1072,1,0.814067,"), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze att"
C16-1324,D10-1023,0,\N,Missing
C16-1324,W03-1017,0,\N,Missing
C16-1324,P14-5010,0,\N,Missing
C16-1324,W15-0505,1,\N,Missing
C16-1324,W14-2105,0,\N,Missing
C16-1324,D15-1255,0,\N,Missing
C16-1324,W14-2109,0,\N,Missing
C16-1324,W14-2106,0,\N,Missing
C16-1324,D15-1110,0,\N,Missing
C16-1324,D15-1072,1,\N,Missing
C16-1324,N16-1165,1,\N,Missing
C16-1324,C16-1158,1,\N,Missing
C16-1324,W03-2102,0,\N,Missing
C16-1324,E12-1085,0,\N,Missing
C16-1324,D15-1050,0,\N,Missing
C18-1318,W17-5115,1,0.938912,"acter, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an app"
C18-1318,C16-1324,1,0.861364,"rsuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2"
C18-1318,D17-1141,1,0.860545,"s: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete argu"
C18-1318,P16-2085,0,0.187524,"ose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges th"
C18-1318,N16-1166,0,0.0279473,"essed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16"
C18-1318,P11-1099,0,0.0515776,"as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for th"
C18-1318,D17-1249,0,0.0183513,"synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al."
C18-1318,D15-1255,0,0.0399843,"ses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argu"
C18-1318,D16-1129,0,0.0156505,". (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Simi"
C18-1318,W17-5102,0,0.0434853,"analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-"
C18-1318,E17-1070,0,0.239536,"s logos, ethos, and pathos. In the realm of the area of computational argumentation, rhetorical strategies are particularly relevant for technologies that synthesize argumentative text and that aim to deliver arguments effectively. Existing argument mining research largely focuses on the logical structure of arguments, identifying their units (premises vs. conclusions) and relations (support vs. attack). Recently, a few studies have tackled strategy-related aspects, such as explicit expressions of ethos (Duthie et al., 2016) and the effects of logical and emotional arguments across audiences (Lukin et al., 2017). So far, however, strategies have not been considered in argumentation synthesis, which altogether has not received much attention (see Section 2). In this paper, we study the role of rhetorical strategies when synthesizing argumentation. In particular, we consider monological argumentative texts where an author seeks to persuade target readers of his or her stance towards a given topic, such as news editorials and persuasive essays. Conceptually, we argue that an author synthesizes a text of such genres in three subsequent steps: 1. Selecting content in terms of argumentative discourse units"
C18-1318,D17-1318,0,0.153505,"tative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhet"
C18-1318,naderi-hirst-2017-classifying,0,0.0521996,"systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously."
C18-1318,D15-1110,1,0.865613,"credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) devel"
C18-1318,D10-1023,0,0.0229266,"t et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political"
C18-1318,W15-0507,0,0.220234,"tib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their sys"
C18-1318,D15-1050,0,0.134517,"Missing"
C18-1318,P15-4019,0,0.205315,"from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exi"
C18-1318,W14-2110,0,0.024727,"ional way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four"
C18-1318,P17-1011,0,0.136561,"tegy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesiz"
C18-1318,D14-1006,0,0.455036,"stration of the speaker’s credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise,"
C18-1318,C16-1158,1,0.888935,"ises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on"
C18-1318,D17-1253,1,0.845016,"e. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effect"
C18-1318,Q17-1016,0,0.067982,"e the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16 persuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authori"
C18-1318,W15-0512,0,0.182313,"work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The D"
C18-1318,D17-1164,0,0.0256897,"et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sen"
C18-1318,W00-1408,0,0.740009,"t properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sentiment or similar (Hu et al., 2017; Shen et al., 2017). Our model is meant to provide an abstract framework to be exploited in such approaches. 3 Model We now delineate our model of rhetorical strategies for synthesizing a monological argumentative text following"
D15-1072,C14-1002,0,0.0189951,"bal sentiment: neutral (3 out of 5) Global sentiment: neutral (2 out of 3) Figure 1. Example web reviews with neutral global sentiment from three domains, taken from the corpora described in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. W"
D15-1072,P05-1015,0,0.0904038,"e locations belong to a predefined training set and two to a validation and a test set each. For all locations, the reviews are evenly distributed over the five TripAdvisor overall scores. In accordance with the product corpus, we see score 4–5 as positive global sentiment, 3 as neutral, and 1–2 as negative. In each review, all main clauses together with their subordinate clauses have been classified as being positive, negative, or neutral. Movie Domain Finally, the third corpus (Mao and Lebanon, 2007) compiles 450 Rotten Tomatoes reviews from the Cornell Movie Review Data scale dataset v1.0 (Pang and Lee, 2005) that refer to two authors. We use the 201 reviews of Scott Renshaw for training and the 249 of Dennis Schwartz for testing. The reviews lack punctuation, capitalization, and their overall ratings. We recovered the overall ratings from the original dataset based on the rating scale 0–2, resulting in 178 positive, 139 neutral, and 133 negative reviews. In each review, Mao and Lebanon (2007) classified all sentences to be very positive, positive, neutral, negative, or very negative, which we reduce to three classes. Analysis of the Generality of the Model We now report on experiments on corpora"
D15-1072,D10-1023,0,0.212093,"Missing"
D15-1072,H05-1043,0,0.0561257,"ed in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., rely"
D15-1072,P10-1114,1,0.897834,"f Web Review Argumentation Henning Wachsmuth and Johannes Kiesel and Benno Stein Faculty of Media, Bauhaus-Universität Weimar, Germany {henning.wachsmuth,johannes.kiesel,benno.stein}@uni-weimar.de Abstract across domains, as illustrated in Figure 1 for a product, a hotel, and a movie review. As a consequence, sentiment analysis suffers from domain dependence (Wu et al., 2010), i.e., high effectiveness is often achieved only in the domain an approach has been specifically modeled for. To adapt to other domains, prior knowledge about these domains or about domain-independent features is needed (Prettenhofer and Stein, 2010). This paper considers the question as to whether the overall argumentation of web reviews can be modeled in a general way in order to increase domain independence in sentiment analysis. We observe that people structure web reviews largely sequentially—in contrast to the complex structures of many other argumentative texts. While the reviewed aspects differ between domains, our assumption is that the overall argumentation of a web review is generally represented by a sequence of local sentiments, called the review’s sentiment flow (Mao and Lebanon, 2007). In particular, we hypothesize that, un"
D15-1072,P13-1160,0,0.0358059,"l words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., relying on deep learning (Socher et al., 2013). We do not compete with such an approach, but we use it to then predict global sentiment. Täckström and McDonald (2011) observe that local and global sentiment correlate, aiming for the opposite direction, though. In (Wachsmuth et al., 2014b), we already compute frequent flows of local sentiment, but we nei"
D15-1072,D13-1170,0,0.00780468,"n impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., relying on deep learning (Socher et al., 2013). We do not compete with such an approach, but we use it to then predict global sentiment. Täckström and McDonald (2011) observe that local and global sentiment correlate, aiming for the opposite direction, though. In (Wachsmuth et al., 2014b), we already compute frequent flows of local sentiment, but we neither analyze their generality, nor do we use them for prediction. The idea of modeling sentiment flow was introduced by Mao and Lebanon (2007) who classify local sentiment based on neighboring local sentiment in a review. When inferring global sentiment from a flow, however, the authors mod"
D15-1072,E14-1051,0,0.0568313,"Missing"
D15-1072,R11-1043,0,0.0289288,"neutral (3 out of 5) Global sentiment: neutral (3 out of 5) Global sentiment: neutral (2 out of 3) Figure 1. Example web reviews with neutral global sentiment from three domains, taken from the corpora described in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align asp"
D15-1072,C14-1053,1,0.711501,"ther argumentative texts. While the reviewed aspects differ between domains, our assumption is that the overall argumentation of a web review is generally represented by a sequence of local sentiments, called the review’s sentiment flow (Mao and Lebanon, 2007). In particular, we hypothesize that, under an adequate model, similar sentiment flows express similar global sentiments, also across domains. All reviews in Figure 1, for instance, express neutral global sentiment starting with positive, continuing with negative, and ending with positive local sentiment. Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review’s length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation. The abstract nature of sentiment flow, however, does not directly achieve do"
D15-1072,H05-2017,0,\N,Missing
D15-1072,N10-1122,0,\N,Missing
D17-1141,baccianella-etal-2010-sentiwordnet,0,\N,Missing
D17-1141,I11-1071,1,\N,Missing
D17-1141,W14-2105,0,\N,Missing
D17-1141,D15-1072,1,\N,Missing
D17-1141,D15-1091,0,\N,Missing
D17-1141,C16-1324,1,\N,Missing
D17-1253,C16-1324,1,0.912549,"Missing"
D17-1253,W16-2808,0,0.013364,"tial structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of gran"
D17-1253,P11-1099,0,0.492762,"ssify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres consid"
D17-1253,C14-1089,0,0.0202512,"approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2010) score the organization of persuasive essays based on sequences of sentence and paragraph functions. We introduced the first explicit computational model of overall argumentation in (Wachsmuth et al., 2014a). There, we compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related a"
D17-1253,P15-4004,0,0.0721504,"Missing"
D17-1253,D15-1255,0,0.414783,"the overall argumentation of a monological argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (monological) overall argumenta"
D17-1253,E06-1015,0,0.0575943,"compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related analysis tasks (Wachsmuth and Stein, 2017). However, flows capture only sequential structure, whereas here we also model the hierarchical structure of overall argumentation. To this end, we make use of kernel methods. Kernel methods are a popular approach for learning on structured data, with several applications in natural language processing (Moschitti, 2006b) including argument mining (Rooney et al., 2012). They employ a similarity function defined between any two input objects that are represented in a taskspecific implicit feature space. The evaluation of such a kernel function relies on the common features of the input objects (Cristianini and ShaweTaylor, 2000). The kernel function encodes knowledge of the task in the form of these features. Several kernel functions have been defined for structured data. To assess the impact of sequential argumentation, we refer to the function of Mooney and Bunescu (2006), which computes common subsequences"
D17-1253,W14-2104,0,0.0162687,"3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al.,"
D17-1253,W16-2813,0,0.0836566,"tab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employ"
D17-1253,W02-1011,0,0.0181603,"Missing"
D17-1253,C16-1312,0,0.369186,"l argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (monological) overall argumentation is important to tackle"
D17-1253,D10-1023,0,0.0892123,"uth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2010) score the organization of persuasive essays based on sequences of sentence and paragraph functions. We introduced the first explicit computational model of overall argumentation in (Wachsmuth et al., 2014a). There, we compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related analysis tasks (Wachsmuth and Stein, 2017). However, flows capture only sequential structure, whereas here we also model the hie"
D17-1253,P15-1053,0,0.0391931,"Missing"
D17-1253,D15-1050,0,0.042587,"the mining of argument units and their relations from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, a"
D17-1253,W15-0509,0,0.0225758,"e the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monol"
D17-1253,C14-1142,0,0.389456,"s of other arguments. For the overall argumentation of a monological argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (m"
D17-1253,C16-1158,1,0.858935,"gumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentatio"
D17-1253,E17-1017,1,0.800826,"Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2"
D17-1253,C14-1053,1,0.929586,"s from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an"
D17-1253,walker-etal-2012-corpus,0,0.034321,"processing, argumentation research deals with the mining of argument units and their relations from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do simila"
D18-2011,D17-1253,1,0.926296,"discussion, SEAS (Lowrance et al., 2000), VUE (Baroni et al., 2015), and Dialectic Map (Niu, 2016) provide a combination of automatic argument analysis and visual argument summaries. With similar intentions, Lexical Episode Plots (Gold et al., 2015), ConToVi (El-Assady et al., 2016), NEREx (El-Assady et al., 2017), and Jentner et al. (2017) visualize specific aspects of transcribed discussions. All these works focus on single arguments or the set of arguments within a single debate or text. In contrast, we present a visualization that summarizes arguments from many different texts. Unlike in (Wachsmuth et al., 2017a), where we illustrated structural argumentation patterns in the texts of a corpus, here we target the content of arguments. As the above-mentioned system ConToVi, we visualize the topic space covered by a set of arguments. While ConToVi provides insights into the flow of aspects during the discussion of a controversial topic, our visualization aims to make arguments on specific aspects easily findable. Moreover, we allow arguments to cover a weighted distribution of multiple aspects rather than only a single aspect. Argument Search with args.me As presented in (Wachsmuth et al., 2017b), the"
D18-2011,W17-5106,1,0.829301,"discussion, SEAS (Lowrance et al., 2000), VUE (Baroni et al., 2015), and Dialectic Map (Niu, 2016) provide a combination of automatic argument analysis and visual argument summaries. With similar intentions, Lexical Episode Plots (Gold et al., 2015), ConToVi (El-Assady et al., 2016), NEREx (El-Assady et al., 2017), and Jentner et al. (2017) visualize specific aspects of transcribed discussions. All these works focus on single arguments or the set of arguments within a single debate or text. In contrast, we present a visualization that summarizes arguments from many different texts. Unlike in (Wachsmuth et al., 2017a), where we illustrated structural argumentation patterns in the texts of a corpus, here we target the content of arguments. As the above-mentioned system ConToVi, we visualize the topic space covered by a set of arguments. While ConToVi provides insights into the flow of aspects during the discussion of a controversial topic, our visualization aims to make arguments on specific aspects easily findable. Moreover, we allow arguments to cover a weighted distribution of multiple aspects rather than only a single aspect. Argument Search with args.me As presented in (Wachsmuth et al., 2017b), the"
D19-1290,N16-1165,1,0.8595,"e people debate or collect arguments for or against controversial topics. Some debate portals are dialogical, such as debate.org, allowing two opponents to debate one topic in rounds. Other debate portals such as debatepedia.org are wiki-like where arguments are listed according to their stance on the topic. Debate portals keep a canonical structure of the arguments considered for each topic (usually a conclusion and a premise). The structure and the wide topic coverage offered by debate portals has made them a suitable resource for research on computational argumentation (Cabrio and Villata; Al-Khatib et al., 2016; Wachsmuth et al., 2017a). 3.1 Argument Frames from Debatepedia.org For the given work, we crawled all arguments from debatepedia.org in order to construct a dataset for the evaluation of frame identification. Debatepe# Topics # Frames # Merged Frames # Arguments 465 1 645 1 623 12 326 Table 1: Counts of topics, frames, merged frames, and arguments in the webis-argument-framing-19 dataset. Frames Economics 119 Public Opinion Environment Feasibility Rights Democracy Crime Politics Security Safety 20 40 60 80 100 #Topics Figure 1: The number of topics in which each of the 10 most frequent frame"
D19-1290,P16-1150,0,0.0251517,"smuth et al., 2923 2017a; Levy et al., 2018; Stab et al., 2018) with the goal of retrieving relevant arguments for an input claim. Use cases for argument search include writing and debating support. In comparison to user queries in conventional search that can often be satisfied by one or a few retrieved documents, these use cases require a broader consideration of the retrieved arguments. Hence, the user of an argument search engine will often investigate both stances and multiple frames on a given topic. While several studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017b), how to aggregate arguments into frames is largely unstudied. The relation between arguments and frames was introduced briefly in some works (Boydstun et al., 2013; Gabrielsen et al., 2011). Still, recent research on computational argumentation largely ignores frames, and a model for aggregating arguments into frames is still missing. Naderi (2016) considered a frame to be an argument and classified sentences in parliamentary speeches into one of seven frames. (Reimers et al., 2019) created a dataset of argument pairs that are labeled according to their similarity. B"
D19-1290,E17-1024,0,0.124406,"Missing"
D19-1290,W17-5105,0,0.0274675,"Missing"
D19-1290,P15-2072,0,0.046065,"the research community.1 2 Related Work Research on framing is scattered across different fields such as media, social, and cognitive studies. Entman (1993) was the first to introduce a formal definition of framing as a way to select and make specific aspects of a topic salient. Subsequent research on framing is concentrated on the effect of using frames in news on a specific audience. One of the open questions is whether frames are topic-specific or generic concepts, or both. Vreese (2005) studied framing in news articles and considered frames to be both of the two. Johnson et al. (2017) and Card et al. (2015), on the other hand, defined frames to be independent of the topic and investigated their usage across different topics. 1 Argument framing dataset: https://webis.de/ data/webis-argument-framing-19.html or https://doi.org/10.5281/zenodo.3373355 Recently, framing caught some attention in the NLP community. Different computational models have been developed for modeling frames in natural language text. Tsur et al. (2015) used topic models on statements released by congress members of the two major parties in the US, Republicans and Democrats. The learned topics were then aggregated into clusters"
D19-1290,C14-1141,0,0.05413,"ss their audience. The constellation of pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the a"
D19-1290,C18-1176,0,0.0426699,"ments from natural language text (Al-Khatib et al., 2016). Most approaches uses supervised classifiers to extract the structure of arguments (conclusion and premise) (Stab and Gurevych, 2014). (Lawerence and Reed, 2017) showed that topic models helps identifying the relevance of a premise to a conclusion when they are trained on topically relevant documents. The stance of the mined arguments is classified as pro or con towards a given topic (Somasundaran and Wiebe, 2010; Bar-Haim et al., 2017). The arguments are then used for applications such as argument search (Wachsmuth et al., 2923 2017a; Levy et al., 2018; Stab et al., 2018) with the goal of retrieving relevant arguments for an input claim. Use cases for argument search include writing and debating support. In comparison to user queries in conventional search that can often be satisfied by one or a few retrieved documents, these use cases require a broader consideration of the retrieved arguments. Hence, the user of an argument search engine will often investigate both stances and multiple frames on a given topic. While several studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et a"
D19-1290,D17-1318,0,0.0504683,"Missing"
D19-1290,naderi-hirst-2017-classifying,0,0.147114,"Missing"
D19-1290,P19-1054,0,0.105236,"veral studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017b), how to aggregate arguments into frames is largely unstudied. The relation between arguments and frames was introduced briefly in some works (Boydstun et al., 2013; Gabrielsen et al., 2011). Still, recent research on computational argumentation largely ignores frames, and a model for aggregating arguments into frames is still missing. Naderi (2016) considered a frame to be an argument and classified sentences in parliamentary speeches into one of seven frames. (Reimers et al., 2019) created a dataset of argument pairs that are labeled according to their similarity. Based on the dataset, they introduced the task of argument clustering which aims at classifying an argument pair with the same topic into similar or dissimilar. The main difference to this work is that no explicit aspect are assigned to the arguments during annotation. 3 Data Debate portals are websites where people debate or collect arguments for or against controversial topics. Some debate portals are dialogical, such as debate.org, allowing two opponents to debate one topic in rounds. Other debate portals s"
D19-1290,W10-0214,0,0.102823,"Missing"
D19-1290,N18-5005,0,0.0752923,"Missing"
D19-1290,D14-1006,0,0.127534,"Missing"
D19-1290,P15-1157,0,0.0725048,"Missing"
D19-1290,W17-5106,1,0.936702,"f pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the arguments into frames. In Section 5, we desc"
D19-1290,E17-1105,1,0.941953,"f pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the arguments into frames. In Section 5, we desc"
D19-5009,W16-3638,0,0.0209312,"Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault, 2016). Until recently, the most effective overall approaches rely on neural network architectures such as CNN and RNN (Badjatiya et al., 2017; Pavlopoulos et al., 2017). On the personal attacks corpus, Pavlopoulos et al. (2017) have developed several very effective deep learning models with word embedding features. We employ the best-performing neural model, but we analyze the effect of adding our new approach (i.e., to unravel the abusiveness search space) that simultaneously helps to improve lexicon-based explainability. • We investigate how to unravel the search space of abusive language based o"
D19-5009,I13-1066,0,0.0312922,"he target of attack, i.e., being the direct recipient or a third party. The experimental results show that our search space unraveling slightly improves over state-ofthe-art single-space classifiers with the additional bonus of a dynamic abusiveness lexicon that can help to explain the classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follo"
D19-5009,D17-1117,0,0.138631,"ting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault, 2016). Until recently, the most effective overall approaches rely on neural network architectures such as CNN and RNN (Badjatiya et al., 2017; Pavlopoulos et al., 2017). On the personal attacks corpus, Pavlopoulos et al. (2017) have developed several very effective deep learning models with word embedding features. We employ the best-performing neural model, but we analyze the effect of adding our new approach (i.e., to unravel the abusiveness search space) that simultaneously helps to improve lexicon-based explainability. • We investigate how to unravel the search space of abusive language based on the underlying offending way. • We develop computational approach that performs the unraveling in practice, and we evaluate it for the classification of Wikipedi"
D19-5009,D14-1162,0,0.0821523,"dard approaches training abusiveness classifiers on all examples at once, we propose to apply a three-stage approach. 5.1 To represent the state of the art, we employ the best-performing model on the personal attack corpus proposed by Pavlopoulos et al. (2017): an RNN model where the basic cell is a GRU. An embedding layer transforms an input word sequence into a word embedding sequence. Then, the model learns a hidden state from the word embeddings. The hidden state is employed to predict the probability of ‘not-attack’ using a linear regression layer. We use 300-dimensional word embeddings (Pennington et al., 2014) pre-trained on the Common Crawl with 840 billion tokens and a vocabulary size of 2.2 million. Out-of-vocabulary words are mapped to one random vector. We use Glorot (Glorot and Bengio, 2010) to initialize the model, with mean-square error as loss function, Adam for optimization (Kingma and Ba, 2014), a learning rate of 0.001, and a batch size of 128. The initial abusive lexicon used for splitting the search space is the complete set of words in the base lexicon of Wiegand et al. (2018) containing 1650 negative polar expressions. This lexicon performed better in our pilot experiments compared"
D19-5009,W17-1101,0,0.0140578,"ossibly in obfuscated form (e.g., “a$$h0le”), or abusiveness can also happen implicitly via sarcasm (e.g., “go back to school, whatever you learned didn’t stick”) or via new racist or abusive codes (e.g., on the platform 4chan, “Google” is used as a slur for black people, “skittle” for Arabs, and “butterfly” for gays).1 Some recent studies have pointed to different types and to the importance of separating them, especially (Waseem et al., 2017). However, the distinction between the different offending dimensions has hardly been investigated for the development of abusive language classifiers (Schmidt and Wiegand, 2017). Accordingly, existing approaches consider the language of all abusive texts irrespective of their offending dimensions as one single search space. They simply train one machine learning model with different linguistic features on this space in order to classify unseen text as being abusive or not. Due to the diversity of language in offending dimensions, we expect such models to often result in limited effectiveness in practice. The reason is that, when learning to detect abusive texts following one way, for instance, the inclusion of training texts following other ways induces noise that di"
D19-5009,W12-2103,0,0.05317,"ddition, the corpus includes manual labels for the target of attack, i.e., being the direct recipient or a third party. The experimental results show that our search space unraveling slightly improves over state-ofthe-art single-space classifiers with the additional bonus of a dynamic abusiveness lexicon that can help to explain the classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offe"
D19-5009,N16-2013,0,0.0158751,"classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault,"
D19-5009,W17-3012,0,0.0215798,"nformatik .uni-halle.de Abstract other person, entity, or group (other recipient). On the other hand, abusive words and phrases may be used explicitly (e.g., “asshole!”), possibly in obfuscated form (e.g., “a$$h0le”), or abusiveness can also happen implicitly via sarcasm (e.g., “go back to school, whatever you learned didn’t stick”) or via new racist or abusive codes (e.g., on the platform 4chan, “Google” is used as a slur for black people, “skittle” for Arabs, and “butterfly” for gays).1 Some recent studies have pointed to different types and to the importance of separating them, especially (Waseem et al., 2017). However, the distinction between the different offending dimensions has hardly been investigated for the development of abusive language classifiers (Schmidt and Wiegand, 2017). Accordingly, existing approaches consider the language of all abusive texts irrespective of their offending dimensions as one single search space. They simply train one machine learning model with different linguistic features on this space in order to classify unseen text as being abusive or not. Due to the diversity of language in offending dimensions, we expect such models to often result in limited effectiveness"
D19-5009,N18-1095,0,0.0509307,"th the predictions of the developed classifier. The first lexicon contains 1650 words and expressions in which 551 of them are abusive, while the second contains 8478 words and expressions with 2989 abusive ones. The results of using the lexicon for detecting the abusive language in micro-posts demonstrate high effectiveness, particularly in cross-domain settings. Data In this section, we detail the data that we employ for the implementation and evaluation of our approach. Specifically, we describe the Wikipedia personal attack corpus (Wulczyn et al., 2017) and the abusive language lexicon of Wiegand et al. (2018). 3.1 Train Wikipedia Personal Attack Corpus Wikipedia is one of the online platforms suffering from abusive language, especially from personal attacks (Shachaf and Hara, 2010). In particular, each Wikipedia article is associated to a so called talk page, where users are solicited to write comments in order to discuss and improve the quality of the article’s content. While the large majority of comments is valuable, some users attack others with texts comprising hate speech and harassment, among others. Our analysis and evaluation are based on the personal attack corpus (Wulczyn et al., 2017)"
D19-5009,H05-1044,0,0.0621967,"Missing"
E12-1058,D07-1019,0,\N,Missing
E12-1058,D07-1090,0,\N,Missing
E17-1017,N16-1165,1,0.380564,"sive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated"
E17-1017,W16-2808,0,0.057931,") Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment sy"
E17-1017,P16-2085,1,0.841124,"Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale, 2007). As a result, diverse quality dimensio"
E17-1017,W15-0514,0,0.0600702,"Missing"
E17-1017,W98-0303,0,0.311573,"nal appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argument"
E17-1017,P12-2041,0,0.119946,"Missing"
E17-1017,P11-1099,1,0.393064,"Missing"
E17-1017,C14-1089,1,0.513151,"tation Quality Assessment in Natural Language Henning Wachsmuth Bauhaus-Universität Weimar Weimar, Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audien"
E17-1017,P16-2089,0,0.0173404,"al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus or"
E17-1017,P16-1150,0,0.41004,"right side of Figure 1 show where the approaches surveyed in Section 2.2 are positioned in the taxonomy. Some dimensions have been tackled multiple times (e.g., clarity), others not at all (e.g., credibility). The taxonomy indicates what sub-dimensions will affect the same high-level dimension. 4 The Dagstuhl-15512 ArgQuality Corpus Finally, we present our new annotated Dagstuhl15512 ArgQuality Corpus for studying argumentation quality based on the developed taxonomy, and we report on a first corpus analysis.3 4.1 Data and Annotation Process Our corpus is based on the UKPConvArgRank dataset (Habernal and Gurevych, 2016), which contains rankings of 25 to 35 textual debate portal arguments for two stances on 16 issues, such as evolution vs. creation and ban plastic water bottles. All ranks were derived from crowdsourced convincingness labels. For every issue/stance pair, we took the five top-ranked texts and chose five further via stratified sampling. Thereby, we covered both high-quality arguments and different levels of lower quality. Two example texts follow below in Figure 2. Before annotating the 320 chosen texts, we carried out a full annotation study with seven authors of this paper on 20 argumentative"
E17-1017,W14-2104,0,0.0646839,". (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy d"
E17-1017,P11-1032,0,0.00976501,"ween the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the roles of practical approaches. It does not require a particular argumentation model, but it rests on the notion of the granularity levels from Section 1. 3.1 Overview of the Theory-based Taxonomy Our ob"
E17-1017,D15-1110,0,0.0252984,"riven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on death penalty. This example reveals three central challeng"
E17-1017,P13-1026,0,0.362263,"Missing"
E17-1017,P14-1144,0,0.0803542,"Missing"
E17-1017,P15-1053,0,0.125852,"Missing"
E17-1017,D10-1023,0,0.426521,"Missing"
E17-1017,D08-1020,0,0.0224306,"brio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of"
E17-1017,W15-0603,0,0.0749477,"ncy, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or rebuttal) turn out to be most effective. Organization is also analyzed by Rahimi et al. (2015) on the same dataset used for the evidence 179 Aspect Quality Dimension Granularity Text Genres Sources Evidence Level of support Sufficiency Argumentation Argument unit Argument Student essays Wikipedia articles Student essays Rahimi et al. (2014) Braunstain et al. (2016) Stab and Gurevych (2017) Rhetoric Argument strength Evaluability Global coherence Organization Persuasiveness Prompt adherence Thesis clarity Winning side Argumentation Argumentation Argumentation Argumentation Argument Argumentation Argumentation Debate Student essays Law comments Student essays Student essays Forum discuss"
E17-1017,D15-1050,0,0.330444,"s under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on de"
E17-1017,W14-2110,0,0.0505174,"al et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the role"
E17-1017,D14-1006,0,0.0178285,"essment. 1 benno.stein@uni-weimar.de Introduction What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the f"
E17-1017,W16-2813,0,0.364991,"Missing"
E17-1017,E17-1092,0,0.242722,"al models and argument-oriented features, they rank sentencelevel argument units according to the level of support they provide for an answer. Unlike classical essay scoring, Rahimi et al. (2014) score an essay’s evidence, a quality dimension of argumentation: it captures how sufficiently the given details support the essay’s thesis. On the dataset of Correnti et al. (2013) with 1569 student essays and scores from 1 to 4, they find that the concentration and specificity of words related to the essay prompt (i.e., the statement defining the discussed issue) impacts scoring accuracy. Similarly, Stab and Gurevych (2017) introduce an essay corpus with 1029 argument-level annotations of sufficiency, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or"
E17-1017,W15-4631,0,0.0408966,"ltužic´ and Šnajder (2015) Relevance Reasonableness le Sufficiency Local sufficiency Prominence ia Evidence Cabrio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also,"
E17-1017,D15-1072,1,0.846424,"Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale,"
E17-1017,C16-1158,1,0.644819,"Missing"
E17-1017,E17-1105,1,0.83682,"ion What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if em"
E17-1017,P16-2032,0,0.105377,"Missing"
E17-1017,N16-1017,0,0.0362025,"Missing"
E17-1105,W14-2109,0,0.0390945,"., 2009). To assess argument relevance objectively, we adapt 1118 a core retrieval technique, recursive link analysis. Due to its wide use, we build upon Google’s original PageRank algorithm (Page et al., 1999), but alternatives such as (Kleinberg, 1999) would also apply. PageRank is sensitive to certain manipulations, such as link farms (Croft et al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for f"
E17-1105,N16-1165,1,0.769121,"people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a retrieval perspective how to assess argument relevance objectively, i.e., without relying on explicit human judgments. Following argumentation theory, we see relevance as a dialectical quality that depends on how beneficial all participants of a discussion deem the use of an argume"
E17-1105,C16-1324,1,0.896205,"Missing"
E17-1105,baccianella-etal-2010-sentiwordnet,0,0.00667877,"e. 2. Frequency. An argument’s relevance corresponds to the frequency of its premises in the graph. This baseline captures popularity, as proposed in related work (see Section 2). 3. Similarity. An argument’s relevance corresponds to the similarity of its premises to its conclusion. We use the Jaccard similarity between all words in the premises and the conclusion. This basic content-oriented baseline quantifies the support of premises. 4. Sentiment. An argument’s relevance corresponds to the positivity of its premises. Here, we sum up the positive values of all premise words in SentiWordNet (Baccianella et al., 2010) and substract all negatives. Also this baseline quantifies the support of premises. 5. Most premises. An argument’s relevance corresponds to its number of premises. This simple baseline captures the amount of support. 6. Random. The relevance is decided randomly. This baseline helps interpreting the results. Experiment For all 32 conclusions of our benchmark rankings, we assessed the relevance of every associated argument with all six approaches—in case of 1.–4. once for each premise aggregation method. For all approaches, we then compared the resulting ranks with the respective benchmark ran"
E17-1105,W15-0511,0,0.0542093,"raph with three potentially relevant arguments for a queried stance. Figure 2: Example for the reuse of an argument’s conclusion as a premise in two other arguments. Figure 1 sketches an argument graph. Given a user query with a stance on a controversial topic, as shown, each argument whose (maybe implicit) conclusion c matches the stance is potentially relevant. Stance classification is outside the scope of this paper. We assess the relevance of arguments with conclusion c. The reuse of such conclusions in other arguments is exemplified in Figure 2. sion, e.g., its opposite can be generated (Bilu et al., 2015) to balance support and attack somehow. In general, the usage of conclusions as premises favors a monotonous assessment (the more the better), which we implement in Section 4. Note that we allow circles in the graph. This might look unwanted as it enables circular reasoning. However, not all arguments use the same inference rule (say, modus ponens). Hence, it is reasonable that they, directly or indirectly, refer to each other. Altogether, our model defines a framework for assessing argument relevance. It is instantiated by concrete mining and graph processing algorithms. An analysis of argume"
E17-1105,W15-0514,0,0.0721614,"Missing"
E17-1105,P12-2041,0,0.401631,"exist (see Section 2). However, they hardly account for the problem that argument quality (and relevance in particular) is often perceived subjectively. Whether a3 , e.g., is more relevant than a1 or less depends on personal judgment: Example a3 . “The death penalty doesn’t deter people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a ret"
E17-1105,P11-1099,0,0.119233,"ts found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014),"
E17-1105,P16-1150,0,0.21223,"(Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) where classical retrieval and argument-related features serve to rank argument units by the level of support they provide in community question answering. More objectively, Boltuži´c and Šnajder (2015) find popular arguments in online debates. However, popularity alone is often not correlated with merit (Govier, 2010). We additionally analyze depend"
E17-1105,C14-1141,0,0.0986088,"ue facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different argume"
E17-1105,W14-2105,0,0.0497307,"l-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support o"
E17-1105,C10-1099,0,0.0306348,"t includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khat"
E17-1105,W15-0513,0,0.0377796,". Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support or attack my stance?” The mo"
E17-1105,P15-1053,0,0.200648,"for drawing the conclusion (Johnson and Blair, 2006). Here, we are interested in an argument’s global relevance, which refers to the benefit of the argument in a discussion (Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) where classical retrieval and argument-related features serve to rank argument units by the level of support they provide in community question answering. More objectively"
E17-1105,W15-0507,0,0.0177859,"worthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph"
E17-1105,D15-1050,0,0.30256,"g., is more relevant than a1 or less depends on personal judgment: Example a3 . “The death penalty doesn’t deter people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a retrieval perspective how to assess argument relevance objectively, i.e., without relying on explicit human judgments. Following argumentation theory, we see relevance"
E17-1105,C14-1142,0,0.0620953,"ce (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support or attack my stance?” The model relies on three principles that aim at the separation of concerns: I. Freedom of Inference. No inference from argument premises to conclusions is challenged. II. Freedom of Mining. No restrictions are made for how to mine and relate argument units. III. Freedom of Assessment. No graph processing method is presupposed t"
E17-1105,W14-4918,0,0.0184974,"al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from"
E17-1105,D15-1072,1,0.754152,"et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-"
E17-1105,E17-1017,1,0.917789,"lly help accepting or rejecting its conclusion. Such relevance is one prerequisite of a cogent argument, along with the accepability of the premises and their sufficiency for drawing the conclusion (Johnson and Blair, 2006). Here, we are interested in an argument’s global relevance, which refers to the benefit of the argument in a discussion (Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) wh"
E17-1105,walker-etal-2012-corpus,0,0.0490101,"ch as link farms (Croft et al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a cr"
E17-3004,P14-5016,0,\N,Missing
E17-3004,C16-1324,1,\N,Missing
I13-1061,C12-1004,0,0.035075,"Missing"
I13-1061,N10-1004,0,0.0229215,"ffective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning language like McClosky et al. (2010). To our knowledge, we are the first to apply it for predicting extraction efficiency. bution of relevant information. We argue that such kinds of uncertainty and lack of a-priori knowledge cannot be tackled offline, but they require to learn and to adapt to the characteristics of input texts to avoid a noticeable efficiency loss. 1.1 Contributions and Outline In this paper, we analyze to what extent the heterogeneity of natural language texts in the distribution of relevant information affects the efficiency of an information extraction pipeline. For a high heterogeneity, we propose an adapta"
I13-1061,W06-1651,0,0.0169375,"013. sists in relating a number of entities to events of predefined types (Cunningham, 2006). Recent research, e.g. (Jean-Louis et al., 2011), and major evaluation tracks, e.g. (Kim et al., 2011), show the ongoing importance of template filling. We consider extraction pipelines that perform filtering, which have a long tradition (Cowie and Lehnert, 1996). Sarawagi (2008) sees the efficient filtering of relevant portions of input texts as a main challenge. In the pipelines we focus on, each algorithm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning langu"
I13-1061,E09-1093,0,0.235869,"Missing"
I13-1061,D11-1142,0,0.012657,"hm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning language like McClosky et al. (2010). To our knowledge, we are the first to apply it for predicting extraction efficiency. bution of relevant information. We argue that such kinds of uncertainty and lack of a-priori knowledge cannot be tackled offline, but they require to learn and to adapt to the characteristics of input texts to avoid a noticeable efficiency loss. 1.1 Contributions and Outline In this paper, we analyze to what extent the heterogeneity of natural language texts in the distribution of r"
I13-1061,P05-1045,0,0.0634701,"Missing"
I13-1061,C10-1127,1,0.835833,"cally valid. Thereby, we limit the online adaptation algorithm to a certain degree in learning linguistic features from the texts, but we gain that we can measure the benefit of online adaptation as a function of the averaged deviation. i=1 We compute exact values σ(C|D) in Section 5 to measure the impact of heterogeneity. In general, the averaged deviation can also be estimated on a sample of texts. For illustration, Table 1 lists the deviations for the three most common named entity types in the German part of the CoNLL’03 corpus (Tjong Kim Sang and De Meulder, 2003), in the Revenue corpus (Wachsmuth et al., 2010), in a sample of the German Wikipedia (the first 10,000 articles according to internal page ID), and in the LFA-11 smartphone corpus, which is a web crawl of blog posts (Wachsmuth and Bujna, 2011). Here, we recognized entities using Stanford NER (Finkel et al., 2005; Faruqui and Pad´o, 2010). Different from other sampling-based efficiency estimations, cf. (Wang et al., 2011), the averaged deviation does not measure the typical characteristics of input texts, but it quantifies how much these characteristics vary. By that, it helps pipeline designers to decide whether an online adaptation of pip"
I13-1061,I11-1071,1,0.845828,"on as a function of the averaged deviation. i=1 We compute exact values σ(C|D) in Section 5 to measure the impact of heterogeneity. In general, the averaged deviation can also be estimated on a sample of texts. For illustration, Table 1 lists the deviations for the three most common named entity types in the German part of the CoNLL’03 corpus (Tjong Kim Sang and De Meulder, 2003), in the Revenue corpus (Wachsmuth et al., 2010), in a sample of the German Wikipedia (the first 10,000 articles according to internal page ID), and in the LFA-11 smartphone corpus, which is a web crawl of blog posts (Wachsmuth and Bujna, 2011). Here, we recognized entities using Stanford NER (Finkel et al., 2005; Faruqui and Pad´o, 2010). Different from other sampling-based efficiency estimations, cf. (Wang et al., 2011), the averaged deviation does not measure the typical characteristics of input texts, but it quantifies how much these characteristics vary. By that, it helps pipeline designers to decide whether an online adaptation of pipeline schedules is needed to ensure efficient ex5 Evaluation We now present controlled experiments with the online adaptation algorithm on text corpora of different heterogeneity. The goal is to s"
I13-1061,I11-1081,0,0.0641743,"Missing"
I13-1061,C12-2125,1,0.925541,"traction can always be approached as a filtering task as discussed in detail in (Wachsmuth et al., 2013b): By filling a template slot, each algorithm in A implicitly classifies certain units of an input text as relevant. Only these units need to be filtered for the next algorithm in π. As a result, a smart schedule π will often significantly improve the overall extraction efficiency. If the input requirements of all algorithms in A are met within π, the effectiveness of Π (in terms of both precision and recall) will be maintained, since the output of Π exactly lies in the filtered text units (Wachsmuth and Stein, 2012).1 When given a big data filtering task, the designer of a pipeline faces two challenges: (1) How to determine the most efficient schedule for a set of extraction algorithms and a collection or a stream of texts? (2) How to maintain efficiency under heterogeneous text characteristics? With regard to the former challenge we resort to existing research (cf. Section 2). The latter becomes an issue where input texts are not fully known or come from different sources as in the web. Moreover, streams of texts can undergo substantial changes in the distriFrom an efficiency viewpoint, information extr"
I13-1061,W11-1801,0,0.0347816,"between two entity types, such as &lt;ORG> was founded in &lt;TIME>. E.g., the sentence “Google was established by two Stanford students.” needs not to be filtered for relation extraction, as it contains no time entity. The schedule of the two implied entity recognition steps will affect the extraction efficiency. 534 International Joint Conference on Natural Language Processing, pages 534–542, Nagoya, Japan, 14-18 October 2013. sists in relating a number of entities to events of predefined types (Cunningham, 2006). Recent research, e.g. (Jean-Louis et al., 2011), and major evaluation tracks, e.g. (Kim et al., 2011), show the ongoing importance of template filling. We consider extraction pipelines that perform filtering, which have a long tradition (Cowie and Lehnert, 1996). Sarawagi (2008) sees the efficient filtering of relevant portions of input texts as a main challenge. In the pipelines we focus on, each algorithm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering"
I13-1061,W03-0419,0,\N,Missing
K18-1044,D17-1141,1,0.855103,"shape the opinion of the masses. Similarly, they can increase or decrease the gap between readers with opposing beliefs (van Dijk, 1995). As such, news editorials represent an important resource for research on argument mining (Mochales and Moens, 2011) and debating technologies (Rinott et al., 2015). On the other hand, a single news editorial rarely changes the stance of a reader completely. Moreover, many editorials do not put an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our cor"
K18-1044,C16-1324,1,0.876787,"Missing"
K18-1044,W09-3723,0,0.519708,"tation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies."
K18-1044,bal-saint-dizier-2010-towards,0,0.463693,"isticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually"
K18-1044,P12-2041,0,0.0331381,"hsmuth et al. (2017b). The authors developed a taxonomy with one main aspect each for logical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in Section 3, the dimension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth e"
K18-1044,N16-1166,0,0.0615486,"Missing"
K18-1044,P16-1150,0,0.429909,"laims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices that aim at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree, includin"
K18-1044,W17-5102,0,0.108097,"s into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually connect the patterns to persuasiveness. For blog posts and forum discussions respectively, previous work has annotated persuasive acts (Anand et al., 2011) and the use of Aristotle’s rhetorical means (Hidey et al., 2017). Still, this would not allow distinguishing effective from 455 ineffective strategies. We fill this gap by presenting the first editorial corpus with persuasion-related annotations of argumentation quality. To obtain a larger corpus size, we rely on the editorials from Sandhaus (2008) rather than those from Al-Khatib et al. (2016). Potash et al. (2017) observe bias in existing corpora towards higher quality for longer arguments. To prevent such bias, we consider only editorials from a narrow length range. Research on argumentation quality has recently been surveyed by Wachsmuth et al. (2017b)"
K18-1044,W15-0505,1,0.875024,"bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually connect the patterns"
K18-1044,C14-1141,0,0.0188643,"three liberals and three conservatives. The annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controver"
K18-1044,D17-1142,0,0.052097,"Missing"
K18-1044,Q13-1028,0,0.0300506,"ical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in Section 3, the dimension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and"
K18-1044,E17-1070,0,0.211526,"not common for political argumentation. Instead, we tackle subjectiveness by letting people with both stances on a discussed issue annotate quality. Cano-Basave and He (2016) point out that persuasive argumentation is about both changing and reinforcing stance — a view that we follow. The authors study the impact of persuasive language of political debates based on poll changes. Such a direct effect on different audiences is not accessible for most argumentative texts, including editorials. Persuasiveness does not only depend on a text itself, but also on the reader’s beliefs and personality. Lukin et al. (2017) find that different types of arguments (rational vs. emotional) are effective depending on the “Big Five” personality traits (Goldberg, 1990). We captured our annotators’ personality traits, too. However, we primarily focus on nine political profiles from left to right (Doherty et al., 2017) in order to represent prior stance. We are not aware of any previous work in computational argumentation considering such profiles so far. 3 A New Model of Argumentation Quality for News Editorials We propose a model that quantifies the argumentation quality of an editorial at the discourse level. Two dim"
K18-1044,D15-1110,0,0.0236464,"annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategie"
K18-1044,P15-1053,0,0.519424,"g emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices that aim at changing or affecting the behavior of others or at strengthening the existing be"
K18-1044,I17-1060,0,0.189454,"fectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and Gurevych (2016) compare the convincingness of arguments with only one stance on a given issue, which circumvents the problem, but does not help for actual persuasion. While Tan et al. (2016) analyze how people are persuaded by others with opposing stance, they restrict their view to good-faith discussions (where people are open to be persuaded) — a s"
K18-1044,I17-1035,0,0.165057,"-level annotations are missing that actually connect the patterns to persuasiveness. For blog posts and forum discussions respectively, previous work has annotated persuasive acts (Anand et al., 2011) and the use of Aristotle’s rhetorical means (Hidey et al., 2017). Still, this would not allow distinguishing effective from 455 ineffective strategies. We fill this gap by presenting the first editorial corpus with persuasion-related annotations of argumentation quality. To obtain a larger corpus size, we rely on the editorials from Sandhaus (2008) rather than those from Al-Khatib et al. (2016). Potash et al. (2017) observe bias in existing corpora towards higher quality for longer arguments. To prevent such bias, we consider only editorials from a narrow length range. Research on argumentation quality has recently been surveyed by Wachsmuth et al. (2017b). The authors developed a taxonomy with one main aspect each for logical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in S"
K18-1044,D15-1050,0,0.0195949,"article that argues in favor of a particular stance on a usually timely controversial issue, such as the relocation of the US embassy in Israel to Jerusalem. Usually, it reflects the political ideology of the newspaper, aiming to persuade readers of the respective stance. Such editorials are said to have the power to shape the opinion of the masses. Similarly, they can increase or decrease the gap between readers with opposing beliefs (van Dijk, 1995). As such, news editorials represent an important resource for research on argument mining (Mochales and Moens, 2011) and debating technologies (Rinott et al., 2015). On the other hand, a single news editorial rarely changes the stance of a reader completely. Moreover, many editorials do not put an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation"
K18-1044,C14-1142,0,0.0228244,"ts of 1000 news editorials, each annotated by three liberals and three conservatives. The annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their re"
K18-1044,C16-1158,1,0.807989,"mension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and Gurevych (2016) compare the convincingness of arguments with only one stance on a given issue, which circumvents the problem, but does not help for actual persuasion. While Tan et al. (2016) analyze how people are persuaded by others with opposing stance, they restrict their view to good-faith discuss"
K18-1044,P17-2039,1,0.933183,"an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices tha"
K18-1044,E17-1017,1,0.885902,"an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices tha"
K18-1044,W03-1017,0,0.614233,"In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive"
N16-1165,E12-1049,0,\N,Missing
N16-1165,D13-1191,0,\N,Missing
N16-1165,E12-1062,0,\N,Missing
N16-1165,P12-2041,0,\N,Missing
N16-1165,P11-1055,0,\N,Missing
N16-1165,D15-1255,0,\N,Missing
N16-1165,W14-2109,0,\N,Missing
N16-1165,C14-1141,0,\N,Missing
N16-1165,reschke-etal-2014-event,0,\N,Missing
N16-1165,C14-1142,0,\N,Missing
N16-1165,D15-1072,1,\N,Missing
N16-1165,W14-2112,0,\N,Missing
N18-1036,jain-etal-2014-corpus,0,\N,Missing
N18-1036,P16-1150,1,\N,Missing
N18-1036,L18-1526,1,\N,Missing
P10-1114,P05-1001,0,0.0399114,"fier fS with training documents written in S and by applying fS to unlabeled documents written in T . For the application of fS under language T different approaches are current practice: machine translation of unlabeled documents from T to S, dictionary-based translation of unlabeled documents from T to S, or language-independent concept modeling by means of comparable corpora. The mentioned approaches have their pros and cons, some of which are discussed below. Here we propose a different approach to crosslanguage text classification which adopts ideas from the field of multi-task learning (Ando and Zhang, 2005a). Our approach builds upon structural correspondence learning, SCL, a recently proposed theory for domain adaptation in the field of natural language processing (Blitzer et al., 2006). Similar to SCL, our approach induces correspondences among the words from both languages by means of a small number of so-called pivots. In our context a pivot is a pair of words, {wS , wT }, from the source language S and the target language T , which possess a similar semantics. Testing the occurrence of wS or wT in a set of unlabeled documents from S and T yields two equivalence classes across these languag"
P10-1114,W06-1615,0,0.835589,"ce: machine translation of unlabeled documents from T to S, dictionary-based translation of unlabeled documents from T to S, or language-independent concept modeling by means of comparable corpora. The mentioned approaches have their pros and cons, some of which are discussed below. Here we propose a different approach to crosslanguage text classification which adopts ideas from the field of multi-task learning (Ando and Zhang, 2005a). Our approach builds upon structural correspondence learning, SCL, a recently proposed theory for domain adaptation in the field of natural language processing (Blitzer et al., 2006). Similar to SCL, our approach induces correspondences among the words from both languages by means of a small number of so-called pivots. In our context a pivot is a pair of words, {wS , wT }, from the source language S and the target language T , which possess a similar semantics. Testing the occurrence of wS or wT in a set of unlabeled documents from S and T yields two equivalence classes across these languages: one class contains the documents where either wS or wT occur, the other class contains the documents where neither wS nor wT occur. Ideally, a pivot splits the set of unlabeled docu"
P10-1114,P07-1056,0,0.163472,"Missing"
P10-1114,P07-1033,0,0.0228537,"Missing"
P10-1114,N09-1068,0,0.0204648,"(2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2006; Jiang and Zhai, 2007). The latter setting is referred to as unsupervised domain adaptation. 1119 Note that, cross-language text classification can be cast as an unsupervised domain adaptation problem by considering each language as a separate domain. Blitzer et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear"
P10-1114,W05-0802,0,0.00897042,"s between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-s"
P10-1114,P06-1070,0,0.0125954,"languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fort"
P10-1114,C04-1071,0,0.0613652,"target-languagecategory-combination a linear classifier is learned on the training set and tested on the test set. The resulting accuracy scores are referred to as upper bound; it informs us about the expected performance on the target task if training data in the target language is available. We chose a machine translation baseline to compare CL-SCL to another cross-language method. Statistical machine translation technology offers a straightforward solution to the problem of cross-language text classification and has been used in a number of cross-language sentiment classification studies (Hiroshi et al., 2004; Bautin et al., 2008; Wan, 2009). Our baseline CL-MT works as follows: (1) learn a linear classifier on the training data, and (2) translate the test documents into the source language,6 (3) predict 6 Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. Th"
P10-1114,oard-1998-comparative,0,0.0332551,"3 states the terminology for cross-language text classification. Section 4 describes our main contribution, a new approach to cross-language text classification based on structural correspondence learning. Section 5 presents experimental results in the context of cross-language sentiment classification. 2 Related Work Cross-Language Text Classification Bel et al. (2003) belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these"
P10-1114,W02-1011,0,0.012607,"Missing"
P10-1114,P09-1054,0,0.0131345,"s-validation, testing for λ all values 10−i , i ∈ [0; 6]. For the pivot prediction task, λ is set to the small value of 10−5 , in order to favor model accuracy over generalizability. The computational bottleneck of CL-SCL is the SVD of the dense parameter matrix W. Here we follow Blitzer et al. (2006) and set the negative values in W to zero, which yields a sparse representation. For the SVD computation the Lanczos algorithm provided by SVDLIBC is employed.4 We investigated an alternative approach to obtain a sparse W by directly enforcing sparse pivot predictors wl through L1-regularization (Tsuruoka et al., 2009), but didn’t pursue this strategy due to unstable results. Since SGD is sensitive to feature scaling the projection θx is post-processed as follows: (1) Each feature of the cross-lingual representation is standardized to zero mean and unit variance, where mean and variance are estimated on DS ∪ Du . (2) The cross-lingual document representations P are scaled by a constant α such that |DS |−1 x∈DS kαθxk = 1. We use Google Translate as word translation oracle, which returns a single translation for each query word.5 Though such a context free translation is suboptimum we do not sanitize the retu"
P10-1114,P09-1027,0,0.516589,"cation. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al. (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2"
P13-1119,C10-2115,1,\N,Missing
P13-1119,P02-1020,0,\N,Missing
P17-2039,W16-2808,1,0.847172,"cy, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. B"
P17-2039,P12-2041,0,0.0279224,"ying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors al"
P17-2039,D16-1129,1,0.101498,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,P16-1150,1,0.106561,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,N13-1132,0,0.0340534,"y dimensions with scores from 1 to 3 (or choose “cannot judge”). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. 4.2 Agreement of the Crowd with Experts First, we checked to what extent lay annotators and experts agree in terms of Krippendorff’s α. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of Wachsmuth et al. (2017a). On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. 253 (a) Crowd / Expert Quality Dimension Cog LA LR LS Eff Cre Emo Cla App Arr Rea GA GR GS OQ Cogency Local acceptability Local relevance Local sufficiency Effectiveness Credibility Emotional appeal Clarity Appropriateness Arrangement Reasonableness Global acceptability Global relevance Global sufficiency Overall quality (b) Crowd 1 / 2 / Expert (c) Crowd 1 / Expert (d) Crowd 2 / Expert Mean MACE Mean MACE Mean MACE Mean MACE .27 .49 .42 .18 .13 .41 .45 .42 .54 .53 .33 .54 .44 –.17 .43 .38 .35 .39 .31 .31 .27 .23 .28 .26 .30 .40 .40 .31 .19 .43 .24 .37 .33 .21"
P17-2039,P15-1053,0,0.080264,"dged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that r"
P17-2039,D15-1050,0,0.125974,"ng theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argument has acceptable premises that are relevant to its conclusion and sufficient to draw the conclusion (Johnson and Blair, 2006). Practitioners object that such quality dimensions are hard to assess for real-life arguments (Habernal and Gurevych, 2016b). Moreover, the normative nature of theory suggests absolute quality ratings, but in practice it seems much easier to state which argument is more convinc"
P17-2039,E17-1017,1,0.313544,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,E17-1105,1,0.205942,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,P16-2032,0,0.0789411,"o cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a foll"
reyes-etal-2010-evaluating,E06-2031,0,\N,Missing
reyes-etal-2010-evaluating,W04-2214,0,\N,Missing
reyes-etal-2010-evaluating,P05-3029,0,\N,Missing
reyes-etal-2010-evaluating,W02-1011,0,\N,Missing
reyes-etal-2010-evaluating,strapparava-valitutti-2004-wordnet,0,\N,Missing
reyes-etal-2010-evaluating,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
S15-2097,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2097,H05-1044,0,\N,Missing
S15-2097,P11-1016,0,\N,Missing
S15-2097,R13-1007,0,\N,Missing
S15-2097,J92-4003,0,\N,Missing
S15-2097,C10-2005,0,\N,Missing
S15-2097,S13-2065,0,\N,Missing
S15-2097,D11-1052,0,\N,Missing
S15-2097,P02-1053,0,\N,Missing
S15-2097,W10-0204,0,\N,Missing
S15-2097,W02-1011,0,\N,Missing
S15-2097,S14-2009,0,\N,Missing
S15-2097,S14-2111,0,\N,Missing
S15-2097,S13-2054,0,\N,Missing
S15-2097,P14-1146,0,\N,Missing
S15-2097,S15-2078,0,\N,Missing
S15-2097,N13-1039,0,\N,Missing
S15-2097,S13-2052,0,\N,Missing
W15-0505,C14-1142,0,\N,Missing
W17-4508,D15-1044,0,\N,Missing
W17-4508,N16-1012,0,\N,Missing
W17-4508,P17-1099,0,\N,Missing
W17-4508,D15-1229,0,\N,Missing
W17-5106,walker-etal-2012-corpus,0,\N,Missing
W17-5106,P12-2041,0,\N,Missing
W17-5106,W14-2105,0,\N,Missing
W17-5106,W14-2107,0,\N,Missing
W17-5106,D15-1255,0,\N,Missing
W17-5106,W15-0514,0,\N,Missing
W17-5106,D15-1110,0,\N,Missing
W17-5106,N16-1007,0,\N,Missing
W17-5106,P16-1150,0,\N,Missing
W17-5106,W17-5115,1,\N,Missing
W17-5115,W02-1001,0,\N,Missing
W17-5115,P03-1054,0,\N,Missing
W17-5115,N12-1003,0,\N,Missing
W17-5115,prasad-etal-2008-penn,0,\N,Missing
W17-5115,D15-1255,0,\N,Missing
W17-5115,W15-0508,0,\N,Missing
W17-5115,C14-1141,0,\N,Missing
W17-5115,D15-1110,0,\N,Missing
W17-5115,C14-1142,0,\N,Missing
W17-5115,W14-2111,0,\N,Missing
W17-5115,W14-2112,0,\N,Missing
W17-5115,L16-1167,0,\N,Missing
W17-5115,C16-1324,1,\N,Missing
W17-5115,P17-1002,0,\N,Missing
W17-5115,E17-1105,1,\N,Missing
W18-6538,P18-1063,0,0.0194833,"e, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined together to train the mo"
W18-6538,N16-1012,0,0.032176,"ation ranks among the oldest synthesis tasks of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first"
W18-6538,K16-1028,0,0.0457146,"Missing"
W18-6538,P17-1099,0,0.0441586,"of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined"
W18-6538,W17-4508,1,0.896057,"Missing"
W19-4025,L18-1173,0,0.0266597,"ersit¨at Jena, Jena, Germany, &lt;firstname.lastname>@uni-jena.de ◦ Faculty of Media, Bauhaus-Universit¨at Weimar, Weimar, Germany, &lt;firstname.lastname>@uni-weimar.de Abstract lems with the lack of systematic support for hierarchically structured tag labels where one label is semantically more general than another (e.g., the general tag anamnesis in relation to more specific ones like family anamnesis). Finally, and this point addresses a more general design desideratum, we encountered a substantial lack of continuous quality control mechanisms in the majority of annotation tools (the WASA tool (AlGhamdi and Diab, 2018) is one of the rare exceptions and shares several design goals with WAT-S L 2.0). This shortcoming requires annotation project managers to reach for external tools for statistical evaluation. As a consequence, shifting back and forth between annotation and evaluation environments slows down the overall progress of the entire annotation project and hampers iterative refinement of annotation guidelines. Yet, a close technical coupling of such testdevelopment cycles within one integrated platform is a particularly fruitful strategy in complex annotation campaigns. As a remedy for these problems,"
W19-4025,J08-4004,0,0.0606187,"nt environments and easy to customize. Plain text files are used as input, each line containing one segment for labeling. Results as well as logging information (e.g., time stamps) are stored in key-value files. These easy to process formats made WAT-S L 1.0 already well-suited for large-scale annotation projects and were further extended by us as described in Section 3.3. 216 Figure 2: Sublabels of anamnesis tag in a secondary drop-down menu shown when the user clicks on the superlabel anamnesis; the bold and underlined letters display the shortcuts of the labels. tation decisions. Following Artstein and Poesio (2008), we prefer it over a range of alternative measures, like Cohen’s κ (Cohen, 1960), which are overly sensitive to individual annotators’ decisions when modeling chance agreement. Based on such kind of statistical evidence, continuous quality monitoring allows annotation project managers to assess the difficulty of tasks, allowing for a swift refinement of annotation guidelines. This feature was implemented by calculating coincidence matrices for each task with DKP RO AGREEMENT (Meyer et al., 2014). Following annotator feedback during early iterations of our annotation project, we also introduce"
W19-4025,rak-etal-2014-interoperability,0,0.0230067,"ping WAT-S L 2.0, an open-source web-based annotation tool for long-segment labeling, hierarchically structured label sets and built-ins for quality control. 1 Introduction In the course of large-scale annotation campaigns on medical full-text corpora, we encountered several shortcomings of the current generation of annotation tools. Labeling long-spanning text segments (e.g., entire sentences or even paragraphs) is a major issue here that is only insufficiently supported by general purpose open-source annotation tools (M¨uller and Strube, 2006; Stenetorp et al., 2012; Bontcheva et al., 2013; Rak et al., 2014; Yimam et al., 2014) which typically aim at annotating (much) shorter text spans for entities and relations. This is especially troublesome given the increasing availability of full texts and even books as input for annotation projects. With annotation schemes becoming more and more conceptually structured, we also faced prob1 https://github.com/webis-de/wat 215 Proceedings of the 13th Linguistic Annotation Workshop, pages 215–219 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics guide (re)definition The user interface provides annotators not only with a single"
W19-4025,E12-2021,0,0.120789,"Missing"
W19-4025,E17-3004,1,0.883369,"Missing"
W19-4025,P14-5016,0,0.0541898,"Missing"
W19-8607,W18-5215,0,0.0251713,"nalysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so far. In our evaluation, we utilize the dataset of Wachsmuth et al. (2018). This dataset contains 260 argumentative"
W19-8607,P16-2085,0,0.137945,", independent of their finances what is the good of a wonderfully outfitted university if it doesn’t actually allow the majority of clever people to broaden their horizons with all that great equipment p5 p6 p7 p8 p9 p10 p11 p12 Topic Should all universities in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical co"
W19-8607,D17-1144,0,0.0659392,"arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific ro"
W19-8607,W16-2816,0,0.0845706,"es in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and s"
W19-8607,D14-1162,0,0.0830054,"Missing"
W19-8607,P16-1150,0,0.0216463,"es, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference"
W19-8607,W15-0507,0,0.0710582,"is, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and stance from the English version of the arg-microtexts corpus (Peldszus and St"
W19-8607,P18-1152,0,0.0201934,"2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where the goal is to convince or persuade a reader (rather than to merely inform or entertain). Holtzman et al. (2018) propose to alleviate incoherence and repetitiveness by training a set of discriminators, which aim to ensure that a text respects the Gricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approac"
W19-8607,P18-1021,0,0.100511,"view of argumentation synthesis that represents argumentative and rhetorical considerations with language modeling. 2. A novel approach that selects, arranges, and phrases ADUs to synthesize strategy-specific arguments for any topic and stance. 3. First experimental evidence that arguments with basic rhetorical strategies can be synthesized computationally.2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where th"
W19-8607,2007.sigdial-1.5,0,0.256035,"each ADU is represented by a cluster label (A–F in Figure 2), where each label represents one ADU type. Now, for each of the strategies, we map each manually-generated sequence of ADUs to a sequence of cluster labels. Using these sequences of labels, we train one separated selection language model for each strategy. For clustering, we rely on topic-independent features that we expect to implicitly encode logical and emotional strategies: (1) psychological meaningfulness (Pennebaker et al., 2015), (2) eight basic emotions (Plutchik, 1980; Mohammad and Turney, 2013), and (3) argumentativeness (Somasundaran et al., 2007). In the following, we elaborate on the concrete features that we extract: Selection Language Model This model handles the selection of a set of n ADUs for a topic-stance pair x and a rhetorical strategy. We approach the selection as a language modeling task where each ADU is a “word” of our language model and each argument a “sentence”. To abstract from topic, the model actually selects ADU types, as explained in the following. 57 Argument corpus Topic+stance 1 … Argument1,1 ... 1-grams P( ) = 0.20 ... P( ) = 0.60 E F 3. Phrasing Regression Model → 2. Arrangement Language Model D A Argumentm,"
W19-8607,D14-1006,0,0.0583418,"t of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We conside"
W19-8607,C18-1318,1,0.571429,"Missing"
W19-8607,N16-1007,0,0.250645,"uments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so"
W19-8607,W15-0512,0,0.148233,"ricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approach is the system of Sato et al. (2015), where a user can enter a claimlike topic along with a stance. The system then generates argumentative paragraphs on specific aspects of the topic by selecting sentences from 10 million news texts of the Gigaword corpus. Potentially relevant aspects are those that trigger evaluative judgment in the reader. The sentences are arranged so that the text starts with a claim sentence and is followed by support sentences, employing the approach of Yanase et al. (2015). The support sentences are ordered by maximizing the semantic connectivity between sentences. Finally, some rephrasing is done in terms of certain aspects of surface realization. In a manual evaluation, however, no text was seen as sounding natural, underlining the difficulty of the task. In contrast to Sato et al. (2015), we learn directly from input data what argumentative discourse units to combine and how to arrange them. We leave surface realization aside to keep the focus on the argument composition. 2 The code for running the experiments is available here: https://github.com/webis-de/"
W19-8607,W00-1408,0,0.695119,"Missing"
W19-8666,W19-8665,0,0.0128685,"valuator script to compute ROUGE scores. Each software and evaluator run on the test set was manually reviewed by organizers for errors and data leakage. After a successful review, https://tac.nist.gov/ 524 5 Evaluating models on TIRA using ROUGE was allowed even after the submission deadline. Thus, a participant’s technical paper may have a variation of the same model with different ROUGE scores, but was not manually evaluated. the scores were shared on a public leaderboard.6 Two participants provided their system descriptions. We did not receive any description for the tldr-bottom-up model. Gehrmann et al. (2019) leveraged fine-tuned language models to generate abstractive summaries. They argue that excessive copying facilitated by the copy-attention mechanism hinders paraphrasing and information compression (abstraction). As part of the TL;DR challenge, they compared two summarization approaches (pseudo-self-attn and transf-seq2seq) demonstrating the effectiveness of transfer learning at generating abstractive summaries. Our manual evaluation confirms that these models generate concise and coherent summaries. Tackling the same problem of excessive copying in pointer-generator models, Choi et al. (201"
W19-8666,W10-0722,0,0.0151451,"ey U for pairwise comparison using Bonferroni correction. 2 unified-pgn 2 38 unified-vae-pgn 4 30 transf-seq2seq 4 27 pseudo-self-attn 12 35 tldr-bottom-up 2 25 seq2seq-baseline 79 14 ts .28 up Sufficiency 2 3 Avg. 60 66 69 53 73 7 Text quality 1 2 2.11 6 2.13 9 2.20 0 1.97 2 2.30 1 1.11 73 68 62 5 8 28 21 8 90 2.52 3 Avg. 26 29 95 90 71 6 1.78 1.78 2.70 2.67 2.29 1.11 0 15 85 2.57 Table 3: Sufficiency and text quality score distribution in the majority category. the models. Furthermore, it may help to identify if non-expert annotators can still produce reliable judgments without a guideline. Gillick and Liu (2010) cautioned that workers have difficulties distinguishing the content of a summary from its text quality. With that in mind, we devised two orthogonal three-level rating scales. With respect to sufficiency, workers could rate a summary as insufficient (incomplete and unrelated to the source text), as barely acceptable (missing the main point, but capturing relevant secondary information), or as sufficient (capturing the main point of the text). In terms of text quality, we distinguished the levels badly written (incoherent or major errors), needs improvement (minor errors breaking the flow, but"
W19-8666,hovy-etal-2006-automated,0,0.0683851,"summarization technology for social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir."
W19-8666,D18-1208,0,0.0297649,"of news. This can be attributed to the ease of obtaining large amounts of news articles alongside suitable summary ground truth, greatly simplifying the corpus construction. However, the summaries found in the currently widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than"
W19-8666,N04-1019,0,0.0522061,"social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir.nist.gov/projects/duc/index.html https:/"
W19-8666,D15-1044,0,0.0377217,"st, the example from the WebisTLDR-17 corpus exhibits higher abstraction, abbreviations and composition of multiple facts into single phrases. (TAC)4 with evaluation as an independent task (Automatically Evaluating Summaries of Peers, AESOP). Most of these efforts were limited to extractive summarization on comparably small datasets from specific domains, such as biomedical records, newswire articles, and opinions, since neural text generation had not yet become mainstream, rendering abstractive summarization much more difficult. The first attempt at abstractive summarization was presented by Rush et al. (2015), which resulted in a subsequent surge in neural summarization research yielding promising results—we refer to Shi et al. (2018) for a comprehensive review. However, as most recent models have been evaluated exclusively on news corpora, our knowledge of their full capabilities is still superficial. Through the TL;DR challenge, we hope to close this gap. 4 3 Survey of Submissions Out of 16 registered participants, we received 5 submissions from 3 participants (2 from industry). In addition, we provided a seq2seq-baseline model with 2 layers, bi-LSTM, 256 hidden units and no attention. Participa"
W19-8666,E17-2007,0,0.0375653,"Missing"
W19-8666,P17-1099,0,0.0391249,"e diversity, the unified-vae-pgn model uses a VAE for generating summaries of the extracted important sentences. This multi-stage architecture preserves a substantial amount of key information while generating acceptable summaries as revealed in our manual evaluation. We refer readers to the system description papers for further details. 4 Evaluation via crowdsourcing to evaluate both the sufficiency and the text quality of a generated abstractive summary. Below, after reviewing both approaches, we report on the results of the participating systems. 4.1 We begin with a novelty analysis as per See et al. (2017), calculating the fraction of n-grams in the summary that are absent from the text as its novelty (Table 2). The ground truth has the highest novelty, underlining the abstractive nature of selfauthored summaries. Next, we used ROUGE (Lin, 2004) for automatic evaluation and report the F1scores.7 From Table 2 it is difficult to draw any conclusions just by looking at ROUGE scores. Furthermore, a key issue of ROUGE is that it does not provide any upper bounds for the quality of a summarization system (Schluter, 2017), thus warranting an extensive manual evaluation of the systems. Model ROUGE 1 2"
W19-8666,W18-6538,1,0.848865,"r the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR challenge (Syed et al., 2018), inviting summarization researchers to test existing models as well as new ones. To ensure reproducibility as well as blind and semi-automatic evaluation, we adopted the cloud-based evaluation platform TIRA (Potthast et al., 2019). In addition to the automatic ROUGE metrics, we evaluate the submissions manually for summary effectiveness and text quality via crowdsourcing. In this paper, we report our findings, discuss what annotators consider when scoring summaries, and outline future directions for abstractive summarization research. With most summarization research focused on the news domai"
W19-8666,W17-4508,1,0.852321,"widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR"
