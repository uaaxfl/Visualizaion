2020.acl-main.492,Q19-1004,0,0.0259235,"tic coefficient: +0.55 × 10−3 . Figure 5: Influence-artifact distribution for an original and negated HANS example. (P: The lawyers saw the professor behind the bankers. H: The lawyers saw / did not see the professor.) Original Negated Lexical overlap coef Negation coef +3.05 × 10−3 +0.53 × 10−3 −1.13 × 10−3 +0.27 × 10−3 Table 6: Average quadratic coefficients of the influence-artifact distribution for all original HANS examples and all negated HANS examples. 6 Related Work Interpreting NLP model predictions by constructing importance scores over the input tokens is a widely adopted approach (Belinkov and Glass, 2019). Since the appearance and rise of attentionbased models, many work naturally inspect attention scores and interpret with them. However, we are aware of the recent discussion over whether attention is a kind of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al."
2020.acl-main.492,D19-1415,0,0.0225552,"can explain any model’s decision by fitting a sparse linear model to the local region of the input example. The main focus of this work is the applicability of influence functions (Koh and Liang, 2017) as an interpretation method in NLP tasks, and to highlight the possibility of using this to surface annotation artifacts. Other methods that can trace the model’s decision back into the training examples include deep weighted averaging classifiers (Card et al., 2019), which make decisions based on the labels of training examples that are most similar to the test input by some distance metrics. Croce et al. (2019) use kernel-based deep architectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited"
2020.acl-main.492,N19-1423,0,0.222138,"-based saliency maps (left) and influence functions (right). Note that this example is classified incorrectly by the model. Positive saliency tokens and highly influential examples may suggest why the model makes the wrong decision; tokens and examples with negative saliency or influence scores may decrease the model’s confidence in making that decision. clude answering the following research questions. RQ 1 We empirically assess whether the approximation to the influence functions (Koh and Liang, 2017) can be reliably used to interpret decisions of deep transformer-based models such as BERT (Devlin et al., 2019). RQ 2 We investigate the degree to which results from the influence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the"
2020.acl-main.492,P18-2006,0,0.0229107,"se kernel-based deep architectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model coul"
2020.acl-main.492,P19-1334,0,0.341383,"nfluence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the effect of hypothesized artifacts (Gururangan et al., 2018; McCoy et al., 2019) on the model’s prediction using influence functions. 2 Explaining Black-box Model Predictions Machine learning models in NLP depend on two factors when making predictions: the input text and the model parameters. Prior attempts to interpret opaque NLP models have typically focused on the input text. Our work investigates the complementary approach of interpreting predictions by analyzing the influence of examples in training data. Saliency maps aim to provide interpretability by highlighting parts of the input text, whereas influence functions seek clues in the model parameters, eventually lo"
2020.acl-main.492,D18-1407,0,0.0345731,"d of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al., 2017) we use in this work, there are some variants that aim to make gradient-based attributions robust to potential noise in the input (Sundararajan et al., 2017; Smilkov et al., 2017). We also note that Feng et al. (2018) find that gradient-based methods sometimes yield counter-intuitive results when iterative input reductions are performed. Other token-level interpretations include input perturbation (Li et al., 2016b) which measure a token’s importance by the effect of removing it, and LIME (Ribeiro et al., 2016) which can explain any model’s decision by fitting a sparse linear model to the local region of the input example. The main focus of this work is the applicability of influence functions (Koh and Liang, 2017) as an interpretation method in NLP tasks, and to highlight the possibility of using this to"
2020.acl-main.492,N18-1146,0,0.0689281,"model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two complementary interpretation methods—gradient-based saliency maps and influence functions—in two text classification tasks: sentiment analysis and NLI. We first validated the reliability of influence functions when used with deep transformer-based models. We found that in a lexicon-driven sentiment analysis task, saliency maps and influence functions are largely consistent with each other. They are not consistent, however, on the task of NLI. We pos"
2020.acl-main.492,N18-2017,0,0.193708,"which results from the influence function are consistent with insights gleaned from gradient-based saliency scores for representative NLP tasks. RQ 3 We explore the application of influence functions as a mechanism to reveal artifacts (or confounds) in training data that might be exploited by models. To the best of our knowledge, this is the first work in NLP to compare interpretation methods that construct saliency maps over inputs with methods that explain predictions via influential training examples. We also propose a new quantitative measurement for the effect of hypothesized artifacts (Gururangan et al., 2018; McCoy et al., 2019) on the model’s prediction using influence functions. 2 Explaining Black-box Model Predictions Machine learning models in NLP depend on two factors when making predictions: the input text and the model parameters. Prior attempts to interpret opaque NLP models have typically focused on the input text. Our work investigates the complementary approach of interpreting predictions by analyzing the influence of examples in training data. Saliency maps aim to provide interpretability by highlighting parts of the input text, whereas influence functions seek clues in the model para"
2020.acl-main.492,P19-1487,0,0.020794,"nce and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two c"
2020.acl-main.492,D19-1275,0,0.0195319,"st activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was expl"
2020.acl-main.492,N16-3020,0,0.897788,"to interpreting black box NLP model predictions, i.e., 1 Code is available at https://github.com/ xhan77/influence-function-analysis. Yulia Tsvetkov Carnegie Mellon University ytsvetko@cs.cmu.edu indicating specific input tokens as being particularly influential for a given prediction. This in turn facilitates the construction of saliency maps over texts, in which words are highlighted with intensity proportional to continuous ‘importance’ scores. Prominent examples of the latter include gradient-based attribution (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017), LIME (Ribeiro et al., 2016), and attention-based (Xu et al., 2015) heatmaps. While widely used and potentially useful for some lexicon-driven tasks (e.g., sentiment analysis), we argue that by virtue of being constrained to highlighting individual input tokens, saliency maps will necessarily fail to explain predictions in more complex semantic tasks involving reasoning, such as natural language inference (NLI), where fine-grained interactions between multiple words or spans are key (Camburu et al., 2018). Moreover, saliency maps are inherently limited as a model debugging tool; they may tell us which inputs the model fo"
2020.acl-main.492,2020.acl-main.386,0,0.0302862,"2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an example in the left part of Fi"
2020.acl-main.492,P18-1079,0,0.0276486,"chitectures 5560 that project test inputs to a space determined by a group of sampled training examples and make explanations through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some ar"
2020.acl-main.492,N19-1357,1,0.86226,"egation coef +3.05 × 10−3 +0.53 × 10−3 −1.13 × 10−3 +0.27 × 10−3 Table 6: Average quadratic coefficients of the influence-artifact distribution for all original HANS examples and all negated HANS examples. 6 Related Work Interpreting NLP model predictions by constructing importance scores over the input tokens is a widely adopted approach (Belinkov and Glass, 2019). Since the appearance and rise of attentionbased models, many work naturally inspect attention scores and interpret with them. However, we are aware of the recent discussion over whether attention is a kind of faithful explanation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Using vanilla attention as interpretation could be more problematic in now ubiquitous deep transformerbased models, such as we use here. Gradient-based saliency maps are locally ‘faithful’ by construction. Other than the vanilla gradients (Simonyan et al., 2014) and the “gradient × input” method (Shrikumar et al., 2017) we use in this work, there are some variants that aim to make gradient-based attributions robust to potential noise in the input (Sundararajan et al., 2017; Smilkov et al., 2017). We also note that Feng et al. (2018) find that gradient-based metho"
2020.acl-main.492,2020.acl-main.409,1,0.827447,"why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an example in the left part of Figure 1. 2.2 Influenc"
2020.acl-main.492,D19-1425,1,0.859481,"or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial learning to demote the artifacts. 7 Conclusion We compared two complementary interpretation methods—gradient-based saliency maps and influence functions—in two text classification tasks: sentiment analysis and NLI. We first validated the reliability of influence functions when used with deep transformer-based models. We found that in a lexicon-driven sentiment analysis task, saliency maps and influence functions are largely consistent with each other. They are not consistent, however, on the task of NLI. We posit that influence fu"
2020.acl-main.492,D16-1011,0,0.0269157,"tions to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of artifacts in data was explored in other text classification tasks (Pryzant et al., 2018; Kumar et al., 2019; Landeiro et al., 2019), focusing on approaches to adversarial l"
2020.acl-main.492,D13-1170,0,0.00867916,"Missing"
2020.acl-main.492,N16-1082,0,0.633017,"hlighting parts of the input text, whereas influence functions seek clues in the model parameters, eventually locating interpretations within the training examples that influenced these estimates. In this section we explain the two interpretation methods in detail.2 2.1 Gradient-based saliency maps As a standard, illustrative ‘explanation-by-inputfeatures’ method, we focus on gradient-based saliency maps, in which the gradient of the loss L is computed with respect to each token t in the input text, and the magnitude of the gradient serves as a feature importance score (Simonyan et al., 2014; Li et al., 2016a). Gradients have the advantage of being locally ‘faithful’ by construction: they tell us how much the loss would change, were we to perturb a token by a small amount. Gradient-based attributions are also agnostic with respect to the model, as long as it is differentiable with respect to inputs. Finally, calculating gradients is computationally efficient, especially compared to methods that require post-hoc input perturbation and function fitting, like LIME (Ribeiro et al., 2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to t"
2020.acl-main.492,D19-3002,0,0.0355319,"Missing"
2020.acl-main.492,D19-1002,0,0.237298,"like LIME (Ribeiro et al., 2016). We are interested in why the model made a particular prediction. We therefore define a loss Lyˆ with respect to the prediction yˆi that the model actually made, rather than the ground truth yi . For each token t ∈ xi , we define a saliency score −∇e(t) Lyˆ · e(t), where e(t) is the embedding of t. This is also referred as the “gradient × input” method in Shrikumar et al. (2017). The “gradient” ∇e(t) Lyˆ captures the sensitivity of the loss to the change in the input embedding, and the “input” 2 Here we focus on interpretability approaches which are faithful (Wiegreffe and Pinter, 2019; Jacovi and Goldberg, 2020; Jain et al., 2020) by construction; other approaches are discussed in §6. 5554 e(t) leverages the sign and magnitude of the input. The final saliency score of each token t would be L1-normalized across all tokens in xi . Unlike Simonyan et al. (2014) and Li et al. (2016a), when scoring features for importance, we do not take the absolute value of the saliency score, as this encodes whether a token is positively influencing the prediction (i.e., providing support the prediction) or negatively influencing the prediction (highlighting counter-evidence). We show an exa"
2020.acl-main.492,P19-1560,0,0.0212169,"ions through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of"
2020.acl-main.492,N19-1112,0,0.0365371,"ions through the most activated training instances. While these methods can similarly identify the ‘influential’ training examples, they require special designs or modifications to the model and could sacrifice the model’s performance and generalizability. Other general methods for model interpretability include adversarial-attack approaches that identify that part of input texts can lead to drastically different model decisions when minimally edited (Ebrahimi et al., 2018; Ribeiro et al., 2018), probing approaches that test internal representations of models for certain tasks and properties (Liu et al., 2019b; Hewitt and Liang, 2019), and generative approaches that make the model jointly extract or generate natural language explanations to support predictions (Lei et al., 2016; Camburu et al., 2018; Liu et al., 2019a; Rajani et al., 2019). Specific to the NLI task, Gururangan et al. (2018) recognize and define some possible artifacts within NLI annotations. McCoy et al. (2019) create a diagnostic dataset that we use in this work and suggest that the model could be exploiting some artifacts in training data based on its poor performance on the diagnostic set. Beyond NLI, the negative influence of"
2020.acl-main.492,N18-1101,0,0.101455,"Missing"
2020.acl-main.754,D17-1268,1,0.838312,"h training language on the current model, namely R(i; θt ). We estimate this value by i sampling a batch of data from each Dtrain to get the training gradient for θt , and use this to calculate the reward for this language. This process is detailed in line 11 of the line 25. Unlike the algorithm in DDS which requires storing n model gradients,3 this approximation does not require extra memory even if n is large, which is important given recent efforts to scale multilingual training to 100+ (Arivazhagan et al., 2019; Aharoni et al., 2019) or even 1000+ lan¨ guages (Ostling and Tiedemann, 2017; Malaviya et al., 2017). 5 k=1 PD(i; ψt) n Dtrain Stabilized Multi-objective Training (10) where Jdev (·) defines the combination of n dev sets, and we simply plug in its definition from Eq. 3. Intuitively, Eq. 10 implies that we should favor the training language i if its gradient aligns with the gradient of the aggregated dev risk of all languages. Implementing the Scorer Update The pseudocode for the training algorithm using MultiDDS can be found in line 25. Notably, we do not update the data scorer ψ on every training step, because it is too computationally expensive for NMT training (Wang et al., 2019b). Instea"
2020.acl-main.754,P19-1583,1,0.937102,"016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due to interference from the heterogeneous nature of the data. This is especially the case for modestly-sized models that are conducive to efficient deployme"
2020.acl-main.754,D09-1092,0,0.0584651,"iverse group. The models trained with MultiDDS-S perform better and have less variance. variance and a higher mean than MultiDDS. Additionally, we compare the learned language distribution of MultiDDS-S and MultiDDS in Fig. 5. The learned language distribution in both plots fluctuates similarly, but MultiDDS has more drastic changes than MultiDDS-S. This is also likely due to the reward of MultiDDS-S having less variance than that of MultiDDS. 7 Related Work Our work is related to the multilingual training methods in general. Multilingual training has a rich history (Schultz and Waibel, 1998; Mimno et al., 2009; Shi et al., 2010; T¨ackstr¨om et al., 2013), but has become particularly prominent in recent years due the ability of neural networks to easily perform multi-task learning (Dong et al., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose lang"
2020.acl-main.754,D18-1103,1,0.948476,"15; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due to interference from the heterogeneous nature of the data. This is especially the case for modestly-sized models that are conduciv"
2020.acl-main.754,D18-1034,1,0.824344,"nguages. Experiments on two sets of languages under both one-to-many and manyto-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.1 1 Introduction Multilingual models are trained to process different languages in a single model, and have been applied to a wide variety of NLP tasks such as text classification (Klementiev et al., 2012; Chen et al., 2018a), syntactic analysis (Plank et al., 2016; Ammar et al., 2016), named-entity recognition (Xie et al., 2018; Wu and Dredze, 2019), and machine translation (MT) (Dong et al., 2015; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github"
2020.acl-main.754,P18-2104,0,0.0193142,"l., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose languages from multilingual data to improve the performance on a particular language, while our work instead aims to train a single model that handles translation between many languages. (Zaremoodi et al., 2018; Wang et al., 2018, 2019a) propose improvements to the model architecture to improve multilingual performance, while MultiDDS is a model-agnostic and optimizes multilingual data usage. Our work is also related to machine learning methods that balance multitask learning (Chen et al., 2018b; Kendall et al., 2018). For example, Kendall et al. (2018) proposes to weigh the training loss from a multitask model based on the uncertainty of each task. Our method focuses on optimizing the multilingual data usage, and is both somewhat orthogonal to and less heuristic than such loss weighting methods. Fi"
2020.acl-main.754,D16-1163,0,0.0546451,"ar et al., 2016), named-entity recognition (Xie et al., 2018; Wu and Dredze, 2019), and machine translation (MT) (Dong et al., 2015; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due"
2020.acl-main.754,N18-2084,1,0.870494,"˜i (x, y) ∼ Dtrain X, Y ← X, Y ∪ x, y end . Train the NMT model for multiple steps for x, y in X, Y do θ← GradientUpdate (θ, ∇θ `(x, y; θ)) end . Estimate the effect of each language R(i; θ) for i from 1 to n do i x0 , y 0 ∼ Dtrain gtrain ← ∇θ `(x0 , y 0 ; θ) θ0 ← GradientUpdate(θ, gtrain ) gdev ← 0 for j from 1 to n do j xd , yd ∼ Ddev gdev ← gdev + ∇θ0 `(xd , yd ; θ0 ) end R(i; θ) ← cos(gdev , gtrain ) end . Optimize P ψ dψ ← ni=1 R(i; θ) · ∇ψ log (PD (i; ψ)) ψ ← GradientUpdate(ψ, dψ ) end 6 6.1 Experimental Evaluation Data and Settings We use the 58-languages-to-English parallel data from Qi et al. (2018). A multilingual NMT model is trained for each of the two sets of language pairs with different level of language diversity: Related: 4 LRLs (Azerbaijani: aze, Belarusian: bel, Glacian: glg, Slovak: slk) and a related HRL for each LRL (Turkish: tur, Russian: rus, Portuguese: por, Czech: ces) Diverse: 8 languages with varying amounts of data, picked without consideration for relatedness (Bosnian: bos, Marathi: mar, Hindi: hin, Macedonian: mkd, Greek: ell, Bulgarian: bul, French: fra, Korean: kor) Statistics of the datasets are in § A.3. For each set of languages, we test two varieties of transl"
2020.acl-main.754,D10-1103,0,0.0349742,"dels trained with MultiDDS-S perform better and have less variance. variance and a higher mean than MultiDDS. Additionally, we compare the learned language distribution of MultiDDS-S and MultiDDS in Fig. 5. The learned language distribution in both plots fluctuates similarly, but MultiDDS has more drastic changes than MultiDDS-S. This is also likely due to the reward of MultiDDS-S having less variance than that of MultiDDS. 7 Related Work Our work is related to the multilingual training methods in general. Multilingual training has a rich history (Schultz and Waibel, 1998; Mimno et al., 2009; Shi et al., 2010; T¨ackstr¨om et al., 2013), but has become particularly prominent in recent years due the ability of neural networks to easily perform multi-task learning (Dong et al., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose languages from multili"
2020.acl-main.754,Q13-1001,0,0.0682599,"Missing"
2020.acl-main.754,P02-1040,0,\N,Missing
2020.acl-main.754,C12-1089,0,\N,Missing
2020.acl-main.754,P15-1166,0,\N,Missing
2020.acl-main.754,Q17-1024,0,\N,Missing
2020.acl-main.754,E17-2102,0,\N,Missing
2020.acl-main.754,I17-2050,0,\N,Missing
2020.acl-main.754,D18-2012,0,\N,Missing
2020.acl-main.754,D18-1326,0,\N,Missing
2020.acl-main.754,N19-1388,0,\N,Missing
2020.acl-main.754,Q18-1039,0,\N,Missing
2020.conll-1.46,2020.lrec-1.223,0,0.0219066,"l factors on users’ CS patterns across Spanglish and Hinglish. 2 Code-Switching Strategies Given that CS is used in very nuanced ways, researchers have been studying how people codeswitch, examining the switch-points of languages syntactically (Poplack, 1980; Solorio and Liu, 2008), prosodically (Fricke et al., 2016), lexically (Kootstra, 2012), pragmatically (Begum et al., 2016), and so forth. Many works have attempted to model code-switching text and speech from a statistical perspective (Garg et al., 2018a,b). Recent works and benchmarks such as Linguistic Codeswitching Evaluation (LinCE) (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have provided a unified platform to evaluate CS data for various NLP tasks across various language pairs. Our work is in line with these recent efforts to provide NLP capabilities to users with diverse linguistic backgrounds. We extend the human–machine CS dialogue system by Ahn et al. (2020) to a new language pair of Hindi-English. In order to better understand the style and usage of languages in a code-switched utterance, we cluster and characterize these utterances by a set of predefined CS strategies. Previous works have mainly identified two commonly us"
2020.conll-1.46,2020.scil-1.32,1,0.521461,"mate goal to enable adaptive codeswitching dialogue agents, in this paper we study user accommodation, i.e., entrainment (Brennan 565 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 565–577 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 and Clark, 1996) in CS human–machine dialogues. Our exploratory analysis of user accommodation will facilitate better development of dialogue agents which can eventually accommodate to users in return. To this end, we adopt a collaborative dialogue framework of Ahn et al. (2020), which converses with Spanish–English (Spanglish) bilinguals. To facilitate a more general analysis, we extend this framework to Hindi–English (Hinglish), a language pair which is typologically distinct from Spanglish and is spoken by millions of people. We begin by providing background on codeswitching (§2) and linguistic accommodation (§3) We then introduce our generalized bilingual dialogue system (§4). In §5, we describe our experimental setup for Hinglish data collection and discuss the data statistics. We later provide our exploratory analysis of language accommodation and other socio-l"
2020.conll-1.46,W18-3210,0,0.0239202,"le adjust their behaviors or speech styles to their conversational partners’ (Giles et al., 1973). Linguistic accommodation has proven to reduce interpersonal distance (Camilleri, 1996) and is correlated with dialogue success and engagement (Nenkova et al., 2008). Although well-studied in the monolingual dialogues (Brennan and Clark, 1996; Niederhoffer and Pennebaker, 2002), it is relatively new in the CS setting. Soto et al. (2018) found rate of code-switching to be accommodated in human–human Spanish-English dialogues. Choice of language when code-switching can also be adapted in dialogues (Bawa et al., 2018). Fricke et al. (2016) further discover that part-of-speech of a CS utterance may impact the following language choice. Our work adds to this field by studying accommodation of language choice for lexical classes. In terms of quantifying accommodation, we adapt a metric from Mizukami et al. (2016) to measure accommodation (we refer it to as global accommodation). Global accommodation extends the score proposed in Nenkova et al. (2008) by aggregating a speaker’s word usage across an entire dialogue and biases it relatively with other non-partners in the corpus.PFor two partners a and b, we deno"
2020.conll-1.46,L16-1260,0,0.0145913,"9 Hindi-English human–machine conversations, (3) adaptation of accommodation metrics and a corresponding analysis of accommodation of language style and choice in CS dialogues, and (4) an exploratory study of linguistic and socio-cultural factors on users’ CS patterns across Spanglish and Hinglish. 2 Code-Switching Strategies Given that CS is used in very nuanced ways, researchers have been studying how people codeswitch, examining the switch-points of languages syntactically (Poplack, 1980; Solorio and Liu, 2008), prosodically (Fricke et al., 2016), lexically (Kootstra, 2012), pragmatically (Begum et al., 2016), and so forth. Many works have attempted to model code-switching text and speech from a statistical perspective (Garg et al., 2018a,b). Recent works and benchmarks such as Linguistic Codeswitching Evaluation (LinCE) (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have provided a unified platform to evaluate CS data for various NLP tasks across various language pairs. Our work is in line with these recent efforts to provide NLP capabilities to users with diverse linguistic backgrounds. We extend the human–machine CS dialogue system by Ahn et al. (2020) to a new language pair of Hindi"
2020.conll-1.46,D18-1346,1,0.835378,"language style and choice in CS dialogues, and (4) an exploratory study of linguistic and socio-cultural factors on users’ CS patterns across Spanglish and Hinglish. 2 Code-Switching Strategies Given that CS is used in very nuanced ways, researchers have been studying how people codeswitch, examining the switch-points of languages syntactically (Poplack, 1980; Solorio and Liu, 2008), prosodically (Fricke et al., 2016), lexically (Kootstra, 2012), pragmatically (Begum et al., 2016), and so forth. Many works have attempted to model code-switching text and speech from a statistical perspective (Garg et al., 2018a,b). Recent works and benchmarks such as Linguistic Codeswitching Evaluation (LinCE) (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have provided a unified platform to evaluate CS data for various NLP tasks across various language pairs. Our work is in line with these recent efforts to provide NLP capabilities to users with diverse linguistic backgrounds. We extend the human–machine CS dialogue system by Ahn et al. (2020) to a new language pair of Hindi-English. In order to better understand the style and usage of languages in a code-switched utterance, we cluster and characterize"
2020.conll-1.46,P17-1162,0,0.0272605,"ally across turns within a single dialogue. For two partners a and b, we can formulate this metric as   local(a,b) (C) = P r TbC |TaC − P r TbC Our bilingual human–machine dialogue system mainly serves two important purposes: (1) collection of CS data and (2) experimentation of new agent strategies. Previous work (Ramanarayanan and Suendermann-Oeft, 2017) developed a rulebased CS dialogue system restricted to a fixed set of prompts. Ahn et al. (2020) proposed a more flexible bilingual system for English-Spanish as an extension of a monolingual goal-oriented collaborative dialogue framework (He et al., 2017), originally designed for the M UTUAL F RIENDS task. This task provides the two conversational partners A and B individually with a knowledge base (KB) of friends, out of which there is exactly one friend common in both KBs. Each friend in the KB has several attributes such as hobby, location of work, etc. The goal of the task is to collaboratively find this mutual friend by text conversations between the two partners–which can be human or machine. The modifications made by Ahn et al. (2020) for extending this monolingual system to support bilingual Spanish-English dialogues were mainly in thr"
2020.conll-1.46,2020.acl-main.329,0,0.0289223,"oss Spanglish and Hinglish. 2 Code-Switching Strategies Given that CS is used in very nuanced ways, researchers have been studying how people codeswitch, examining the switch-points of languages syntactically (Poplack, 1980; Solorio and Liu, 2008), prosodically (Fricke et al., 2016), lexically (Kootstra, 2012), pragmatically (Begum et al., 2016), and so forth. Many works have attempted to model code-switching text and speech from a statistical perspective (Garg et al., 2018a,b). Recent works and benchmarks such as Linguistic Codeswitching Evaluation (LinCE) (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have provided a unified platform to evaluate CS data for various NLP tasks across various language pairs. Our work is in line with these recent efforts to provide NLP capabilities to users with diverse linguistic backgrounds. We extend the human–machine CS dialogue system by Ahn et al. (2020) to a new language pair of Hindi-English. In order to better understand the style and usage of languages in a code-switched utterance, we cluster and characterize these utterances by a set of predefined CS strategies. Previous works have mainly identified two commonly used code-switching (CS) strategies:"
2020.conll-1.46,2020.acl-main.169,1,0.80705,"of the languages, switching from one MatL to another. In our work, we focus on the Hindi-English language pair. We experiment with 4 CS strategies ins (1) EN−−→HI (inserting English phrases into Hindi ins MatL), (2) HI−−→EN (inserting Hindi phrases into alt English MatL), (3) HI−→EN (alternating from alt Hindi MatL to English MatL), and (4) EN−→HI (alternating from English MatL to Hindi MatL). CS is also observed more often in informal and casual settings than formal ones (Sitaram et al., 2019). We test this hypothesis by inducing informality in the agent’s strategies. Although recent works (Madaan et al., 2020) have introduced neural methods to induce informality, we deploy a simple way to moderate formality by adding discourse markers (e.g. “so”, “well”) at the beginning and ending of sentences. These markers are independent of context and syntax (Schiffrin, 1988), and are often associated with informality (Jucker, 2002). Thus, we define four more agent strategies by infusing informality (+ Informality) in each of the previously described 4 CS strategies. 566 3 Measuring Accommodation in Dialogue Communication Accommodation Theory posits that people adjust their behaviors or speech styles to their"
2020.conll-1.46,W16-3640,0,0.0201085,"monolingual dialogues (Brennan and Clark, 1996; Niederhoffer and Pennebaker, 2002), it is relatively new in the CS setting. Soto et al. (2018) found rate of code-switching to be accommodated in human–human Spanish-English dialogues. Choice of language when code-switching can also be adapted in dialogues (Bawa et al., 2018). Fricke et al. (2016) further discover that part-of-speech of a CS utterance may impact the following language choice. Our work adds to this field by studying accommodation of language choice for lexical classes. In terms of quantifying accommodation, we adapt a metric from Mizukami et al. (2016) to measure accommodation (we refer it to as global accommodation). Global accommodation extends the score proposed in Nenkova et al. (2008) by aggregating a speaker’s word usage across an entire dialogue and biases it relatively with other non-partners in the corpus.PFor two partners a and b, we denote Ea,b = − w∈V |P ra (w) − P rb (w) |for a given word class V (where P r(w) is the empirical probability of word w). Denoting the set of non-partners for the speaker a by Na , we define ratio as   E(a,b) > E(a,np) 1 ratio(E(a,b) , E(a,np) ) = 0.5 E(a,b) = E(a,np)   0 E(a,b) < E(a,np) (and Ta"
2020.conll-1.46,P08-2043,0,0.0251553,"These markers are independent of context and syntax (Schiffrin, 1988), and are often associated with informality (Jucker, 2002). Thus, we define four more agent strategies by infusing informality (+ Informality) in each of the previously described 4 CS strategies. 566 3 Measuring Accommodation in Dialogue Communication Accommodation Theory posits that people adjust their behaviors or speech styles to their conversational partners’ (Giles et al., 1973). Linguistic accommodation has proven to reduce interpersonal distance (Camilleri, 1996) and is correlated with dialogue success and engagement (Nenkova et al., 2008). Although well-studied in the monolingual dialogues (Brennan and Clark, 1996; Niederhoffer and Pennebaker, 2002), it is relatively new in the CS setting. Soto et al. (2018) found rate of code-switching to be accommodated in human–human Spanish-English dialogues. Choice of language when code-switching can also be adapted in dialogues (Bawa et al., 2018). Fricke et al. (2016) further discover that part-of-speech of a CS utterance may impact the following language choice. Our work adds to this field by studying accommodation of language choice for lexical classes. In terms of quantifying accommo"
2020.conll-1.46,D19-5558,0,0.0265839,"syntactic, grammatical and morphological levels (Sitaram et al., 2019). Code-switching has been studied in linguistics and sociolinguistics for decades (Poplack, 1980; Gumperz, 1982; Milroy et al., 1995; Auer, 1 The code and data is available at https://github. com/TanmayParekh/commonDost 2013; Gardner-Chloros and Weston, 2015) since it reveals various linguistic and socio-cultural behaviours (Heller, 1982). However, NLP studies of written CS are limited to social media texts, rather than natural conversation, and tend to focus on single sentences, rather than be contextualized in a dialogue (Rabinovich et al., 2019). Advances in dialogue research (Vinyals and Le, 2015; Zhang et al., 2020; Serban et al., 2016) have enabled conversational AI technologies for human– machine interactions, like Alexa and Siri. Although these technologies are pervasive, they still have limited abilities to accommodate to the user, and they do not account for the ubiquity of multilingual communication. Due to the lack of code-switching abilities in existing language technologies, there has been limited work in studying linguistic accommodation in written CS dialogues. With the ultimate goal to enable adaptive codeswitching dial"
2020.conll-1.46,D08-1102,0,0.0480537,"stem easily generalizable to a new CS language pair, (2) a new dataset, C OMMON D OST, comprising of 439 Hindi-English human–machine conversations, (3) adaptation of accommodation metrics and a corresponding analysis of accommodation of language style and choice in CS dialogues, and (4) an exploratory study of linguistic and socio-cultural factors on users’ CS patterns across Spanglish and Hinglish. 2 Code-Switching Strategies Given that CS is used in very nuanced ways, researchers have been studying how people codeswitch, examining the switch-points of languages syntactically (Poplack, 1980; Solorio and Liu, 2008), prosodically (Fricke et al., 2016), lexically (Kootstra, 2012), pragmatically (Begum et al., 2016), and so forth. Many works have attempted to model code-switching text and speech from a statistical perspective (Garg et al., 2018a,b). Recent works and benchmarks such as Linguistic Codeswitching Evaluation (LinCE) (Aguilar et al., 2020) and GLUECoS (Khanuja et al., 2020) have provided a unified platform to evaluate CS data for various NLP tasks across various language pairs. Our work is in line with these recent efforts to provide NLP capabilities to users with diverse linguistic backgrounds."
2020.conll-1.46,2020.acl-demos.30,0,0.0326858,"itching has been studied in linguistics and sociolinguistics for decades (Poplack, 1980; Gumperz, 1982; Milroy et al., 1995; Auer, 1 The code and data is available at https://github. com/TanmayParekh/commonDost 2013; Gardner-Chloros and Weston, 2015) since it reveals various linguistic and socio-cultural behaviours (Heller, 1982). However, NLP studies of written CS are limited to social media texts, rather than natural conversation, and tend to focus on single sentences, rather than be contextualized in a dialogue (Rabinovich et al., 2019). Advances in dialogue research (Vinyals and Le, 2015; Zhang et al., 2020; Serban et al., 2016) have enabled conversational AI technologies for human– machine interactions, like Alexa and Siri. Although these technologies are pervasive, they still have limited abilities to accommodate to the user, and they do not account for the ubiquity of multilingual communication. Due to the lack of code-switching abilities in existing language technologies, there has been limited work in studying linguistic accommodation in written CS dialogues. With the ultimate goal to enable adaptive codeswitching dialogue agents, in this paper we study user accommodation, i.e., entrainment"
2020.emnlp-main.359,N19-1388,0,0.024822,"er on downstream tasks—it has been shown that multilingual models can generalize to target languages even when labeled training data is only available in the source language (typically English) on a wide range of tasks (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). However, multilingual models are not equally beneficial for all languages. Conneau et al. (2019) demonstrated that including more languages in a single model can improve performance for lowresource languages but hurt performance for highresource languages. Similarly, recent work (Johnson et al., 2017; Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019) in multilingual neural machine translation (NMT) also observed performance degradation on high-resource language pairs. In multi-task learning (Ruder, 2017), this phenomenon is known as negative interference or negative transfer (Wang et al., 2019), where training multiple tasks jointly hinders the performance on individual tasks. Despite these empirical observations, little prior work analyzed or showed how to mitigate negative interference in multilingual language models. Particularly, it is natural to ask: (1) Can negative interference occur for low-resource lang"
2020.emnlp-main.359,D19-1165,0,0.0172133,"ages through higherorder gradients. In other words, language-specific parameters are not agnostic of other languages anymore without violating the language-specific requirement. This is because, in Eq. 3, while ∇θ(t) is based on the i-th language only, the validation loss is computed for all languages. Finally, in the second phase, we update θ based on the new φ(t+1) : θ (t+1) = θ (t) − β∇θ(t) Ltrain (θ (t) , φ(t+1) ) 4.2 (5) Evaluation While our method is generic, we evaluate it applied on bilingual models with adapter networks. Adapters have been effectively utilized in multilingual models (Bapna et al., 2019), and we choose them for practical consideration of limiting perlanguage capacity. Unlike prior works that finetune adapters for adaptation, we train them jointly with shared parameters during pretraining. We follow Houlsby et al. (2019) and insert language-specific adapters after attention and feedforward layers. We leave a more thorough investigation of how to better pick language-specific structures for future work. For downstream task evaluation, we finetune all layers. Notice that computing the gradient of gradient in Eq. 3 doubles the memory requirement. In practice, we utilize the finit"
2020.emnlp-main.359,2020.tacl-1.30,0,0.0514589,"for finetuning details.) We use the WikiAnn (Pan et al., 2017) NER dataset, which is a sequence labelling task built automatically from Wikipedia. A linear layer with softmax classifier is added on top of pretrained models to predict the label for each word based on its first subword. We report the F1 score. POS Similar to NER, POS is also a sequence labelling task but with a focus on synthetic knowledge. In particular, we use the Universal Dependencies treebanks (Nivre et al., 2018). Task-specific layers are the same and we report F1, as in NER. We choose to use the TyDiQA-GoldP QA dataset (Clark et al., 2020) that covers typologically diverse languages. Similar to popular QA dataset such as SQuAD (Rajpurkar et al., 2018), this is a span prediction task where task-specific linear classifiers are used to predict start/end positions of the answer. Standard metrics of F1 and Exact Match (EM) are reported. NLI XNLI (Conneau et al., 2018) is probably the most popular cross-lingual benchmark. Notice that the original dataset only contains training data for English. Consequently, we only evaluate this task on the zero-shot transfer setting while we consider both settings for the rest of other tasks. 3.3 R"
2020.emnlp-main.359,P19-4007,0,0.0250315,"Missing"
2020.emnlp-main.359,D18-1269,0,0.0417825,"R, POS is also a sequence labelling task but with a focus on synthetic knowledge. In particular, we use the Universal Dependencies treebanks (Nivre et al., 2018). Task-specific layers are the same and we report F1, as in NER. We choose to use the TyDiQA-GoldP QA dataset (Clark et al., 2020) that covers typologically diverse languages. Similar to popular QA dataset such as SQuAD (Rajpurkar et al., 2018), this is a span prediction task where task-specific linear classifiers are used to predict start/end positions of the answer. Standard metrics of F1 and Exact Match (EM) are reported. NLI XNLI (Conneau et al., 2018) is probably the most popular cross-lingual benchmark. Notice that the original dataset only contains training data for English. Consequently, we only evaluate this task on the zero-shot transfer setting while we consider both settings for the rest of other tasks. 3.3 Results and Analysis In Table 2 and 3, we report our results on NER, POS and QA together with XLM-100, which is trained on 100 languages and contains 827M parameters. In particular, we observe that monolin4441 Model ar ru sw te avg NER (F1) hi te Model Within-language Monolingual Mono JointPair + ffn + attn + adpt + share adpt +"
2020.emnlp-main.359,D19-1252,0,0.0294703,"Missing"
2020.emnlp-main.359,D19-1167,0,0.0194639,"performance in the target task. Others show it is important to remedy negative transfer in multi-source settings (Ge et al., 2014; Wang and Carbonell, 2018). In this work, we study negative transfer in multilingual models, where languages contain heavily unbalanced training data and exhibit complex intertask relatedness. In addition, our work is related to methods that measure similarity between cross-lingual representations. For example, existing methods utilize statistical metrics to examine cross-lingual embeddings such as singular vector canonical correlation analysis (Raghu et al., 2017; Kudugunta et al., 2019), eigenvector similarity (Søgaard et al., 2018), 6 Conclusion We present the first systematic study of negative interference in multilingual models and shed light on its causes. We further propose a method and show it can improve cross-lingual transferability by mitigating negative interference. While prior efforts focus on improving sharing and cross-lingual alignment, we provide new insights and a different perspective on unsharing and resolving language conflicts. Acknowledgments We want to thank Jaime Carbonell for his support on the early stage of this project. We also would like to thank"
2020.emnlp-main.359,2021.ccl-1.108,0,0.0949542,"Missing"
2020.emnlp-main.359,D18-1398,0,0.0286767,"t + share adpt + meta adpt 67.1 42.5 48.5 67.8 67.9 68.5 73.5 51.4 50.7 73.7 73.4 74.8 69.2 40.7 41.2 69.5 70.0 70.2 61.5 36.2 33.3 62.2 61.8 64.5 62.3 34.8 35.1 59.7 62.2 61.5 66.7 41.1 41.8 66.6 67.1 67.9 XLM 68.2 75.2 72.3 65.4 58.1 67.8 and centered kernel alignment (Kornblith et al., 2019; Wu et al., 2019). While these methods focus on testing latent representations, we directly compare similarity of neural network structures through network pruning. Finally, our work is related to meta learning, which sets a meta task to learn model initialization for fast adaptation (Finn et al., 2017; Gu et al., 2018; Flennerhag et al., 2019), data selection (Wang et al., 2020a), and hyperparameters (Baydin et al., 2018). In our case, the meta task is to mitigate negative interference. Table 6: XNLI results (Accuracy). 5 Related Work Unsupervised multilingual language models such as mBERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019; Conneau et al., 2019) work surprisingly well on many NLP tasks without parallel training signals (Pires et al., 2019; Wu and Dredze, 2019). A line of follow-up work (Wu et al., 2019; Artetxe et al., 2019; Karthikeyan et al., 2020) study what contributes to the cros"
2020.emnlp-main.359,N19-1392,0,0.0241274,"l training signals (Pires et al., 2019; Wu and Dredze, 2019). A line of follow-up work (Wu et al., 2019; Artetxe et al., 2019; Karthikeyan et al., 2020) study what contributes to the cross-lingual ability of these models. They show that vocabulary overlap is not required for multilingual models, and suggest that abstractions shared across languages emerge automatically during pretraining. Another line of research investigate how to further improve these shared knowledge, such as applying post-hoc alignment (Wang et al., 2020b; Cao et al., 2020) and utilizing better calibrated training signal (Mulcaire et al., 2019; Huang et al., 2019). While prior work emphasize how to share to improve transferability, we study multilingual models from a different perspective of how to unshare to resolve language conflicts. Our work is also related to transfer learning (Pan and Yang, 2010) and multi-task learning (Ruder, 2017). In particular, prior work have observed (Rosenstein et al., 2005) and studied (Wang et al., 2019) negative transfer, such that transferring knowledge from source tasks can degrade the performance in the target task. Others show it is important to remedy negative transfer in multi-source settings"
2020.emnlp-main.359,P17-1178,0,0.0203912,"more (over 100) languages. We use the Adam optimizer (Kingma and Ba, 2014) and exploit the same learning rate schedule as Lample and Conneau (2019). We train each model with 4 NVIDIA V100 GPUs with 32GB of memory. Using mixed precision, we fit a batch of 128 for each GPU and the total batch size is 512. Each epoch contains 10k steps and we train for 50 epochs. For evaluation, we consider four downstream tasks: named entity recognition (NER), part-ofspeech tagging (POS), question answering (QA), and natural language inference (NLI). (See Appendix A for finetuning details.) We use the WikiAnn (Pan et al., 2017) NER dataset, which is a sequence labelling task built automatically from Wikipedia. A linear layer with softmax classifier is added on top of pretrained models to predict the label for each word based on its first subword. We report the F1 score. POS Similar to NER, POS is also a sequence labelling task but with a focus on synthetic knowledge. In particular, we use the Universal Dependencies treebanks (Nivre et al., 2018). Task-specific layers are the same and we report F1, as in NER. We choose to use the TyDiQA-GoldP QA dataset (Clark et al., 2020) that covers typologically diverse languages"
2020.emnlp-main.359,2020.acl-main.754,1,0.874889,"Missing"
2020.emnlp-main.359,P19-1493,0,0.0758915,"Missing"
2020.emnlp-main.359,P18-2124,0,0.0184869,"Missing"
2020.emnlp-main.359,D19-1077,0,0.163427,"ailable at https://github.com/ iedwardwangi/MetaAdapter. transfer learning by pretraining a single shared Transformer model (Vaswani et al., 2017) jointly on multiple languages. The goals of multilingual modeling are not limited to improving language modeling in low-resource languages (Lample and Conneau, 2019), but also include zero-shot crosslingual transfer on downstream tasks—it has been shown that multilingual models can generalize to target languages even when labeled training data is only available in the source language (typically English) on a wide range of tasks (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). However, multilingual models are not equally beneficial for all languages. Conneau et al. (2019) demonstrated that including more languages in a single model can improve performance for lowresource languages but hurt performance for highresource languages. Similarly, recent work (Johnson et al., 2017; Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019) in multilingual neural machine translation (NMT) also observed performance degradation on high-resource language pairs. In multi-task learning (Ruder, 2017), this phenomenon is known as negative interference or"
2020.emnlp-main.359,P16-1162,0,0.00582906,".4 86.3 65.7 76.9 66.8 Table 2: NER and POS results. We observe negative interference when monolingual models outperform multilingual models. Besides, adding language-specific layers (e.g. ffn) mitigates interference but sacrifices transferability. high-resource languages {Arabic (Ar), French (Fr), Russian (Ru)} and three low-resource languages {Hindi (Hi), Swahili (Sw), Telugu (Te)} (see Table 1 for their statistics). We choose these six languages based their data availability in downstream tasks. We use Wikipedia as training data with statistics shown in Table 1. For each model, we use BPE (Sennrich et al., 2016) to learn 32k subword vocabulary shared between languages. For multilingual models, we sample language proportionally 1 to Pi = ( PLiLj ) T , where Li is the size of the j training corpus for i-th language pair and T is the temperature. Each model is a standard Transformer (Vaswani et al., 2017) with 8 layers, 12 heads, 512 embedding size and 2048 hidden dimension for the feedforward layer. Notice that we specifically consider a smaller model capacity to be comparable with existing models with larger capacity but also include much more (over 100) languages. We use the Adam optimizer (Kingma an"
2020.emnlp-main.359,P18-1072,0,0.026702,"Missing"
2020.emnlp-main.422,2020.lrec-1.497,0,0.0292855,"Missing"
2020.emnlp-main.44,W19-3804,0,0.259622,"ough word associations (Greenwald et al., 1998). However, the implicit nature of bias suggests that human annotations for bias detection may not be reliable, which motivates an unsupervised approach. The goals of our work align with prior work in NLP that has examined biases in real-world data. However, prior work examines bias at a broad corpus level or relies on supervised models. While corpus-level analyses, e.g. associations between gendered words and stereotypes, can be insightful (Bolukbasi et al., 2016; Fast et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Friedman et al., 2019; Chaloner and Maldonado, 2019), they are difficult to interpret over short text spans. They also often rely on human-defined “known” stereotypes, such as lists of traditionally male and female occupations obtained through crowd-sourcing, which restricts analysis to a narrow surface-level domain. Similarly, supervised approaches can provide insight into carefully defined types of bias (Wang and Potts, 2019; Breitfeller et al., 2019; Sap et al., 2020), but they rely on human annotations tasks, which are difficult to design or generalize to other domains, especially because social concepts differ across contexts and cultures"
2020.emnlp-main.44,P19-1167,0,0.023312,"aggressions (Breitfeller et al., 2019) and involves carefully constructed annotations schemes that are difficult to generalize to other data sets or types of bias. In contrast, our unsupervised approach is not limited to any particular domain and does not rely on human annotations, which can be subjective. Less-supervised approaches focus on corpuslevel analyses, such as associations between gendered terms and occupational stereotypes (Wagner et al., 2015; Bolukbasi et al., 2016; Fu et al., 2016; Joseph et al., 2017; Nakandala et al., 2017; Friedman et al., 2019; Chaloner and Maldonado, 2019; Hoyle et al., 2019). Methodologies for identifying gender-related differences in text have varied, including word-embedding similarity (Bolukbasi et al., 2016), language model perplexity (Fu et al., 2016), and predictive words identified by logistic regression (Nakandala et al., 2017). These metrics are meaningful over a corpus-level, but are often difficult to interpret over short text spans. Additionally, none of these methods focus on controlling for confounds. While matching is a well-established method for controlling for confounding variables in causality literature (Rosenbaum and Rubin, 1983, 1985; Stuart"
2020.emnlp-main.44,P19-1357,0,0.169629,"Missing"
2020.emnlp-main.44,2020.acl-main.474,0,0.0601868,"Missing"
2020.emnlp-main.44,W19-3803,0,0.091579,"e human perceptions through word associations (Greenwald et al., 1998). However, the implicit nature of bias suggests that human annotations for bias detection may not be reliable, which motivates an unsupervised approach. The goals of our work align with prior work in NLP that has examined biases in real-world data. However, prior work examines bias at a broad corpus level or relies on supervised models. While corpus-level analyses, e.g. associations between gendered words and stereotypes, can be insightful (Bolukbasi et al., 2016; Fast et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Friedman et al., 2019; Chaloner and Maldonado, 2019), they are difficult to interpret over short text spans. They also often rely on human-defined “known” stereotypes, such as lists of traditionally male and female occupations obtained through crowd-sourcing, which restricts analysis to a narrow surface-level domain. Similarly, supervised approaches can provide insight into carefully defined types of bias (Wang and Potts, 2019; Breitfeller et al., 2019; Sap et al., 2020), but they rely on human annotations tasks, which are difficult to design or generalize to other domains, especially because social concepts diffe"
2020.emnlp-main.44,D19-1425,1,0.912152,"in the pair. In this way, we balance the training set of COM TXT in terms of how predictive the confounding variable O TXT is of the target attribute W GEN . 598 3.2 Controlling Latent Confounding Variables through Adversarial Training While propensity matching is a desirable way to control for confounding variables because of established literature, matching is only possible for observed variables (Gu and Rosenbaum, 1993; Rosenbaum, 1988). In our data, while O TXT is observed, W TRAITS is not possible to match on (further discussion in §4). Instead, we use an adversarial objective drawn from Kumar et al. (2019) to encourage the model to ignore W TRAITS. Confound representation While we cannot explicitly enumerate W TRAITS, we know that they are associated with the identity of OW, and we can infer them from COM TXT addressed to OW. We use associations between OW and COM TXT to derive a feature vector for each COM TXTi that reflects W TRAITSi . The latent confounds to demote are represented as multinomial distributions, derived from log-odds scores (Monroe et al., 2008). For each label OW = k and each word type w in all COM TXT, we calculate the log-odds score lo(w, k) ∈ R, where higher scores indicat"
2020.emnlp-main.44,P19-1159,0,0.0153458,"concepts remain difficult to define and identify, especially for non-experts. Social biases appear to be a natural component of human cognition that allow people to make judgments efficiently (Kahneman et al., 1982). As a result, they are often implicit—people are unaware of their own biases (Blair, 2002; Bargh, 1999)—and manifest subtly, e.g., as microaggressions or condescension (Huckin, 2002; Sue, 2010). Much NLP literature has examined biases in data, algorithms, or model performance, and the negative pipeline between them: models absorb and amplify data biases, which impacts performance (Sun et al., 2019). However, little work has looked 1 Code and pre-trained models are available at https: //github.com/anjalief/unsupervised_ gender_bias further up the pipeline and relied on the assumption that biases in data originate in human cognition. In contrast, this assumption motivates our work: an unsupervised approach to detecting implicit gender bias in text. Text provides an ideal avenue for studying bias, because human cognition is closely tied to natural language. Psychology studies often examine human perceptions through word associations (Greenwald et al., 1998). However, the implicit nature of"
2020.emnlp-main.44,L18-1445,1,0.820448,"oni. We create a 66-term list of substitutions from existing resources (Zhao et al., 2018; Bolukbasi et al., 2016) as well as our observations of the data. We also use substitutions to remove the names of addressees from comment, replacing OW’s “Firstname” and “Lastname” with “hnamei” in COM TXT. We do not attempt to identify nicknames, as the confound demotion method described in §3.2 should already mitigate the influence of individual names, and we perform the substitution as merely an extra precaution. 4 Experimental Setup Our primary data is the Facebook subsection of the RtGender corpus (Voigt et al., 2018). The data contains two subsections: Politicians (400K posts and 13.9M replies addressed to 412 then-current U.S. 599 5 Evaluation We train our model to predict W GEN from COM TXT , employing propensity matching over O TXT , word substitutions over COM TXT , and W TRAITS demotion. We focus on evaluating how well our model controls for confounds and whether or not it captures gendered language. Successful demotion of confounds would suggest that our model learns to identify text indicative of gender bias. Observed Confounding Variable Demotion In Figure 2, we show log-odds scores, measuring ass"
2020.emnlp-main.44,2020.acl-main.486,0,0.554689,"tween gendered words and stereotypes, can be insightful (Bolukbasi et al., 2016; Fast et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Friedman et al., 2019; Chaloner and Maldonado, 2019), they are difficult to interpret over short text spans. They also often rely on human-defined “known” stereotypes, such as lists of traditionally male and female occupations obtained through crowd-sourcing, which restricts analysis to a narrow surface-level domain. Similarly, supervised approaches can provide insight into carefully defined types of bias (Wang and Potts, 2019; Breitfeller et al., 2019; Sap et al., 2020), but they rely on human annotations tasks, which are difficult to design or generalize to other domains, especially because social concepts differ across contexts and cultures (Dong et al., 2019). Our work offers a new approach to surfacing gender bias that does not require direct supervision and is meaningful at a sentence or paragraph level. We create a model that takes text in the 2nd -person perspective as input and predicts the gender of the person the text is addressed to. If the classifier predicts the gender of the addressee with high confidence based only on the text directed to them"
2020.emnlp-main.44,D18-1301,0,0.0140056,"educing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.1 1 Introduction Despite widespread documentation of the negative impacts of bias, stereotypes, and prejudice (Krieger, 1990; Goldin, 1990; Steele and Aronson, 1995; Logel et al., 2009; Schluter, 2018), these concepts remain difficult to define and identify, especially for non-experts. Social biases appear to be a natural component of human cognition that allow people to make judgments efficiently (Kahneman et al., 1982). As a result, they are often implicit—people are unaware of their own biases (Blair, 2002; Bargh, 1999)—and manifest subtly, e.g., as microaggressions or condescension (Huckin, 2002; Sue, 2010). Much NLP literature has examined biases in data, algorithms, or model performance, and the negative pipeline between them: models absorb and amplify data biases, which impacts perfo"
2020.emnlp-main.44,D19-1385,0,0.272929,"hile corpus-level analyses, e.g. associations between gendered words and stereotypes, can be insightful (Bolukbasi et al., 2016; Fast et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Friedman et al., 2019; Chaloner and Maldonado, 2019), they are difficult to interpret over short text spans. They also often rely on human-defined “known” stereotypes, such as lists of traditionally male and female occupations obtained through crowd-sourcing, which restricts analysis to a narrow surface-level domain. Similarly, supervised approaches can provide insight into carefully defined types of bias (Wang and Potts, 2019; Breitfeller et al., 2019; Sap et al., 2020), but they rely on human annotations tasks, which are difficult to design or generalize to other domains, especially because social concepts differ across contexts and cultures (Dong et al., 2019). Our work offers a new approach to surfacing gender bias that does not require direct supervision and is meaningful at a sentence or paragraph level. We create a model that takes text in the 2nd -person perspective as input and predicts the gender of the person the text is addressed to. If the classifier predicts the gender of the addressee with high confi"
2020.emnlp-main.44,N18-2003,0,0.0300929,"ng set. We normalize these vectors to obtain multinomial probability distributions which reflect COM TXT i ’s association with each OW individual. Thus, when we demote this vector during training, we force the classifier to learn features that are indicative of the group W GEN and not features that are indicative of individual members of this group Overt Signals Finally, we control for overt signals using word substitutions that replace gendered terms with more neutral language, for example woman → hpersoni and man → hpersoni. We create a 66-term list of substitutions from existing resources (Zhao et al., 2018; Bolukbasi et al., 2016) as well as our observations of the data. We also use substitutions to remove the names of addressees from comment, replacing OW’s “Firstname” and “Lastname” with “hnamei” in COM TXT. We do not attempt to identify nicknames, as the confound demotion method described in §3.2 should already mitigate the influence of individual names, and we perform the substitution as merely an extra precaution. 4 Experimental Setup Our primary data is the Facebook subsection of the RtGender corpus (Voigt et al., 2018). The data contains two subsections: Politicians (400K posts and 13.9M"
2020.emnlp-main.44,D19-1176,1,\N,Missing
2020.emnlp-main.622,P19-1357,0,0.0411291,"Missing"
2020.emnlp-main.622,N19-1423,0,0.0225029,"ple correctly. This alone can show that the outlier training example has a dubious label, regardless of its relationship to the probing example. For consistency, we define the influence of a training example to the mis-prediction of disguised offenses as: I(xtrn ) = L(θ, xtrn , ytrn ).9 3 3.1 Experiments Setup We use a popular toxic language detection tool, Perspective API by Jigsaw and Google, as the compromised classifier C. It builds upon a convolutional neural network with pretrained word embeddings and proprietary large labeled data. For the student model C 0 , we use a BERT-based model (Devlin et al., 2019), initialized with the pretrained weights and fine-tuned on our training set. Below we instantiate our training set D0 and a probing set P of veiled offenses. SBIC – Social Bias Inference Corpus (Sap et al., 2020) is a dataset containing 45K social media posts with crowdsourced annotations of offensiveness, intention, and targeted group from a variety of origins, including hate speech, offensive language, and microaggressions, and selected dangerous threads on Reddit (e.g., r/darkJokes) and 8 More influence metrics can be found in Yeh et al. (2018), Khanna et al. (2019), and Barshan et al. (20"
2020.emnlp-main.622,2020.acl-main.486,0,0.0438688,"is-prediction of disguised offenses as: I(xtrn ) = L(θ, xtrn , ytrn ).9 3 3.1 Experiments Setup We use a popular toxic language detection tool, Perspective API by Jigsaw and Google, as the compromised classifier C. It builds upon a convolutional neural network with pretrained word embeddings and proprietary large labeled data. For the student model C 0 , we use a BERT-based model (Devlin et al., 2019), initialized with the pretrained weights and fine-tuned on our training set. Below we instantiate our training set D0 and a probing set P of veiled offenses. SBIC – Social Bias Inference Corpus (Sap et al., 2020) is a dataset containing 45K social media posts with crowdsourced annotations of offensiveness, intention, and targeted group from a variety of origins, including hate speech, offensive language, and microaggressions, and selected dangerous threads on Reddit (e.g., r/darkJokes) and 8 More influence metrics can be found in Yeh et al. (2018), Khanna et al. (2019), and Barshan et al. (2020). We leave the exploration of them in our framework to future work. 9 Note that this method does not require probing examples. hate sites. We use SBIC as our base dataset. We consider three attributes in SBIC p"
2020.emnlp-main.622,2020.emnlp-main.44,1,0.836946,"veiled toxicity is hard: deep semantic analysis and large datasets are needed. But veiled offenses are not represented in existing toxicity datasets (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018) and building a new dataset is expensive: candidates for annotation cannot be filtered through lexicons and random sampling of social media posts will surface only a tiny fraction of relevant examples (Breitfeller et al., 2019). Moreover, since biased text is often unconscious and subjective, untrained annotators might mislabel it due to their own biases (Breitfeller et al., 2019; Field and Tsvetkov, 2020). Toxic Language in Disguise Toxic language has been recognized as a severe problem in the online social communities. While great efforts have been made to detect and prevent the spread of overt trolling, hate speech, abusive language, and toxic comments (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), they often build upon lexicon-based approaches (Waseem and Hovy, 2016; Davidson et al., 2017) and thus are ineffective at detecting forms of veiled toxicity; e.g., codewords (Taylor et al., 2017), novel forms of offense (Jain et al., 2018), and subtle and often unintentional manifestations"
2020.emnlp-main.622,W17-1101,0,0.0213947,"on cannot be filtered through lexicons and random sampling of social media posts will surface only a tiny fraction of relevant examples (Breitfeller et al., 2019). Moreover, since biased text is often unconscious and subjective, untrained annotators might mislabel it due to their own biases (Breitfeller et al., 2019; Field and Tsvetkov, 2020). Toxic Language in Disguise Toxic language has been recognized as a severe problem in the online social communities. While great efforts have been made to detect and prevent the spread of overt trolling, hate speech, abusive language, and toxic comments (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), they often build upon lexicon-based approaches (Waseem and Hovy, 2016; Davidson et al., 2017) and thus are ineffective at detecting forms of veiled toxicity; e.g., codewords (Taylor et al., 2017), novel forms of offense (Jain et al., 2018), and subtle and often unintentional manifestations of social bias such as microaggressions and condescension (Breitfeller et al., 2019; Wang and Potts, 2019). In this work, we focus on disguised toxic language that is often undetected by existing tools.2 It 1 Our code is available at https://github.com/ xhan77/veiled-toxicity-dete"
2020.emnlp-main.622,2020.acl-main.492,1,0.810956,"model. We can then use the chain rule to measure how this change in the model parameters would in turn affect the loss of the probing input: dL(θ, xprb , yˆprb ) dθ = ∇θ L(θ, xprb , yˆprb ) · , dtrn dtrn where yˆprb is the wrong label for the probing example, since we want to know which training examples lead to a wrong prediction of the probing disguised offense. The final influence of a train example to a probing example is defined as: dL(θ,xprb ,ˆ yprb ) I(xtrn , xprb ) = − . More details of dtrn influence functions and their applications in NLP can be found in Koh and Liang (2017) and Han et al. (2020).7 Gradient product Computing the inverse Hessian Hθ−1 in the influence functions is expensive and requires approximations if the model is nonconvex. If we ignore the inverse Hessian term, the calculation reduces to the dot product between the gradient of the training loss L(θ, xtrn , ytrn ) and the gradient of the probing loss L(θ, xprb , yˆprb ). 7733 7 Implementation details can be found in the Appendix. This method is discussed in Pruthi et al. (2020). Specifically, we adopt the TrackIn method, which defines the influence as: k X I(xtrn , xprb ) = ∇θ L(θi , xtrn , ytrn ) i=1 ·∇θ L(θi , xpr"
2020.emnlp-main.622,D19-1385,0,0.0269748,"n the online social communities. While great efforts have been made to detect and prevent the spread of overt trolling, hate speech, abusive language, and toxic comments (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), they often build upon lexicon-based approaches (Waseem and Hovy, 2016; Davidson et al., 2017) and thus are ineffective at detecting forms of veiled toxicity; e.g., codewords (Taylor et al., 2017), novel forms of offense (Jain et al., 2018), and subtle and often unintentional manifestations of social bias such as microaggressions and condescension (Breitfeller et al., 2019; Wang and Potts, 2019). In this work, we focus on disguised toxic language that is often undetected by existing tools.2 It 1 Our code is available at https://github.com/ xhan77/veiled-toxicity-detection. 2 We use the terms veiled and disguised toxicity interchangeably in this work. We propose a framework to surface veiled offenses and improve toxicity classifiers that are compromised in detecting them. It requires a small set of labeled probing examples to surface orders of magnitude more disguised offenses missed by the classifier, through interpretable ML techniques tracking the influence of training examples on"
2020.emnlp-main.622,N16-2013,0,0.035052,"detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.1 Warning: this paper contains examples that may be offensive or upsetting. 1 Detecting veiled toxicity is hard: deep semantic analysis and large datasets are needed. But veiled offenses are not represented in existing toxicity datasets (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018) and building a new dataset is expensive: candidates for annotation cannot be filtered through lexicons and random sampling of social media posts will surface only a tiny fraction of relevant examples (Breitfeller et al., 2019). Moreover, since biased text is often unconscious and subjective, untrained annotators might mislabel it due to their own biases (Breitfeller et al., 2019; Field and Tsvetkov, 2020). Toxic Language in Disguise Toxic language has been recognized as a severe problem in the online social communities. While great efforts have bee"
2020.emnlp-main.622,I08-1048,0,0.0344248,"e influence as: k X I(xtrn , xprb ) = ∇θ L(θi , xtrn , ytrn ) i=1 ·∇θ L(θi , xprb , yˆprb ), where θi is the checkpoint of the model at each training epoch. The intuition behind this method is to approximate the total reduction in the probing loss L(θ, xprb , yˆprb ) during the training process when the training example xtrn is used. More details on TrackIn can be found in Pruthi et al. (2020).8 Training loss Our last method to define the influence of training examples can be considered as a baseline which is often used in active learning as ‘uncertainty-based sampling’ (Lewis and Gale, 1994; Zhu et al., 2008). The intuition is that a training example with a high loss (low confidence) could indicate that the model struggles to predict that example correctly. This alone can show that the outlier training example has a dubious label, regardless of its relationship to the probing example. For consistency, we define the influence of a training example to the mis-prediction of disguised offenses as: I(xtrn ) = L(θ, xtrn , ytrn ).9 3 3.1 Experiments Setup We use a popular toxic language detection tool, Perspective API by Jigsaw and Google, as the compromised classifier C. It builds upon a convolutional n"
2020.ngt-1.7,D19-1387,0,0.0191781,".2 Implementation Details We use the Transformer-BASE model (Vaswani et al., 2017) as the underlying architecture for our model (E, D1 , D2 , extractive summarization model for distillation and baselines). We refer the reader to Vaswani et al. (2017) for hyperparameter details. In the input article, a special token hSEPi is added at the beginning of each sentence to mark sentence boundaries. For the CNN/DailyMail corpus, the monolingual extractive summarization used in the distillation objective has the same architecture as the encoder E and is trained the CNN/DailyMail corpus constructed by (Liu and Lapata, 2019). To train the encoder with Ldis , we take the final hidden representation of each hSEPi token and apply a 2-layer feed-forward network with ReLU activation in the middle layer and sigmoid at the final layer to get qi for each sentence i (see §2.2). For the Gigaword dataset, because the inputs and outputs are typically short, we choose keywords rather than sentences as the prediction unit. Specifically, we first use TextRank (Mihalcea and Tarau, 2004) to extract all the keywords from the source document. Then, for each keyword i that appears in the target summary, the gold label qi in equation"
2020.ngt-1.7,N19-1204,0,0.0286628,"Missing"
2020.ngt-1.7,W04-3252,0,0.0242028,"summarization used in the distillation objective has the same architecture as the encoder E and is trained the CNN/DailyMail corpus constructed by (Liu and Lapata, 2019). To train the encoder with Ldis , we take the final hidden representation of each hSEPi token and apply a 2-layer feed-forward network with ReLU activation in the middle layer and sigmoid at the final layer to get qi for each sentence i (see §2.2). For the Gigaword dataset, because the inputs and outputs are typically short, we choose keywords rather than sentences as the prediction unit. Specifically, we first use TextRank (Mihalcea and Tarau, 2004) to extract all the keywords from the source document. Then, for each keyword i that appears in the target summary, the gold label qi in equation 1 is assigned to 1, and qi is assigned to 0 for keywords that do not appear in the target side. We share the parameters of the bottom four layers of the decoder in the multi-task setting. We use the T RIGRAM model in (Wieting et al., 2019b,a) to measure the cross-lingual sentence semantic similarities. As pointed out in §2, after the pre-training stage, we only use D1 for XLS. The final results are obtained using only E and D1 . We use two metrics fo"
2020.ngt-1.7,N19-4007,1,0.875699,"glish–German ROUGE-2 ROUGE-L XSIM 28.19 32.17 11.40 13.85 25.77 29.43 - - - - - 37.38 40.23 41.25 42.19* 42.51* 42.83* 42.49 17.96 22.32 22.40 22.91* 22.96 23.30* 23.29* 33.85 36.59 37.93 38.74* 38.98* 39.29* 38.95 45.17 48.77 49.20* 49.65 50.85* 49.88* 23.06 36.83 37.74* 38.32* 38.69* 38.19* 8.40 17.62 18.40* 18.46 18.76* 18.17 21.28 35.54 36.34* 36.86* 37.20* 36.72* 43.41 49.21 49.53* 49.66 50.17* 49.64 Table 1: Performance of different models. The highest scores are in bold and statistical significance compared with the best baseline is indicated with * (p &lt;0.05, computed using compare-mt (Neubig et al., 2019)). X SIM is computed between the target language system outputs and the source language reference summaries. Method MLE- XLS +E XTRACT +D IS MLE- XLS + MT +E XTRACT +D IS translate-then-summarize (T RAN -S UM) pipelines. These results are taken from Zhu et al. (2019). MLE- XLS We pre-train E and D1 with only Lxls without any fine-tuning. MLE- XLS + MT We pre-train E, D1 and D2 with Lxls + Lmt without using Ldis . This is the best performing model in (Zhu et al., 2019). We show their reported results as well as results from our re-implementation. ROUGE-2 17.96 19.58 20.47 22.40 21.58 22.48 ROUG"
2020.ngt-1.7,N18-2102,0,0.0423548,"Missing"
2020.ngt-1.7,W14-3302,0,0.0819191,"Missing"
2020.ngt-1.7,D15-1044,0,0.0464972,"we obtain sentence representations for both yˆtgt and ysrc and treat the cosine similarity between the two representations as the reward r(·). 3 3.1 Experimental Setup Datasets We evaluate our models on English–Chinese and English–German article-summary datasets. The English–Chinese dataset is created by Zhu et al. (2019), constructed using the CNN/DailyMail monolingual summarization corpus (Hermann et al., 2015). The training, validation and test sets consist of about 364K, 3K and 3K samples, respectively. The English–German dataset is our contribution, constructed from the Gigaword dataset (Rush et al., 2015). We sample 2.48M training, 2K validation and 2K test samples from the dataset. Pseudoparallel corpora for both language pairs are constructed by translating the summaries to the target language (and filtered after back-translation; see §2). This is done for training, validation as well as test sets. These two pseudo-parallel training sets are used for pre-training with Lxls . Translated Chinese and German summaries of the test articles are then post-edited by human annotators to construct the test set for evaluating XLS. We refer the readers to (Zhu et al., 2019) for more details. For the Eng"
2020.ngt-1.7,P16-1162,0,0.00816734,"is done for training, validation as well as test sets. These two pseudo-parallel training sets are used for pre-training with Lxls . Translated Chinese and German summaries of the test articles are then post-edited by human annotators to construct the test set for evaluating XLS. We refer the readers to (Zhu et al., 2019) for more details. For the English–Chinese dataset, we use word-based segmentation for the source (articles in English) and character-based segmentation for the target (summaries in Chinese) as in (Zhu et al., 2019). For the English–German dataset, byte-pair encoding is used (Sennrich et al., 2016) with 60K merge operations. For machine translation and training the XSIM model, we sub-sample 5M sentences from the WMT2017 Chinese–English and 3.2 Implementation Details We use the Transformer-BASE model (Vaswani et al., 2017) as the underlying architecture for our model (E, D1 , D2 , extractive summarization model for distillation and baselines). We refer the reader to Vaswani et al. (2017) for hyperparameter details. In the input article, a special token hSEPi is added at the beginning of each sentence to mark sentence boundaries. For the CNN/DailyMail corpus, the monolingual extractive su"
2020.ngt-1.7,P18-1063,0,0.0470355,"Missing"
2020.ngt-1.7,P19-1305,0,0.0625985,"ement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines.1 1 Model Summary-ZH Lrl Gold Reference-EN Lcls Generated Reference-ZH Figure 1: Along with minimizing the XLS crossentropy loss Lxls , we also apply reinforcement learning to optimize the model by directly comparing the outputs with gold references in the source language. Prior studies have attempted to train XLS models in an end-to-end fashion, through knowledge distillation from pre-trained machine translation (MT) or monolingual summarization models (Ayana et al., 2018; Duan et al., 2019), but these approaches have been only shown to work for short outputs. Alternatively, Zhu et al. (2019) proposed to automatically translate source-language summaries in the training set thereby generating pseudo-reference summaries in the target language. With this parallel dataset of source documents and target summaries , an end-to-end model is trained to simultaneously summarize and translate using a multi-task objective. Although the XLS model is trained end-to-end, it is trained on MT-generated reference translations and is still prone to compounding of translation and summarization error"
2020.ngt-1.7,P11-1155,0,0.0669516,"ry in the target language and the gold summary in the source language. Additionally, to better initialize our XLS Introduction Cross-lingual text summarization (XLS) is the task of compressing a long article in one language into a summary in a different language. Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task. Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the summary into the target language or viceversa (Wan et al., 2010; Wan, 2011). Both of these approaches require separately trained summarization and translation models, and suffer from error propagation (Zhu et al., 2019). 1 https://github.com/zdou0830/ crosslingual_summarization_semantic. 60 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 60–68 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d which takes an article in the source language xsrc as input and generates its summary in a pre-specified target language yˆtgt = f (xsrc ; θ). Here, θ are the learnable parameters of"
2020.ngt-1.7,D18-1443,0,0.0345114,"Missing"
2020.ngt-1.7,P10-1094,0,0.16757,"ne-generated summary in the target language and the gold summary in the source language. Additionally, to better initialize our XLS Introduction Cross-lingual text summarization (XLS) is the task of compressing a long article in one language into a summary in a different language. Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task. Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the summary into the target language or viceversa (Wan et al., 2010; Wan, 2011). Both of these approaches require separately trained summarization and translation models, and suffer from error propagation (Zhu et al., 2019). 1 https://github.com/zdou0830/ crosslingual_summarization_semantic. 60 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 60–68 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d which takes an article in the source language xsrc as input and generates its summary in a pre-specified target language yˆtgt = f (xsrc ; θ). Here, θ are the learnable p"
2020.ngt-1.7,P19-1427,0,0.359203,"odel is trained to simultaneously summarize and translate using a multi-task objective. Although the XLS model is trained end-to-end, it is trained on MT-generated reference translations and is still prone to compounding of translation and summarization errors. In this work, we propose to train an end-to-end XLS model to directly generate target language summaries given the source articles by matching the semantics of the predictions with the semantics of the source language summaries. To achieve this, we use reinforcement learning (RL) with a bilingual semantic similarity metric as a reward (Wieting et al., 2019b). This metric is computed between the machine-generated summary in the target language and the gold summary in the source language. Additionally, to better initialize our XLS Introduction Cross-lingual text summarization (XLS) is the task of compressing a long article in one language into a summary in a different language. Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task. Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the su"
2020.ngt-1.7,P18-1013,0,0.043354,"Missing"
2020.ngt-1.7,P19-1453,0,0.40284,"odel is trained to simultaneously summarize and translate using a multi-task objective. Although the XLS model is trained end-to-end, it is trained on MT-generated reference translations and is still prone to compounding of translation and summarization errors. In this work, we propose to train an end-to-end XLS model to directly generate target language summaries given the source articles by matching the semantics of the predictions with the semantics of the source language summaries. To achieve this, we use reinforcement learning (RL) with a bilingual semantic similarity metric as a reward (Wieting et al., 2019b). This metric is computed between the machine-generated summary in the target language and the gold summary in the source language. Additionally, to better initialize our XLS Introduction Cross-lingual text summarization (XLS) is the task of compressing a long article in one language into a summary in a different language. Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task. Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the su"
2020.ngt-1.7,D19-1623,0,0.0367944,"Missing"
2020.ngt-1.7,W04-1013,0,0.106516,"in performance by up to 0.5 ROUGE-L points. Through extensive analyses and human evaluation, we show that when the bilingual semantic similarity reward is used, our model generates summaries that are more accurate, longer, more fluent, and more relevant than summaries generated by baselines. 2 • Use a machine translation (MT) model to generate pseudo reference summaries (˜ ytgt ) by translating ysrc to the target language. Then, translate y˜tgt back to the source language using a target-to-source MT model and discard the examples with high reconstruction errors, which are measured with ROUGE (Lin, 2004) scores. The details of this step can be found in Zhu et al. (2019). • Pre-train the model parameters θ using a multitask objective based on MT and monolingual summarization objectives with some simple yet effective techniques as described in §2.2. • Further fine-tune the model using reinforcement learning with bilingual semantic similarity metric (Wieting et al., 2019b) as reward, which is described in §2.3. 2.2 Here, we describe the second step of our algorithm (Figure 2). The pre-training loss we use is a weighted combination of three objectives. Similarly to Zhu et al. (2019), we use an XL"
2020.ngt-1.7,D15-1012,0,0.020325,"i-task pre-training also helps in improving the performance while at the same using fewer parameters and hence a smaller memory footprint. Effect of Summary Lengths Next, we study how different baselines and our model performs with respect to generating summaries (in Chinese) of different lengths, in terms of number of characters. As shown in Figure 3, after fine-tuning the model with RL, our proposed model becomes 5 Related Work Most previous work on cross-lingual text summarization utilize either the summarize-then-translate or translate-then-summarize pipeline (Wan et al., 2010; Wan, 2011; Yao et al., 2015; Ouyang et al., 65 Output 18 Ref Sup A bill to raise the legal age to buy cigarettes was voted into law Wednesday by the City Council. New York is the largest US city to raise the purchase age above the federal limit of 18-years-old. The law is expected to go into effect early next year. New York has become the largest purchase age in the United States. New York is not the first city to raise the legal drinking age. 18 RLROUGE 18 21 21 New York has become the largest purchase age in the United States, and the legal age has increased from 18 to 21. The City Council approved a law on Wednesday"
2020.scil-1.32,W18-3219,0,0.0576668,"Missing"
2020.scil-1.32,C18-1319,0,0.0643179,"Missing"
2020.scil-1.32,W18-3210,0,0.383612,"Missing"
2020.scil-1.32,L16-1260,0,0.434662,"Missing"
2020.scil-1.32,W18-3204,1,0.873366,"Missing"
2020.scil-1.32,W18-3211,1,0.875399,"Missing"
2020.scil-1.32,C82-1023,0,0.44955,"aneous speech (Deuchar et al., 2014), and a Twitter corpus (Molina et al., 2016). Examples of an utterance in each strategy along with the distribution of these strategies in both Twitter and Miami corpora are given in Table 1. We follow Muysken’s (2000) approach. The first strategy from Muysken (2000) is Insertional code-switching, which follows the Myers-Scotton framework of a Matrix Language (MatL) and an Embedded Language (EmbL). The structure and grammar of the MatL is maintained while inserting the EmbL (often single words or phrases) in certain spots (Myers-Scotton, 1993). According to Joshi (1982), closed class items such as determiners, quantifiers, etc., would remain in the MatL. This has also been shown to be more commonly used when the speakers are not equally proficient in both languages (Deuchar et al., 2007). We experiment with two conditions: (1) retaining the grammar of English while insertins ing Spanish nouns (SP !EN), and (2) using Spanish grammar while inserting English nouns ins (EN !SP). Next, we experiment with Alternational codeswitching, when the two languages remain more separate and alternate after clauses. Switch-points adhere to constituent boundaries (Sankoff and"
2020.scil-1.32,N13-2012,0,0.0155114,"rder to learn how these varieties may be linguistically or functionally comparative to our findings. The implications of our current work, which reveal which CS strategies are more entrainable than others, could help CS agents adapt to users and to better parse and predict user utterances with a more informed CS language model.13 Future agents should incorporate different CS strategies dynamically within a single conversation that entrain to the user. In order to move beyond a rule13 This approach is similar to a method where ASR systems that lexically entrain users can lower ASR error rates (Levitan, 2013). based agent, in future work we can leverage neural language generation systems (e.g., Park and Tsvetkov, 2019) trained on CS data. From here, we can usher in an era of bilingual dialogue systems that brings human–computer interactions to a more personalized space. 8 Acknowledgments We acknowledge helpful input from the anonymous reviewers. We also thank Gayatri Bhat, Cindy Blanco, Anjalie Field, Melinda Fricke, Shirley Anugrah Hayati, He He, Sachin Kumar, Chan Young Park, Anat Prior, Sai Krishna Rallabandi, Shruti Rijhwani, Shuly Wintner, and Yiheng Zhou for fruitful discussions. Finally, we"
2020.scil-1.32,D14-1098,0,0.0258074,"Missing"
2020.scil-1.32,P17-1162,0,0.0689247,"t need to collect examples of multilingual human–computer dialogues, a resource that does not yet exist. To collect human–computer dialogues in a controlled manner, we (1) modify an existing goaloriented dialogue framework to code-switch; (2) create multiple instances of code-switching dialogue systems, where each instance follows one pre-defined strategy of CS as described in §3; and (3) analyze collected dialogues and study how people communicate differently with dialogue agents following a particular strategy. We begin by modifying an existing goaloriented collaborative dialogue framework (He et al., 2017). The framework implements a scenario of discussing mutual friends given a knowledge base, private to each interlocutor. Each of the interlocutors has a list of friends with attributes such as hobby and major. Only one friend is the same across both lists, and the goal is to find that mutual friend via collaborative discussion over text chat. We extend this framework to a bilingual Spanish–English goal-oriented collaborative dialogue. In our bilingual interface, users see the private table of friends and attributes in both Spanish and English. To code-switch in language generation, we add modi"
2020.scil-1.32,P08-2043,0,0.642632,"Missing"
2020.scil-1.32,D19-5626,1,0.822923,"he implications of our current work, which reveal which CS strategies are more entrainable than others, could help CS agents adapt to users and to better parse and predict user utterances with a more informed CS language model.13 Future agents should incorporate different CS strategies dynamically within a single conversation that entrain to the user. In order to move beyond a rule13 This approach is similar to a method where ASR systems that lexically entrain users can lower ASR error rates (Levitan, 2013). based agent, in future work we can leverage neural language generation systems (e.g., Park and Tsvetkov, 2019) trained on CS data. From here, we can usher in an era of bilingual dialogue systems that brings human–computer interactions to a more personalized space. 8 Acknowledgments We acknowledge helpful input from the anonymous reviewers. We also thank Gayatri Bhat, Cindy Blanco, Anjalie Field, Melinda Fricke, Shirley Anugrah Hayati, He He, Sachin Kumar, Chan Young Park, Anat Prior, Sai Krishna Rallabandi, Shruti Rijhwani, Shuly Wintner, and Yiheng Zhou for fruitful discussions. Finally, we sincerely thank our annotator, Joshua Baumgarten. This work was supported by NSF grant IIS-1812327 and NSF GRFP"
2020.scil-1.32,P18-1143,0,0.115955,"Missing"
2020.scil-1.32,D19-5558,0,0.111858,"Missing"
2020.scil-1.32,W18-5009,0,0.0387168,"Missing"
2020.scil-1.32,P17-1180,0,0.20533,"Missing"
2020.scil-1.32,D16-1121,0,0.0449551,"Missing"
2020.scil-1.32,D08-1102,0,0.182558,"Missing"
2020.scil-1.32,W18-3201,0,0.0683939,"Missing"
2020.scil-1.43,C18-1135,0,0.0408748,"Missing"
2020.scil-1.43,D17-1118,0,0.0177716,"t applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguistic perspective, formalized in terms of distributional semantics. Hypothesis 1 Neologisms are more likely to emerge in sparser areas of the semantic space. This corresponds to the supply-driven neology hypothesis: we assume that areas of the space that contain fewer semantically related wor"
2020.scil-1.43,W16-2506,1,0.922812,"ord embeddings into the HISTORICAL space, we multiply them by the obtained rotation matrix R. 4.4 Control set selection To test our hypotheses, we collect an alternative set of words and analyze how certain statistical properties of their neighbors differ from those of neighbors of neologisms. At this stage it is important to control for non-semantic confounding factors that might affect the word distribution in the semantic space. One such factor is word frequency: it has been shown that embeddings of words of similar frequency tend to be closer in the embedding space (Schnabel et al., 2015; Faruqui et al., 2016), which results in very dense clusters, or hubs, of words with high cosine similarity (Radovanovi´c et al., 2010; Dinu et al., 2014). We choose to also restrict our control set to only include words that did not substantially grow or decline in frequency over the HISTORICAL period in order to prevent selecting counterparts that only share similar frequency in the MODERN subcorpus (e.g., due to recent topical relevance), but exhibit significant fluctuation prior to that period. In particular, we refrain from selecting words that emerged in language right before our HISTORI CAL - MODERN split. W"
2020.scil-1.43,P16-1141,0,0.143841,"s rather than modeling their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced e"
2020.scil-1.43,P18-2032,0,0.0218419,"spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguistic perspective, formalized in terms of distributional semantics. Hypothesis 1 Neologisms are more likely to emerge in sparser areas of the semantic space. This corresponds to the supply-driven neology hypothesis: we assume that areas of the space that contain fewer semantically related words are likely to give birth to new ones so as to fill in the ‘semantic gaps’. Word embeddings give us a natural way of formalizing this: since semantically related words have been shown to populate the same regions"
2020.scil-1.43,C18-1117,0,0.0134403,"their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguis"
2020.scil-1.43,reese-etal-2010-wikicorpus,0,0.0316468,"Missing"
2020.scil-1.43,D15-1036,0,0.0235291,"r). To project MODERN word embeddings into the HISTORICAL space, we multiply them by the obtained rotation matrix R. 4.4 Control set selection To test our hypotheses, we collect an alternative set of words and analyze how certain statistical properties of their neighbors differ from those of neighbors of neologisms. At this stage it is important to control for non-semantic confounding factors that might affect the word distribution in the semantic space. One such factor is word frequency: it has been shown that embeddings of words of similar frequency tend to be closer in the embedding space (Schnabel et al., 2015; Faruqui et al., 2016), which results in very dense clusters, or hubs, of words with high cosine similarity (Radovanovi´c et al., 2010; Dinu et al., 2014). We choose to also restrict our control set to only include words that did not substantially grow or decline in frequency over the HISTORICAL period in order to prevent selecting counterparts that only share similar frequency in the MODERN subcorpus (e.g., due to recent topical relevance), but exhibit significant fluctuation prior to that period. In particular, we refrain from selecting words that emerged in language right before our HISTOR"
2020.scil-1.43,D18-1467,0,0.104121,", there are factors that predict where new words emerge other than the availability of semantic space. Demand, we hypothesize, plays a role as well as supply. Most existing computational research on the mechanisms of neology focuses on discovering sociolinguistic factors that predict acceptance of emerging words into the mainstream language and growth of their usage, typically in online social communities (Del Tredici and Fern´andez, 2018). The sociolinguistic factors can include geography (Eisenstein, 2017), user demographics (Eisenstein et al., 2012, 2014), diversity of linguistic contexts (Stewart and Eisenstein, 2018) or word form (Kershaw et al., 2016). To the best of our knowledge, there is no prior work focused on discovering factors predictive of the emergence of new words rather than modeling their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shi"
2020.semeval-1.230,W18-3507,1,0.834655,"York Times.3 We scraped articles from 2018 to mid-2019 to ensure that the articles follow the same topical distribution as that of the training articles. We provide the unlabeled data, split to sentences, and train BERT on masked LM and next sentence prediction losses (Devlin et al., 2019). We leverage the pipeline provided by the Huggingface transformers library.4 2.4 Class-Imbalance Since our corpus suffers from class imbalance in both sentence classification and word-level span identification task, we associate higher weight to the loss incurred by samples of the minority class. Following Khosla (2018), we calculate these weights as inverse of the normalized P frequency of samples of each class and plug them into the cross-entropy loss. Namely L = − N1 N n=1 wn ∗ lossn , where wn is the weight associated with the loss for each sample. 3 Experimental Setup We used the BERT model from hugging-face library which was then fine-tuned on the corpus. Since the test-set was not made available during the competition, we hold out a small part of the training data as dev-set which was used to tune the models. The submitted model was chosen based on its Span-level Normalized F1-Score on the validation"
2020.semeval-1.230,2020.lrec-1.94,1,0.732679,"s-entropy loss with sigmoid activation for sentence-level classification, whereas the token-level classification is trained using a cross-entropy loss with softmax activation. The losses Lsent , Ltok are jointly optimized using hyper-parameter α to control the strength of both losses: L = αLsent + (1 − α)Ltok . 2.2 (4) Additional Features We incorporate additional lexical, syntactic, linguistic, and topical features at word, sentence and document levels to better inform the model. Syntactic Features: Inspired by prior work that leveraged syntactic features effectively (Vashishth et al., 2018; Kumar et al., 2020), and motivated by the observation that many propaganda spans are persuasive or idiomatic phrases, we extract phrasal features from constituency parse trees to explicitly incorporate structural syntactic information. We encode the path from a word to the root in the parse tree as a dc dimensional embedding.2 Stanford CoreNLP (Manning et al., 2014) was used to extract the constituency parses as well as part-of-speech tags that were also used as features. Affective and Semantic Features: In addition to structural cues, prior work has shown that propaganda is marked with affective and emotional w"
2020.semeval-1.230,P14-5010,0,0.0026891,"orporate additional lexical, syntactic, linguistic, and topical features at word, sentence and document levels to better inform the model. Syntactic Features: Inspired by prior work that leveraged syntactic features effectively (Vashishth et al., 2018; Kumar et al., 2020), and motivated by the observation that many propaganda spans are persuasive or idiomatic phrases, we extract phrasal features from constituency parse trees to explicitly incorporate structural syntactic information. We encode the path from a word to the root in the parse tree as a dc dimensional embedding.2 Stanford CoreNLP (Manning et al., 2014) was used to extract the constituency parses as well as part-of-speech tags that were also used as features. Affective and Semantic Features: In addition to structural cues, prior work has shown that propaganda is marked with affective and emotional words and phrases (Gupta et al., 2019; Alhindi et al., 2019). Motivated by that, we append to word embeddings of the ith token (after BERT) features extracted using affective lexicons, including the LIWC (Tausczik and Pennebaker, 2010) dictionary, NRC lexicons of Word Emotion, VAD and Affect Intensity (Mohammad and Turney, 2013; Mohammad, 2018a; Mo"
2020.semeval-1.230,P18-1017,0,0.0118142,"nning et al., 2014) was used to extract the constituency parses as well as part-of-speech tags that were also used as features. Affective and Semantic Features: In addition to structural cues, prior work has shown that propaganda is marked with affective and emotional words and phrases (Gupta et al., 2019; Alhindi et al., 2019). Motivated by that, we append to word embeddings of the ith token (after BERT) features extracted using affective lexicons, including the LIWC (Tausczik and Pennebaker, 2010) dictionary, NRC lexicons of Word Emotion, VAD and Affect Intensity (Mohammad and Turney, 2013; Mohammad, 2018a; Mohammad, 2018b). We also assign a score to each token that corresponds to the frequency of the word in the propaganda spans as opposed to the non-propaganda ones. For example, words like ‘invader’, will have a high score, as it is salient to propaganda spans. Furthermore, we incorporate semantic class features. These include named entities (such as Person, Place, Temporal Expressions) as identified by the CoreNLP NER tagger (Manning et al., 2014) and finer-grained topical categories from Empath (Fast et al., 2016) (such as Government, War, Violence). These word-level features (Fword ) are"
2020.semeval-1.230,L18-1027,0,0.0287622,"nning et al., 2014) was used to extract the constituency parses as well as part-of-speech tags that were also used as features. Affective and Semantic Features: In addition to structural cues, prior work has shown that propaganda is marked with affective and emotional words and phrases (Gupta et al., 2019; Alhindi et al., 2019). Motivated by that, we append to word embeddings of the ith token (after BERT) features extracted using affective lexicons, including the LIWC (Tausczik and Pennebaker, 2010) dictionary, NRC lexicons of Word Emotion, VAD and Affect Intensity (Mohammad and Turney, 2013; Mohammad, 2018a; Mohammad, 2018b). We also assign a score to each token that corresponds to the frequency of the word in the propaganda spans as opposed to the non-propaganda ones. For example, words like ‘invader’, will have a high score, as it is salient to propaganda spans. Furthermore, we incorporate semantic class features. These include named entities (such as Person, Place, Temporal Expressions) as identified by the CoreNLP NER tagger (Manning et al., 2014) and finer-grained topical categories from Empath (Fast et al., 2016) (such as Government, War, Violence). These word-level features (Fword ) are"
2020.semeval-1.230,D17-1317,0,0.061674,"ers are not able to identify it, but their opinions are shaped according to the propagandist’s hidden agenda (Barrón-Cedeño et al., 2019); it is thus very difficult to identify propaganda automatically. However, automatic identification and analysis of propaganda in news articles and on social media are essential to understand propaganda at scale and develop approaches to countering it (King et al., 2017; Starbird, 2018; Field et al., 2018). Prior research on propaganda detection focused primarily on identifying propaganda at a document level, due to the dearth of finer-grained labelled data (Rashkin et al., 2017; Barrón-Cedeño et al., 2019). This has resulted in classifying all news articles from a particular source as propaganda (or hoax, disinformation, etc.), which is often not the case, and which can obfuscate a finer-grained analysis of propagandistic strategies (Da San Martino et al., 2019; Horne et al., 2018). Recently, Da San Martino et al. (2019) carried out a seminal task of fine-grained propaganda detection and curated a dataset consisting of about 550 news articles. The dataset contains word-span level annotations provided by high-quality professionals along with additional information ab"
2020.semeval-1.230,D18-1157,1,0.846797,"tok ). (3) We use a cross-entropy loss with sigmoid activation for sentence-level classification, whereas the token-level classification is trained using a cross-entropy loss with softmax activation. The losses Lsent , Ltok are jointly optimized using hyper-parameter α to control the strength of both losses: L = αLsent + (1 − α)Ltok . 2.2 (4) Additional Features We incorporate additional lexical, syntactic, linguistic, and topical features at word, sentence and document levels to better inform the model. Syntactic Features: Inspired by prior work that leveraged syntactic features effectively (Vashishth et al., 2018; Kumar et al., 2020), and motivated by the observation that many propaganda spans are persuasive or idiomatic phrases, we extract phrasal features from constituency parse trees to explicitly incorporate structural syntactic information. We encode the path from a word to the root in the parse tree as a dc dimensional embedding.2 Stanford CoreNLP (Manning et al., 2014) was used to extract the constituency parses as well as part-of-speech tags that were also used as features. Affective and Semantic Features: In addition to structural cues, prior work has shown that propaganda is marked with affe"
2020.socialnlp-1.2,W17-1101,0,0.054745,"al training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification. 1 Introduction The prevalence of toxic comments on social media and the mental toll on human moderators has generated much interest in automated systems for detecting hate speech and abusive language (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), especially language that targets particular social groups (Silva et al., 2016; Mondal et al., 2017; Mathew et al., 2019). However, deploying these systems without careful consideration of social context can increase bias, marginalization, and exclusion (Bender and Friedman, 2018; Waseem and Hovy, 2016). Most datasets currently used to train hate speech classifiers were collected through crowdsourced annotations (Davidson et al., 2017; Founta et al., 2018), despite the risk of annotator bias. Waseem (2016) show that non-experts are more likely to label text as abusiv"
2020.socialnlp-1.2,P17-2009,1,0.835226,"e language. After adversarial training, the FPR rate decreases with only minor changes in accuracy. However, checkpoints with lower FPR rates also often have lower accuracy. While Tables 2 and 3 suggest that our model does achieve a balance between these Related Work Preventing neural models from absorbing or even amplifying unwanted artifacts present in datasets is indispensable towards building machine learning systems without unwanted biases. One thread of work focuses on removing bias at the data level, through reducing annotator bias (Sap et al., 2019) and augmenting imbalanced datasets (Jurgens et al., 2017). Dixon et al. (2018) propose an unsupervised method based on balancing the training set and employing a proposed measurement for mitigating unintended bias in text classification models. Webster et al. (2018) present a gender-balanced dataset with ambiguous name-pair pronouns to provide diversity coverage for realworld data. In addition to annotator bias, sampling 11 AAE Accuracy AAE Accuracy single adversary 0.80 income-level in any other text classification task. Overall, our approach has the potential to improve fairness and reduce bias in NLP models. multiple adversaries DWMW17 0.75 0.70"
2020.socialnlp-1.2,D19-1425,1,0.915609,", and Sap et al. (2019) show how lack of social context in annotation tasks further increases the risk 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 7–14 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 xi is the input text, yi is the label for the target attribute and zi is label of the protected attribute. The (xi , yi ) tuples are used to train the classifier C, and the (xi , zi ) tuple is used to train the adversary D. We adapt a two-phase training procedure from Kumar et al. (2019). We use this procedure because Kumar et al. (2019) show that their model is more effective than alternatives in a setting similar to ours, where the lexical indicators of the target and protected attributes are closely connected (e.g., words that are common in non-abusive AAE and are also common in abusive language datasets). In the first phase (pre-training), we use the standard supervised training objective to update encoder H and classifier C: to predict a target attribute (here, toxicity) without basing predictions on a protected attribute (here, AAE). Although we aim at preserving the ut"
2020.socialnlp-1.2,W16-5618,0,0.149308,"s for detecting hate speech and abusive language (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), especially language that targets particular social groups (Silva et al., 2016; Mondal et al., 2017; Mathew et al., 2019). However, deploying these systems without careful consideration of social context can increase bias, marginalization, and exclusion (Bender and Friedman, 2018; Waseem and Hovy, 2016). Most datasets currently used to train hate speech classifiers were collected through crowdsourced annotations (Davidson et al., 2017; Founta et al., 2018), despite the risk of annotator bias. Waseem (2016) show that non-experts are more likely to label text as abusive than expert annotators, and Sap et al. (2019) show how lack of social context in annotation tasks further increases the risk 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 7–14 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 xi is the input text, yi is the label for the target attribute and zi is label of the protected attribute. The (xi , yi ) tuples are used to train the classifier C, and the (xi , zi )"
2020.socialnlp-1.2,N16-2013,0,0.043035,"nimally affecting the performance of hate speech classification. 1 Introduction The prevalence of toxic comments on social media and the mental toll on human moderators has generated much interest in automated systems for detecting hate speech and abusive language (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), especially language that targets particular social groups (Silva et al., 2016; Mondal et al., 2017; Mathew et al., 2019). However, deploying these systems without careful consideration of social context can increase bias, marginalization, and exclusion (Bender and Friedman, 2018; Waseem and Hovy, 2016). Most datasets currently used to train hate speech classifiers were collected through crowdsourced annotations (Davidson et al., 2017; Founta et al., 2018), despite the risk of annotator bias. Waseem (2016) show that non-experts are more likely to label text as abusive than expert annotators, and Sap et al. (2019) show how lack of social context in annotation tasks further increases the risk 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 7–14 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://"
2020.socialnlp-1.2,Q18-1042,0,0.0206795,"ur model does achieve a balance between these Related Work Preventing neural models from absorbing or even amplifying unwanted artifacts present in datasets is indispensable towards building machine learning systems without unwanted biases. One thread of work focuses on removing bias at the data level, through reducing annotator bias (Sap et al., 2019) and augmenting imbalanced datasets (Jurgens et al., 2017). Dixon et al. (2018) propose an unsupervised method based on balancing the training set and employing a proposed measurement for mitigating unintended bias in text classification models. Webster et al. (2018) present a gender-balanced dataset with ambiguous name-pair pronouns to provide diversity coverage for realworld data. In addition to annotator bias, sampling 11 AAE Accuracy AAE Accuracy single adversary 0.80 income-level in any other text classification task. Overall, our approach has the potential to improve fairness and reduce bias in NLP models. multiple adversaries DWMW17 0.75 0.70 7 0 5 10 FDCL18 15 20 0 5 10 Epochs 15 20 We gratefully thank anonymous reviewers, Maarten Sap, and Dallas Card for their help with this work. The second author of this work is supported by the NSF Graduate Re"
2020.socialnlp-1.2,P18-2005,0,0.0168973,"Methodology Our goal is to train a model that can predict a target attribute (abusive or not abusive language), but that does not base decisions off of confounds in data that result from protected attributes (e.g., AAE dialect). In order to achieve this, we use an adversarial objective, which discourages the model from encoding information about the protected attribute. Adversarial training is widely known for successfully adapting models to learn representations that are invariant to undesired attributes, such as demographics and topics, though they rarely disentangle attributes completely (Li et al., 2018; Elazar and Goldberg, 2018; Kumar et al., 2019; Lample et al., 2019; Landeiro et al., 2019). min D min H,C N 1 X L(D(H(xi )), zi ) N 1 N i=1 N X α · L(C(H(xi )), yi )+ i=1 (2) (3) (1 − α) · L(D(H(xi )), 0.5) Unlike Kumar et al. (2019), we introduce a hyper-parameter α, which controls the balance between the two loss terms in Equation 3. We find that α is crucial for correctly training the model (we detail this in §3). We first train the adversary to predict the protected attribute from the text representations outputted by the encoder. We then train the encoder to “fool” the adversary by gene"
2020.socialnlp-1.2,N19-1060,0,0.0397946,"PITU-Carnegie Mellon University-Subgrant-009246-2019-10-01 for supporting this research. 0.92 0.90 0.88 Acknowledgements Figure 2: Validation accuracy on AAE prediction of the adversary in the whole training process. The green line denotes the training setting of one adversary and the orange line denotes the training setting of multiple adversaries. strategies also result in topic and author bias in datasets of abusive language detection, leading to decreased classification performance when testing in more realistic settings, necessitating the adoption of cross-domain evaluation for fairness (Wiegand et al., 2019). A related thread of work on debiasing focuses at the model level (Zhao et al., 2019). Adversarial training has been used to remove protected features from word embeddings (Xie et al., 2017; Zhang et al., 2018) and intermediate representations for both texts (Elazar and Goldberg, 2018; Zhang et al., 2018) and images (Edwards and Storkey, 2015; Wang et al., 2018). Though previous works have documented that adversarial training fails to obliterate protected features, Kumar et al. (2019) show that using multiple adversaries more effectively forces the removal. Along similar lines, multitask lear"
2020.socialnlp-1.2,P19-1163,0,0.254831,"pecially language that targets particular social groups (Silva et al., 2016; Mondal et al., 2017; Mathew et al., 2019). However, deploying these systems without careful consideration of social context can increase bias, marginalization, and exclusion (Bender and Friedman, 2018; Waseem and Hovy, 2016). Most datasets currently used to train hate speech classifiers were collected through crowdsourced annotations (Davidson et al., 2017; Founta et al., 2018), despite the risk of annotator bias. Waseem (2016) show that non-experts are more likely to label text as abusive than expert annotators, and Sap et al. (2019) show how lack of social context in annotation tasks further increases the risk 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 7–14 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 xi is the input text, yi is the label for the target attribute and zi is label of the protected attribute. The (xi , yi ) tuples are used to train the classifier C, and the (xi , zi ) tuple is used to train the adversary D. We adapt a two-phase training procedure from Kumar et al. (2019). We"
2020.socialnlp-1.2,N19-1064,0,0.0755985,"Missing"
2020.socialnlp-1.2,D17-1323,0,0.0675497,"Missing"
2020.socialnlp-1.2,D16-1120,0,\N,Missing
2020.socialnlp-1.2,Q18-1041,0,\N,Missing
2021.acl-long.149,W17-3001,0,0.058196,"Missing"
2021.acl-long.149,W19-3504,0,0.136189,"different systems. As the training data for the shared task was standardized, all models were trained on the same data. However, participants could have used external training data or pre-trained embeddings, so a more detailed investigation of results is needed to ascertain which factors most contribute to disparate performance. Model Outputs Several papers focus on model outcomes, and how NLP systems could perpetuate and amplify bias if they are deployed: • Classifiers trained on common abusive language data sets are more likely to label tweets containing characteristics of AAE as offensive (Davidson et al., 2019; Sap et al., 2019). • Classifiers for abusive language are more likely to label text containing identity terms like ‘black’ as offensive (Dixon et al., 2018). • GPT outputs text with more negative sentiment when prompted with AAE -like inputs (Groenwold et al., 2020). Social Analyses of Outputs While the examples in this section primarily focus on racial biases in trained NLP systems, other work (e.g. included in ‘Social Science/Social Media’ in Table 1) uses NLP tools to analyze race in society. Examples include examining how commentators describe football players of different races (Merullo"
2021.acl-long.149,N19-1304,0,0.0631039,"Missing"
2021.acl-long.149,P11-1137,0,0.0929969,"Missing"
2021.acl-long.149,2020.acl-main.560,0,0.0280032,"uct the same research are perceived as doing cutting-edge research that demands attention and recognition.” While we draw examples about race from HCI in the absence of published work on these topics in NLP, the lack of linguistic diversity in NLP research similarly demonstrates how representation does not necessarily imply inclusion. Although researchers from various parts of the world (Asia, in particular) do have some numerical representation among ACL authors, attendees, and fellows, NLP research overwhelmingly favors a small set of languages, with a heavy skew towards European languages (Joshi et al., 2020) and ‘standard’ language varieties (Kumar et al., 2021). 5.3 People use models Finally, NLP research produces technology that is used by people, and even work without direct applications is typically intended for incorporation into application-based systems. With the recognition that technology ultimately affects people, researchers on ethics in NLP have increasingly called for considerations of whom technology might harm and suggested that there are some NLP technologies that should not be built at all. In the context of perpetuating racism, examples include criticism of tools for predicting"
2021.acl-long.149,I17-1093,0,0.0670635,"Missing"
2021.acl-long.149,P19-1357,0,0.0543411,"Missing"
2021.acl-long.149,W12-1008,0,0.065274,"Missing"
2021.acl-long.149,W19-3806,0,0.0575746,"Missing"
2021.acl-long.149,2020.lrec-1.180,0,0.0405077,"Missing"
2021.acl-long.149,2020.acl-main.487,0,0.0153243,"pipelines • Research on race is restricted to a narrow subset of tasks and definitions of race, which can mask harms and falsely reify race as ‘natural’ • Traditionally underrepresented people are excluded from the research process, both as consumers and producers of technology Furthermore, while we focus on race, which we note has received substantially less attention than gender, many of the observations in this work hold for social characteristics that have received even less attention in NLP research, such as socioeconomic class, disability, or sexual orientation (Mendelsohn et al., 2020; Hutchinson et al., 2020). Nevertheless, none of these challenges can be addressed without direct engagement with marginalized communities of color. NLP researchers can draw on precedents for this type of engagement from other fields, such as participatory design and value sensitive design models (Friedman et al., 2013). Additionally, numerous organizations already exist that serve as starting points for partnerships, such as Black in AI, Masakhane, Data for Black Lives, and the Algorithmic Justice League. Finally, race and language are complicated, and while readers may look for clearer recommendations, no one data s"
2021.acl-long.149,W18-1501,0,0.077133,"Missing"
2021.acl-long.149,2020.acl-main.483,0,0.0552242,"Missing"
2021.acl-long.149,S18-2005,0,0.0284194,"st activists assign different offensive language labels to tweets than figure-eight workers, demonstrating that annotators’ lived experiences affect data annotations. Models Some papers have found evidence that model instances or architectures can change the racial biases of outputs produced by the model. Sommerauer and Fokkens (2019) find that the word embedding associations around words like ‘race’ and ‘racial’ change not only depending on the model architecture used to train embeddings, but also on the specific model instance used to extract them, perhaps because of differing random seeds. Kiritchenko and Mohammad (2018) examine gender and race biases in 200 sentiment analysis systems submitted to a shared task and find different levels of bias in different systems. As the training data for the shared task was standardized, all models were trained on the same data. However, participants could have used external training data or pre-trained embeddings, so a more detailed investigation of results is needed to ascertain which factors most contribute to disparate performance. Model Outputs Several papers focus on model outcomes, and how NLP systems could perpetuate and amplify bias if they are deployed: • Classif"
2021.acl-long.149,2020.gebnlp-1.2,0,0.0319953,"r reflect the experience of people who face discrimination along both axes (e.g. Black women) (Crenshaw, 1989). A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in any other NLP research areas. Further, several of these papers use NLP technology to examine or validate theories on intersectionality; they do not draw from theory on intersectionality to critically examine NLP models. These omissions can mask harms: Jiang and Fellbaum (2020) provide an example using word embeddings of how failing to consider intersectionality can render invisible people marginalized in multiple ways. Numerous directions remain for exploration, such as how ‘debiasing’ models along one social dimension affects other dimensions. Surveys in HCI offer further frameworks on how to incorporate identity and intersectionality into computational research (Schlesinger et al., 2017; Rankin and Thomas, 2019). 4.3 NLP research on race is restricted to specific tasks and applications Finally, Table 1 reveals many common NLP applications where race has not been"
2021.acl-long.149,W19-3823,1,0.822607,"s are not intended to be exhaustive, and in §4 we describe some of the ways that NLP literature has failed to engage with race, but nevertheless, we present them as evidence that NLP systems perpetuate harmful biases along racialized lines. Data A substantial amount of prior work has already shown how NLP systems, especially word embeddings and language models, can absorb and amplify social biases in data sets (Bolukbasi et al., 2016; Zhao et al., 2017). While most work focuses on gender bias, some work has made similar observations about racial bias (Rudinger et al., 2017; Garg et al., 2018; Kurita et al., 2019). These studies focus on how training data might describe racial minorities in biased ways, for example, by examining words associated with terms like ‘black’ or traditionally European/African American names (Caliskan et al., 2017; Manzini et al., 2019). Some studies additionally capture who is described, revealing under-representation in training data, sometimes tangentially to primary research questions: Rudinger et al. (2017) suggest that gender bias may be easier to identify than racial or ethnic bias in Natural Language Inference data sets because of data sparsity, and Caliskan et al. (20"
2021.acl-long.149,2020.alw-1.17,0,0.0369369,"Missing"
2021.acl-long.149,S19-1010,0,0.0543842,"Missing"
2021.acl-long.149,W19-3655,0,0.104867,"Missing"
2021.acl-long.149,W17-3503,0,0.05535,"Missing"
2021.acl-long.149,W14-2702,0,0.135659,"Missing"
2021.acl-long.149,D19-1580,0,0.0620699,"Missing"
2021.acl-long.149,2021.acl-long.416,0,0.063973,"Missing"
2021.acl-long.149,2020.emnlp-main.154,0,0.0316555,"neralize poorly to Twitter (WoodDoughty et al., 2018), and names common among Black and white children were not distinctly different prior to the 1970s (Fryer Jr and Levitt, 2004; Sweeney, 2013). We focus on these 3 data sets as they were most common in the papers we surveyed, but we note that others exist. Preot¸iuc-Pietro and Ungar (2018) provide a data set of tweets with self-identified race of their authors, though it is little used in subsequent work and focused on demographic prediction, rather than evaluating model performance gaps. Two recently-released data sets (Nadeem et al., 2020; Nangia et al., 2020) provide crowd-sourced pairs of more- and less-stereotypical text. More work is needed to understand any privacy concerns and the strengths and limitations of these data (Blodgett et al., 2021). Additionally, some papers collect domain-specific data, such as self-reported race in an online community (Loveys et al., 2018), or crowd-sourced annotations of perceived race of football players (Merullo et al., 2019). While these works offer clear contextualization, it is difficult to use these data sets to address other research questions. 4.2 Classification schemes operationalize race as a fixed, s"
2021.acl-long.149,W17-1609,0,0.0685667,"Missing"
2021.acl-long.149,R19-1123,0,0.0408206,"Missing"
2021.acl-long.149,P19-1163,0,0.420823,"ety have shifted considerably in the last half century; see King (2020) for an overview. 6 https://bit.ly/2Yv07IL 7 https://bit.ly/3j2weZA 1907 generated by them. Data Labels Although model biases are often blamed on raw data, several of the papers we survey identify biases in the way researchers categorize or obtain data annotations. For example: • Annotation schema Returning to Blodgett et al. (2018), this work defines new parsing standards for formalisms common in AAE, demonstrating how parsing labels themselves were not designed for racialized language varieties. • Annotation instructions Sap et al. (2019) show that annotators are less likely to label tweets using AAE as offensive if they are told the likely language varieties of the tweets. Thus, how annotation schemes are designed (e.g. what contextual information is provided) can impact annotators’ decisions, and failing to provide sufficient context can result in racial biases. • Annotator selection Waseem (2016) show that feminist/anti-racist activists assign different offensive language labels to tweets than figure-eight workers, demonstrating that annotators’ lived experiences affect data annotations. Models Some papers have found eviden"
2021.acl-long.149,2020.acl-main.486,0,0.0777685,"Missing"
2021.acl-long.149,W17-1611,0,0.0438955,"Missing"
2021.acl-long.149,2020.acl-main.468,0,0.0716647,"Missing"
2021.acl-long.149,2020.nuse-1.15,0,0.0815321,"Missing"
2021.acl-long.149,W18-1114,0,0.0754465,"Missing"
2021.acl-long.149,W17-2912,0,0.103014,"Missing"
2021.acl-long.149,W17-3009,0,0.057878,"Missing"
2021.acl-long.149,2020.socialnlp-1.2,1,0.924697,", they only capture a narrow subset of the multiple dimensions of race (§2). For example, none of them capture selfidentified race. While observed race is often appropriate for examining discrimination and some types of disparities, it is impossible to assess potential harms and benefits of NLP systems without assessing their performance over text generated by and directed to people of different races. The corpus from Blodgett et al. (2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race. Furthermore, names and hand-selected identity terms are not sufficient for uncovering model bias. De-Arteaga et al. (2019) show this in examining gender bias in occupation classification: when overt indicators like names and pronouns are scrubbed from the data, performance gaps and potential allocational harms still remain. Names also 8 We provide further counts of what racial categories papers use and how they operationalize them in Appendix B. generalize poorly. While identity terms can"
2021.acl-long.149,W19-8633,0,0.0178138,"e a narrow subset of the multiple dimensions of race (§2). For example, none of them capture selfidentified race. While observed race is often appropriate for examining discrimination and some types of disparities, it is impossible to assess potential harms and benefits of NLP systems without assessing their performance over text generated by and directed to people of different races. The corpus from Blodgett et al. (2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race. Furthermore, names and hand-selected identity terms are not sufficient for uncovering model bias. De-Arteaga et al. (2019) show this in examining gender bias in occupation classification: when overt indicators like names and pronouns are scrubbed from the data, performance gaps and potential allocational harms still remain. Names also 8 We provide further counts of what racial categories papers use and how they operationalize them in Appendix B. generalize poorly. While identity terms can be examined acro"
2021.acl-short.16,D18-1366,0,0.371718,"he vocabulary from the MT model, making adaptation easier. Now, to finetune this model to generate TGT, we need TGT embeddings. Since the TGT monolingual corpus is small, training fasttext vectors on this corpus from scratch will lead (as we show) to low-quality embeddings. Leveraging the relatedness of STD and TGT and their vocabulary overlap, we use STD embeddings to transfer knowledge to TGT embeddings: for each character ngram in the TGT corpus, we initialize its embedding with the corresponding STD embedding, if available. We then continue training fasttext on the TGT monolingual corpus (Chaudhary et al., 2018). Last, we use a supervised embedding alignment method (Lample et al., 2018a) to project the learned TGT embeddings in the same space as STD. STD and TGT are expected to have a large lexical overlap, so we use identical tokens in both varieties as supervision for this alignment. The obtained embeddings, due to transfer learning from STD, inject additional knowledge in the model. Finally, to obtain a SRC→TGT model, we finetune f on psuedo-parallel SRC – TGT data. Using a STD→SRC MT model (a back-translation model 111 trained using large STD – SRC parallel data with standard settings) we (back)-"
2021.acl-short.16,2020.emnlp-main.214,0,0.248347,"ing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens between STD and TGT, thus enabling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparam"
2021.acl-short.16,2020.emnlp-main.480,0,0.0478527,"Missing"
2021.acl-short.16,W19-6721,0,0.0282889,"i et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when"
2021.acl-short.16,2020.findings-emnlp.283,0,0.0383765,"Missing"
2021.acl-short.16,D19-1632,0,0.015424,", Belarusian, Nynorsk, and the four Arabic varieties. For reference, note that the EN→RU, EN→MSA, and EN→NO models are relatively strong, yielding BLEU scores of 24.3, 21.2, and 24.9, respectively. Synthetic Setup Considering STD and TGT as the same language is sub-optimal, as is evident from the poor performance of the non-adapted S UP ( SRC→STD ) model. Clearly, special attention ought to be paid to language varieties. Direct unsupervised translation from SRC to TGT performs poorly as well, confirming previously reported results of the ineffectiveness of such methods on unrelated languages (Guzmán et al., 2019). 5 Additional ablation results are listed in Appendix C. Translating SRC to TGT by pivoting through STD achieves much better performance owing to strong U NSUP ( STD→TGT ) models that leverage the similarities between STD and TGT. However, when resources are scarse (e.g., with 10K monolingual sentences as opposed to 1M), this performance gain considerably diminishes. We attribute this drop to overfitting during the pre-training phase on the small TGT monolingual data. Ablation results (Appendix C) also show that in such low-resource settings the learned embeddings are of low quality. Finally,"
2021.acl-short.16,2020.wmt-1.68,0,0.0324819,"Missing"
2021.acl-short.16,2011.eamt-1.19,0,0.178881,"Missing"
2021.acl-short.16,2020.acl-main.448,0,0.0253534,"and aligning the two embedding spaces. (3) The resulting model is finetuned with pseudo-parallel SRC→TGT data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on t"
2021.acl-short.16,P12-2059,0,0.0329472,"Missing"
2021.acl-short.16,2020.acl-main.156,0,0.0322545,"GT, but we show that they improve the overall performance. We discuss the implications of this noise in §4. 3 Experimental Setup Datasets We experiment with two setups. In the first (synthetic) setup, we use English (EN) as SRC, Russian (RU) as STD, and Ukrainian (UK) and Belarusian (BE) as TGTs. We sample 10M EN - RU sentences from the WMT’19 shared task (Ma et al., 2019), and 80M RU sentences from the CoNLL’17 shared task to train embeddings. To simulate lowresource scenarios, we sample 10K, 100K and 1M UK sentences from the CoNLL’17 shared task and BE sentences from the OSCAR corpus (Ortiz Suárez et al., 2020). We use TED dev/test sets for both languages pairs (Cettolo et al., 2012). The second (real world) setup has two language sets: the first one defines English as SRC, with Modern Standard Arabic (MSA) as STD and four Arabic varieties spoken in Doha, Beirut, Rabat and Tunis as TGTs. We sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the Engl"
2021.acl-short.16,P02-1040,0,0.110582,"abling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embedd"
2021.acl-short.16,W18-6319,0,0.0133174,"and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embeddings are replaced with TGT embeddings. The TGT embe"
2021.acl-short.16,D17-1266,0,0.0391541,"Missing"
2021.acl-short.16,2020.acl-demos.14,0,0.0283906,"how that in such low-resource settings the learned embeddings are of low quality. Finally, L ANG VAR MT consistently outperforms all baselines. Using 1M UK sentences, it achieves similar performance (for EN→UK) to the softmax ablation of our method, S OFTMAX, and small gains over unsupervised methods. However, in lower resource settings our approach is clearly better than the strongest baselines by over 4 BLEU points for UK (10K) and 3.9 points for BE (100K). To identify potential sources of error in our proposed method, we lemmatize the generated translations and test sets and evaluate BLEU (Qi et al., 2020). Across all data sizes, both UK and BE achieve a substantial increase in BLEU (up to +6 BLEU; see Appendix D for details) compared to that obtained on raw text, indicating morphological errors in the translations. In future work, we will investigate whether we can alleviate this issue by considering TGT embeddings based on morphological features of tokens (Chaudhary et al., 2018). Real-world Setup The effectiveness of L ANG VAR MT is pronounced in this setup with a dramatic improvement of more than 18 BLEU points over unsupervised baselines when translating into Doha Arabic. We hypothesize th"
2021.acl-short.16,2020.emnlp-main.365,0,0.0279739,"arallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpu"
2021.acl-short.16,2021.eacl-main.115,0,0.0357741,"e sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BP"
2021.acl-short.16,2020.acl-main.704,0,0.0247475,"data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on translation from English to Ukrainian, Belarusian, Nynorsk, and Arabic dialects with varying amounts of m"
2021.acl-short.16,P16-1162,0,0.0290828,"ences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens"
2021.acl-short.16,W07-0705,0,0.168003,"Missing"
2021.crac-1.13,2020.acl-main.140,0,0.0540253,"Missing"
2021.crac-1.13,doddington-etal-2004-automatic,0,0.0753121,"the SpanBERT representation. With Table 5: Examples of coreferent span pairs missed by Baseline the introduction of SpanBERT, there was a marked (CL), identified by our model (CL + RL + SL). In these cases, performance improvement for several NLU tasks we can see that wordpiece tokenization is likely misleading including coreference resolution. Joshi et al. (2020) the baseline model, since the spans in each pair have few wordpieces in common. showed that SpanBERT could be fine-tuned to perform well on several datasets, e.g., GLU and ACE FP Non-coreferent Span Pair Examples (Wang et al., 2018; Doddington et al., 2004). She underwent an open laparoscopy . . . The patient is now However, Lu and Ng (2020) found that Spanadmitted for exploratory laparotomy BERT coreference resolvers generally rely more Right heart catheterization and coronary angiography on October 15 . . . urgently transferred by Dr. Lenni Factor for on mentions than context, so they are susceptipossible angioplasty ble to small perturbations (e.g., changing all the 78-yo male with atrial fibrillation. . . Mechanical mitral names/nominal mentions in the test set). More genvalve: Anticoagulation was reversed He had a cardiac catheterization pe"
2021.crac-1.13,N15-1184,0,0.0240222,"Missing"
2021.crac-1.13,2020.acl-main.740,0,0.0491307,"Missing"
2021.crac-1.13,2020.tacl-1.5,0,0.231269,"antecedent, span of concept knowledge to more efficiently adapt representations are used by the model to (1.) select coreference models to a new domain. We dea set of candidate mentions and (2.) select an anvelop methods to improve the span representatecedent from the candidates for the given mention. tions via (1) a retrofitting loss to incentivize Thus, a high-quality span representation encodes span representations to satisfy a knowledgebased distance function and (2) a scaffolding the semantic meaning of the span tokens and their loss to guide the recovery of knowledge from local context. Joshi et al. (2020) introduced Spanthe span representation. By integrating these BERT, a pre-training method extending BERT, delosses, our model is able to improve our basesigned to improve performance on span-selection line precision and F-1 score. In particular, we tasks that involves masking contiguous spans rather show that incorporating knowledge with endthan tokens. Span representations are derived by to-end coreference models results in better perconcatenating the pre-trained transformer outputs formance on the most challenging, domainat the boundary tokens with an attention-weighted specific spans1 . vec"
2021.crac-1.13,D19-1588,0,0.055197,"Missing"
2021.crac-1.13,2020.emnlp-main.302,0,0.208102,"ls of Reference, Anaphora and Coreference (CRAC 2021), pages 121–131 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics 2021). Neural models have also been criticized for erence resolution since syntactic constituents are largely relying on shallow heuristics in the text, sug- often coreferent. Spans belonging to the same congesting this data-driven learning method requires cept within our concept knowledge usually coremany target examples to learn a new target distri- fer, so we generalize this technique to a broad, bution (Lu and Ng, 2020; Rosenman et al., 2020). knowledge-based lexicon in our domain adaptation setting. While our retrofitting loss integrates relThe presence of out-of-vocabulary words in a new domain can create additional challenges. Span- ative knowledge into the span representation, we are able to supplement the span representation with BERT uses wordpiece tokenization, which can lead global meaning using the scaffolding loss. to misleading meaning representation for spans To evaluate our models, we take OntoNotes as when a single wordpiece belongs to spans with different meanings (Joshi et al., 2020; Poerner et al., our source doma"
2021.crac-1.13,2020.wnut-1.3,0,0.0286541,"le tracking a patient’s progress and treatment history. However, coreference arcs, and other linguistic features of clinical notes contain acronyms and medical termi- coreference (Kahardipraja et al., 2020). The best nology. Annotating new training data for every do- coreference performance and span representations main of interest is expensive and time-consuming, are obtained by training the end-to-end model with and coreference models trained on existing bench- SpanBERT using labeled coreference data. mark datasets perform worse on other domains When adapting a coreference model to a new do(Srivastava et al., 2020; Xu and Choi, 2020; Joshi main, fine-tuning or continued training can greatly et al., 2020). In this work, we develop a domain- improve performance, but this approach can be adaptation model for coreference resolution that computationally expensive and requires a large 1 amount of labelled documents from the target doCode publicly available at https://github.com/ nupoorgandhi/i2b2-coref-public main (Gururangan et al., 2020; Xia and Van Durme, 121 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 121–131 c Punta Cana, Dominican Re"
2021.crac-1.13,D18-1412,0,0.0256096,"ur distance function to be composed of two elements: coreference information and concept knowledge. dT (si , sj ) = αc dc (si , sj ) + αk dk (si , sj ) RL = X 1 X ˆ `i , x ˆ `j )| |dT ` (s`i , s`j ) − d(x |r` | ` i,j Here |r` |denotes the number of span pairs internal to one document, which we use to normalize, ` identifies the document that span si , sj belongs to, and the function d is cosine distance. Concept Identification as an Auxiliary Task We introduce a concept identification auxiliary task to guide the model to construct a span representation from which the concept can be recovered. Swayamdipta et al. (2018) introduces the notion of a “scaffold” or auxiliary supervised loss function that is related to the primary task. Since coreferring spans nearly always belong to the same concept in our concept knowledge, concepts are a good choice for a scaffold. By sharing SpanBERT parameters optimizing for the scaffold loss and the overall coreference loss, we are able to encode the concept type in the span representation. Auxiliary Scaffolding Loss (SL) Following from Swayamdipta et al. (2018), we assign a distribution over the set of concepts Variables αc , αk each denote weights that we tune, and T refer"
2021.crac-1.13,2020.codi-1.4,0,0.0370056,"schedel et al., 2013). However, in many real world settings where coreference reso- expressiveness of the SpanBERT representation is apparent from extrinsic coreference performance, lution would be valuable, text differs greatly from but also through probing tasks that have shown these standard datasets. For example, coreference that span representations can capture headedness, resolution over clinical notes can enable tracking a patient’s progress and treatment history. However, coreference arcs, and other linguistic features of clinical notes contain acronyms and medical termi- coreference (Kahardipraja et al., 2020). The best nology. Annotating new training data for every do- coreference performance and span representations main of interest is expensive and time-consuming, are obtained by training the end-to-end model with and coreference models trained on existing bench- SpanBERT using labeled coreference data. mark datasets perform worse on other domains When adapting a coreference model to a new do(Srivastava et al., 2020; Xu and Choi, 2020; Joshi main, fine-tuning or continued training can greatly et al., 2020). In this work, we develop a domain- improve performance, but this approach can be adaptati"
2021.crac-1.13,2021.acl-short.3,0,0.0279583,"han tokens. Span representations are derived by to-end coreference models results in better perconcatenating the pre-trained transformer outputs formance on the most challenging, domainat the boundary tokens with an attention-weighted specific spans1 . vector over the span tokens. These representations 1 Introduction are fed into a coreference resolution model, thus integrating SpanBERT into an end-to-end coreferRecent work has achieved high performance on ence resolution system. coreference resolution in standard benchmark SpanBERT is able to capture coreference strucdatasets like OntoNotes (Kirstain et al., 2021; Joshi ture implicitly in rich span representations. The et al., 2020; Weischedel et al., 2013). However, in many real world settings where coreference reso- expressiveness of the SpanBERT representation is apparent from extrinsic coreference performance, lution would be valuable, text differs greatly from but also through probing tasks that have shown these standard datasets. For example, coreference that span representations can capture headedness, resolution over clinical notes can enable tracking a patient’s progress and treatment history. However, coreference arcs, and other linguistic f"
2021.crac-1.13,D17-1018,0,0.0872756,"s a pre-training method extending BERT that masks contiguous spans and also trains the span boundary representations to predict the masked span. The span representation hi is the concatenation of the two SpanBERT transformer states of the span endpoints (first and last word pieces) xSTART(i) , xEND(i) and an attention vector x ˆi computed over all the word pieces in the span (Joshi et al., 2019, 2020).   hi = xSTART(i) , xEND(i) , x ˆi , φ(i) The attention vector x ˆi is intended to best represent the internal span itself (e.g. head word), whereas the endpoints better represent the context (Lee et al., 2017). This suggests that the x ˆi component of the overall span representation is the most natural part of the span to align with global, non-contextual knowledge. For the set of possible spans, we first produce span representations. The span representation is learned as a part of the neural end-to-end framework intro- 2.3 Integrating Knowledge into Span Representation duced in Lee et al. (2017). Given span representations, each span representation hi is assigned a We aim to create a span representation such that unary mention score. The mention score reflects knowledge can be easily aligned with"
2021.crac-1.13,2020.emnlp-main.298,0,0.0918782,"Missing"
2021.crac-1.13,2020.findings-emnlp.71,0,0.0613107,"Missing"
2021.crac-1.13,W18-5446,0,0.061469,"Missing"
2021.crac-1.13,2020.emnlp-main.686,0,0.0313263,"rogress and treatment history. However, coreference arcs, and other linguistic features of clinical notes contain acronyms and medical termi- coreference (Kahardipraja et al., 2020). The best nology. Annotating new training data for every do- coreference performance and span representations main of interest is expensive and time-consuming, are obtained by training the end-to-end model with and coreference models trained on existing bench- SpanBERT using labeled coreference data. mark datasets perform worse on other domains When adapting a coreference model to a new do(Srivastava et al., 2020; Xu and Choi, 2020; Joshi main, fine-tuning or continued training can greatly et al., 2020). In this work, we develop a domain- improve performance, but this approach can be adaptation model for coreference resolution that computationally expensive and requires a large 1 amount of labelled documents from the target doCode publicly available at https://github.com/ nupoorgandhi/i2b2-coref-public main (Gururangan et al., 2020; Xia and Van Durme, 121 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 121–131 c Punta Cana, Dominican Republic, November 10"
2021.crac-1.13,W14-1104,0,0.0218896,". . . decreased hematocrit prior to his humeral fixation surgery An angiogram was done which disclosed possible subsegmental pulmonary emboli of the upper lobes as well . . . patient was bolused with intravenous heparin due to concern for pulmonary embolism there may be no concept knowledge available when our model is deployed. There has been some limited work in domain adaptation for coreference resolution. Yang et al. (2012) adapts a model trained on the MUC-6 and ACE 2005 datasets to the biomedical domain using an active learning approach, applying data augmentation and pruning techniques. Zhao and Ng (2014) propose a feature-based active learning method to learn cross-domain knowledge. Unlike these works, we take advantage of the modern expressive power of the SpanBERT representation. With Table 5: Examples of coreferent span pairs missed by Baseline the introduction of SpanBERT, there was a marked (CL), identified by our model (CL + RL + SL). In these cases, performance improvement for several NLU tasks we can see that wordpiece tokenization is likely misleading including coreference resolution. Joshi et al. (2020) the baseline model, since the spans in each pair have few wordpieces in common."
2021.eacl-main.204,C16-1065,0,0.0285477,"Hofstede et al. (2005) defined culture as the collective mind which “distinguishes the members of one group of people from another.” Cultural idiosyncrasies affect and shape people’s beliefs and behaviors. Linguists have particularly focused on the relationship between culture and language, revealing in qualitative case studies how cultural differences are manifested as linguistic variations (Siegel, 1977). Quantifying cross-cultural similarities from linguistic patterns has largely been unexplored in NLP, with the exception of studies that focused on cross-cultural differences in word usage (Garimella et al., 2016; Lin et al., 2018). In this work, we aim to quantify cross-cultural similarity, focusing *The first three authors contributed equally. on semantic and pragmatic differences across languages.1 We devise a new distance measure between languages based on linguistic proxies of culture. We hypothesize that it can be used to select transfer languages and improve cross-lingual transfer learning, specifically in pragmaticallymotivated tasks such as sentiment analysis, since expressions of subtle sentiment or emotion—such as subjective well-being (Smith et al., 2016), anger (Oster, 2019), or irony (Ka"
2021.eacl-main.204,goldhahn-etal-2012-building,0,0.0162101,"e aligned emotion word vector of language ltf . 3 Feature Analysis In this section, we evaluate the proposed pragmatically-motivated features intrinsically. Throughout the analyses, we use 16 languages listed in Figure 4 which are later used for extrinsic evaluation (§5). 3.1 Implementation Details We used multilingual word tokenizers from NLTK and RDR POS Tagger (Nguyen et al., 2014) for most of the languages except for Arabic, Chinese, Japanese, and Korean, where we used PyArabic, Jieba, Kytea, and Mecab, respectively. For monolingual corpora, we used the news-crawl 1M corpora from Leipzig (Goldhahn et al., 2012) for both LCR and LTQ. We used bilingual dictionaries from Choe et al. (2020) and TED talks corpora (Qi et al., 2018) for both parallel corpora and an additional monolingual corpus for LTQ. We focused on bigrams and trigrams and set k, the number of extracted MWEs, to 500. We followed Lample et al. (2018) to generate the supervised cross-lingual word embeddings for ESD. Figure 1: Plot of languages in ptr and vtr plane. Languages are color-coded according to the cultural areas defined in Siegel (1977). 3.2 LCR and Language Context-level ptr approximates how often discourse entities are indexed"
2021.eacl-main.204,E17-1025,0,0.0281183,"16; Lin et al., 2018). In this work, we aim to quantify cross-cultural similarity, focusing *The first three authors contributed equally. on semantic and pragmatic differences across languages.1 We devise a new distance measure between languages based on linguistic proxies of culture. We hypothesize that it can be used to select transfer languages and improve cross-lingual transfer learning, specifically in pragmaticallymotivated tasks such as sentiment analysis, since expressions of subtle sentiment or emotion—such as subjective well-being (Smith et al., 2016), anger (Oster, 2019), or irony (Karoui et al., 2017)—have been shown to vary significantly by culture. We focus on three distinct aspects in the intersection of language and culture, and propose features to operationalize them. First, every language and culture rely on different levels of context in communication. Western European languages are generally considered low-context languages, whereas Korean and Japanese are considered high-context languages (Hall, 1989). Second, similar cultures construct and construe figurative language similarly (Casas and Campoy, 1995; Vulanovi´c, 2014). Finally, emotion semantics is similar between languages tha"
2021.eacl-main.204,D19-5505,0,0.0204459,"we use NDCG@3. We train and evaluate the model using leave-oneout cross-validation: where one language is set aside as the test language while other languages are used to train the ranking model. Among the training languages, each language is posited in turn as the target language while others are the transfer languages. 5 5.1 Experiments Baselines SA Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual extension of BERT pretrained with 104 different languages, has shown strong results in various text classification tasks in crosslingual settings (Sun et al., 2019; Xu et al., 2019; Li et al., 2019). We use mBERT to conduct zeroshot cross-lingual transfer and to extract optimal transfer language rankings: fine-tune mBERT on transfer language data and test it on target language data. The performance is measured by the macro F1 score on the test set. L ANG R ANK L ANG R ANK (Lin et al., 2019) uses 13 features to train the ranking model: The dataset size in transfer language (tf size), target language (tg size), and the ratio between the two (ratio size); Type-token-ratio (ttr) which measures lexical diversity and word overlap for lexical similarity between a pair of languages; various dist"
2021.eacl-main.204,P18-1066,0,0.142591,"efined culture as the collective mind which “distinguishes the members of one group of people from another.” Cultural idiosyncrasies affect and shape people’s beliefs and behaviors. Linguists have particularly focused on the relationship between culture and language, revealing in qualitative case studies how cultural differences are manifested as linguistic variations (Siegel, 1977). Quantifying cross-cultural similarities from linguistic patterns has largely been unexplored in NLP, with the exception of studies that focused on cross-cultural differences in word usage (Garimella et al., 2016; Lin et al., 2018). In this work, we aim to quantify cross-cultural similarity, focusing *The first three authors contributed equally. on semantic and pragmatic differences across languages.1 We devise a new distance measure between languages based on linguistic proxies of culture. We hypothesize that it can be used to select transfer languages and improve cross-lingual transfer learning, specifically in pragmaticallymotivated tasks such as sentiment analysis, since expressions of subtle sentiment or emotion—such as subjective well-being (Smith et al., 2016), anger (Oster, 2019), or irony (Karoui et al., 2017)—"
2021.eacl-main.204,E17-2002,1,0.892939,"Missing"
2021.eacl-main.204,N19-1112,0,0.0257017,"f language are correlates of cultural and pragmatic difference. Model Probing Fine-tuning pretrained models to downstream tasks has become the de facto standard in various NLP tasks, and their success has promoted the development of their multilingual extensions (Devlin et al., 2019; Lample and Conneau, 2019). While the performance gains from these models are undeniable, their learning dynamics remain obscure. This issue has prompted various probing methods designed to test what kind of linguistic information the models retain, including syntactic and semantic knowledge (Conneau et al., 2018; Liu et al., 2019; Ravishankar et al., 2019; Tenney et al., 2019). Similarly, our features can be employed as a touchstone to evaluate a model’s knowledge in cross-cultural pragmatics. Investigating how different pretraining tasks affect the 2410 learning of pragmatic knowledge will also be an interesting direction of research. 8 Conclusion In this work, we propose three pragmaticallyinspired features that capture cross-cultural similarities that arise as linguistic patterns: language context-level ratio, literal translation quality, and emotion semantic distance. Through feature analyses, we examine whether o"
2021.eacl-main.204,D17-1268,0,0.0218622,"ich measures lexical diversity and word overlap for lexical similarity between a pair of languages; various distances between a language pair from the URIEL database (geographic geo, genetic gen, inventory inv, syntactic syn, phonological phon and featural feat). DEP We adopt the setting from Ahmad et al. (2018) to perform cross-lingual zero-shot transfer. We train deep biaffine attentional graph-based models (Dozat and Manning, 2016) which achieved state-of-the-art performance in dependency parsing for many languages. The performance is evaluated using labeled attachment scores (LAS). MTV EC Malaviya et al. (2017) proposed to learn a language representation while training a neural machine translation (NMT) system in a simliar fashion to Johnson et al. (2017). During training, a language token is prepended to the source sentence and the learned token’s embedding becomes the language vector. Bjerva et al. (2019) has shown that such language representations contain various types of linguistic information ranging from word order to typological information. We used the one released by Malaviya et al. (2017) which has the dimension of 512. 4.4 Ranking Model & Evaluation Ranking Model For the language ranking"
2021.eacl-main.204,D18-1103,0,0.0117795,"17 samples) was too small to train an effective model. ilarity by comparing the nearest neighborhood of words in different languages, showing that words in some domains (e.g., time, quantity) exhibit higher cross-lingual alignment than other domains (e.g., politics, food, emotions). Jackson et al. (2019) represented each language as a network of emotion concepts derived from their colexification patterns and measured the similarity between networks. Auxiliary Language Selection in Cross-lingual tasks There has been active work on leveraging multiple languages to improve cross-lingual systems (Neubig and Hu, 2018; Ammar et al., 2016). Adapting auxiliary language datasets to the target language task can be practiced through either language-selection or data-selection. Previous work on language-selection mostly relied on leveraging syntactic or semantic resemblance between languages (e.g. ngram overlap) to choose the best transfer languages (Zoph et al., 2016; Wang and Neubig, 2019). Our approach extends this line of work by leveraging cross-cultural pragmatics, an aspect that has been unexplored by prior work. 7 Future Directions Typology of Cross-cultural Pragmatics The features proposed here provide"
2021.eacl-main.204,E14-2005,0,0.0217551,"n semantics, emotions are scattered into different positions. We thus define ESD as the average cosine distance between languages: X ESD(ltf , ltg ) = cos(vtf,e , vtg,e )/|E| e∈E where E is the set of emotion concepts and vtf,e is the aligned emotion word vector of language ltf . 3 Feature Analysis In this section, we evaluate the proposed pragmatically-motivated features intrinsically. Throughout the analyses, we use 16 languages listed in Figure 4 which are later used for extrinsic evaluation (§5). 3.1 Implementation Details We used multilingual word tokenizers from NLTK and RDR POS Tagger (Nguyen et al., 2014) for most of the languages except for Arabic, Chinese, Japanese, and Korean, where we used PyArabic, Jieba, Kytea, and Mecab, respectively. For monolingual corpora, we used the news-crawl 1M corpora from Leipzig (Goldhahn et al., 2012) for both LCR and LTQ. We used bilingual dictionaries from Choe et al. (2020) and TED talks corpora (Qi et al., 2018) for both parallel corpora and an additional monolingual corpus for LTQ. We focused on bigrams and trigrams and set k, the number of extracted MWEs, to 500. We followed Lample et al. (2018) to generate the supervised cross-lingual word embeddings f"
2021.eacl-main.204,N18-2084,0,0.0229676,"motivated features intrinsically. Throughout the analyses, we use 16 languages listed in Figure 4 which are later used for extrinsic evaluation (§5). 3.1 Implementation Details We used multilingual word tokenizers from NLTK and RDR POS Tagger (Nguyen et al., 2014) for most of the languages except for Arabic, Chinese, Japanese, and Korean, where we used PyArabic, Jieba, Kytea, and Mecab, respectively. For monolingual corpora, we used the news-crawl 1M corpora from Leipzig (Goldhahn et al., 2012) for both LCR and LTQ. We used bilingual dictionaries from Choe et al. (2020) and TED talks corpora (Qi et al., 2018) for both parallel corpora and an additional monolingual corpus for LTQ. We focused on bigrams and trigrams and set k, the number of extracted MWEs, to 500. We followed Lample et al. (2018) to generate the supervised cross-lingual word embeddings for ESD. Figure 1: Plot of languages in ptr and vtr plane. Languages are color-coded according to the cultural areas defined in Siegel (1977). 3.2 LCR and Language Context-level ptr approximates how often discourse entities are indexed with pronouns rather than left conjecturable from context. Similarly, vtr estimates the rate at which predicates appe"
2021.eacl-main.204,W19-6205,0,0.0901867,"Missing"
2021.eacl-main.204,N19-1162,0,0.0232242,"(zh). The output ranking rˆ fr is compared to the ground truth ranking r fr which is determined by the zero-shot performance z of cross-lingual models. Task Setting We define our task as the language ranking problem: given the target language ltg , we want to rank a set of n candidate transfer languages (1) (n) Ltf ={ltf , . . . , ltf } by their usefulness when transferred to ltg , which we refer to as transferability (illustrated in Figure 3). The effectiveness of cross-lingual transfer is often measured by evaluating the joint training or zero-shot transfer performance (Wu and Dredze, 2019; Schuster et al., 2019). In this work, we quantify the effectiveness as the zero-shot transfer performance, following Lin et al. (2019). Our goal is to train a model that ranks available transfer languages in Ltf by their transferability for a target language ltg . To train the ranking model, we first need to find the ground-truth transferability rankings, which operate as the model’s training data. We evaluate the zero-shot performance ztf,tg by training a taskspecific cross-lingual model solely with transfer language ltf and testing on ltg . After evaluating ztf,tg for each candidate transfer language in Ltf , we"
2021.eacl-main.204,P19-1583,0,0.011495,"from their colexification patterns and measured the similarity between networks. Auxiliary Language Selection in Cross-lingual tasks There has been active work on leveraging multiple languages to improve cross-lingual systems (Neubig and Hu, 2018; Ammar et al., 2016). Adapting auxiliary language datasets to the target language task can be practiced through either language-selection or data-selection. Previous work on language-selection mostly relied on leveraging syntactic or semantic resemblance between languages (e.g. ngram overlap) to choose the best transfer languages (Zoph et al., 2016; Wang and Neubig, 2019). Our approach extends this line of work by leveraging cross-cultural pragmatics, an aspect that has been unexplored by prior work. 7 Future Directions Typology of Cross-cultural Pragmatics The features proposed here provide three dimensions in a provisional quantitative cross-linguistic typology of pragmatics in language. Having been validated, both intrinsically and extrinsically, they can be used in studies as a stand-in for cross-cultural similarity. They also open a new avenue of research, raising questions about what other quantitative features of language are correlates of cultural and"
2021.eacl-main.204,D19-1077,0,0.0241612,"an (ru), and Chinese (zh). The output ranking rˆ fr is compared to the ground truth ranking r fr which is determined by the zero-shot performance z of cross-lingual models. Task Setting We define our task as the language ranking problem: given the target language ltg , we want to rank a set of n candidate transfer languages (1) (n) Ltf ={ltf , . . . , ltf } by their usefulness when transferred to ltg , which we refer to as transferability (illustrated in Figure 3). The effectiveness of cross-lingual transfer is often measured by evaluating the joint training or zero-shot transfer performance (Wu and Dredze, 2019; Schuster et al., 2019). In this work, we quantify the effectiveness as the zero-shot transfer performance, following Lin et al. (2019). Our goal is to train a model that ranks available transfer languages in Ltf by their transferability for a target language ltg . To train the ranking model, we first need to find the ground-truth transferability rankings, which operate as the model’s training data. We evaluate the zero-shot performance ztf,tg by training a taskspecific cross-lingual model solely with transfer language ltf and testing on ltg . After evaluating ztf,tg for each candidate transf"
2021.eacl-main.204,N19-1242,0,0.0140461,"MAP. Similarly, we use NDCG@3. We train and evaluate the model using leave-oneout cross-validation: where one language is set aside as the test language while other languages are used to train the ranking model. Among the training languages, each language is posited in turn as the target language while others are the transfer languages. 5 5.1 Experiments Baselines SA Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual extension of BERT pretrained with 104 different languages, has shown strong results in various text classification tasks in crosslingual settings (Sun et al., 2019; Xu et al., 2019; Li et al., 2019). We use mBERT to conduct zeroshot cross-lingual transfer and to extract optimal transfer language rankings: fine-tune mBERT on transfer language data and test it on target language data. The performance is measured by the macro F1 score on the test set. L ANG R ANK L ANG R ANK (Lin et al., 2019) uses 13 features to train the ranking model: The dataset size in transfer language (tf size), target language (tg size), and the ratio between the two (ratio size); Type-token-ratio (ttr) which measures lexical diversity and word overlap for lexical similarity between a pair of langu"
2021.eacl-main.204,D16-1163,0,0.0173001,"on concepts derived from their colexification patterns and measured the similarity between networks. Auxiliary Language Selection in Cross-lingual tasks There has been active work on leveraging multiple languages to improve cross-lingual systems (Neubig and Hu, 2018; Ammar et al., 2016). Adapting auxiliary language datasets to the target language task can be practiced through either language-selection or data-selection. Previous work on language-selection mostly relied on leveraging syntactic or semantic resemblance between languages (e.g. ngram overlap) to choose the best transfer languages (Zoph et al., 2016; Wang and Neubig, 2019). Our approach extends this line of work by leveraging cross-cultural pragmatics, an aspect that has been unexplored by prior work. 7 Future Directions Typology of Cross-cultural Pragmatics The features proposed here provide three dimensions in a provisional quantitative cross-linguistic typology of pragmatics in language. Having been validated, both intrinsically and extrinsically, they can be used in studies as a stand-in for cross-cultural similarity. They also open a new avenue of research, raising questions about what other quantitative features of language are cor"
2021.eacl-main.204,C10-2144,1,0.664743,"and metaphors (K¨ovecses, 2003, 2010). For example, like father like son in English can be translated word-by-word into a similar idiom tel p`ere tel fils in French. However, in Japanese, a similar idiom 蛙の子は蛙 (Kaeru no ko wa kaeru) “A frog’s child is a frog.” cannot be literally translated. Literal translation quality (LTQ) feature quantifies how well a given language pair’s MWEs are preserved in literal (word-by-word) translation, using a bilingual dictionary. A well-curated list of MWEs is not available for the majority of languages. We thus follow an automatic extraction approach of MWEs (Tsvetkov and Wintner, 2010). First, a variant of pointwise mutual information, PMI3 (Daille, 1994) is used to extract noisy lists of top-scoring n-grams from two large monolingual corpora from different domains, and intersecting the lists filters out domain-specific n-grams and retains the language-specific top-k MWEs. Then, a bilingual dictionary between ltf and ltg and a parallel corpus between the pair are used. 3 For each n-gram in ltg ’s MWEs, we search for its literal translations extracted using the dictionary in parallel sentences containing the n-gram. For any word in the n-gram, if there is a translation in th"
2021.eacl-main.220,W03-0501,0,0.0856166,"ed (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.1 1 Introduction Text summarization aims at identifying important information in long source documents and expressing it in human readable summaries. Two prominent methods of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where important sentences in the source article are selected to form a summary, and abstractive (Rush 1 Code and data available at: https://github.com/ vidhishanair/structured_summarizer et al., 2015; See et al., 2017), where the model restructures and rephrases essential content into a paraphrased summary. State of the art approaches to abstractive summarization employ neural encoder-decoder methods that encode the source document as a sequence of tokens producing latent document representations and decode the summary conditioned on the representations. Recent studi"
2021.eacl-main.220,P18-1063,0,0.0192659,"he right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization ("
2021.eacl-main.220,D16-1053,0,0.0445439,"Missing"
2021.eacl-main.220,N18-2097,0,0.0182117,"l., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in"
2021.eacl-main.220,P19-1062,0,0.0609612,"Missing"
2021.eacl-main.220,P19-1630,0,0.0153658,"t al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framewo"
2021.eacl-main.220,D18-1443,0,0.0326571,"Missing"
2021.eacl-main.220,P18-1013,0,0.0160938,"ctures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntac"
2021.eacl-main.220,P19-1206,0,0.0163096,"ic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in neural abstractive summarization. We introduce a novel explicit-at"
2021.eacl-main.220,D18-1208,0,0.0606948,"Missing"
2021.eacl-main.220,P14-2052,0,0.0320829,"rds or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et"
2021.eacl-main.220,D19-1051,0,0.0254577,"Missing"
2021.eacl-main.220,2020.acl-main.703,0,0.0908276,"Missing"
2021.eacl-main.220,D18-1441,0,0.0150896,"shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our"
2021.eacl-main.220,N15-1114,0,0.0243291,"o either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structu"
2021.eacl-main.220,D19-1387,0,0.0336493,"der. We use the Adagrad optimizer (Duchi et al., 2011) with a learning rate of 0.15 and an initial accumulator value of 0.1. We do not use dropout and use 4 The best results from Gehrmann et al. (2018) outperform DiffMask experiment, but they use inference-time hard masking which can be applied on ours. Our baselines also exclude Reinforcement Learning (RL) based systems as they are not directly comparable, but our approach can be introduced in an encoder-decoder based RL system. Since we do not incorporate any pretraining, we do not compare with recent contextual representation based models (Liu and Lapata, 2019). 5 https://github.com/atulkum/pointer_ summarizer 2578 Model Pointer-Generator (See et al., 2017) Pointer-Generator + Coverage (See et al., 2017) Graph Attention (Tan et al., 2017) Pointer-Generator + DiffMask (Gehrmann et al., 2018) ROUGE 1 36.44 39.53 38.10 38.45 ROUGE 2 15.66 17.28 13.90 16.88 ROUGE L 33.42 36.38 34.00 35.81 Pointer-Generator (Re-Implementation) Pointer-Generator + Coverage (Re-Implementation) Latent-Structure (LS) Attention Explicit-Structure (ES) Attention LS + ES Attention 35.55 39.07 39.52 39.63 39.62 15.29 16.97 16.94 16.98 17.00 32.05 35.87 36.71 36.72 36.95 Table 1:"
2021.eacl-main.220,R11-1066,0,0.0334585,"ntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in neural abstractive summarization. We introduce a novel explicit-attention module which incorporates external linguistic structur"
2021.eacl-main.220,K16-1028,0,0.0553552,"Missing"
2021.eacl-main.220,D18-1206,0,0.122169,"Missing"
2021.eacl-main.220,W08-1404,0,0.0610077,"et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) ind"
2021.eacl-main.220,D15-1044,0,0.0718388,"wn his appointment , though he felt broncos were moving in the right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural"
2021.eacl-main.220,P17-1099,0,0.32103,"es by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.1 1 Introduction Text summarization aims at identifying important information in long source documents and expressing it in human readable summaries. Two prominent methods of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where important sentences in the source article are selected to form a summary, and abstractive (Rush 1 Code and data available at: https://github.com/ vidhishanair/structured_summarizer et al., 2015; See et al., 2017), where the model restructures and rephrases essential content into a paraphrased summary. State of the art approaches to abstractive summarization employ neural encoder-decoder methods that encode the source document as a sequence of tokens producing latent document representations and decode the summary conditioned on the representations. Recent studies suggest that these models suffer from several key challenges. First, since standard training datasets are derived from news articles, model outputs are strongly affected by the layout bias of the articles, with models relying on the leading s"
2021.eacl-main.220,C18-1146,0,0.0169814,"tences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and aug"
2021.eacl-main.220,P17-1108,0,0.0588999,"Missing"
2021.eacl-main.220,N16-1174,0,0.0611917,"Schmidhuber, 1997; Vaswani et al., 2017). The encoder produces a set of hidden representations {h}. A decoder maps the previously generated token yt−1 to a hidden state and computes a soft attention probability distribution p(at |x, y1:t−1 ) over encoder hidden states. A distribution p over the vocabulary is computed at every time step t and the network is trained using the negative log likelihood loss: losst = −log p(yt ). StructSum modifies the above architecture as follows. We aggregate the token representations from the encoder to form sentence representations as in hierarchical encoders (Yang et al., 2016). We then use implicit- and explicit-structure attention modules to augment the sentence representations with sentence dependency information, leveraging both a learned latent structure and an external structure from other NLP modules. The attended vectors are then passed to the decoder, which produces the output abstractive summary. In the rest of this section, we describe our framework architecture, shown in Figure 1, in detail. 2.1 Sentence Representations We consider an encoder which takes a sequence of words in a sentence si = {w} as input and produces contextual hidden representation for"
2021.eacl-main.220,D18-1088,0,0.0188402,"eakened side put on fine show to crown his appointment , though he felt broncos were moving in the right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowd"
2021.emnlp-main.570,W14-3302,0,0.0824889,"Missing"
2021.emnlp-main.570,W18-6111,0,0.0202721,"ne axis in each plot corresponds to the performance on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying eithe"
2021.emnlp-main.570,W18-6433,0,0.154861,"s like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence ma"
2021.emnlp-main.570,W17-4705,0,0.101209,"mple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word ea"
2021.emnlp-main.570,D13-1174,0,0.0180435,"ality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-forme"
2021.emnlp-main.570,2020.emnlp-main.422,1,0.904164,"ic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual grammar rules that identify specific areas to improve the model’s grammaticality. L’ AMBRE relies on a grammatical description of the language, similar to those linguists and language educators have been producing for decades when they document a language or create teaching materials. Specifically, we consider rules describing morphosyntax, including agreement, case assignment, and verb form selection. Following Chaudhary et al. (2020), we describe a procedure to automatically extract these rules from existing dependency treebanks (§3) with high precision.3 When evaluating NLG outputs, adherence to these rules can be assessed through dependency parses (Figure 1). However, off-the-shelf dependency parsers are trained on grammatically sound text and are not well-suited for parsing ungrammatical (or noisy) text (Hashemi and Hwa, 2016) such as that generated by NLG systems. We propose a method to train more robust dependency parsers and morphological feature taggers by synthesizing morphosyntactic errors in existing treebanks ("
2021.emnlp-main.570,L16-1102,0,0.0697468,"Missing"
2021.emnlp-main.570,2020.acl-demos.10,0,0.0306615,"n dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific do"
2021.emnlp-main.570,W19-7814,0,0.0276629,"gen- rules as ras (PRON, VERB, subj) → CasePRON = Nom and ras (PRON, VERB, obj) → CasePRON = Acc. 7 Many linguists also produce highly formal accounts of Our hypothesis is that certain syntactic construcgrammatical phenomena. However, many of these formalisms are difficult to implement computationally because they are tions require specific morphological feature selecequivalent (in the most egregious cases) to Turing machines. tion from one of their constituents (e.g., pronoun 8 We use the Surface-Syntactic Universal Dependencies subjects need to be in nominative case, but pronoun (SUD) 2.5 (Gerdes et al., 2019). See A.1 for a comparison of UD and SUD. objects only allow for genitive or accusative case in 7133 Greek).9 This implies that the “local” distribution that a specific construction requires will be different from a “global” distribution of morphological feature values computed over the whole treebank. Figure 2 presents an example for German-GSD. We can automatically discover these rules by finding such cases of distortion. First, we obtain a global distribution (G(fx ) = p(fx )) that captures the empirical distribution of the values of a morphological feature f on POS x over the whole treeban"
2021.emnlp-main.570,H05-1085,0,0.160341,"e Russian and German GEC corpora for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In c"
2021.emnlp-main.570,D16-1182,0,0.0236581,"arguably would be even more effective if combined with hand-curated descriptions created by linguists. We leave this as an interesting direction for future work. In our code, we provide detailed instructions for adding new rules. 4 Parsing Noisy Text Within our evaluation framework, we rely on parsers to generate the dependency trees of potentially malformed or noisy sentences from NLG systems. However, publicly available parsers are typically trained on clean and grammatical text from UD treebanks, and may not generalize to noisy inputs (Daiber and van der Goot, 2016; Sakaguchi et al., 2017; Hashemi and Hwa, 2016, 2018). Therefore, it is necessary to ensure that parsers are robust to any morphology-related errors in the input 9 This class of rules are also often lexicalized, depending on text. Ideally, the tagger should accurately identify the lexeme of either the head or the dependent. In the example the morphological features of incorrect word forms, S.1 of Figure 1, the object phrase lange Bücher (‘long Book’) while the dependency parser remains robust to such is inflected in the accusative case because of the verb lesen (‘read’). Other constructions might require the object declined noise. To this"
2021.emnlp-main.570,D19-1279,0,0.0232009,"n UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LAS (with similar gains on UAS) and UFe"
2021.emnlp-main.570,2020.acl-main.126,0,0.0166514,"gically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG will-1 SG long-ACC . PL Book-ACC . PL read-PTCP S.2 *Ich werden langen Bücher lesen I-NOM .1 SG will-1 PL long-DAT ."
2021.emnlp-main.570,W04-1013,0,0.0247647,"metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG wi"
2021.emnlp-main.570,2020.acl-main.490,0,0.0197465,"s 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence may trigger a grammatical error later in the sentence). Most of these methods, with the exception of Mueller et al. (2020), focus only on English or translation to/from English. In this paper, we propose L’ AMBRE, a metric that both evaluates the grammatical well-formedness of text in a fine-grained fashion and can be applied to text from multiple languages. We use widely available dependency parsers to tag and parse target text, and then compute our metric by identifying language-specific morphosyntactic errors in text (a schematic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual gr"
2021.emnlp-main.570,W18-6450,0,0.0246572,"idelyt-BLEU (Ataman et al., 2020) measures BLEU on outputs used rule-based proofreading software to detect tagged using a morphological analyzer. 7137 en→ cs 1 de et fi ru tr WMT’18 0.84 all ragree 0.91 ras 0.78 -0.06 0.07 -0.10 0.68 0.83 0.62 0.86 0.96 0.77 0.86 0.71 0.89 0.58 0.64 -0.31 WMT’19 all 0.80 0.89 ragree ras 0.70 0.16 0.14 0.13 - 0.85 0.87 0.82 0.57 0.70 0.45 - 0.95 0.95 0.95 0.93 0.93 0.95 0.95 0.95 0.9 ’15 ’16 ’17 ’18 ’19 ’14 ’15 ’16 ’17 ’18 ’19 : systems average ◦: system •: reference For evaluating MT systems, we use the data from the Metrics Shared Task in WMT 2018 and 2019 (Ma et al., 2018, 2019). This corpus includes outputs from all participating systems on the test sets from the News Translation Shared Task (Bojar et al., 2018; Barrault et al., 2019). Our study focuses on systems that translate from English to morphologically-rich target languages: Czech, Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the ref"
2021.emnlp-main.570,W19-5302,0,0.0157703,"Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negative correlations for German, and See A.5 for the corresponding scatter plots. German 0.95 0.94 0.95 0.94 0.9 ’14 Table 3: With a few exceptions, our grammar-based metrics correlate well wit"
2021.emnlp-main.570,D18-1151,0,0.025453,"ut, limiting their applicability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the ot"
2021.emnlp-main.570,2020.acl-main.448,0,0.0251423,"Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negativ"
2021.emnlp-main.570,D19-5545,0,0.0482666,"Missing"
2021.emnlp-main.570,D16-1228,0,0.158847,"gnment and verb form choice, ras ). Analysis: In both languages, we find agreement rules to be of higher quality than case and verb form assignment ones. This phenomenon is more pronounced in German where many case assignment rules are lexeme-dependent, as discussed in §3. Importantly, our proposed robust parsers lead to clear gains in error identification recall, compared to the pre-trained ones (“Original” vs. “Robust” in Table 2). Given the complexity of the errors present in text from non-native learners and the well-known incompleteness of GEC corpora in listing all possible corrections (Napoles et al., 2016), combined with the prevalence of typos and the dataset’s domain difference compared to the 15 We use the train portion due to its large size, therefore parser’s training data, our error identification modgives a better estimate of our L’ AMBRE performance. Note ule performs quite well. that, in this experiment, we do not aim to compare against state-of-the-art GEI tools. To understand where L’ AMBRE fails, we man7136 ually inspected a sample of false positives. First, we notice that tokens with typos are often erroneously tagged and parsed. Our augmentation is only equipped to handle (correct"
2021.emnlp-main.570,2020.acl-demos.14,0,0.0602168,"Missing"
2021.emnlp-main.570,Q19-1001,0,0.0252876,"e on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying either the subject or the verb to agree with the other constituent. To"
2021.emnlp-main.570,Q16-1013,0,0.0163878,"ese metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating tex"
2021.emnlp-main.570,P17-2030,0,0.0584137,"Missing"
2021.emnlp-main.570,2020.acl-main.263,0,0.020855,"os and Neubig (2019), but we leave this for future work. For evaluation, we induce noise into the dev portions of the treebanks and test the robustness of off-the-shelf taggers and parsers from Stanza (Qi et al., 2020) (indicative results on Czech, Greek, and Turkish are shown in Figure 4). Along with the overall scores on the dev set, we also report the results only on the altered word forms (“Altered Forms”). Across the three languages, we notice 12 For each token, we first map the morphological feature annotations in the original UD schema to the UniMorph schema (McCarthy et al., 2018). 13 Tan et al. (2020) follows similar methodology using English-only LemmInflect tool, but our approach is scalable to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train spl"
2021.emnlp-main.570,P08-1059,0,0.0600684,"ra for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explic"
2021.emnlp-main.570,2020.acl-main.660,0,0.0163098,"igure 5: A diachronic study of grammatical wellformedness of WMT English→X systems’ outputs. The systems in general are becoming more fluent. In the last two years the best systems produce as wellformed outputs as the reference translations. Turkish (WMT18). In the case of German, a significant number of case assignment rules are dependent on the lexeme (as noted in §3) and we expect future work on lexicalized rules to partially address this drawback. In Turkish, the low parser quality plays a significant role and highlights the need for further work on parsing morphologically-rich languages (Tsarfaty et al., 2020). Last, we note that human judgments, unlike L’ AMBRE, incorporate both well-formedness and adequacy (with respect to the source). Therefore, we recommend using L’ AMBRE in tandem with standard MT metrics to obtain a good indication of overall performance, both during model training and evaluation. We additionally perform a correlation analysis of L’ AMBRE with perplexity, BLEU and chrF on the WMT system outputs (A.5 in Appendix). As expected, we see a strong negative correlation with perplexity (low perplexity and high L’ AMBRE). For BLEU and chrF, the results are quite similar to the correla"
2021.emnlp-main.570,Q19-1040,0,0.0140367,"ability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniqu"
2021.emnlp-main.570,E17-2060,0,0.0233458,"rammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring access to trained MT models. Comparison with Other Metrics: We also compare L’ AMBRE to other metrics that capture fluency and/or grammatical well-formedness, namely perplexity as computed by large language mode"
2021.emnlp-main.570,P16-1162,0,0.00537509,"ture work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring"
2021.emnlp-main.570,K17-3009,0,0.0289797,"to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LA"
2021.emnlp-main.64,2020.acl-main.386,0,0.0980244,"Model XLNet-Base S ELF E XPLAIN-XLNet + LIL S ELF E XPLAIN-XLNet + GIL S ELF E XPLAIN-XLNet + GIL + LIL 93.4 94.3 94.0 94.6 RoBERTa-Base S ELF E XPLAIN-RoBERTa + LIL S ELF E XPLAIN-RoBERTa + GIL S ELF E XPLAIN-RoBERTa + GIL + LIL 94.8 94.8 94.8 95.1 Table 3: Ablation: S ELF E XPLAIN-XLNet and S ELF E XPLAIN-RoBERTa base models on SST-2. 4 Explanation Evaluation Explanations are notoriously difficult to evaluate quantitatively (Doshi-Velez et al., 2017). A good model explanation should be (i) relevant to the current input and predictions and (ii) understandable to humans (DeYoung et al., 2020; Jacovi and Goldberg, 2020; Wiegreffe et al., 2020; Jain et al., 2020). Towards this, we evaluate whether the explanations along the following diverse criteria: • Sufficiency – Do explanations sufficiently reflect the model predictions? Classification Results : We first evaluate the utility of classification models after incorporating 6 Accuracy • Plausibility – Do explanations appear plausible and understandable to humans? https://cogcomp.seas.upenn.edu/Data/QA/QC/ 840 • Trustability – Do explanations improve human trust in model predictions? From S ELF E XPLAIN, we extracted (i) Most relevant local concepts: these ar"
2021.emnlp-main.64,W18-5408,0,0.041297,"Missing"
2021.emnlp-main.64,N19-1357,0,0.0288508,"nsparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing global 836 Proceedings of the 2021 Conferen"
2021.emnlp-main.64,2020.acl-main.409,0,0.0533416,"XPLAIN-XLNet + GIL S ELF E XPLAIN-XLNet + GIL + LIL 93.4 94.3 94.0 94.6 RoBERTa-Base S ELF E XPLAIN-RoBERTa + LIL S ELF E XPLAIN-RoBERTa + GIL S ELF E XPLAIN-RoBERTa + GIL + LIL 94.8 94.8 94.8 95.1 Table 3: Ablation: S ELF E XPLAIN-XLNet and S ELF E XPLAIN-RoBERTa base models on SST-2. 4 Explanation Evaluation Explanations are notoriously difficult to evaluate quantitatively (Doshi-Velez et al., 2017). A good model explanation should be (i) relevant to the current input and predictions and (ii) understandable to humans (DeYoung et al., 2020; Jacovi and Goldberg, 2020; Wiegreffe et al., 2020; Jain et al., 2020). Towards this, we evaluate whether the explanations along the following diverse criteria: • Sufficiency – Do explanations sufficiently reflect the model predictions? Classification Results : We first evaluate the utility of classification models after incorporating 6 Accuracy • Plausibility – Do explanations appear plausible and understandable to humans? https://cogcomp.seas.upenn.edu/Data/QA/QC/ 840 • Trustability – Do explanations improve human trust in model predictions? From S ELF E XPLAIN, we extracted (i) Most relevant local concepts: these are the top ranked phrases based on r(nt)1:J f"
2021.emnlp-main.64,D19-1455,0,0.0634909,"Missing"
2021.emnlp-main.64,P18-1249,0,0.0265728,"inary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based on Transformer-XL (Dai et al., 2019) architecture. We incorporate S ELF E XPLAIN into RoBERTa and XLNet, and use the above encoders without the GIL and LIL layers as the baselines. We generate parse trees (Kitaev and Klein, 2018) to extract target concepts for the input and follow same pre-processing steps as the original encoder configurations for the rest. We also maintain the hyperparameters and weights from the pre-training of the encoders. The architecture with GIL and LIL modules are fine-tuned on datasets described in §3. For the number of global influential concepts K, we consider two settings K = 5, 10. We also perform hyperparameter tuning on α, β = {0.01, 0.1, 0.5, 1.0} and report results on the best model configuration. All models were trained on an NVIDIA V-100 GPU. GIL and LIL layers in Table 2. Across t"
2021.emnlp-main.64,D16-1011,0,0.0494078,"Missing"
2021.emnlp-main.64,D19-1523,0,0.0401229,"Missing"
2021.emnlp-main.64,N16-1082,1,0.863053,"extracted explanations without the rest of the input. An explanation that achieves high accuracy using this classifier is indicative of its ability to recover the original model prediction. We evaluate the explanations on the sentiment analysis task. Explanations from S ELF E X PLAIN are incorporated to the FRESH framework and we compare the predictive accuracy of the explanations in comparison to baseline explanation methods. Following Jain et al. (2020), we use the same experimental setup and saliency-based baselines such as attention (Lei et al., 2016; Bastings et al., 2019) and gradient (Li et al., 2016) based explanation methods. From Table 47 , we observe that S ELF E XPLAIN explanations from LIL and GIL show high predictive performance compared to all the baseline methods. Additionally, GIL explanations outperform full-text (an explanation that uses all of the input sample) performance, which is often considered an upper-bound for span-based explanation approaches. We hypothesize that this is because GIL explanation concepts from the training data are very relevant to help disambiguate the input text. In summary, outputs from S ELF E X PLAIN are more predictive of the label compared to pri"
2021.emnlp-main.64,C02-1150,0,0.19989,"a S ELF E XPLAIN-RoBERTa (K=5) S ELF E XPLAIN-RoBERTa (K=10) 94.8 95.1 95.1 53.5 54.3 54.1 97.0 97.6 97.6 89.0 89.4 89.2 96.2 96.3 96.3 Table 2: Performance comparison of models with and without GIL and LIL layers. All experiments used the same encoder configurations. We use the development set for SST-2 results (test set of SST-2 is part of GLUE benchmark) and test sets for - SST-5, TREC-6, TREC-50 and SUBJ α, β = 0.1 for all the above settings. same dataset as before, but modifies it into a finergrained 5-class classification task. (iii) TREC-6 6 : a question classification task proposed by Li and Roth (2002), where each question should be classified into one of 6 question types. (iv) TREC-50: a fine-grained version of the same TREC-6 question classification task with 50 classes (v) SUBJ: subjective/objective binary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based"
2021.emnlp-main.64,2021.ccl-1.108,0,0.110428,"Missing"
2021.emnlp-main.64,D15-1166,0,0.0609058,"we show that our interpreted model outputs are perceived as more trustworthy, understandable, and adequate for explaining model decisions compared to previous approaches to explainability. This opens an exciting research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that reli"
2021.emnlp-main.64,P19-1334,0,0.0354235,"ion datasets show that S ELF E X PLAIN facilitates interpretability without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option Th"
2021.emnlp-main.64,P05-1015,0,0.354132,"onfigurations. We use the development set for SST-2 results (test set of SST-2 is part of GLUE benchmark) and test sets for - SST-5, TREC-6, TREC-50 and SUBJ α, β = 0.1 for all the above settings. same dataset as before, but modifies it into a finergrained 5-class classification task. (iii) TREC-6 6 : a question classification task proposed by Li and Roth (2002), where each question should be classified into one of 6 question types. (iv) TREC-50: a fine-grained version of the same TREC-6 question classification task with 50 classes (v) SUBJ: subjective/objective binary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based on Transformer-XL (Dai et al., 2019) architecture. We incorporate S ELF E XPLAIN into RoBERTa and XLNet, and use the above encoders without the GIL and LIL layers as the baselines. We generate parse trees (Kitaev and Klein, 2018) to extract target concept"
2021.emnlp-main.64,D19-1425,1,0.855195,"at S ELF E X PLAIN facilitates interpretability without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors el"
2021.emnlp-main.64,D13-1170,0,0.0109943,"Missing"
2021.emnlp-main.64,2020.acl-main.432,0,0.0318201,"fister, 2020) may provide greater transparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing glob"
2021.emnlp-main.64,P19-1487,0,0.0223033,"extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy (DOE) National interpreting a given sample as a weighted sum of Nuclear Security Administration (NNSA) Office the training samples while Croce et al. (2019) iden- of Defense Nuclear Nonproliferation Research tifies influential training samples using a kernel- and Development (DNN R&D"
2021.emnlp-main.64,N16-3020,0,0.419917,"facts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors elevated the movie predicted sentiment: positive Input Word Attributions SelfExplain The fantastic actors elevated the movie Top relevant concepts fantastic actors (0.7) elevated (0.1).. Influential training concepts fabulous acting (0.4) stunning (0.2) .. Figure 1: A sample of interpretable concepts from S ELF E XPLAIN for a binary sentiment analysis task. Compared to saliency-map style word attributions, S ELF E XPLAIN can provide explanations via concepts in the input sample and the concepts in the training data for already-trained models, inh"
2021.emnlp-main.64,D15-1044,0,0.0527456,"utputs are perceived as more trustworthy, understandable, and adequate for explaining model decisions compared to previous approaches to explainability. This opens an exciting research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy ("
2021.emnlp-main.64,P19-1282,0,0.0205655,"aakkola, 2018; Arik and Pfister, 2020) may provide greater transparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 20"
2021.emnlp-main.64,D19-1002,0,0.0329771,"ion capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing global 836 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.64,D19-1420,0,0.0304888,"Missing"
2021.emnlp-main.64,D08-1004,0,0.0655718,"g research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy (DOE) National interpreting a given sample as a weighted sum of Nuclear Security Administration (NNSA) Office the training samples while Croce et al. (2019) iden- of Defense Nuclear"
2021.emnlp-main.64,D17-1323,0,0.0306271,"without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors elevated the movie predicted sentiment: positive"
2021.findings-acl.338,D18-1316,0,0.10155,"leverage adversarial and counterfactual examples (Kaushik et al., 2020; Srivastava et al., 2020). A reliable method for creating counterfactual data is to collect human-written adversarial negative responses (Sai et al., 2020), but it is expensive, time-consuming, and difficult to scale. Our goal is to create reliable automatic methods for synthesizing adversarial negative responses. The most common approach to generating natural language adversarial examples is to paraphrase or insert typos, synonyms, or words relevant to the context in the inputs (Iyyer et al., 2018; Ebrahimi et al., 2018; Alzantot et al., 2018; Zhang et al., 2019). In open domain conversations, however, a context can have a wide range of possible responses with varied forms and semantics. Small lexical 3867 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3867–3883 August 1–6, 2021. ©2021 Association for Computational Linguistics Error category Description Sample responses C-ent Incorrect entities or actors (R,G) Context: I am so happy that you are doing okay. Response: My friend is always happy. C-time Incorrect Time expressions (R) Incorrect subject or object of verbs or presence of one or more in"
2021.findings-acl.338,W05-0909,0,0.633628,"es conditioned on random contexts. Key-context Our approach that generates responses conditioned on test context keywords and random context history. Key-sem Our approach similar to Key-context which additionally conditions on words semantically related to the keywords in the context. For each context, adversarial train sets are created by adding 5 random negative responses to the set of 5 negative responses created from the above approaches. If an approach create more than 5 responses, we randomly select 5 from them. For dialogue evaluation, we compare the above approaches with BLEU, METEOR (Banerjee and Lavie, 2005), embedding based metrics SkipThought (Kiros et al., 2015), Vec Extrema (Forgues et al., 2014), and RUBER (Tao et al., 2018) and BERTScore (Zhang et al., 2020a). 4.2.4 Models We experiment with following architectures for ranking and evaluation models in our experiments: 1) Bert (Devlin et al., 2019). We use the SA-Bert model (Gu et al., 2020), 2) Electra (Clark et al., 2020), pre-trained with a replaced token detection objective and employs a generator-discriminator framework, and 3) Poly-encoders (Humeau et al., 2020), allows for fast real-time inference by precomputing each candidate respon"
2021.findings-acl.338,D19-1195,0,0.0429381,"Missing"
2021.findings-acl.338,2020.acl-main.225,0,0.0131606,"which we discuss next. 3.1 Mask-and-fill Approach This approach modifies and corrupts original utterances related to a context as shown in Figure 1. It consists of two steps: 1) masking, where one or more tokens of an original utterance are masked out; and 2) infilling, where the masked out tokens are substituted with new tokens. For a context C, the set of original utterances consists of: • Set of ground truth responses of the context - Rg . • Set of utterances from the context - Uc . • Set of retrieved responses based on context - Re . Masking: We use the hierarchical masking function from Donahue et al. (2020) which selectively masks spans at the granularities of words, n-grams, and sentences. We apply the masking function to each utterance multiple times to get up to 3 masked versions per utterance. Each utterance is constrained to have at least two masked spans. The spans are selected randomly for masking following Donahue et al. (2020). Infilling: We extend the Infilling Language Model (ILM) from Donahue et al. (2020) for dialogue Figure 1: Mask-and-fill approach using ILM model. ILM is trained to infill n-grams in place of blanks in a response. Tokens after [infill] replace the [blank] tokens."
2021.findings-acl.338,P18-2006,0,0.132572,"prior work proposed to leverage adversarial and counterfactual examples (Kaushik et al., 2020; Srivastava et al., 2020). A reliable method for creating counterfactual data is to collect human-written adversarial negative responses (Sai et al., 2020), but it is expensive, time-consuming, and difficult to scale. Our goal is to create reliable automatic methods for synthesizing adversarial negative responses. The most common approach to generating natural language adversarial examples is to paraphrase or insert typos, synonyms, or words relevant to the context in the inputs (Iyyer et al., 2018; Ebrahimi et al., 2018; Alzantot et al., 2018; Zhang et al., 2019). In open domain conversations, however, a context can have a wide range of possible responses with varied forms and semantics. Small lexical 3867 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3867–3883 August 1–6, 2021. ©2021 Association for Computational Linguistics Error category Description Sample responses C-ent Incorrect entities or actors (R,G) Context: I am so happy that you are doing okay. Response: My friend is always happy. C-time Incorrect Time expressions (R) Incorrect subject or object of verbs or pre"
2021.findings-acl.338,D17-1215,0,0.0283635,"e response. PersonaChat dataset (Zhang et al., 2018) is a corpus of human-human personaconditioned conversations consisting of 8938 dialogues in the train set. We sample 2 random context-response pairs from each dialogue with a total of 17876 contexts for training. We prepend the persona utterances to the dialogue contexts in our experiments. Since there is no human-created adversarial test set available for PersonaChat dataset, we construct an artificial adversarial dataset by randomly selecting an utterance from the dialog context and inserting it in the set of candidate responses following Jia and Liang (2017) and Whang et al. (2021). The adversarial test set for each context consists of the ground truth response, one utterance selected from the dialog context, and 8 random negative responses. The random test set consists of 9 random negative responses. 4.2.2 Metrics For classification task, we report the accuracy following (Sai et al., 2020). For ranking task, we report standard ranking metrics - Recall Rn @k and mean reciprocal rank (MRR). For DailyDialog++, n is 6 in Recall as candidates consist of one positive response with 5 negative responses. For PersonaChat, n is 10. For both classification"
2021.findings-acl.338,2021.naacl-main.240,1,0.72834,"s the response either inappropriate or incoherent to the context, but it remains topically 3874 similar to the context. In Key-sem the dialogue acts, some entities and other tokens of the generated response depend on a random context the response is conditioned on, which also makes the response inappropriate or incoherent to the context. 5 Related Work Dialogue response ranking and evaluation are important tasks in dialogue domain because even the recent large pretrained-language model based architectures (Zhang et al., 2020b; Humeau et al., 2020; Adiwardana et al., 2020; Roller et al., 2021; Gupta et al., 2021) have been shown to be susceptible to creating inconsistent, ungrammatical and incoherent responses (Roller et al., 2021). Traditional word-overlap based metrics like BLEU have been shown to be ineffective for dialogue response scoring (Liu et al., 2016; Gupta et al., 2019). Recently trainable metrics such as ADEM (Lowe et al., 2017), RUBER (Ghazarian et al., 2019) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversar"
2021.findings-acl.338,W19-5944,1,0.924946,"int by the size of the dataset, and they provide lesser coverage over categories Ccont, C-strat and C-lang (Table 1), our approaches have no such constraints. Performance on dialogue evaluation To study the performance of various approaches on real systems, we compare them on the task of Dialogue evaluation or scoring. We measure the correlation between the scores predicted by the approaches in Table 4 with human provided ratings. Reference based metrics like BLEU-2, METEOR, SkipThought and Vec Extrema achieve very low correlations, similar to findings reported in prior art (Liu et al., 2016; Gupta et al., 2019). BERTScore and RUBER achieve moderate correlation. Our approach Key-sem achieves the best correlations, followed by Mask-and-fill. BM25’s performance is lower than that of our approaches, but it is higher than the Random and Semi-hard approaches. Although Token-subs did not achieve high performance on the classification and ranking tasks, it performs well on this task. This is likely because real model outputs contains more of 3873 Context Random Mask-and-fill Key-sem Human A: Julia, will you be my wife? B: I’m sorry, Steven. C: Please, Julia, I have made proposal to you five times . I really"
2021.findings-acl.338,2020.findings-emnlp.7,0,0.0212432,"19) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversarial or counterfactual data creation techniques have been proposed for applications such as evaluation (Gardner et al., 2020; Madaan et al., 2020), attacks (Ebrahimi et al., 2018; Wallace et al., 2019; Jin et al., 2020), explanations (Goodwin et al., 2020; Ross et al., 2020) or training models to be robust against spurious patterns and biases (Garg et al., 2019; Huang et al., 2020). Adversarial examples are crafted through operations such as adding noisy characters (Ebrahimi et al., 2018; Pruthi et al., 2019), paraphrasing (Iyyer et al., 2018), replacing with synonyms (Alzantot et al., 2018; Jin et al., 2020), rule based token-level transformations (Kryscinski et al., 2020), or inserting words relevant to the context (Zhang et al., 2019). While these approaches are optimized to change the predictions of a target model by perturbing the inputs, our approaches are more general and are not optimized towards any target model. Polyjuice (Wu et al., 2021) and FactCC (Kryscins"
2021.findings-acl.338,D19-1128,0,0.0438725,"Missing"
2021.findings-acl.338,I17-1099,0,0.0256078,"hat generate negative examples for training more robust dialogue systems. These generated adversarial responses have high content similarity with the contexts but are either incoherent, inappropriate or not fluent. Our approaches are fully data-driven and can be easily incorporated in existing models and datasets. Experiments on classification, ranking and evaluation tasks across multiple datasets demonstrate that our approaches outperform strong baselines in providing informative negative examples for training dialogue systems.1 1 Introduction Due to growing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select a"
2021.findings-acl.338,2020.emnlp-main.741,0,0.13326,"t (Reimers and Gurevych, 2019) for semantic similarity calculation with α set to the recommended value of 0.07. Token-subs (Kryscinski et al., 2020) Training data is generated by applying a series of rule-based transformations on the positive responses. Transformations include pronoun, entity and number swapping, sentence negation and noise injection. BM25 Top responses returned by BM25 (Robertson and Zaragoza, 2009) based on similarity with the context. Any ground truth response is removed from this response set if present by chance. This baseline is inspired from Karpukhin et al. (2020) and Lin et al. (2020) and has shown strong performance in passage and response retrieval. Mask-and-fill Our approach that infills utterances conditioned on random contexts. Key-context Our approach that generates responses conditioned on test context keywords and random context history. Key-sem Our approach similar to Key-context which additionally conditions on words semantically related to the keywords in the context. For each context, adversarial train sets are created by adding 5 random negative responses to the set of 5 negative responses created from the above approaches. If an approach create more than 5 re"
2021.findings-acl.338,D16-1230,0,0.0901112,"Missing"
2021.findings-acl.338,P17-1103,0,0.0162341,"xt. 5 Related Work Dialogue response ranking and evaluation are important tasks in dialogue domain because even the recent large pretrained-language model based architectures (Zhang et al., 2020b; Humeau et al., 2020; Adiwardana et al., 2020; Roller et al., 2021; Gupta et al., 2021) have been shown to be susceptible to creating inconsistent, ungrammatical and incoherent responses (Roller et al., 2021). Traditional word-overlap based metrics like BLEU have been shown to be ineffective for dialogue response scoring (Liu et al., 2016; Gupta et al., 2019). Recently trainable metrics such as ADEM (Lowe et al., 2017), RUBER (Ghazarian et al., 2019) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversarial or counterfactual data creation techniques have been proposed for applications such as evaluation (Gardner et al., 2020; Madaan et al., 2020), attacks (Ebrahimi et al., 2018; Wallace et al., 2019; Jin et al., 2020), explanations (Goodwin et al., 2020; Ross et al., 2020) or training models to be robust against spurious patterns an"
2021.findings-acl.338,2020.acl-main.64,0,0.0179941,"are important tasks in dialogue domain because even the recent large pretrained-language model based architectures (Zhang et al., 2020b; Humeau et al., 2020; Adiwardana et al., 2020; Roller et al., 2021; Gupta et al., 2021) have been shown to be susceptible to creating inconsistent, ungrammatical and incoherent responses (Roller et al., 2021). Traditional word-overlap based metrics like BLEU have been shown to be ineffective for dialogue response scoring (Liu et al., 2016; Gupta et al., 2019). Recently trainable metrics such as ADEM (Lowe et al., 2017), RUBER (Ghazarian et al., 2019) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversarial or counterfactual data creation techniques have been proposed for applications such as evaluation (Gardner et al., 2020; Madaan et al., 2020), attacks (Ebrahimi et al., 2018; Wallace et al., 2019; Jin et al., 2020), explanations (Goodwin et al., 2020; Ross et al., 2020) or training models to be robust against spurious patterns and biases (Garg et al., 2019; Huang et al., 2020). Adversarial examp"
2021.findings-acl.338,2021.naacl-main.383,1,0.7317,"egorization, we manually analyzed responses present in outputs of generative models, candidates of retrieval sets, and human written adversarial dialogue responses (Sai et al., 2020). Categories C-ent, C-time and C-cont are errors related to various inconsistencies and logical flaws in the responses and indicate poor response appropriateness. Categories C-speaker, C-follow and C-strat are error types specific to the dialogue setting and indicate poor response coherence. Category C-lang indicates poor response fluency. Our categorization of errors is inspired by the categorization suggested by Pagnoni et al. (2021) for factuality of summarization, and Higashinaka et al. (2019); Ko et al. 3868 (2019) and Sato et al. (2020) for dialogue. These categories inform our approaches as well as error analysis. 3 Methodology For a given dialogue context C and its gold response Rg , our goal is to generate an adversarial response Ra such that while achieving high scores from dialogue ranking or evaluation models, it should not be a valid response to the context C. Dialogue ranking and evaluation models trained with such hard synthetic negative responses should learn to associate response relevance with features bey"
2021.findings-acl.338,D14-1162,0,0.0895174,"false negatives during training can lead to the model learning to misclassify appropriate responses. However due to the open-ended nature of dialogue responses, preventing generation of false negatives is not trivial. In addition to conditioning on random contexts, we incorporate the following mechanisms during infilling to further reduce false negative generation: • Semantics of substitution: We only select token substitutions which were not present in the tokens which were blanked. We also lower the generation probability of the blanked tokens’ top 10 related words based on GloVe embedding (Pennington et al., 2014) similarity by a factor of 100. This ensures that the blanks are not infilled by the originally blanked tokens or any related words. • Degree of substitution - To ensure that the gen3869 Training [context] How long did it take you to get your license? [keywords] month [sep] license [response] It took me 1 month to get the license Testing [context] We should visit the park today. [keywords] license [response] We will bring our license and documents. Figure 2: Keyword-guided approach for adversarial response generation. During training, the model learns to generate a response conditioned on its"
2021.findings-acl.338,P19-1561,0,0.0164517,"obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversarial or counterfactual data creation techniques have been proposed for applications such as evaluation (Gardner et al., 2020; Madaan et al., 2020), attacks (Ebrahimi et al., 2018; Wallace et al., 2019; Jin et al., 2020), explanations (Goodwin et al., 2020; Ross et al., 2020) or training models to be robust against spurious patterns and biases (Garg et al., 2019; Huang et al., 2020). Adversarial examples are crafted through operations such as adding noisy characters (Ebrahimi et al., 2018; Pruthi et al., 2019), paraphrasing (Iyyer et al., 2018), replacing with synonyms (Alzantot et al., 2018; Jin et al., 2020), rule based token-level transformations (Kryscinski et al., 2020), or inserting words relevant to the context (Zhang et al., 2019). While these approaches are optimized to change the predictions of a target model by perturbing the inputs, our approaches are more general and are not optimized towards any target model. Polyjuice (Wu et al., 2021) and FactCC (Kryscinski et al., 2020) proposed approaches for modelagnostic general-purpose counterfactual generation. These approaches change the mode"
2021.findings-acl.338,D19-1410,0,0.0146247,"pproaches consistently perform the best across all model architectures compared to baselines on the Adversarial test set, just short of models trained with human created adversarial data. Poly-encoder’s accuracy is not available as it ranks candidates relative to each other. Human (Sai et al., 2020) Human written adversarial responses. Random Responses sampled from random contexts. Semi-hard (Li et al., 2019) Sampling scheme which selects samples from a batch based on their similarity scores with a margin of α from the positive response score. We perform static sampling and use Sentence-Bert (Reimers and Gurevych, 2019) for semantic similarity calculation with α set to the recommended value of 0.07. Token-subs (Kryscinski et al., 2020) Training data is generated by applying a series of rule-based transformations on the positive responses. Transformations include pronoun, entity and number swapping, sentence negation and noise injection. BM25 Top responses returned by BM25 (Robertson and Zaragoza, 2009) based on similarity with the context. Any ground truth response is removed from this response set if present by chance. This baseline is inspired from Karpukhin et al. (2020) and Lin et al. (2020) and has show"
2021.findings-acl.338,2021.eacl-main.24,0,0.0116957,"radictions which makes the response either inappropriate or incoherent to the context, but it remains topically 3874 similar to the context. In Key-sem the dialogue acts, some entities and other tokens of the generated response depend on a random context the response is conditioned on, which also makes the response inappropriate or incoherent to the context. 5 Related Work Dialogue response ranking and evaluation are important tasks in dialogue domain because even the recent large pretrained-language model based architectures (Zhang et al., 2020b; Humeau et al., 2020; Adiwardana et al., 2020; Roller et al., 2021; Gupta et al., 2021) have been shown to be susceptible to creating inconsistent, ungrammatical and incoherent responses (Roller et al., 2021). Traditional word-overlap based metrics like BLEU have been shown to be ineffective for dialogue response scoring (Liu et al., 2016; Gupta et al., 2019). Recently trainable metrics such as ADEM (Lowe et al., 2017), RUBER (Ghazarian et al., 2019) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content"
2021.findings-acl.338,2021.findings-acl.336,0,0.0191944,"Missing"
2021.findings-acl.338,W17-7301,0,0.017565,"rsarial response space and can create false negative responses. Adversarial semantic collisions (Song et al., 2020) aims to generate texts that are semantically unrelated but judged as similar by NLP models to expose model vulnerabilities. However, the outputs which are unrelated to the context are not useful for adversarial training as they are easy to classify. Finally, negative sampling strategies have also been studied for creating hard negative samples in context of visual embeddings (Faghri et al., 2018; Guo et al., 2018), knowledge graphs (Kotnis and Nastase, 2017), document retrieval (Saeidi et al., 2017; Karpukhin et al., 2020) and response retrieval (Li et al., 2019; Lin et al., 2020). In this work we compare and build upon past work and are the first to propose generative approaches for adversarial negative response creation in dialogue. 6 Conclusion This paper introduces approaches for synthesizing adversarial negative responses for training more robust dialogue response ranking and evaluation models. To synthesize a rich and comprehensive set of responses, we present and analyze categories of errors which affect the models. Our proposed approaches do not require any manual annotation and"
2021.findings-acl.338,2020.tacl-1.52,0,0.289987,"nking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate responses according to coherence and appropriateness. Ranking and evaluation models are generally trained using true positive responses and randomly selected negative responses, which raises two issues. First, random negative candidates often have low content similarity with the context, and thus models learn to associate response coherence and appropriateness with content similarity (Yuan et al., 2019; Whang et al., 2021; Sai et al., 2020). In real systems, generated response candidates tend to be more similar in terms of content, and so other factors (e.g., time expressions, dialogue acts, inconsistencies) tend to be more important. Second, randomly selecting candidates as negative examples in an open domain context can result in false negatives, leading to misclassification of appropriate responses. To make dialogue models more robust to the spurious pattern of content similarity, prior work proposed to leverage adversarial and counterfactual examples (Kaushik et al., 2020; Srivastava et al., 2020). A reliable method for crea"
2021.findings-acl.338,2020.acl-main.55,0,0.106766,"ing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate responses according to coherence and appropriateness. Ranking and evaluation models are generally trained using true positive responses and randomly selected negative responses, which raises two issues. First, random negative candidates often have low content similarity with the context, and thus models learn to associate response coherence and appropriateness with content similarity (Yuan et al., 2019; Whang et al., 2021; Sai et al., 2020). In real systems, generated response candidates tend to be more similar in terms of content, and so o"
2021.findings-acl.338,2020.acl-main.183,0,0.017037,"aining more robust dialogue systems. These generated adversarial responses have high content similarity with the contexts but are either incoherent, inappropriate or not fluent. Our approaches are fully data-driven and can be easily incorporated in existing models and datasets. Experiments on classification, ranking and evaluation tasks across multiple datasets demonstrate that our approaches outperform strong baselines in providing informative negative examples for training dialogue systems.1 1 Introduction Due to growing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate responses according to"
2021.findings-acl.338,2020.emnlp-main.344,0,0.0344266,"sed approaches for modelagnostic general-purpose counterfactual generation. These approaches change the model’s prediction by creating small edits through substitutions and insertions to the inputs. They are not applicable to our setting where we aim to flip the gold label, that is, convert a valid response to an adversarial response, while the model prediction should ideally remain the same to create hard training examples. Furthermore small perturbations do not provide good coverage over the adversarial response space and can create false negative responses. Adversarial semantic collisions (Song et al., 2020) aims to generate texts that are semantically unrelated but judged as similar by NLP models to expose model vulnerabilities. However, the outputs which are unrelated to the context are not useful for adversarial training as they are easy to classify. Finally, negative sampling strategies have also been studied for creating hard negative samples in context of visual embeddings (Faghri et al., 2018; Guo et al., 2018), knowledge graphs (Kotnis and Nastase, 2017), document retrieval (Saeidi et al., 2017; Karpukhin et al., 2020) and response retrieval (Li et al., 2019; Lin et al., 2020). In this wo"
2021.findings-acl.338,D19-1221,0,0.0153547,"en shown to be ineffective for dialogue response scoring (Liu et al., 2016; Gupta et al., 2019). Recently trainable metrics such as ADEM (Lowe et al., 2017), RUBER (Ghazarian et al., 2019) and USR (Mehri and Eskenazi, 2020) have been proposed for these tasks. However, since they are trained using negative samples obtained from random contexts, they are also prone to the spurious pattern of content similarity. Adversarial or counterfactual data creation techniques have been proposed for applications such as evaluation (Gardner et al., 2020; Madaan et al., 2020), attacks (Ebrahimi et al., 2018; Wallace et al., 2019; Jin et al., 2020), explanations (Goodwin et al., 2020; Ross et al., 2020) or training models to be robust against spurious patterns and biases (Garg et al., 2019; Huang et al., 2020). Adversarial examples are crafted through operations such as adding noisy characters (Ebrahimi et al., 2018; Pruthi et al., 2019), paraphrasing (Iyyer et al., 2018), replacing with synonyms (Alzantot et al., 2018; Jin et al., 2020), rule based token-level transformations (Kryscinski et al., 2020), or inserting words relevant to the context (Zhang et al., 2019). While these approaches are optimized to change the"
2021.findings-acl.338,J19-1005,0,0.019904,"egative examples for training dialogue systems.1 1 Introduction Due to growing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate responses according to coherence and appropriateness. Ranking and evaluation models are generally trained using true positive responses and randomly selected negative responses, which raises two issues. First, random negative candidates often have low content similarity with the context, and thus models learn to associate response coherence and appropriateness with content similarity (Yuan et al., 2019; Whang et al., 2021; Sai et al., 2020). In real systems, genera"
2021.findings-acl.338,D19-1011,0,0.0385099,"Missing"
2021.findings-acl.338,P19-1559,0,0.109579,"nd counterfactual examples (Kaushik et al., 2020; Srivastava et al., 2020). A reliable method for creating counterfactual data is to collect human-written adversarial negative responses (Sai et al., 2020), but it is expensive, time-consuming, and difficult to scale. Our goal is to create reliable automatic methods for synthesizing adversarial negative responses. The most common approach to generating natural language adversarial examples is to paraphrase or insert typos, synonyms, or words relevant to the context in the inputs (Iyyer et al., 2018; Ebrahimi et al., 2018; Alzantot et al., 2018; Zhang et al., 2019). In open domain conversations, however, a context can have a wide range of possible responses with varied forms and semantics. Small lexical 3867 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3867–3883 August 1–6, 2021. ©2021 Association for Computational Linguistics Error category Description Sample responses C-ent Incorrect entities or actors (R,G) Context: I am so happy that you are doing okay. Response: My friend is always happy. C-time Incorrect Time expressions (R) Incorrect subject or object of verbs or presence of one or more incorrect entities or c"
2021.findings-acl.338,P18-1205,0,0.0474112,"tive examples for training more robust dialogue systems. These generated adversarial responses have high content similarity with the contexts but are either incoherent, inappropriate or not fluent. Our approaches are fully data-driven and can be easily incorporated in existing models and datasets. Experiments on classification, ranking and evaluation tasks across multiple datasets demonstrate that our approaches outperform strong baselines in providing informative negative examples for training dialogue systems.1 1 Introduction Due to growing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate r"
2021.findings-acl.338,2020.acl-demos.30,0,0.416808,"roach similar to Key-context which additionally conditions on words semantically related to the keywords in the context. For each context, adversarial train sets are created by adding 5 random negative responses to the set of 5 negative responses created from the above approaches. If an approach create more than 5 responses, we randomly select 5 from them. For dialogue evaluation, we compare the above approaches with BLEU, METEOR (Banerjee and Lavie, 2005), embedding based metrics SkipThought (Kiros et al., 2015), Vec Extrema (Forgues et al., 2014), and RUBER (Tao et al., 2018) and BERTScore (Zhang et al., 2020a). 4.2.4 Models We experiment with following architectures for ranking and evaluation models in our experiments: 1) Bert (Devlin et al., 2019). We use the SA-Bert model (Gu et al., 2020), 2) Electra (Clark et al., 2020), pre-trained with a replaced token detection objective and employs a generator-discriminator framework, and 3) Poly-encoders (Humeau et al., 2020), allows for fast real-time inference by precomputing each candidate response representation once, and then ranking candidate responses for retrieval by attending to the context. 4.3 Results and Discussion In this section, we compare"
2021.findings-acl.338,2020.acl-main.4,0,0.0174346,"ion task comprises of scoring or rating a response for its quality. For this task, we report the correlation of model scores with human provided ratings. We leverage the human ratings released by the following sources: 1) 600 ratings for response “sensibility” from (Zhao and Kawahara, 2020) with inter-rater agreement &gt; 0.6 (Krippendorff’s α (Krippendorff, 2018)). The responses consist of outputs from hierarchical recurrent encoder decoder (HRED) model with Attention (Serban et al., 2016) and Variational HRED model with attention (Serban et al., 2017); 2) 700 ratings for response quality from (Zhao et al., 2020). The responses are from 6 different generative models - Seq-2-Seq (Sutskever et al., 2014), attentional Seq-2-Seq, HRED, VHRED, GPT2-small, and GPT2-medium (Wolf et al., 2019) with greedy decoding, ancestral sampling, and nucleus sampling based decoding (Holtzman et al., 2020). The inter-rater agreement is 0.815 (Krippendorff’s α), and 3) Since the first two sources do not cover retrieval model outputs, we additionally collect quality ratings for 100 responses from a retrieval model’s (Poly-Encoder (Humeau et al., 2020)) selected responses and 100 human written responses with moderate inter-a"
2021.findings-acl.338,P18-1103,0,0.0132039,"iding informative negative examples for training dialogue systems.1 1 Introduction Due to growing availability of dialogue corpora (Li et al., 2017; Zhang et al., 2018; Smith et al., 2020) and the advancement of neural architectures (Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019), dialogue systems have achieved considerable success. As typically formulated, dialogue models generate one or more candidate responses 1 Code and data are publicly available https: //github.com/prakharguptaz/Adv_gen_ dialogue to a provided context, consisting of past dialogue turns. Dialogue ranking (Zhou et al., 2018; Wu et al., 2019) and evaluation models (Tao et al., 2018; Yi et al., 2019; Sato et al., 2020), in turn, are deployed to select and score candidate responses according to coherence and appropriateness. Ranking and evaluation models are generally trained using true positive responses and randomly selected negative responses, which raises two issues. First, random negative candidates often have low content similarity with the context, and thus models learn to associate response coherence and appropriateness with content similarity (Yuan et al., 2019; Whang et al., 2021; Sai et al., 2020). In re"
2021.findings-emnlp.288,2020.emnlp-main.656,0,0.0714434,"Missing"
2021.findings-emnlp.288,2020.emnlp-main.44,1,0.887221,"Missing"
2021.findings-emnlp.288,D19-1176,1,0.839088,"Missing"
2021.findings-emnlp.288,D19-1481,0,0.0429927,"Missing"
2021.findings-emnlp.288,D19-1107,0,0.0635706,"Missing"
2021.findings-emnlp.288,N19-1166,0,0.0209439,"ce, 2016). Moderated comments offer significant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset w"
2021.findings-emnlp.288,P19-1357,1,0.835507,"independently taken out of context. Pavlopoulos et al. (2020) challenged this assumption and examined if context matters in toxic language detection. While they found a significant number of human annotation labels were changed when context is additionally given, they could not find evidence that context actually improves the performance of classifiers. Our work also examines the importance of context, but we do not limit our scope to toxic language detection and investigate a broader set of community norm violation ranging from formatting issues to trolling. Beyond Incivility and Hate Speech Jurgens et al. (2019) claims “abusive behavior online falls along a spectrum, and current approaches focus only on a narrow range” and urges to expand the scope of problems in online abuse. Most work on online conversation has been focused on certain types of rule violation such as incivility and toxic language (e.g., Zhang et al., 2018; Chang and Danescu-Niculescu-Mizil, 2019; Almerekhi et al., 2020). In this work, we focus on a broader concept of community norm violation and provide a new dataset and tasks to facilitate future research in this direction. 7 Conclusion comments are made, and, moreover, have focuse"
2021.findings-emnlp.288,2020.acl-main.483,0,0.0203578,"omments offer significant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset we anonymize individual"
2021.findings-emnlp.288,2020.acl-main.486,0,0.01354,"ant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset we anonymize individual usernames and per"
2021.findings-emnlp.288,W19-3507,0,0.034109,"-sensitive content violation types that actually occur in the wild. To enable future research on detecting community-specific norm violations, we constructed a dataset that retrieves online conversation threads and comments deleted by moderators, categorized by community norm violations. We discuss ethical considerations related to protecting user privacy in §2. Additionally, we acknowledge that the dataset itself can incorporate unintentional biases. For example, it can incorporate moderators’ biases in deciding which comments are selected to be removed (Binns et al., 2017; Myers West, 2018; Shen and Rose, 2019). The unmoderated comments can include norm-violating comments that were missed by the moderators (Chandrasekharan et al., 2018). By constructing a large scale dataset that spans multiple subreddits and moderators’ teams we partially mitigate these concerns. To investigate this further, future work could incorporate an additional evaluation procedure with test sets containing held-out moderators (cf. Geva et al., 2019). Online communities establish their own norms for what is acceptable behavior. However, current NLP methods for identifying unacceptable behavior have largely overlooked the con"
2021.findings-emnlp.288,N16-2013,0,0.0347584,"paired unmoderated conversations as a control set. Each moderated conversation is matched with up to two unmoderated conversations from the same post and with most 4 Ranked by number of subscribers as of April 2021 5 Prior work has created datasets used to detect sinWe were unable to retrieve an additional 21K removed gle types of norm violations in social media mes- norm-violating comments, which were unavailable in the PushShift archive. We still include these corresponding consages (e.g. incivility, hate speech or hostility) versations in our data release as they can be useful in the task (Waseem and Hovy, 2016; Founta et al., 2018). of forecasting future norm violations. 3387 Rule Types Advertising Moderation Enforcement similar conversation lengths as the target moderated conversation. Ethical Considerations for Protecting User Privacy Our dataset focuses, in part, on comments that moderators have viewed as objectionable and therefore removed. While these moderated comments are still publicly available, their use requires additional ethical reflection and precautions to preserve the dignity and privacy of users (Townsend and Wallace, 2016). Moderated comments offer significant benefit to the study"
2021.findings-emnlp.288,P18-1125,0,0.0205568,"ves the performance of classifiers. Our work also examines the importance of context, but we do not limit our scope to toxic language detection and investigate a broader set of community norm violation ranging from formatting issues to trolling. Beyond Incivility and Hate Speech Jurgens et al. (2019) claims “abusive behavior online falls along a spectrum, and current approaches focus only on a narrow range” and urges to expand the scope of problems in online abuse. Most work on online conversation has been focused on certain types of rule violation such as incivility and toxic language (e.g., Zhang et al., 2018; Chang and Danescu-Niculescu-Mizil, 2019; Almerekhi et al., 2020). In this work, we focus on a broader concept of community norm violation and provide a new dataset and tasks to facilitate future research in this direction. 7 Conclusion comments are made, and, moreover, have focused on a relatively small set of unacceptable behaviors such as incivility. In this work, we introduce a new dataset, N ORM V IO, of 51K conversations grounded with community-specific judgements of which rule is violated. Using this data, we develop new models for detecting context-sensitive rule violations, demonstra"
2021.findings-emnlp.288,2020.acl-main.396,0,0.219554,"uch as Meta-rules and Trolling; we the performance was relatively more uniform and speculate that this decreased performance is due to additional context did not contribute as much. This the increased number of parameters from adding context encoder layer to process conversation his- result suggests that providing full text of rules may help resolve certain ambiguous comments and thus tory and future work with more examples of these violations may substantially improve performance. the model rely less on the additional context. This result for history greatly expands an analysis 5 Analysis by Pavlopoulos et al. (2020) that found minimal performance gain when adding a single prior com- How many violations do current systems ment to identify toxicity; while we too find minimal miss? In part due to their targeted focus, improvement for Incivility and Harassment norms, the P ERSPECTIVE and I NCIVIL H ATE baseline adding history does improve the recognition for models miss a substantial proportion of the total other norm violations (e.g., Format and Content) norm violations. Figure 6 shows the confusion maindicating that prior context can be useful. trices of the violation detection task, where labels While the"
2021.findings-emnlp.374,P18-2103,0,0.0131293,"s can faton, 2018; Guidotti et al., 2018), and the tendency cilitate the discovery of the model’s reliance on to learn spurious correlations, in addition to the true signals of the task (Leino et al., 2019; Sagawa et al., frequent spurious patterns. For example, in natural 2020b). Both of these lead to corrosive outcomes, language inference models an over-reliance on lexical signals can be revealed via feature attribution from reduced performance on datasets in which (Gururangan et al., 2018), via instance attribution the confounds no longer hold (Jia and Liang, 2017; Gururangan et al., 2018; Glockner et al., 2018; Mc- (Han et al., 2020), or through a combination of Coy et al., 2019; Kumar et al., 2019; Clark et al., thereof (Pezeshkpour et al., 2021a). In this work, we investigate a closer interaction between inter2019), to pernicious biases in model decisions (Sun et al., 2019; Blodgett et al., 2020; Field et al., 2021), pretability and model robustness. Our research hypothesis is that interpretations and to overall reduced trust in technology (Ribeiro that discover confounds can be incorporated at et al., 2016; Ehsan et al., 2019). training time, to proactively guide the model to1 This work was done"
2021.findings-emnlp.374,N18-2017,0,0.0245686,"erpretability and robust generharmful issues are their lack of interpretability (Lipalization are not unrelated. Interpretations can faton, 2018; Guidotti et al., 2018), and the tendency cilitate the discovery of the model’s reliance on to learn spurious correlations, in addition to the true signals of the task (Leino et al., 2019; Sagawa et al., frequent spurious patterns. For example, in natural 2020b). Both of these lead to corrosive outcomes, language inference models an over-reliance on lexical signals can be revealed via feature attribution from reduced performance on datasets in which (Gururangan et al., 2018), via instance attribution the confounds no longer hold (Jia and Liang, 2017; Gururangan et al., 2018; Glockner et al., 2018; Mc- (Han et al., 2020), or through a combination of Coy et al., 2019; Kumar et al., 2019; Clark et al., thereof (Pezeshkpour et al., 2021a). In this work, we investigate a closer interaction between inter2019), to pernicious biases in model decisions (Sun et al., 2019; Blodgett et al., 2020; Field et al., 2021), pretability and model robustness. Our research hypothesis is that interpretations and to overall reduced trust in technology (Ribeiro that discover confounds ca"
2021.findings-emnlp.374,2020.acl-main.492,1,0.907403,"ility in NLP pretability, and their reliance on spurious cormodels are (1) feature attribution—identifying imrelations. Prior work proposed various apportant tokens in the input span, e.g. via saliency proaches to interpreting the black-box models maps (Li et al., 2016; Ribeiro et al., 2016); and (2) to unveil the spurious correlations, but the research was primarily used in human-computer instance attribution—explaining the model deciinteraction scenarios. It still remains underexsions as a function of influential training data (Koh plored whether or how such model interpretaand Liang, 2017; Han et al., 2020; Pruthi et al., tions can be used to automatically “unlearn” 2020b). Both lines of research aim to help users confounding features. In this work, we probuild trust in the model by showing the rationale pose influence tuning—a procedure that leverbehind the model decision. ages model interpretations to update the model parameters towards a plausible interpretation Approaches to demoting the influence of spuri(rather than an interpretation that relies on spuous confounds in the data include (1) model-based rious patterns in the data) in addition to learnapproaches to learn confound-invariant re"
2021.findings-emnlp.374,N19-1357,0,0.0300033,"ompare with the adversarial training method with gradient reversal in Pryzant et al. (2018) as a baseline, since both methods perform explicit demotions to some known confounds in the data used by the model. Future work can explore comparisons and potential combinations with other approaches addressing the spurious correlations. Interpreting NLP models by token-level importance scores over the input span is a widely adopted approach (Belinkov and Glass, 2019). These scores can be gradient-based (Simonyan et al., 2014; Li 8 Conclusion et al., 2016), attention weights if supported by the model (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), or weights from a linear model fitting the NLP models that build upon deep neural networks local region of a deep model (Ribeiro et al., 2016). are notoriously opaque about their decision process. The models can achieve better performance or learn Though instance attribution methods can be used to more efficiently if supervisions are provided for unveil problems of the model reflected by the imthese feature importance scores (Ross et al., 2017; plausible interpretations, a novel research question Zhong et al., 2019; Pruthi et al., 2020a). is whether or how the mo"
2021.findings-emnlp.374,D17-1215,0,0.0297009,"Lipalization are not unrelated. Interpretations can faton, 2018; Guidotti et al., 2018), and the tendency cilitate the discovery of the model’s reliance on to learn spurious correlations, in addition to the true signals of the task (Leino et al., 2019; Sagawa et al., frequent spurious patterns. For example, in natural 2020b). Both of these lead to corrosive outcomes, language inference models an over-reliance on lexical signals can be revealed via feature attribution from reduced performance on datasets in which (Gururangan et al., 2018), via instance attribution the confounds no longer hold (Jia and Liang, 2017; Gururangan et al., 2018; Glockner et al., 2018; Mc- (Han et al., 2020), or through a combination of Coy et al., 2019; Kumar et al., 2019; Clark et al., thereof (Pezeshkpour et al., 2021a). In this work, we investigate a closer interaction between inter2019), to pernicious biases in model decisions (Sun et al., 2019; Blodgett et al., 2020; Field et al., 2021), pretability and model robustness. Our research hypothesis is that interpretations and to overall reduced trust in technology (Ribeiro that discover confounds can be incorporated at et al., 2016; Ehsan et al., 2019). training time, to pr"
2021.findings-emnlp.374,S18-2005,0,0.0158143,"dataset and a linguistic probing dataset, 12 Importantly, the trials are considered failed based on their training set performance (0.5 accuracy equivalent to random guess), not based on the dev set or test set performance. When deploying to an unknown test set, we would know when to re-train the model based on its known training performance. 4405 but the potential application of our approach can be more impactful than the current tasks. For example, our method might be helpful for identifying and mitigating gender biases and racial biases in sentiment analysis or toxicity detection systems (Kiritchenko and Mohammad, 2018; Sap et al., 2019), by modeling the problem as a deconfounding task. One potential drawback is that these natural cases would inevitably require some extent of extra human annotations. However, we also believe the human feedback in NLP (Settles, 2011; Kaushik et al., 2020; Wang et al., 2021) is a crucial and controllable way to tackle model’s exploitation of spurious correlations in the data, which happens as a result of the absence of proper supervision. Furthermore, if we define the influence objective differently in §4, e.g. modeling which groups of examples should be influential to the pr"
2021.findings-emnlp.374,D19-1425,1,0.781817,"ng the rationale pose influence tuning—a procedure that leverbehind the model decision. ages model interpretations to update the model parameters towards a plausible interpretation Approaches to demoting the influence of spuri(rather than an interpretation that relies on spuous confounds in the data include (1) model-based rious patterns in the data) in addition to learnapproaches to learn confound-invariant representaing to predict the task labels. We show that in tions, e.g., adversarial training (Pryzant et al., 2018; a controlled setup, influence tuning can help Elazar and Goldberg, 2018; Kumar et al., 2019); (2) deconfounding the model from spurious patdata-based approaches to balance the training data, terns in data, significantly outperforming base1 e.g., counterfactual data augmentation (Zmigrod line methods that use adversarial training. et al., 2019; Kaushik et al., 2020); (3) optimization 1 Introduction approaches to account for worst-case scenarios, e.g., distributionally robust optimization (Sagawa Despite the huge success of contemporary deep et al., 2020a); and (4) post-processing approaches, learning models and various applications that they such as model ensembling (Clark et al., 201"
2021.findings-emnlp.374,N16-1082,0,0.217778,"ngineering, University of Washington {xhan77, yuliats}@cs.washington.edu Abstract Consequently, multiple approaches have been proposed to alleviate the issues of the growing inAmong the most critical limitations of deep scrutability and brittleness of the models. Two learning NLP models are their lack of interprominent approaches to interpretability in NLP pretability, and their reliance on spurious cormodels are (1) feature attribution—identifying imrelations. Prior work proposed various apportant tokens in the input span, e.g. via saliency proaches to interpreting the black-box models maps (Li et al., 2016; Ribeiro et al., 2016); and (2) to unveil the spurious correlations, but the research was primarily used in human-computer instance attribution—explaining the model deciinteraction scenarios. It still remains underexsions as a function of influential training data (Koh plored whether or how such model interpretaand Liang, 2017; Han et al., 2020; Pruthi et al., tions can be used to automatically “unlearn” 2020b). Both lines of research aim to help users confounding features. In this work, we probuild trust in the model by showing the rationale pose influence tuning—a procedure that leverbehind"
2021.findings-emnlp.374,P19-1334,0,0.0432605,"Missing"
2021.findings-emnlp.374,2021.naacl-main.75,0,0.172649,"ions, in addition to the true signals of the task (Leino et al., 2019; Sagawa et al., frequent spurious patterns. For example, in natural 2020b). Both of these lead to corrosive outcomes, language inference models an over-reliance on lexical signals can be revealed via feature attribution from reduced performance on datasets in which (Gururangan et al., 2018), via instance attribution the confounds no longer hold (Jia and Liang, 2017; Gururangan et al., 2018; Glockner et al., 2018; Mc- (Han et al., 2020), or through a combination of Coy et al., 2019; Kumar et al., 2019; Clark et al., thereof (Pezeshkpour et al., 2021a). In this work, we investigate a closer interaction between inter2019), to pernicious biases in model decisions (Sun et al., 2019; Blodgett et al., 2020; Field et al., 2021), pretability and model robustness. Our research hypothesis is that interpretations and to overall reduced trust in technology (Ribeiro that discover confounds can be incorporated at et al., 2016; Ehsan et al., 2019). training time, to proactively guide the model to1 This work was done at Carnegie Mellon University. wards avoiding the confounds and improving genCode is available at https://github.com/xhan77/ influence-tun"
2021.findings-emnlp.374,N18-1146,0,0.331649,"h aim to help users confounding features. In this work, we probuild trust in the model by showing the rationale pose influence tuning—a procedure that leverbehind the model decision. ages model interpretations to update the model parameters towards a plausible interpretation Approaches to demoting the influence of spuri(rather than an interpretation that relies on spuous confounds in the data include (1) model-based rious patterns in the data) in addition to learnapproaches to learn confound-invariant representaing to predict the task labels. We show that in tions, e.g., adversarial training (Pryzant et al., 2018; a controlled setup, influence tuning can help Elazar and Goldberg, 2018; Kumar et al., 2019); (2) deconfounding the model from spurious patdata-based approaches to balance the training data, terns in data, significantly outperforming base1 e.g., counterfactual data augmentation (Zmigrod line methods that use adversarial training. et al., 2019; Kaushik et al., 2020); (3) optimization 1 Introduction approaches to account for worst-case scenarios, e.g., distributionally robust optimization (Sagawa Despite the huge success of contemporary deep et al., 2020a); and (4) post-processing approaches,"
2021.findings-emnlp.374,2021.emnlp-main.64,1,0.77453,"s do not facilitate a systematic to those trained with the baseline methods; (3) influimprovement for the model based on the plausibil- ence tuning can still be reliable in lower-resource ity of the interpretations, which is a gap addressed setups. Future work will explore more datasets and by this work. Models designed with explicit inter- tasks, and other optimization methods. Additionpretability considerations like deep weighted aver- ally, we will explore guiding the model to learn to aging classifiers (Card et al., 2019) and SelfExplain promote core attributes of the task in addition to (Rajagopal et al., 2021) may also support instance demoting the spurious confounds. 4406 Acknowledgments We thank Haoping Bai, Sachin Kumar, and the anonymous EMNLP reviewers and area chairs for helpful discussions of this work. This material is based upon work funded by the DARPA CMO under Contract No. HR001120C0124. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof. References Elnaz Barshan, Marc-Etienne Brunet, and G. Dziugaite. 2020. RelatIF: Identifying explanatory training examples via relative influence. In Proc. A"
2021.findings-emnlp.374,2021.hcinlp-1.8,0,0.0402751,"based on its known training performance. 4405 but the potential application of our approach can be more impactful than the current tasks. For example, our method might be helpful for identifying and mitigating gender biases and racial biases in sentiment analysis or toxicity detection systems (Kiritchenko and Mohammad, 2018; Sap et al., 2019), by modeling the problem as a deconfounding task. One potential drawback is that these natural cases would inevitably require some extent of extra human annotations. However, we also believe the human feedback in NLP (Settles, 2011; Kaushik et al., 2020; Wang et al., 2021) is a crucial and controllable way to tackle model’s exploitation of spurious correlations in the data, which happens as a result of the absence of proper supervision. Furthermore, if we define the influence objective differently in §4, e.g. modeling which groups of examples should be influential to the probing instance and to what extent, we may be able to implicitly promote the core attributes in the task in addition to demoting the confounds. 7 Related Work attribution, though the flexibility of the model architecture can be more limited. One key use case of the proposed influence tuning fr"
2021.findings-emnlp.374,2020.emnlp-main.16,0,0.047615,"Missing"
2021.findings-emnlp.374,D19-1002,0,0.0229553,"ial training method with gradient reversal in Pryzant et al. (2018) as a baseline, since both methods perform explicit demotions to some known confounds in the data used by the model. Future work can explore comparisons and potential combinations with other approaches addressing the spurious correlations. Interpreting NLP models by token-level importance scores over the input span is a widely adopted approach (Belinkov and Glass, 2019). These scores can be gradient-based (Simonyan et al., 2014; Li 8 Conclusion et al., 2016), attention weights if supported by the model (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), or weights from a linear model fitting the NLP models that build upon deep neural networks local region of a deep model (Ribeiro et al., 2016). are notoriously opaque about their decision process. The models can achieve better performance or learn Though instance attribution methods can be used to more efficiently if supervisions are provided for unveil problems of the model reflected by the imthese feature importance scores (Ross et al., 2017; plausible interpretations, a novel research question Zhong et al., 2019; Pruthi et al., 2020a). is whether or how the model training can benefit Unli"
2021.findings-emnlp.374,2021.eacl-main.291,0,0.0290518,"during the decision process. Other works that aim at preventing neural models from using the spurious attributes include Elazar and Goldberg (2018) and Pryzant et al. (2018) which operate over a known set of confounds, and Kumar et al. (2019) which models unknown, latent confounds. They often involve the idea of learning invariant features across domains through adversarial training (Ganin et al., 2016; Xie et al., 2017). Spurious correlations can also be mitigated by data-based, optimization-based, and postprocessing methods (Zmigrod et al., 2019; Kaushik et al., 2020; Sagawa et al., 2020a; Yaghoobzadeh et al., 2021; Clark et al., 2019). In this work, we mainly compare with the adversarial training method with gradient reversal in Pryzant et al. (2018) as a baseline, since both methods perform explicit demotions to some known confounds in the data used by the model. Future work can explore comparisons and potential combinations with other approaches addressing the spurious correlations. Interpreting NLP models by token-level importance scores over the input span is a widely adopted approach (Belinkov and Glass, 2019). These scores can be gradient-based (Simonyan et al., 2014; Li 8 Conclusion et al., 2016"
2021.findings-emnlp.374,N16-3020,0,0.349565,"rsity of Washington {xhan77, yuliats}@cs.washington.edu Abstract Consequently, multiple approaches have been proposed to alleviate the issues of the growing inAmong the most critical limitations of deep scrutability and brittleness of the models. Two learning NLP models are their lack of interprominent approaches to interpretability in NLP pretability, and their reliance on spurious cormodels are (1) feature attribution—identifying imrelations. Prior work proposed various apportant tokens in the input span, e.g. via saliency proaches to interpreting the black-box models maps (Li et al., 2016; Ribeiro et al., 2016); and (2) to unveil the spurious correlations, but the research was primarily used in human-computer instance attribution—explaining the model deciinteraction scenarios. It still remains underexsions as a function of influential training data (Koh plored whether or how such model interpretaand Liang, 2017; Han et al., 2020; Pruthi et al., tions can be used to automatically “unlearn” 2020b). Both lines of research aim to help users confounding features. In this work, we probuild trust in the model by showing the rationale pose influence tuning—a procedure that leverbehind the model decision. ag"
2021.findings-emnlp.374,P19-1161,0,0.0139443,"ork is to deconfound the model from relying on spurious attributes during the decision process. Other works that aim at preventing neural models from using the spurious attributes include Elazar and Goldberg (2018) and Pryzant et al. (2018) which operate over a known set of confounds, and Kumar et al. (2019) which models unknown, latent confounds. They often involve the idea of learning invariant features across domains through adversarial training (Ganin et al., 2016; Xie et al., 2017). Spurious correlations can also be mitigated by data-based, optimization-based, and postprocessing methods (Zmigrod et al., 2019; Kaushik et al., 2020; Sagawa et al., 2020a; Yaghoobzadeh et al., 2021; Clark et al., 2019). In this work, we mainly compare with the adversarial training method with gradient reversal in Pryzant et al. (2018) as a baseline, since both methods perform explicit demotions to some known confounds in the data used by the model. Future work can explore comparisons and potential combinations with other approaches addressing the spurious correlations. Interpreting NLP models by token-level importance scores over the input span is a widely adopted approach (Belinkov and Glass, 2019). These scores can"
2021.findings-emnlp.63,D19-1165,0,0.027544,"t, there are limited datasets and benchmarks that evaluate NLP models’ ability to generalize to unseen dialect variations. Therefore, we only test our method on NER and POS tagging tasks because they have the best language coverage. It is an important future direction to construct high-quality datasets that consider language and dialect variations. Second, our method has slower inference speed due to test time computation. Future work can aim to reduce the cost by algorithmic or hardware innovations. Our work is related to parameter efficient fine- Acknowledgement tuning of pretrained models (Bapna et al., 2019; Pfeiffer et al., 2020b; Li and Liang, 2021; Guo This material is based on work supported by the et al., 2021). Specifically, (Üstün et al., 2020; National Science Foundation under grants 2040926 Karimi Mahabadi et al., 2021) make adapters more and 2125201. XW is supported by the Apple PhD generalizable by learning a parameter generator, fellowship. The authors would like to thank Laura while our work aims to utilize existing pretrained Rimell, Sachin Kumar and Hieu Pham for helpful adapters without further training. Pfeiffer et al. discussions on the drafts of the paper. (2021) propose to le"
2021.findings-emnlp.63,2020.coling-main.579,0,0.0997332,"Missing"
2021.findings-emnlp.63,2020.acl-main.747,0,0.0305789,"nguages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and trans- timate, and language varieties constantly emerge in ferred to other languages. Several recent w"
2021.findings-emnlp.63,2021.naacl-main.184,0,0.084255,"Missing"
2021.findings-emnlp.63,N19-1423,0,0.19552,"e embedding space for task Ti . MAD-X trains the adapters Ti (·) and Lj (·) in two steps. First, for each language Lj , its adapter Adapter Ensembling As a first solution to this Lj is inserted into M to replace the output of each problem, we propose an extremely simple strategy layer h with Lj (h). The resulting model, which we of averaging the transformed outputs of multiple denote as Lj ◦ M, is trained on unlabeled data in language adapters. Specifically, we use both the Lj using an unsupervised objective such as masked source language adapter Lsrc and adapters from language modeling (MLM; Devlin et al., 2019). related languages with similar linguistic properties Second, for each task Ti , its adapter Ti is inserted to the new language. Let R be the set of the source on top of a src language adapter Lsrc . The resulting and related language adapters. To do inference on model Ti ◦ Lsrc ◦ M is trained on the downstream a task T for the new language Lnew , we transform task Ti in language Lsrc . After these two steps, 1 Ti ◦ Lj ◦ M can be used to perform zero-shot crosshttps://adapterhub.ml/ 731 the output h of each layerP in M with the language adapters as Lavg (h) = R1 R i=1 Li (h). Entropy Minimize"
2021.findings-emnlp.63,2021.acl-long.353,0,0.0524787,"Missing"
2021.findings-emnlp.63,P17-1178,0,0.0159875,"..., T-1 do . Calculate entropy H(x, α) ← Entropy(T ◦ Lwavg (h, αt ) ◦ M) . Calculate gradient g t = ∇α H(x; αt ) . Update weighting αt+1 ← Update(αt , g t ) end . Calculate final prediction yˆ ← Predict(T ◦ Lwavg (h, αT ) ◦ M) Related hi is ru Additional Test en,ar en,de en mr,bn,ta,bho fo,no,da be,uk,bg Table 1: Test language groups and their corresponding language adapters. Adapters from languages in the first two columns are applied to the test languages in the third column. conduct experiments on named entity recognition (NER) and part-of-speech tagging (POS). We use the WikiAnn dataset (Pan et al., 2017) for NER and Universial Treebank 2.0 for POS tagging (Nivre et al., 2018). Model We use the mBERT (Devlin et al., 2019) model, which shows good performance for lowresource languages on the structured prediction tasks (Pfeiffer et al., 2020b; Hu et al., 2020). We use the English annotated data to train the task adapter. Each experiment is run with 3 different random seeds and we report the average performance. More details can be found in Appendix A. Languages Due to the scarcity of datasets for dialects, we focus on three groups of closely related languages to simulate the setup of language va"
2021.findings-emnlp.63,2021.eacl-main.39,0,0.0208731,"ining and storing extra parameters. Its performance is also not consistent across languages and tasks, likely because it is only trained on English labeled data. 50 70.0 67.5 Baselines We compare with several baselines: 1) En: the English adapter; 2) Related: the best performing related language adapter; 3) Continual learning (CL): we use the English language adapter and update its parameters using the entropy loss for each test input; 4) Fusion: learn another set of key, value and query parameters in each layer that uses the layer output as a query to mix together the output of each adapter (Pfeiffer et al., 2021). Since we do not use labeled data in the new language, we train the fusion parameters on English labeled data. 4.1 mr no F1 3.0 F1 gain by adding English F1 gain over ensemble Table 2: F1 of the baselines and our methods for each language group. EMEA-s1 updates the adapter weights with a single gradient step while EMEA-s10 updates for 10 steps. New adapter EMEA 1k 10k 50k 100k Monolingual Data 40 1k 10k 50k 100k Monolingual Data Figure 4: Comparison to training adapter on different amount of monolingual data. provements or is comparable to the best baseline on other languages. EMEA delivers f"
2021.findings-emnlp.63,2020.emnlp-demos.7,1,0.859962,"Missing"
2021.findings-emnlp.63,2021.acl-long.378,0,0.0665002,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.617,1,0.857427,"Missing"
2021.findings-emnlp.63,2020.acl-main.244,0,0.0241551,"dapters to support a new language Lnew , which is not in {L1 , L2 , ..., Ln } on a given task T without training a new adapter for Lnew . Related Language Adapters One potential solution is to find the most related language Lrel ∈ {L1 , L2 , ..., Ln } and then use T ◦ Lrel ◦ M to do inference in Lnew . However, this has two disadvantages. First, the task adapter T is only trained in the setting of T ◦ Lsrc ◦ M, so it might not generalize well to the test time setting of T ◦ Lrel ◦ M (as shown in § 4.1). Second, while the pretrained model M may be relatively robust against distribution shifts (Hendrycks et al., 2020), the specialized language adapters might make the model brittle to language variations because they are trained for specific languages. Our experiments in § 4.1 show that this solution indeed leads to poor performance. To facilitate our discussion, we briefly summarize the MAD-X framework (Pfeiffer et al., 2020b) for zero-shot cross-lingual transfer and identify its shortcomings. The goal of MAD-X is to fine-tune a multilingual pretrained model M to m downstream tasks T1 , T2 , ..., Tm , each of which could be in n languages L1 , L2 , ..., Ln . To this end, MAD-X relies on language and task a"
2021.findings-emnlp.63,D19-1252,0,0.025998,"m zero-shot transfer by first training language-level adapters on monolingual data in different languages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and t"
2021.findings-emnlp.63,2021.acl-long.47,1,0.82652,"Missing"
2021.findings-emnlp.63,2021.eacl-main.6,0,0.0305732,"lts in Tab. 2 showing en as the best individual adapter. For the hi language group, the ar adapter tends to have the least benefit, probably because it has a different script from the languages we test on. 5 Related Work beled data to combine pretrained multitask adapters whereas our method does not require any training or labeled data. While we focus on language adapters in this work, our method is also applicable to ensembling domain or task adapters. Finally, our method is inspired by the test time adaptation framework proposed for image classification (Sun et al., 2020; Wang et al., 2021; Kedia and Chinthakindi, 2021). Instead of adapting a single model, we focus on efficient utilization of many pre-trained language adapters to improve the model’s robustness to language variations. 6 Discussion and Conclusion Language and dialect cannot be simply categorized into monolithic entities. Thus a truly intelligent NLP system should be able to recognize and adapt to personalized language varieties after it is trained and deployed. However, the standard system evaluation is built on the assumption that an NLP model is fixed once it is trained. In this paper, we focus on a specific case of this general problem—we f"
2021.findings-emnlp.63,C18-1113,0,0.0405423,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.180,0,0.0424408,"Missing"
2021.mrl-1.15,Q17-1010,0,0.706352,"he PARAvMF Model Let the language to paraphrase in be L1 . Our goal is to learn a mapping f (x; θ) parameterized by θ. f takes a text x = (x1 , x2 , · · · , xm ) containing m words as input, which can be a sentence or a segment in L1 . It then generates y = (y1 , y2 , . . . , yn ) of length n in the same language such that x and y are paraphrases. That is, y represents the same meaning as x using different phrasing. We assume that no direct supervision data is available, but there exists a bilingual parallel corpus between L1 and another language L2 . We are also given pre-trained embeddings (Bojanowski et al., 2017) for words in both L1 and L2 . The dimension of both the embedding spaces is d. We use a standard transformer-based encoderdecoder model (Vaswani et al., 2017) as the underlying architecture for f . As visualized in the system diagram presented in the Appendix, f is jointly trained to perform three tasks with a shared encoder and decoder: (1) translation from L1 to L2 , (2) translation from L2 to L1 and (3) reconstructing the input text in L1 (autoencoding).2 Towards our primary goal of meaning preservation, the translation objectives help the encoder map the inputs in both the languages to a"
2021.mrl-1.15,N06-2009,0,0.107944,"hrasing aims to rewrite text while preserving its meaning and achieving a different surface re- Since parallel paraphrasing data is not available even in otherwise high-resource languages like alization. It is an eminently practical task, useful in educational applications (Inui et al., 2003; Pe- French, we focus on an unsupervised approach. tersen and Ostendorf, 2007; Pavlick and Callison- Using bilingual parallel corpora, we adapt multilingual machine translation (Johnson et al., 2017) to Burch, 2016; Xu et al., 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior work mentation in a plethora of other tasks (Berant and Liang, 2014; Romano et al., 2006; Fadaee et al., required multiple language pairs and more data to stabilize training (Tiedemann and Scherrer, 2019; 2017; Jin et al., 2018; Hou et al., 2018). Buck et al., 2018; Guo et al., 2019; Thompson and Generating diverse and coherent paraphras"
2021.mrl-1.15,P17-2090,0,0.0605751,"Missing"
2021.mrl-1.15,N13-1092,0,0.131137,"Missing"
2021.mrl-1.15,P06-1114,0,0.0394631,"hile preserving its meaning and achieving a different surface re- Since parallel paraphrasing data is not available even in otherwise high-resource languages like alization. It is an eminently practical task, useful in educational applications (Inui et al., 2003; Pe- French, we focus on an unsupervised approach. tersen and Ostendorf, 2007; Pavlick and Callison- Using bilingual parallel corpora, we adapt multilingual machine translation (Johnson et al., 2017) to Burch, 2016; Xu et al., 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior work mentation in a plethora of other tasks (Berant and Liang, 2014; Romano et al., 2006; Fadaee et al., required multiple language pairs and more data to stabilize training (Tiedemann and Scherrer, 2019; 2017; Jin et al., 2018; Hou et al., 2018). Buck et al., 2018; Guo et al., 2019; Thompson and Generating diverse and coherent paraphrases Post, 2020). To encourag"
2021.mrl-1.15,C18-1105,0,0.0151301,", 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior work mentation in a plethora of other tasks (Berant and Liang, 2014; Romano et al., 2006; Fadaee et al., required multiple language pairs and more data to stabilize training (Tiedemann and Scherrer, 2019; 2017; Jin et al., 2018; Hou et al., 2018). Buck et al., 2018; Guo et al., 2019; Thompson and Generating diverse and coherent paraphrases Post, 2020). To encourage diversity, we propose to is a difficult task. Unlike in machine translareplace the final softmax layer in the decoder with tion, where naturally occurring parallel data in a layer that learns to predict word vectors (Kumar the form of translated news, books and talks are and Tsvetkov, 2019). We show that predicting into available in abundance on the web, naturally occurword meaning representations increases diversity ring paraphrase corpora are scarce. Most common in paraph"
2021.mrl-1.15,K19-1005,0,0.0384658,"Missing"
2021.mrl-1.15,W03-1602,0,0.165255,"on two laninput, which has an effect opposite to the intended guages using a battery of computational metrics as well as in human assessment.1 one in paraphrasing. In this work, we introduce PARAvMF – a sim1 Introduction ple and effective method of training paraphrasing models by generating into embedding spaces (§2). Paraphrasing aims to rewrite text while preserving its meaning and achieving a different surface re- Since parallel paraphrasing data is not available even in otherwise high-resource languages like alization. It is an eminently practical task, useful in educational applications (Inui et al., 2003; Pe- French, we focus on an unsupervised approach. tersen and Ostendorf, 2007; Pavlick and Callison- Using bilingual parallel corpora, we adapt multilingual machine translation (Johnson et al., 2017) to Burch, 2016; Xu et al., 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior wor"
2021.mrl-1.15,P17-4012,0,0.0186786,"arly stopping with the autoencoding objective. We use IWSLT’16 test set for automatic evaluation consisting of 2331 samples in En and Fr each. For human evaluation we subsample 200 sentences from this set. We tokenize and truecase all the data using Moses preprocessing scripts (Koehn et al., 2007). We conduct additional experiments with a larger En–Fr corpus constructed using a 2M sentence-pair subset of the combination of the WMT’10 Gigaword (Tiedemann, 2012) and the OpenSubtitles corpora (Lison and Tiedemann, 2016). Implementation We modify the standard seq2seq transformer model in OpenNMT (Klein et al., 2017) to generate word embeddings (Kumar and Tsvetkov, 2019), and train it with the vMF loss with respect to target vectors. We initialize and fix the input embeddings of the encoder and decoder with off-the-shelf (sub-word based) fasttext embeddings (Bojanowski et al., 2017) for both En and Fr and align the embeddings to encourage crosslingual sharing (Artetxe et al., 2018). With a vocabulary size of 50K for each language, the combined vocabulary size of the encoder and the decoder is 100K. Both encoder and decoder consist of 6 layers with 4 attention heads. The model is optimized using Adam (King"
2021.mrl-1.15,P07-2045,0,0.00855698,"Missing"
2021.mrl-1.15,L16-1147,0,0.0284706,"the total number of training examples. 167 toencoding. We use the L1 side of the IWSLT’16 dev set for early stopping with the autoencoding objective. We use IWSLT’16 test set for automatic evaluation consisting of 2331 samples in En and Fr each. For human evaluation we subsample 200 sentences from this set. We tokenize and truecase all the data using Moses preprocessing scripts (Koehn et al., 2007). We conduct additional experiments with a larger En–Fr corpus constructed using a 2M sentence-pair subset of the combination of the WMT’10 Gigaword (Tiedemann, 2012) and the OpenSubtitles corpora (Lison and Tiedemann, 2016). Implementation We modify the standard seq2seq transformer model in OpenNMT (Klein et al., 2017) to generate word embeddings (Kumar and Tsvetkov, 2019), and train it with the vMF loss with respect to target vectors. We initialize and fix the input embeddings of the encoder and decoder with off-the-shelf (sub-word based) fasttext embeddings (Bojanowski et al., 2017) for both En and Fr and align the embeddings to encourage crosslingual sharing (Artetxe et al., 2018). With a vocabulary size of 50K for each language, the combined vocabulary size of the encoder and the decoder is 100K. Both encode"
2021.mrl-1.15,E17-1083,0,0.0722544,"ch thresholds: 0.95, 0.9, 0.85, selected empirically such that the sample size is sufficiently large. Baselines Although unsupervised methods of paraphrasing with only monolingual data have been explored in recent works (Gupta et al., 2018; Yang et al., 2019; Roy and Grangier, 2019; Patro et al., 2018; Park et al., 2019) they have not been shown to outperform translation based baselines (West et al., 2020). Hence we compare our proposed approach with translation-based baselines only. 4 Results First, we compare with bilingual pivoting baselines Automatic evaluation We observe in table 1 that (Mallinson et al., 2017a,b) which pipeline two sepPARAvMF outperforms all baselines in meaningarate translation models, L1 → L2 , and L2 → L1 . 4 We use two bilingual pivoting baselines, one based Before computing the PTED, we prune the tree to a max on continuous-output model (BP- V MF; the out- height of 3, and discard all the terminal nodes. We employ Stanford CoreNLP (Manning et al., 2014) for parsing and put vectors of the first model are first converted to APTED algorithm for edit distance (Pawlik and Augsten, discrete tokens before being fed to the next) and 2015). 168 Model E NGLISH BS↑ MET.↑ F RENCH BS↑ MET"
2021.mrl-1.15,P14-5010,0,0.00352606,"based baselines (West et al., 2020). Hence we compare our proposed approach with translation-based baselines only. 4 Results First, we compare with bilingual pivoting baselines Automatic evaluation We observe in table 1 that (Mallinson et al., 2017a,b) which pipeline two sepPARAvMF outperforms all baselines in meaningarate translation models, L1 → L2 , and L2 → L1 . 4 We use two bilingual pivoting baselines, one based Before computing the PTED, we prune the tree to a max on continuous-output model (BP- V MF; the out- height of 3, and discard all the terminal nodes. We employ Stanford CoreNLP (Manning et al., 2014) for parsing and put vectors of the first model are first converted to APTED algorithm for edit distance (Pawlik and Augsten, discrete tokens before being fed to the next) and 2015). 168 Model E NGLISH BS↑ MET.↑ F RENCH BS↑ MET.↑ BP-CE BP- V MF PARACE PARAvMF 75.0 72.1 83.5 88.6 69.4 65.5 82.3 87.2 75.0 72.2 87.4 91.6 67.5 64.2 81.6 86.4 Table 1: Meaning-preservation in generated paraphrases. BS: BertScore, MET: METEOR preservation. Both pivoting based baselines perform poorly on average. This is a consequence of error propagation exacerbated in BP- V MF5 . As a result, a very small fraction o"
2021.mrl-1.15,2020.acl-main.615,0,0.0202927,"ess this issue by using only a small random sample of the total training sentences for training 2 To bias the model against always decoding in the other language, unlike in Johnson et al. (2017); Tiedemann and Scherrer (2019), we provide a language-specific start token in the encoder input, in addition to the decoder input. with this objective.3 Second, we find that cross-entropy loss used to train the model results in peaky distributions at each decoding step where the target words get most of the probability mass. This distribution being another signal of overfitting also reduces diversity (Meister et al., 2020). We find in our preliminary experiments, that prior work to address this issue by augmenting diversity inducing objectives to the training loss (Vijayakumar et al., 2018) often comes at a cost of reducing meaning preservation. In this work, we propose using a different training loss which naturally promotes output diversity. We follow Kumar and Tsvetkov (2019), and instead of treating each word w in the vocabulary as a discrete unit, we represent it using a unit-normalized pre-trained vector e learned using monolingual corpora (Bojanowski et al., 2017). At each decoding step, instead of predi"
2021.mrl-1.15,D17-1026,1,0.90244,"Missing"
2021.mrl-1.15,Q16-1029,0,0.0266842,"tive method of training paraphrasing models by generating into embedding spaces (§2). Paraphrasing aims to rewrite text while preserving its meaning and achieving a different surface re- Since parallel paraphrasing data is not available even in otherwise high-resource languages like alization. It is an eminently practical task, useful in educational applications (Inui et al., 2003; Pe- French, we focus on an unsupervised approach. tersen and Ostendorf, 2007; Pavlick and Callison- Using bilingual parallel corpora, we adapt multilingual machine translation (Johnson et al., 2017) to Burch, 2016; Xu et al., 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior work mentation in a plethora of other tasks (Berant and Liang, 2014; Romano et al., 2006; Fadaee et al., required multiple language pairs and more data to stabilize training (Tiedemann and Scherrer, 2019; 2017; Jin et al., 2018; Hou et"
2021.mrl-1.15,P16-1049,0,0.0113681,"otherwise high-resource languages like alization. It is an eminently practical task, useful in educational applications (Inui et al., 2003; Pe- French, we focus on an unsupervised approach. tersen and Ostendorf, 2007; Pavlick and Callison- Using bilingual parallel corpora, we adapt multilingual machine translation (Johnson et al., 2017) to Burch, 2016; Xu et al., 2016), information retrieval monolingual translation. We propose to train this (Duboue and Chu-Carroll, 2006; Harabagiu and Hickl, 2006; Fader et al., 2014), in dialogue sys- model with translation and autoencoding objectives. tems (Yan et al., 2016), as well as for data aug- The latter helps simplify the training setup by using only one language pair, whereas prior work mentation in a plethora of other tasks (Berant and Liang, 2014; Romano et al., 2006; Fadaee et al., required multiple language pairs and more data to stabilize training (Tiedemann and Scherrer, 2019; 2017; Jin et al., 2018; Hou et al., 2018). Buck et al., 2018; Guo et al., 2019; Thompson and Generating diverse and coherent paraphrases Post, 2020). To encourage diversity, we propose to is a difficult task. Unlike in machine translareplace the final softmax layer in the dec"
2021.mrl-1.15,D19-1309,0,0.0171033,"ive of quality paraphrasing but will falsely contribute to high diversity scores if averaged across the entire test set. We thus measure the diversity only on subsets of the test set for which the strongest baseline (PARACE) and our model generate meaning-preserving paraphrases measured using BERTScore thresholds. We report the diversity scores for three such thresholds: 0.95, 0.9, 0.85, selected empirically such that the sample size is sufficiently large. Baselines Although unsupervised methods of paraphrasing with only monolingual data have been explored in recent works (Gupta et al., 2018; Yang et al., 2019; Roy and Grangier, 2019; Patro et al., 2018; Park et al., 2019) they have not been shown to outperform translation based baselines (West et al., 2020). Hence we compare our proposed approach with translation-based baselines only. 4 Results First, we compare with bilingual pivoting baselines Automatic evaluation We observe in table 1 that (Mallinson et al., 2017a,b) which pipeline two sepPARAvMF outperforms all baselines in meaningarate translation models, L1 → L2 , and L2 → L1 . 4 We use two bilingual pivoting baselines, one based Before computing the PTED, we prune the tree to a max on conti"
2021.mrqa-1.16,P18-1161,0,0.0387897,"Missing"
2021.mrqa-1.16,D13-1160,0,0.160387,"Missing"
2021.mrqa-1.16,P17-1171,0,0.0575816,"Missing"
2021.mrqa-1.16,D18-1052,0,0.048236,"Missing"
2021.mrqa-1.16,P19-1436,0,0.0353792,"Missing"
2021.mrqa-1.16,Q19-1026,0,0.0161548,"wer spans. When finetuning1 , the retriever is trained using distant supervision with passages containing the target answer as positive and the reader is supervised using human annotated short answer spans (Lee et al., 2019). We follow the same design and optimization setup of finetuning REALM on QA. We explore the limits of various experiment choices by introducing simple changes to the training and inference setup. Table 1 compares results from our replicated experiments of REALM to prior published results and shows that our experiments produce similar results on the Natural Questions (NQ) (Kwiatkowski et al., 2019) dataset. Detailed analysis across other metrics is in in A.2. 2.1 Scaling the Training Setup REALM performs an approximate MIPS for retrieving the top c relevant documents based on a retrieval score, Sretr (pi , q) = h> q hpi where hq and hpi are question and passage representations respectively. The system is finetuned in practice on a single machine with a 12GB GPU with batch size 1. While this is modest use of resources, we show that this results in suboptimal training. We begin by scaling the REALM system during training. We perform exact MIPS search by leveraging the efficiency of large"
2021.mrqa-1.16,2020.acl-main.703,0,0.0643864,"Missing"
2021.naacl-main.240,P98-1013,0,0.649941,"semantic structure and underlying intents of the exemplar responses. To achieve ﬂuent and contextually-appropriate generated responses that adhere to the semantic structure of exemplars and capture their high-level goals, we use the frame semantics of the exemplars to guide the generation of responses. The central idea 3 Model of frame semantics is frames, which are semantic abstractions describing universal categories of Our model EDGE extends a dialogue generation events, concepts, and relationships, based on the model TransferTransfo (Wolf et al., 2019) to conlinguistic resource FrameNet (Baker et al., 1998). trol generation by including semantic frames from Frame semantics provide a higher-level represen- an exemplar response in addition to the dialogue tation of individual tokens in the response exem- history. TransferTransfo is based on the transplars based on the purpose of those tokens in the former architecture and ﬁne-tunes a generative preresponse. For instance, the tokens ‘hear’, ‘say’, trained model (GPT) (Radford, 2018) with two ‘see’, ‘smell’, ‘feel’, all share a similar purpose of objective functions: (1) a language modelling obtheir semantic frame label ‘Perception’, such that jecti"
2021.naacl-main.240,D19-5602,0,0.0638526,"Missing"
2021.naacl-main.240,N19-1124,0,0.0391937,"Missing"
2021.naacl-main.240,D19-1195,0,0.342693,"ur body Vegan food is very healthy. Exemplar Frames Responses I want to drink milk as well. D ESIRING I NGESTION F OOD You want to eat some vegan food? We eat a lot of vegetables. It’s delicious. We like to eat organic food. Table 1: EDGE generates responses to dialogue contexts by conditioning the response generation on the semantic frames of existing response exemplars to create coherent and controlled replies. ing on response exemplars, we can generate coherent responses that follow the intents of the exemplars without manually labeling vast amounts of data. Current exemplar-based methods (Cai et al., 2019b,a; Wu et al., 2019) have two key drawbacks: 1 Introduction (1) the models often overﬁt to the training data, Large pre-trained language models (Radford et al., then produce incoherent responses by copying ir2019; Devlin et al., 2019) currently used to power relevant tokens from exemplar responses into the dialogue generation systems produce increasingly generated responses, and (2) the models often learn ﬂuent and appropriate responses for novel dialogue to ignore the exemplars, then produce responses contexts (Wolf et al., 2019; Zhang et al., 2020; that are not controlled by the strategic e"
2021.naacl-main.240,P19-1285,0,0.0704878,"Missing"
2021.naacl-main.240,N19-1423,0,0.0274677,"Missing"
2021.naacl-main.240,E17-1059,0,0.0707238,"Missing"
2021.naacl-main.240,W18-2706,0,0.061827,"Missing"
2021.naacl-main.240,P19-1254,0,0.0450536,"Missing"
2021.naacl-main.240,D19-1190,0,0.0395912,"Missing"
2021.naacl-main.240,2020.acl-main.22,0,0.0367152,"Missing"
2021.naacl-main.240,N18-1025,0,0.0649497,"Missing"
2021.naacl-main.240,W19-5944,1,0.839369,"uman-rated and automatic metrics that capture aspects of response quality individually (e.g., is the response grammatically correct?) and with respect to the context (e.g., is the response a valid continuation of the preceding conversation?). We additionally consider how well the responses adhere to the semantic structure of the retrieved response exemplars. 5.1.1 Evaluation Metrics Word overlap metrics have been shown to correlate poorly with human judgements of quality of responses (Liu et al., 2016) as they don’t account for all the plausible responses for any given conversational context (Gupta et al., 2019). We therefore conducted human evaluations to capture aspects of 5 https://kaggle.com/rtatman/fraudulent-email-corpus https://parl.ai/projects/polyencoder 6 3021 http://github.com/huggingface/transfer-learning-conv-ai https://github.com/jcyk/seqgen/tree/master/ranker Model Dist-2 Dist-3 MaUdE Coherent Fluent Consistent Interesting Semantics Retrieval GPT2-Gen LSTM-Tokens LSTM-Frames GPT2-Tokens EDGE (Ours) 0.294 0.249 0.182 0.185 0.254 0.278 0.526 0.494 0.380 0.392 0.513 0.571 0.921 0.905 0.890 0.901 0.927 0.922 2.41 2.42 2.04∗ 2.36∗ 2.19∗ 2.52 2.61 2.55 2.10∗ 2.30∗ 2.47∗ 2.63 2.48 2.41∗ 2.11∗"
2021.naacl-main.240,P19-1255,0,0.0410994,"Missing"
2021.naacl-main.240,N16-1014,0,0.0710817,"kers per context for all 7 models, with a total of 2100 ratings. The Cohen’s Kappa (Cohen, 1968) value for inter-annotator agreement is 0.45 for the annotations, indicating moderate agreement. We also evaluate the models using an unreferenced automated evaluation metric MaUdE (Sinha et al., 2020) which uses large pre-trained language models to extract latent representations of utterances and is trained using Noise Contrastive Estimation. It has shown high correlation with human judgements on criteria such as interestingness and ﬂuency. For measuring diversity of responses we calculate Dist-n (Li et al., 2016). It is the ratio of distinct n-grams to total number n-grams for all the responses from a model. Metric 1 Exemplar 5 Exemplars 10 Exemplars GPT2-Gen Dist-2 Dist-3 0.240 0.481 0.129 0.327 0.096 0.270 LSTM-Tokens SemCov Avg BLEU-2 Dist-2 Dist-3 0.347 0.216 0.184 0.387 0.354 0.214 0.104 0.267 0.360 0.214 0.080 0.223 0.620 0.170 0.155 0.409 0.625 0.161 0.118 0.344 EDGE SemCov Avg BLEU-2 Dist-2 Dist-3 0.650 0.192 0.274 0.569 Table 3: EDGE shows higher semantic coverage (SemCov) with the exemplar responses while showing lower lexical overlap (lower Avg BLEU-2). EDGE also achieves higher diversity ("
2021.naacl-main.240,2020.acl-main.428,0,0.152804,"nd, when LSTM-Tokens inappropriately copies tokens (top left and bottom right examples), the responses often become incoherent (e.g., copying “singer” and “violinist” results in “i’ve got a singer, but i was the violinist.”). Although EDGE generates context speciﬁc responses which generally adhere to the semantics of the exemplars, EDGE still occasionally diverges from the exemplar response. For instance, the model can hallucinate details irrelevant to the context (the word “bank” in the bottom left example), a problem common in neural generative models (Tian et al., 2019; Dušek et al., 2020; Li et al., 2020). Model GPT2-Gen EDGE Coherence 2.10 2.39 Intent Engagement 33.0 79.7 70.1 87.3 Table 6: Human evaluation of Coherence (reported from 1-low to 3-high), Intent (Follows Intent reported as a percentage), and Engagement (reported as a percentage) in the Anti-Scam setting. an exhaustive set of exemplar responses. Further, when such systems directly apply a pre-written response to a novel dialogue context, the response can be incoherent. We demonstrate an application of EDGE in the anti-scam domain where we generate a variety of coherent responses to novel dialogue contexts that capture the high-le"
2021.naacl-main.240,I17-1099,0,0.251646,"f-the-art Poly-encoder model (Humeau et al., 2020) to retrieve response candidates and then select the highest ranked response as the exemplar response. We add the semantic frame sequence from the exemplar response as the input along with the context of the conversation. The model then creates a response which is controlled by the semantic frames from the exemplar, and coherent with the context of the conversation. 4 Experimental Setup We compared our model to existing generative and retrieval-based approaches in two settings: (1) open-domain dialogue generation using the Dailydialog dataset (Li et al., 2017), and (2) goaloriented anti-scam dialogue generation using a set of fraudulent emails (Radev, 2008) as prompts and a small set of intent-speciﬁc anti-scam response exemplars to inform responses. For the anti-scam domain, we investigated exemplar conditioned responses in a case without domain-speciﬁc training (i.e. zero-shot generation). 4.1 Datasets Open-Domain We use the Dailydialog dataset (Li et al., 2017), which consists of 13,118 daily conversations covering topics such as culture, education, tourism and health. The validation and test sets have 1000 conversations each. We consider maximu"
2021.naacl-main.240,D16-1230,0,0.0837506,"Missing"
2021.naacl-main.240,N18-2078,0,0.0274471,"g et al., 2020), Meena (Adiwardana et al., 2020) and Blenderbot (Roller et al., 2020) are capable of generating interesting and human-like responses, our focus is on controlling the response generation process by conditioning on exemplars. By using semantic frames from exemplar responses, our method ﬂexibly captures intents implicitly present in the exemplar frames, and exercises ﬁne-grained semantic control over generation of new responses based on these exemplars. Semantics-Based Generation has reemerged for use in various tasks such as paraphrasing (Wang et al., 2019), machine translation (Marcheggiani et al., 2018) and story generation (Tu et al., 2019; Fan et al., 2019). Semantic representations such as semantic frames and semantic role labels provide abstractions that capture the underlying meanings of different surface realizations (e.g., paraphrases, other languages). We are the ﬁrst to explicitly model frame semantic representations (Fillmore, 1982) in dialogue generation. 7 Conclusion We present EDGE, an exemplar-based generative dialogue model. By generating responses that preserve semantic structures from exemplars, EDGE maintains desired qualities of dialogue systems including intents and strat"
2021.naacl-main.240,2020.acl-main.64,0,0.0528865,"d 3 the highest) on the following criteria: • Coherent Does the response serve as a valid continuation of the preceding conversation? • Interesting Is the response dull or interesting? • Fluent Is the response naturally written, grammatical correct and non-repetitive? • Consistent Does the response make logical sense given the context and by itself? • Uses semantics Does the response share similar concepts with the retrieved response? The annotators were shown a conversational context and responses to rate, and were provided more detailed instructions and examples for each criteria, following Mehri and Eskenazi (2020). We collected ratings from 3 workers per context for all 7 models, with a total of 2100 ratings. The Cohen’s Kappa (Cohen, 1968) value for inter-annotator agreement is 0.45 for the annotations, indicating moderate agreement. We also evaluate the models using an unreferenced automated evaluation metric MaUdE (Sinha et al., 2020) which uses large pre-trained language models to extract latent representations of utterances and is trained using Noise Contrastive Estimation. It has shown high correlation with human judgements on criteria such as interestingness and ﬂuency. For measuring diversity o"
2021.naacl-main.240,Q18-1027,0,0.0178713,"(Wolf et al., 2019; Zhang et al., 2020; that are not controlled by the strategic exemplars. Budzianowski and Vulic´, 2019). However, the genTo generate locally coherent responses that erated responses are often uninformative or incon- also adhere to high-level dialogue constraints, we sistent with high-level constraints of a dialogue present EDGE, a model that uses the semantic strucsystem and the tasks it supports. Prior work added ture of an exemplar response, instead of the tokens high-level control for speciﬁc intents such as po- of the exemplar response, to guide generation (Taliteness (Niu and Bansal, 2018), emotions (Zhong ble 1). For a novel dialogue context, we retrieve et al., 2019) and persona (Song et al., 2019) through a human-written response exemplar and represent a ﬁxed set of coarse labels, but these methods re- it using its semantic frames (Fillmore, 1982). We quire manually labelling data for each new intent. then incorporate the dialogue context and the seOne approach for adding control over response mantic frames of the response exemplars in a powintents is to use response exemplars that are hand- erful pre-trained conditional language model (Radwritten or strategically curated to"
2021.naacl-main.240,P18-1123,0,0.0432422,"Missing"
2021.naacl-main.240,N19-1263,0,0.0391677,"Missing"
2021.naacl-main.240,N19-1269,0,0.0419335,"Missing"
2021.naacl-main.240,2020.acl-main.220,0,0.0184601,"and by itself? • Uses semantics Does the response share similar concepts with the retrieved response? The annotators were shown a conversational context and responses to rate, and were provided more detailed instructions and examples for each criteria, following Mehri and Eskenazi (2020). We collected ratings from 3 workers per context for all 7 models, with a total of 2100 ratings. The Cohen’s Kappa (Cohen, 1968) value for inter-annotator agreement is 0.45 for the annotations, indicating moderate agreement. We also evaluate the models using an unreferenced automated evaluation metric MaUdE (Sinha et al., 2020) which uses large pre-trained language models to extract latent representations of utterances and is trained using Noise Contrastive Estimation. It has shown high correlation with human judgements on criteria such as interestingness and ﬂuency. For measuring diversity of responses we calculate Dist-n (Li et al., 2016). It is the ratio of distinct n-grams to total number n-grams for all the responses from a model. Metric 1 Exemplar 5 Exemplars 10 Exemplars GPT2-Gen Dist-2 Dist-3 0.240 0.481 0.129 0.327 0.096 0.270 LSTM-Tokens SemCov Avg BLEU-2 Dist-2 Dist-3 0.347 0.216 0.184 0.387 0.354 0.214 0"
2021.naacl-main.240,D19-5605,0,0.0172849,"and Blenderbot (Roller et al., 2020) are capable of generating interesting and human-like responses, our focus is on controlling the response generation process by conditioning on exemplars. By using semantic frames from exemplar responses, our method ﬂexibly captures intents implicitly present in the exemplar frames, and exercises ﬁne-grained semantic control over generation of new responses based on these exemplars. Semantics-Based Generation has reemerged for use in various tasks such as paraphrasing (Wang et al., 2019), machine translation (Marcheggiani et al., 2018) and story generation (Tu et al., 2019; Fan et al., 2019). Semantic representations such as semantic frames and semantic role labels provide abstractions that capture the underlying meanings of different surface realizations (e.g., paraphrases, other languages). We are the ﬁrst to explicitly model frame semantic representations (Fillmore, 1982) in dialogue generation. 7 Conclusion We present EDGE, an exemplar-based generative dialogue model. By generating responses that preserve semantic structures from exemplars, EDGE maintains desired qualities of dialogue systems including intents and strategies implicitly present in the curate"
2021.naacl-main.240,D17-1228,0,0.057765,"Missing"
2021.naacl-main.240,W18-5713,0,0.0460703,"Missing"
2021.naacl-main.240,D18-1356,0,0.0154205,"eir underlying intent and add context speciﬁc details where appro- emotion (Zhou et al., 2018), politeness (Niu and priate (e.g., “inﬂuence the decision of the ministry” Bansal, 2018) and style (Keskar et al., 2019) in the last example). Thus, EDGE’s key advan- through coarse-level labels or control phrases (Wu tages over prior approaches are its controllability et al., 2020). Some traditional approaches used and zero-shot performance. templates to control the generation of text (Reiter 3025 et al., 2005; McRoy et al., 2003). Some recent approaches learn templates from the data and exemplars (Wiseman et al., 2018; Ye et al., 2020; Yang et al., 2020). We explore the common case of response exemplars instead of inﬂexible templates or coarse labels to guide the dialogue response generation. Although state-of-the-art models pretrained on large dialogue corpus such as DialoGPT (Zhang et al., 2020), Meena (Adiwardana et al., 2020) and Blenderbot (Roller et al., 2020) are capable of generating interesting and human-like responses, our focus is on controlling the response generation process by conditioning on exemplars. By using semantic frames from exemplar responses, our method ﬂexibly captures intents impl"
2021.naacl-main.240,2020.acl-main.531,0,0.028495,"peciﬁc details where appro- emotion (Zhou et al., 2018), politeness (Niu and priate (e.g., “inﬂuence the decision of the ministry” Bansal, 2018) and style (Keskar et al., 2019) in the last example). Thus, EDGE’s key advan- through coarse-level labels or control phrases (Wu tages over prior approaches are its controllability et al., 2020). Some traditional approaches used and zero-shot performance. templates to control the generation of text (Reiter 3025 et al., 2005; McRoy et al., 2003). Some recent approaches learn templates from the data and exemplars (Wiseman et al., 2018; Ye et al., 2020; Yang et al., 2020). We explore the common case of response exemplars instead of inﬂexible templates or coarse labels to guide the dialogue response generation. Although state-of-the-art models pretrained on large dialogue corpus such as DialoGPT (Zhang et al., 2020), Meena (Adiwardana et al., 2020) and Blenderbot (Roller et al., 2020) are capable of generating interesting and human-like responses, our focus is on controlling the response generation process by conditioning on exemplars. By using semantic frames from exemplar responses, our method ﬂexibly captures intents implicitly present in the exemplar frames"
2021.naacl-main.240,2020.acl-demos.30,0,0.662793,"unts of data. Current exemplar-based methods (Cai et al., 2019b,a; Wu et al., 2019) have two key drawbacks: 1 Introduction (1) the models often overﬁt to the training data, Large pre-trained language models (Radford et al., then produce incoherent responses by copying ir2019; Devlin et al., 2019) currently used to power relevant tokens from exemplar responses into the dialogue generation systems produce increasingly generated responses, and (2) the models often learn ﬂuent and appropriate responses for novel dialogue to ignore the exemplars, then produce responses contexts (Wolf et al., 2019; Zhang et al., 2020; that are not controlled by the strategic exemplars. Budzianowski and Vulic´, 2019). However, the genTo generate locally coherent responses that erated responses are often uninformative or incon- also adhere to high-level dialogue constraints, we sistent with high-level constraints of a dialogue present EDGE, a model that uses the semantic strucsystem and the tasks it supports. Prior work added ture of an exemplar response, instead of the tokens high-level control for speciﬁc intents such as po- of the exemplar response, to guide generation (Taliteness (Niu and Bansal, 2018), emotions (Zhong"
2021.naacl-main.383,P98-1013,0,0.0933451,"aset, we aim to both assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In §4 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in §5 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work. 2 Typology of Factual Errors factual errors further detailing these three levels. This typology is theoretically grounded in frame semantics (Fillmore et al., 1976; Baker et al., 1998; Palmer et al., 2005) and linguistic discourse analysis (Brown and Yule, 1983). Examples for each category are shown in Table 1. 2.1 Semantic Frame Errors A semantic frame is a schematic representation of an event, relation, or state, which consists of a predicate and a list of participants, called frame elements (Baker et al., 1998). A semantic frame has both core and non-core frame elements (FE). Core frame elements are essential to the meaning of the frame, while non-core (e.g. location, time) provide additional descriptive information. Our first three categories capture factual errors in"
2021.naacl-main.383,2021.eacl-main.220,1,0.815473,"Missing"
2021.naacl-main.383,2020.acl-main.703,0,0.0156907,"ctive and include more factual errors on average (Maynez et al., 2020). For a diverse set of model summaries, we collect publicly available model outputs from different summarization models with differing factuality capabilities. For the CNN/DM dataset, we use model outputs from a LSTM Seq-to-Seq model (S2S) (Rush et al., 2015), a Pointer-Generator Network (PGN) model (See et al., 2017), a Bottom-Up Summarization (BUS) model (Gehrmann et al., 2018), a Bert based Extractive-Abstractive model (BertSum) (Liu and Lapata, 2019) and a jointly pretrained transformer based encoder-decoder model BART (Lewis et al., 2020). For the XSum dataset, we collect model outputs from a Topic-Aware CNN Model (Narayan et al., 2018), a Pointer-Generator Network (PGN) model, a randomly initialized (TransS2S) (Vaswani et al., 2017) and one initialized with Bert-Base (BertS2S) (Devlin et al., 2019).2 Details of the models used are provided in §A.1. Annotation Collection Using the above model generated summaries, we collect human annotations from three independent annotators for 250 articles from each dataset (with a total of 1250 model outputs on CNN/DM and 1000 on XSum). We annotate each sentence of a summary to break the ju"
2021.naacl-main.383,W04-1013,0,0.279081,"Missing"
2021.naacl-main.383,D19-1387,0,0.0152776,"crucial to identify discourse level errors. On the other hand, XSum summaries are more abstractive and include more factual errors on average (Maynez et al., 2020). For a diverse set of model summaries, we collect publicly available model outputs from different summarization models with differing factuality capabilities. For the CNN/DM dataset, we use model outputs from a LSTM Seq-to-Seq model (S2S) (Rush et al., 2015), a Pointer-Generator Network (PGN) model (See et al., 2017), a Bottom-Up Summarization (BUS) model (Gehrmann et al., 2018), a Bert based Extractive-Abstractive model (BertSum) (Liu and Lapata, 2019) and a jointly pretrained transformer based encoder-decoder model BART (Lewis et al., 2020). For the XSum dataset, we collect model outputs from a Topic-Aware CNN Model (Narayan et al., 2018), a Pointer-Generator Network (PGN) model, a randomly initialized (TransS2S) (Vaswani et al., 2017) and one initialized with Bert-Base (BertS2S) (Devlin et al., 2019).2 Details of the models used are provided in §A.1. Annotation Collection Using the above model generated summaries, we collect human annotations from three independent annotators for 250 articles from each dataset (with a total of 1250 model"
2021.naacl-main.383,2020.acl-main.173,0,0.310192,"rate the factual correctness of summaries and fail to highly fluent but often factually unreliable correlate with the human judgements of factualoutputs. This motivated a surge of metrics ity (Falke et al., 2019; Kryscinski et al., 2019). attempting to measure the factuality of autoMore recent metrics proposed to improve the evalumatically generated summaries. Due to the lack of common benchmarks, these metrics ation of summarization factuality (Kryscinski et al., cannot be compared. Moreover, all these 2020; Durmus et al., 2020; Wang et al., 2020; methods treat factuality as a binary concept Maynez et al., 2020) cannot be compared due to and fail to provide deeper insights on the the lack of common benchmarks. More critically, kinds of inconsistencies made by different while these approaches differ in the way they model systems. To address these limitations, we factuality, they all consider factuality as a binary devise a typology of factual errors and use it to concept, labeling summaries of any length as faccollect human annotations of generated summaries from state-of-the-art summarization tual or non-factual. They do not provide any finesystems for the CNN/DM and XSum datasets. grained understand"
2021.naacl-main.383,D18-1206,0,0.201088,"to this typology achieving near perfect agreement with experts. We collect FRANK, the resulting dataset, to benchmark factuality metrics and state-of-art summarization systems. in the text. This typology also provides us with the means to categorize the types of errors made by summarization systems, helping us gain deeper insights than simply categorizing content as factual or hallucinated. We define an annotation protocol of factuality based on our typology and collect a dataset of human judgements over a diverse set of model generated summaries on the CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets (§3). Through this dataset, we aim to both assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In §4 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in §5 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work. 2 Typology of Factual Errors factual errors further detailing these three levels. This typology is theoretically grounded in frame se"
2021.naacl-main.383,J05-1004,0,0.16912,"assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In §4 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in §5 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work. 2 Typology of Factual Errors factual errors further detailing these three levels. This typology is theoretically grounded in frame semantics (Fillmore et al., 1976; Baker et al., 1998; Palmer et al., 2005) and linguistic discourse analysis (Brown and Yule, 1983). Examples for each category are shown in Table 1. 2.1 Semantic Frame Errors A semantic frame is a schematic representation of an event, relation, or state, which consists of a predicate and a list of participants, called frame elements (Baker et al., 1998). A semantic frame has both core and non-core frame elements (FE). Core frame elements are essential to the meaning of the frame, while non-core (e.g. location, time) provide additional descriptive information. Our first three categories capture factual errors in each of these componen"
2021.naacl-main.383,P02-1040,0,0.111309,"Missing"
2021.naacl-main.383,D19-1410,0,0.0475446,"Missing"
2021.naacl-main.383,2020.tacl-1.18,0,0.0566495,"Missing"
2021.naacl-main.383,D15-1044,0,0.0486138,"nd XSum datasets as they present different characteristics. CNN/DM summaries are longer, with three sentences on average, while XSum has only single sentence summaries. Having longer summaries is crucial to identify discourse level errors. On the other hand, XSum summaries are more abstractive and include more factual errors on average (Maynez et al., 2020). For a diverse set of model summaries, we collect publicly available model outputs from different summarization models with differing factuality capabilities. For the CNN/DM dataset, we use model outputs from a LSTM Seq-to-Seq model (S2S) (Rush et al., 2015), a Pointer-Generator Network (PGN) model (See et al., 2017), a Bottom-Up Summarization (BUS) model (Gehrmann et al., 2018), a Bert based Extractive-Abstractive model (BertSum) (Liu and Lapata, 2019) and a jointly pretrained transformer based encoder-decoder model BART (Lewis et al., 2020). For the XSum dataset, we collect model outputs from a Topic-Aware CNN Model (Narayan et al., 2018), a Pointer-Generator Network (PGN) model, a randomly initialized (TransS2S) (Vaswani et al., 2017) and one initialized with Bert-Base (BertS2S) (Devlin et al., 2019).2 Details of the models used are provided i"
bhatia-etal-2014-unified,J98-2001,0,\N,Missing
bhatia-etal-2014-unified,C88-1044,0,\N,Missing
bhatia-etal-2014-unified,W13-2234,1,\N,Missing
C10-2144,C10-1002,1,0.846835,"Galon Prime Minister Avshalom Vilan Bar On Meir Shitrit Limor Livnat Attorney General thanks a lot Gaza Strip Type NNC GT NNC-GT PN NNC PN PN NNC PN PN PN PN N-ADJ N-ADJ NNC-GT Table 1: Results: extracted MWEs 4 Evaluation MWEs are notoriously hard to define, and no clear-cut criteria exist to distinguish between MWEs and other frequent collocations. In order to evaluate the utility of our methodology, we conducted three different types of evaluations that we detail below and in Section 5. First, we use a small annotated corpus of Hebrew noun-noun constructions that was made available to us (Al-Haj and Wintner, 2010). The corpus consists of 463 high-frequency bi-grams of the same syntactic construction; of those, 202 are tagged as MWEs (in this case, noun compounds) and 258 as non-MWEs. This corpus consolidates the annotation of three annotators: only instances on which all three agreed were included. Since it includes both positive and negative instances, this corpus facilitates a robust evaluation of precision and recall. Of the 202 positive examples, only 121 occur in our parallel corpus; of the 258 negative examples, 91 occur in our corpus. We therefore limit the discussion to those 212 examples whose"
C10-2144,W03-1812,0,0.0171821,"Missing"
C10-2144,W03-1809,0,0.030972,"shed by their idiosyncratic behavior. Morphologically, some MWEs allow some of their constituents to freely inflect while restricting (or preventing) the inflection of other constituents. In some cases MWEs may allow constituents to undergo non-standard morphological inflections that they would not undergo in isolation. Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003). Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications. Handling MWEs correctly is beneficial for a variety of applications, including information retrieval, building ontologies, text alignment, and machine translation. Identifying MWEs and extracting them from corpora is therefore both important and difficult. In Hebrew (which is the subject of our research), this is even more challenging due to two reasons: the rich and complex morphology of the language; and the dearth of existing language resources, in particular parallel"
C10-2144,W05-0706,0,0.0218724,"Missing"
C10-2144,W09-2901,0,0.0152265,"e for medium-density languages (Varga et al., 2005). We assume the following resources: a small bilingual, sentencealigned parallel corpus; large monolingual corpora in both languages; morphological processors (analyzers and disambiguation modules) for the two languages; and a bilingual dictionary. Our experimental setup is Hebrew-English. We use a small parallel corpus (Tsvetkov and Wintner, 2010) consisting of 19,626 sentences, mostly from newspapers. The corpus consists of 271,787 English tokens (14,142 types) and 280,508 Hebrew tokens (12,555 types), and is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dictionary consist1258 ing of 78,313 translation pairs. Some of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the c"
C10-2144,W02-1801,0,0.327881,"Missing"
C10-2144,P89-1010,0,0.514191,"Missing"
C10-2144,W03-0305,0,0.0229613,"of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the corpora Automatic word alignment algorithms are noisy, and given a small parallel corpus such as ours, data sparsity is a serious problem. To minimize the parameter space for the alignment algorithm, we attempt to reduce language specific differences by pre-processing the parallel corpus. The importance of this phase should not be underestimated, especially for alignment of two radically different languages such as English and Hebrew (Dejean et al., 2003). Hebrew,1 like other Semitic languages, has a rich, complex and highly productive morphology. Information pertaining to gender, number, definiteness, person, and tense is reflected morphologically on base forms of words. In addition, prepositions, conjunctions, articles, possessives, etc., may be concatenated to word forms as prefixes or suffixes. This results in a very large number of possible forms per lexeme. We therefore tokenize the parallel corpus and then remove punctuation. We analyze the Hebrew corpus morphologically and select the most appropriate analysis in context. Adopting this"
C10-2144,W06-1203,0,0.027267,"Missing"
C10-2144,kirschenbaum-wintner-2010-general,1,0.746778,"is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dictionary consist1258 ing of 78,313 translation pairs. Some of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the corpora Automatic word alignment algorithms are noisy, and given a small parallel corpus such as ours, data sparsity is a serious problem. To minimize the parameter space for the alignment algorithm, we attempt to reduce language specific differences by pre-processing the parallel corpus. The importance of this phase should not be underestimated, especially for alignment of two radically different languages such as English and Hebrew (Dejean et al., 2003). Hebrew,1 like other Semitic languages, has a rich, complex and highly productive morphology. Information pertaining"
C10-2144,2005.mtsummit-papers.11,0,0.0293425,"Missing"
C10-2144,lavie-etal-2004-significance,0,0.0240886,"ality of word alignment, especially in the case of MWEs, is rather low, we remove “translations” that are longer than four words (these are most often wrong). We then associate each extracted MWE in Hebrew with all its possible English translations. The result is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omiss"
C10-2144,2004.tmi-1.1,1,0.829747,"ality of word alignment, especially in the case of MWEs, is rather low, we remove “translations” that are longer than four words (these are most often wrong). We then associate each extracted MWE in Hebrew with all its possible English translations. The result is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omiss"
C10-2144,J03-1002,0,0.0031543,"alignment) in parallel texts: either MWEs (which trigger 1:n or n:m alignments); or language-specific differences (e.g., the source language lexically realizes notions that are realized morphologically, syntactically or in some other way in the target language); or noise (e.g., poor translations, low-quality sentence alignment, and inherent limitations of word alignment algorithms). This motivation induces the following algorithm. Given a parallel, sentence-aligned corpus, it is first pre-processed as described above, to reduce the effect of language-specific differences. We then use Giza++ (Och and Ney, 2003) to wordalign the text, employing union to merge the alignments in both directions. We look up all 1:1 alignments in the dictionary. If the pair exists in our bilingual dictionary, we remove it from the sentence and replace it with a special symbol, ‘*’. Such word pairs are not parts of MWEs. If the pair is not in the dictionary, but its alignment score is very high (above 0.5) and it is sufficiently frequent (more than 5 occurrences), we add the pair to the dictionary but also retain it in the sentence. Such pairs are still candidates for being (parts of) MWEs. Example 1 Figure 1-a depicts a"
C10-2144,P02-1040,0,0.108273,"sult is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omissions that are transliterated by the MT system) is translated with the new dictionary as “His teachers also hate to the Zionism and besmirch his Herzl and David Ben-Gurion”; a phrase originally translated as “when so” is now properly translated as “likewise”; a"
C10-2144,tsvetkov-wintner-2010-automatic,1,0.82909,"on of English MWEs, along with their translations to Hebrew. This, again, contributes to the task of enriching our existing bilingual dictionary. 3.2 Resources Our methodology is in principle languageindependent and appropriate for medium-density languages (Varga et al., 2005). We assume the following resources: a small bilingual, sentencealigned parallel corpus; large monolingual corpora in both languages; morphological processors (analyzers and disambiguation modules) for the two languages; and a bilingual dictionary. Our experimental setup is Hebrew-English. We use a small parallel corpus (Tsvetkov and Wintner, 2010) consisting of 19,626 sentences, mostly from newspapers. The corpus consists of 271,787 English tokens (14,142 types) and 280,508 Hebrew tokens (12,555 types), and is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dic"
C10-2144,W07-1104,0,0.0392883,"Missing"
C10-2144,W06-2405,0,0.0265577,"Missing"
C10-2144,D07-1110,0,0.257706,"Missing"
C10-2144,W09-2904,0,0.0872671,"Missing"
C10-2144,J90-1003,0,\N,Missing
C10-2144,resnik-1998-parallel,0,\N,Missing
C10-2144,J93-1007,0,\N,Missing
C10-2144,W04-0412,0,\N,Missing
C10-2144,W07-1106,0,\N,Missing
C10-2144,J03-3002,0,\N,Missing
C10-2144,W04-0404,0,\N,Missing
C10-2144,W06-1204,0,\N,Missing
C10-2144,P07-2045,0,\N,Missing
C10-2144,W09-2907,0,\N,Missing
C10-2144,J00-2004,0,\N,Missing
C10-2144,P99-1068,0,\N,Missing
C10-2144,D11-1077,1,\N,Missing
C10-2144,N10-1029,0,\N,Missing
C10-2144,D11-1034,1,\N,Missing
C10-2144,W05-0603,0,\N,Missing
C10-2144,2005.mtsummit-posters.11,0,\N,Missing
C10-2144,1999.mtsummit-1.79,0,\N,Missing
C14-1100,bhatia-etal-2014-unified,1,0.727454,"es or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in Chinese (a language without articles), where the existential construction can be used to express indefinite subjects and the ba- construction can be used to express definite direct objects (Chen, 2004). Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites expressing these functions. We refer to these as communicative functions of definiteness, following Bhatia et al. (2014). Croft (2003, pp. 6–7) shows that even when two languages have access to the same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French translations (both languages use definite as well as indefinite articles) such as: (1) He showed extreme care. (unmarked) Il montra un soin extrême. (indef.) (2) I love artichokes and asparagus. (unmarked) J’aime les artichauts et les asperges. (def.) (3) His brother became a soldier. (indef.) Son frère est devenu"
C14-1100,D13-1174,1,0.849382,"ness. After preprocessing the text with a dependency parser and coreference resolver, which is described in §6.1, we extract several kinds of percepts for each NP. 4.2.1 Basic Words of interest. These are the head within the NP, all of its dependents, and its governor (external to the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing the dependency path upward from the head. For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a 3 Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection. As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are excluded from regularization. 5 See Theorem 1.2 in Breiman (2001) for details. 4 1063 binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we have additional features specific to the first and the last one. Moreover, to better capture tense, aspect and modality, we collect the attached verb’s auxiliaries. We also make note of the negative particle (with"
C14-1100,P05-1066,0,0.0178084,"rom some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000),"
C14-1100,C88-1044,0,0.577504,"to predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listen"
C14-1100,2007.mtsummit-papers.29,0,0.0509283,"ic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identifica"
C14-1100,T87-1035,0,0.537601,"aced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they are spoken. In co"
C14-1100,C10-1068,0,0.0128025,"se, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that re"
C14-1100,P06-1077,0,0.0109354,"matical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While defini"
C14-1100,W00-0708,0,0.0373875,"(Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distingu"
C14-1100,C02-1139,0,0.0446422,"dded or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share"
C14-1100,C00-2162,0,0.0170846,"s made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of"
C14-1100,W12-3807,1,0.89797,"Missing"
C14-1100,N13-1071,0,0.0612652,"Missing"
C14-1100,D10-1032,0,0.0256045,"gh we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in multiple communicative functions for each grammatical construction. Other attempts have also been made to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation, Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by prepositions. 8 Conclusion We have presented a data-driven approach to modeling the relationship between universal communicative functions associated with (in)definiteness and their lexical/grammatical realization in a particular language. Our feature-rich classifiers can give insights into this relationship as well as predict communicative functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear classifier com"
C14-1100,P10-1005,0,0.108554,"tediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling o"
C14-1100,N10-1018,0,0.0157261,"(b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which"
C14-1100,P13-1045,0,0.0784612,"Missing"
C14-1100,W13-2234,1,0.798697,"s input to (or jointly with) the coreference task. Applications such as information extraction and dialogue processing could be expected to benefit not only from coreference information, but also from some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of"
C14-1100,P02-1039,0,0.00839924,"to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme predictio"
C14-1100,2007.iwslt-1.3,0,0.0123911,"rmation (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly"
D11-1077,W03-1809,0,0.0384817,"ituents to freely inflect while restricting (or preventing) the inflection of other constituents. In some cases MWEs may allow constituents to undergo non-standard morphological inflections that 836 Shuly Wintner Department of Computer Science University of Haifa shuly@cs.haifa.ac.il they would not undergo in isolation. Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003). Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications. Correct handling of MWEs has been proven beneficial for various applications, including information retrieval, building ontologies, text alignment, and machine translation. We propose a novel architecture for identifying MWEs of various types and syntactic categories in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs r"
D11-1077,W07-1101,0,0.018996,"lds a significant improvement in performance over using pure frequency. Several works address the lexical fixedness or syntactic fixedness of (certain types of) MWEs in order to extract them from texts. An expression is considered lexically fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moir´on (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages). Our approach only requires morphological processing and a bilingual dictionary, which are more readil"
D11-1077,W02-1801,0,0.245988,"r future research. 1 To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used, in Hebrew lexicographic order, are abgdhwzxTiklmns‘pcqrˇst. 837 2 Related Work Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Pecina (2008) compares 55 different association measures in ranking German AdjN and PP-Verb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang et al., 2002; Villavicencio et al., 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs. Soon, however, it became clear that mere cooccurrence measurements are not enough to identify MWEs, and their linguistic properties should be exploited as well (Piao et al., 2005). Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs. Ramisch et al. (2008) evaluate a number of association measures on the task of identifying English Verb-Parti"
D11-1077,J90-1003,0,0.177306,"Missing"
D11-1077,E06-1043,0,0.0144426,"ly fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moir´on (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages). Our approach only requires morphological processing and a bilingual dictionary, which are more readilyavailable for several languages. Note also that these approaches target a specific syntactic construction, whereas ours is adequate for various types of MWEs. Several properties of Hebrew MWEs are described by Al-Haj (2010); Al-Haj and Wintner (2010"
D11-1077,W10-3712,0,0.0295339,"and of course semantic (Al-Haj, 2010). They are also extremely diverse: for example, on the semantic dimension alone, MWEs cover an entire spectrum, ranging from frozen, fixed idioms to free combinations of words (Bannard et al., 2003). Such a complex task calls for a combination of multiple approaches, and much research indeed suggests “hybrid” approaches to MWE identification 2 For simplicity, we focus on bi-grams of tokens (MWEs of length 2) in this work; the methodology, however, is easily extensible to longer n-grams. (Duan et al., 2009; Weller and Fritzinger, 2010; Ramisch et al., 2010; Hazelbeck and Saito, 2010). We believe that Bayesian Networks provide an optimal architecture for expressing various pieces of knowledge aimed at MWE identification, for the following reasons (Heckerman, 1995): • In contrast to many other classification methods, BN can learn (and express) causal relationships between features. This facilitates better understanding of the problem domain. • BN can encode not only statistical data, but also prior domain knowledge and human intuitions, in the form of interdependencies among features. We do indeed use this possibility here. 3.2 Linguistically-motivated Features Based on the"
D11-1077,J03-1002,0,0.0138554,"tner, 2010); similarly, when one of the constituents of a MWE is a conjunction, the entire expression is very likely to be frozen. 841 where k is the number of nodes in the BN (other than Xmwe ) and pai is the set of parents of Xi . 3.4 Automatic Generation of Training Data For training we need samples of positive and negative instances of MWEs, each associated with a vector of the values of all features discussed in Section 3.2. We generate this training material automatically. We use a small Hebrew-English bilingual corpus (Tsvetkov and Wintner, 2010a). We word-align the corpus with Giza++ (Och and Ney, 2003), and then apply the (completely unsupervised) algorithm of Tsvetkov and Wintner (2010b), which extracts MWE candidates from the aligned corpus and re-ranks them using statistics computed from a large monolingual corpus. The core idea behind this method is that MWEs tend to be translated in nonliteral ways; in a parallel corpus, words that are 1:1 aligned typically indicate literal translations and are hence unlikely constituents of MWEs. The result is a set of 134,001 Hebrew bi-gram types (from the bilingual corpus), classified as either 1:1 aligned (implying they are likely not MWEs) or unal"
D11-1077,N03-2027,0,0.441083,"Missing"
D11-1077,tsvetkov-wintner-2010-automatic,1,0.755171,"the SVM reflect several morphological and morpho-syntactic properties of such constructions. The resulting classifier performs much better than a na¨ıve baseline, reducing over one third of the errors. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: indeed, we demonstrate that our general methodology, trained on automaticallygenerated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same dataset. Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bil"
D11-1077,C10-2144,1,0.908499,"the SVM reflect several morphological and morpho-syntactic properties of such constructions. The resulting classifier performs much better than a na¨ıve baseline, reducing over one third of the errors. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: indeed, we demonstrate that our general methodology, trained on automaticallygenerated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same dataset. Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bil"
D11-1077,W07-1104,0,0.0318665,"Missing"
D11-1077,D07-1110,0,0.136173,"To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used, in Hebrew lexicographic order, are abgdhwzxTiklmns‘pcqrˇst. 837 2 Related Work Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Pecina (2008) compares 55 different association measures in ranking German AdjN and PP-Verb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang et al., 2002; Villavicencio et al., 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs. Soon, however, it became clear that mere cooccurrence measurements are not enough to identify MWEs, and their linguistic properties should be exploited as well (Piao et al., 2005). Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs. Ramisch et al. (2008) evaluate a number of association measures on the task of identifying English Verb-Particle Constructions and German"
D11-1077,N07-1051,0,\N,Missing
D11-1077,J93-1007,0,\N,Missing
D11-1077,W04-0412,0,\N,Missing
D11-1077,J93-2003,0,\N,Missing
D11-1077,W07-1106,0,\N,Missing
D11-1077,W06-1203,0,\N,Missing
D11-1077,W04-0404,0,\N,Missing
D11-1077,W06-1204,0,\N,Missing
D11-1077,P05-2003,0,\N,Missing
D11-1077,W03-1812,0,\N,Missing
D11-1077,W09-2907,0,\N,Missing
D11-1077,D11-1067,0,\N,Missing
D11-1077,W04-0406,0,\N,Missing
D11-1077,N10-1029,0,\N,Missing
D11-1077,bouamor-etal-2012-identifying,0,\N,Missing
D11-1077,2005.mtsummit-posters.11,0,\N,Missing
D11-1077,kirschenbaum-wintner-2010-general,1,\N,Missing
D11-1077,C10-1002,1,\N,Missing
D11-1077,P00-1056,0,\N,Missing
D15-1161,P14-2131,0,0.0497195,"uage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning words representations tha"
D15-1161,D14-1082,0,0.00523589,"local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextu"
D15-1161,P14-1129,0,0.0182429,"pic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the le"
D15-1161,D14-1012,0,0.0194663,"data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depends on both the syntactic relation from nicely, which narrows down the list of candidates to verbs, and on the semant"
D15-1161,P12-1092,0,0.059041,"e a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Co"
D15-1161,D13-1176,0,0.00900077,"g global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted wor"
D15-1161,D14-1108,1,0.740924,"ction words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently"
D15-1161,N15-1144,1,0.0648252,"ver, this model does not scale well as b increases as it requires V × dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: X c= ai (wi )wi (2) i∈[−b,b]−{0} (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer P > v∈V exp v Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013"
D15-1161,N15-1142,1,0.386366,"isbon, Portugal Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning wo"
D15-1161,P14-1140,0,0.0283868,"Missing"
D15-1161,D07-1043,0,0.00495254,"embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015) that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes. These results are consistent with our observations found in Table 1, in rows “Skip-ngram” and “SSkip-ngram”. We can observe that our attention based CBOW model (row “CBOW Attention”) improves over these results for both tasks and also the original CBOW model (row “CBOW”). 1369 CBOW Skip-ngram SSkip-ngram CBOW Attention POS Induction 50.40 33.86"
D15-1161,D13-1170,0,0.00265235,"sented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which is a binary classification task for movie reviews. We simply use the mean of the word vectors of words in a sentence, and use them as features in an `2 regularized logistic regression classifier. We use the standard training/dev/test split and report accuracy on the test set in table 1. We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax. This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic task"
D15-1161,P10-1040,0,0.138833,"better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depen"
D15-1161,D15-1176,1,\N,Missing
D15-1161,D07-1096,0,\N,Missing
D15-1243,P12-1015,0,0.0404941,"i et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Eac"
D15-1243,J90-1003,0,0.350901,"which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ Uk ΣVk T (Landauer and Dumais, 1997). 3 https://code.google.com/p/word2vec 4 https://github.com/wlin12/wang2vec 5 http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semant"
D15-1243,W06-1670,0,0.0317102,"Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0."
D15-1243,P14-5004,1,0.611603,"sis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6 https://github.com/mfaruqui/ retrofitting 7 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8 http://qwone.com/~jason/20Newsgroups 2051 2014a).9 The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. 5 Results To test the efficiency of QVEC in capturing the semantic content of word vectors, we evaluate how well QVEC’s scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings. We begin with comparison of QVEC with one extrinsic task"
D15-1243,P15-2076,1,0.309428,"Missing"
D15-1243,N15-1184,1,0.864444,"Missing"
D15-1243,N13-1092,0,0.0368039,"Missing"
D15-1243,D14-1012,0,0.0362549,"of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequent"
D15-1243,J15-4004,0,0.243621,"Missing"
D15-1243,N15-1142,1,0.522248,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,H93-1061,0,0.140934,"our model obtains high correlation (0.34 ≤ r ≤ 0.89) with the extrinsic tasks (§5). 2 Linguistic Dimension Word Vectors The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re2049 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb le"
D15-1243,I08-2105,0,0.0125447,"thods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· VB . MOTI"
D15-1243,D14-1162,0,0.112475,"Missing"
D15-1243,D15-1036,0,0.233502,"Missing"
D15-1243,N13-1076,1,0.805767,"Missing"
D15-1243,D13-1170,0,0.0376388,"ed lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequently, it is not clear how to score a non-interp"
D15-1243,P14-1024,1,0.829115,"Missing"
D15-1243,tsvetkov-etal-2014-augmenting-english,1,0.809802,"Missing"
D15-1243,P10-1040,0,0.162082,"f word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at htt"
D15-1243,P14-1074,0,0.0118254,"9 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classif"
D15-1243,D13-1196,0,0.0612772,"nexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguisti"
D15-1243,D15-1161,1,0.0792052,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,P14-2131,0,\N,Missing
D18-1393,N15-1171,0,0.0790227,"Missing"
D18-1393,P15-2072,0,0.263398,"inally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Recent work has attempted to address these conceptual challenges by defining broad framing categories. The Policy Frames Codebook defines a set of 15 frames (one of which is “Other”) commonly used in media for a broad range of issues (Boydstun et al., 2013). In a follow-up work, the authors use these frames to build The Media Frames Corpus (MFC), which consists of articles related to 3 issues: immigration, tobacco, and same-sex marriage (Card et al., 2015). About 11,900 articles are hand-annotated with frames: annotators highlight spans of text related to each frame in the codebook and assign a single “primary frame” to each document. However, the MFC, like other prior framing analyses, relies heavily on labor-intensive manual annotations. The primary automated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard"
D18-1393,D16-1148,0,0.270422,"omated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Co"
D18-1393,J90-1003,0,0.15656,"Missing"
D18-1393,P17-1092,0,0.195841,"relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Corpus. However, becaus"
D18-1393,D17-1292,0,0.014393,"d terrorist will simply be replaced “and everything will start afresh - explosions, chases, roundups...unlucky businessmen, successful terrorists”. The articles 3577 portray the U.S. as an unsafe place to live, making Russia seem like a preferable home. A third type of article also presents Russia as safe by downplaying U.S. military threat: “the missile defense system of the USA does not pose a real threat to Russia’s strategic nuclear forces.” or describing the growth of Russian technology compared to ‘impotent’ American counterparts. to sentiment of social media posts (Nardo et al., 2016). Kang et al. (2017) combine text and Granger causality for a different task: automatically explaining causes of time series events. Our study differs from past work in that we reverse the direction: rather than using news articles to model changes in economic data, we use economic data to show changes in news articles. 6 7 Related Work Most studies on Russian media manipulation focus on state-owned television networks, such as Channel 1 and RT. Strategies identified in these outlets include spreading confusion (Paul and Matthews, 2016) and “selection attribution”, in which negative economic events are attributed"
D18-1393,P15-1157,0,0.566412,"uarterly or yearly level. 3572 of examples. Finally, we use this method to contextualize strategies of media manipulation in the Izvestia corpus. 3.1 Background on Framing Analyses While agenda-setting broadly refers to what topics a text covers, framing refers to which attributes of those topics are highlighted. Several aspects of framing make the concept difficult to analyze. First, just defining framing has been “notoriously slippery” (Boydstun et al., 2013). Frames can occur as stock phrases, i.e. “death tax” vs. “estate tax”, but they can also occur as broader associations or sub-topics (Tsur et al., 2015; McCombs, 2002). Frames also need to be distinguished from similar concepts, like sentiment and stance. For example, the same frame can be used to take different stances on an issue: one politician might argue that immigrants boost the economy by starting new companies that create jobs, while another might argue that immigrants hurt the economy by taking jobs away from U.S. citizens (Baumer et al., 2015; Gamson and Modigliani, 1989). Finally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Re"
D19-1176,W18-5105,0,0.0447521,"Missing"
D19-1176,J08-4004,0,0.232989,"heme often co-exists with “Attribution of Stereotype,” but is distinct in that its focus is on redefining the target’s sense of identity. 2.3 Annotation Results Three annotators familiar with the theoretical background and prior research on microagressions performed an open coding procedure after examining the S ELF MA data to codify the typology and determine annotation guidelines. All three labeled 200 instances of the dataset to estimate agreement. Despite the difficulty of the task, annotators had moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Suppl"
D19-1176,P19-1243,1,0.835717,"towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational means (e.g., Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018; Field et al., 2019; Field and Tsvetkov, 2019). Further, our focus on gender also allows us to reliably recruit crowdworkers across the gender spectrum, whereas other social categories such as race or religion are more difficult to recruit in a balanced proportion through traditional mechanisms. However, despite this current focus, both the typology and annotation approach are designed for 6 Offensiveness 6 Offensiveness Category Attributive Institutionalized Teaming Othering 4 2 4 2 0 -4 -2 0 2 4 0 -4 Discrepancy -2 0 Discrepancy Figure 4: Offensiveness (y-axis) vs Discrepancy (x-axis) of perceived offensiveness between annotator gender."
D19-1176,W16-4301,0,0.0304306,"ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational means (e.g., Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018; Field et al., 2019; Field and Tsvetkov, 2019). Further, our focus on gender also allows us to reliably recruit crowdworkers across the gender spectrum, whereas other social categories such as race or religion are more difficult to recruit in a balanced proportion through traditional mechanisms. However, despite this current focus, both the typology and annotation approach are designed for 6 Offensiveness 6 Offensiveness Category Attributive Institutionalized Teaming Othering 4 2 4 2 0 -4 -2 0 2 4 0 -4 Discrepancy -2 0 Discrepancy Figure 4: Offensiveness (y-axis) vs Discrepan"
D19-1176,W17-2902,0,0.0761968,"sufficiently represented in our corpus, and (3) comprehensive over all distinct microaggression types in the corpus. Four key themes were identified in our analysis of the MA S data: Attributive, Institutionalized, Teaming, and Othering (see Table 1). We discuss each of these next.2 The Attributive theme covers instances where a microaggression attributes a stereotype to an individual based on their identity. These stereotypes may have inherently negative connotations (“lazy”), but may also be neutral (“liking pink”) or positive (“strong”), which complements recent work on benevolent sexism (Jha and Mamidi, 2017). The Institutionalized theme reflects larger institutionalized biases, such as in employment or law enforcement. The Teaming theme is derived from the term forced teaming, coined by de Becker (1997) to describe a strategy of abuse where the abusers frames themselves as being on the same team as the victim. The Othering theme covers MA S which revolve around framing the target in relation to some “othered” group. This theme often co-exists with “Attribution of Stereotype,” but is distinct in that its focus is on redefining the target’s sense of identity. 2.3 Annotation Results Three annotators"
D19-1176,N18-2099,0,0.027849,"[SelfMA] [Random] 6 6 Offensiveness Offensiveness representative of all types of MA S. However, our crowdsourcing approach does provide an effective way to surface MA S and our dual objective approach, which uses both annotator discrepancy and offensiveness, provides complementary views into what statements could be perceived as MA S. Additional iterations of this procedure are likely to improve microaggression recognition and substantially increase the seize of the corpus. We note that one option is to have workers generate examples, rather than rate (e.g., Xu et al., 2013; Su et al., 2016; Jiang et al., 2018); however, such a process raises ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational mean"
D19-1176,P19-1357,1,0.772049,"lasting harmful impacts on their targets. Qualitative interviews suggest that the subtlety of MA S may cause even greater levels of situational stress than overt aggression (Sue, 2010; Nadal et al., 2014). Introduction Toxicity and offensiveness are not always expressed with toxic language. While a substantial community effort has rightfully focused on identifying, preventing, and mitigating overtly toxic, profane, and hateful language (Schmidt and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017), with surveys of the area also overlooking this important and challenging task of recognizing this subtle toxicity (van Aken et al., 2018; Salminen et a"
D19-1176,passonneau-etal-2012-masc,0,0.0219217,"sible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts where at least two annotators agreed (183 posts). And where there were no agreement (17 posts), the annotators determine the labels through a follow-up adjudication process.3 After this process, another 1,100 posts were singly annotated, distributed across the three annotators, for a total of 1,300 posts. Relative percentages of each theme and examples comments are shown in Table 3. We also show the distribution of different axes of discrimination among the"
D19-1176,Q16-1005,0,0.0406361,"erceived offensiveness to pick the next batch of posts for the second round of annotation. The classifier was given, as positive examples, posts with ≥ 0.25 discrepancy8 between the averaged perceived offensiveness of the dominant group and the marginalized groups. Other posts are considered negative examples. The feature sets that we used are motivated by prior work on gender bias and power dynamics. The following seven feature categories are used: (1) unigram and bigram features to capture lexical patterns, (2) two categories of formal and informal words derived from the formality corpus of Pavlick and Tetreault (2016), (3) the gendered occupations lexicon of Bolukbasi et al. (2016), grouped across definitional and stereotypical gender (male, female, neutral), (4) the gender stereotype lexicon of Fast et al. (2016), (5) the gender lexicon for social media from (Sap et al., 2014), (6) a manually-compiled corpus of gendered words, extended from seed list from https://www.hrc.org/resources/glossary-of-terms and (7) a manually-compiled sentiment lexicon, inspired by LIWC. To facilitate reproducibility, all lexicons will be available in the software release. 8 The threshold was chosen empirically to ensure enoug"
D19-1176,D17-1244,0,0.0207354,"d moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts where at least two annotators agreed (183 posts). And where there were no agreement (17 posts), the annotators determine the labels through a follow-up adj"
D19-1176,P16-1030,0,0.0490384,"determine annotation guidelines. All three labeled 200 instances of the dataset to estimate agreement. Despite the difficulty of the task, annotators had moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts"
D19-1176,D14-1121,0,0.0185744,"s are considered negative examples. The feature sets that we used are motivated by prior work on gender bias and power dynamics. The following seven feature categories are used: (1) unigram and bigram features to capture lexical patterns, (2) two categories of formal and informal words derived from the formality corpus of Pavlick and Tetreault (2016), (3) the gendered occupations lexicon of Bolukbasi et al. (2016), grouped across definitional and stereotypical gender (male, female, neutral), (4) the gender stereotype lexicon of Fast et al. (2016), (5) the gender lexicon for social media from (Sap et al., 2014), (6) a manually-compiled corpus of gendered words, extended from seed list from https://www.hrc.org/resources/glossary-of-terms and (7) a manually-compiled sentiment lexicon, inspired by LIWC. To facilitate reproducibility, all lexicons will be available in the software release. 8 The threshold was chosen empirically to ensure enough number of positive examples. Experimental Setup We designed the annotation task with two purposes in mind: (1) We want to test our hypothesis that there is discrepancy in how annotators of different genders perceive MA S. By comparing the distribution of discrepa"
D19-1176,D17-1323,0,0.0307307,"timent tools can label these comments as being positive. As a result, applications using such techniques to promote inoffensive content may potentially promote MA S. Such biased online content is then used 1664 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1664–1674, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics as training data in NLP tools—dialogue systems, question answering, and others—thereby perpetuating and amplifying biases (Zhao et al., 2017). Computational modeling of MA S is challenging for many reasons. MA S are subjective, context-sensitive, and expressed subtly in language (as shown in Figure 1); there are no welldefined annotation guidelines, no corpus of MA S for training and evaluating a model, and prior computational approaches to detecting overtly offensive speech are likely not suitable for classifying MA S. Moreover, they are diluted in the ocean of social media content, and it would be infeasible to create a corpus of diverse types of MA S just by crowdsourcing annotations of randomly sampled social media posts. This"
D19-1176,W17-1101,0,0.0264363,"unconsciously expresses a prejudiced attitude toward a member of a marginalized group such as a racial minority.” Though subtle, MA S have been shown to have lasting harmful impacts on their targets. Qualitative interviews suggest that the subtlety of MA S may cause even greater levels of situational stress than overt aggression (Sue, 2010; Nadal et al., 2014). Introduction Toxicity and offensiveness are not always expressed with toxic language. While a substantial community effort has rightfully focused on identifying, preventing, and mitigating overtly toxic, profane, and hateful language (Schmidt and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017"
D19-1176,D16-1054,0,0.0712136,"Missing"
D19-1176,L18-1445,1,0.855654,"ost. They are then asked rate if they found the response offensive and if so, to what degree was it offensive on a seven-point Likert scale. This scale of offensiveness allows us to capture cases where a microaggression is perceived as offensive by both annotator groups (genders), but to different degrees. To test our hypothesis and quantify how certain posts could be perceived differently by annotators of different genders, each annotation task includes a demographic question.6 Data To focus the data on interactions where gender may be a salient variable, Reddit data was drawn from RtGender (Voigt et al., 2018), which has a post-and-reply interactions where the gender of each author has been inferred with high confidence. After preliminary analyses, the initial data was randomly selected from 28 subreddits based on their focus around gender issues (e.g,. Relationships, AskWomen).7 For the first round of annotation, we used these four subsets of posts to be annotated for of6 Following best practices in gender elicitation (Jaroszewski et al., 2018), we include a third option of “nonbinary, genderqueer, or otherwise”, which has a freeform text box. We opted not to ask about cis or trans status in the d"
D19-1176,W17-3012,0,0.0246784,"and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017), with surveys of the area also overlooking this important and challenging task of recognizing this subtle toxicity (van Aken et al., 2018; Salminen et al., 2018; Fortuna and Nunes, 2018). Indeed, as Figure 1 suggests, current popular tools for toxic language detection do not recognize the toxicity of MA S and further, sentiment tools can label these comments as being positive. As a result, applications using such techniques to promote inoffensive content may potentially promote MA S. Such biased online content is then used 1664 Proceedings of the 2019 Conference on Empirical Methods in Natura"
D19-1176,W13-2515,0,0.0152818,"0 2 4 2 4 Discrepancy Discrepancy [SelfMA] [Random] 6 6 Offensiveness Offensiveness representative of all types of MA S. However, our crowdsourcing approach does provide an effective way to surface MA S and our dual objective approach, which uses both annotator discrepancy and offensiveness, provides complementary views into what statements could be perceived as MA S. Additional iterations of this procedure are likely to improve microaggression recognition and substantially increase the seize of the corpus. We note that one option is to have workers generate examples, rather than rate (e.g., Xu et al., 2013; Su et al., 2016; Jiang et al., 2018); however, such a process raises ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gende"
D19-1425,W19-5201,0,0.0146563,"arners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pattern as the ones obtained with attention scores. In line with results in Table 7, salient words for ALT"
D19-1425,D18-1002,0,0.413214,"to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that demotes latent confounds and r"
D19-1425,D18-1395,1,0.84017,"itialized at random and learned) and passed these embeddings to a bidirectional LSTM encoder (one layer for each direction) with attention (h(x); Pryzant et al., 2018). Each LSTM layer had a hidden dimension of 128. We used two layered feed forward networks with a tanh activation function in the middle layer (of size 256), followed by a softmax in the final layer, as c(.) and adv(.). 5.3 Baselines We consider several baselines that are intended to capture the stylistic features of the texts, explicitly avoiding content. 4157 Linear classifier with content-independent features (LR) Replicating Goldin et al. (2018), we trained a logistic regression classifier with three types of features: function words, POS trigrams, and sentence length, all of which are reflective of the style of writing. We deliberately avoided using content features (e.g., word frequencies). Classification with no adversary on masked texts (LO - TOP -K) We mask the top-K words (based on log-odds scores) in both the train and the test sets (as in §2); we train the classification model again without training adv(.). After masking the top words, we expect patterns of writing style (and, therefore, L1) to become more apparent. Adversari"
D19-1425,N19-1357,0,0.0385091,"The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO -"
D19-1425,S18-2005,0,0.0313045,"on about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.1 1 Introduction Text classification systems based on neural networks are biased towards learning frequent spurious correlations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on th"
D19-1425,N18-1146,0,0.149534,"e aim of this task is to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that de"
D19-1425,Q18-1024,1,0.909604,"ral Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorithm aimed at learning textual representations that are invariant to the c"
D19-1425,P18-2005,0,0.061756,"Missing"
D19-1425,W17-5007,0,0.1775,"4153 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorit"
D19-1425,K17-1018,0,0.170131,"Missing"
D19-1425,P19-1282,1,0.840889,"y L2, English included). The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative wor"
D19-1425,P14-1017,0,0.0780898,"Missing"
D19-1425,W13-1706,0,0.0450352,"Missing"
D19-1425,W07-0602,0,0.0433251,"Missing"
D19-1425,L18-1445,1,0.878121,"Missing"
D19-1425,D19-1002,0,0.0360982,"rminers is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pat"
D19-1425,U09-1008,0,0.0905086,"Missing"
D19-1425,D11-1148,0,0.0606105,"Missing"
D19-1425,D17-1323,0,0.0536227,"orrelations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on the task of native language identification (L1ID), which aims at automatically identifying the native language (L1) of an individual based on their language production in a second language (L2, English in this work). The aim of this task is to discover styl"
D19-5621,D15-1166,0,0.0361906,"Missing"
D19-5621,Q17-1010,0,0.0486559,"u and uorth be λ3 u and λ4 uorth Since these are orthogonal, x decomposes as λ3 u + λ4 uorth + y where y is some vector orthogonal to both u and uorth (y = 0 when d = 2). Using the decomposed forms of u ˆ and uorth in the margin-based loss, the second argument of equation 1 becomes 4 We follow Kumar and Tsvetkov (2019) to conduct experiments on neural machine translation. Datasets We evaluate our models on IWSLT’16 (Cettolo et al., 2015) French→English and German→English datasets. We pretrain target embeddings on a large English-language corpus (4B+ tokens) using FastText on default settings (Bojanowski et al., 2017) and L2 -normalize the embeddings. Vocabulary sizes are limited to 50000. We follow Kumar and Tsvetkov (2019) in using the standard development (tst2013 and tst2014) and test (tst2015 and tst2016) sets associated with the parallel corpora and in processing the data; train, development and test splits contain roughly 200K, 2300 and 2200 parallel sentences each. Setup We use a neural machine translation system with attention (Bahdanau et al., 2015), set up to match that described in Kumar and Tsvetkov (2019). The encoder and decoder are 1layer bidirectional and 2-layer LSTMs with 1024dimensional"
D19-5621,P16-1101,0,0.0156613,"es in which the pretrained embedding closest to u ˆ is not u), the average cosine similarity between predicted embeddings and their nearest pretrained embeddings falls from vMF to SMP (0.91 to 0.86), while that between the predicted and target embeddings rises (0.39 to 0.42). This is accompanied by increases in accuracy @2, @5 and @10 (Table 2). 6 Related Work Pretrained embeddings trained in an unsupervised manner (Mikolov et al., 2013a) are used as input and intermediate representations of data for natural language processing tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016), sentiment analysis (Tang et al., 2016) and dependency parsing (He et al., 2018). We build on (Kumar and Tsvetkov, 2019), one of the first instances of using pretrained embeddings as model outputs for complex sequencegeneration tasks. Closely related work on embedding prediction includes zero-shot learning for word translation (Nakashole, 2018; Conneau et al., 2018) and image labeling (Lazaridou et al., 2015), as well as rare word prediction (Pinter et al., 2018) and classification (Card et al., 2019). Margin-based losses are commonly used to train neural networks that predict dense vectors f"
D19-5621,D18-1047,0,0.0257262,"Related Work Pretrained embeddings trained in an unsupervised manner (Mikolov et al., 2013a) are used as input and intermediate representations of data for natural language processing tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016), sentiment analysis (Tang et al., 2016) and dependency parsing (He et al., 2018). We build on (Kumar and Tsvetkov, 2019), one of the first instances of using pretrained embeddings as model outputs for complex sequencegeneration tasks. Closely related work on embedding prediction includes zero-shot learning for word translation (Nakashole, 2018; Conneau et al., 2018) and image labeling (Lazaridou et al., 2015), as well as rare word prediction (Pinter et al., 2018) and classification (Card et al., 2019). Margin-based losses are commonly used to train neural networks that predict dense vectors for classification tasks, and have long been used in computer vision. Standard formulations include contrastive (Hadsell et al., 2006) and triplet (Schroff et al., 2015) losses; triplet loss is identical to the max-margin framework we use. Other closely re7 Conclusion We explore the use of margin-based loss functions to train continuous-output n"
D19-5621,P02-1040,0,0.104905,"25.4∗†‡ ± 0.3 25.3∗‡ ± 0.5 Table 1: Experimental results. Means and standard deviations of BLEU scores across 4 runs of each experiment, for the (1) discrete-output baseline, (2) continuous-output models trained using vMF, most informative negative example (Lazaridou et al., 2015) and negative sampling, and (3) proposed syn-margin losses constructed using vector projection and vector difference, on IWSLT’16 Fr→En and De→En datasets. Asterisks, daggers and double daggers indicate significant gains over vMF, most informative negative sample and negative sampling respectively (p = 0.05). scores (Papineni et al., 2002) over 4 runs of each experiment. Baselines and benchmarks We compare synmargin losses constructed using projection (SMP) and difference (SMD) techniques against: (1) vMF loss (specifically, the negative log-likelihood formulation in the original paper), (2) marginbased loss averaged over 5 negative samples drawn uniformly at random from the pretrained word embeddings and (3) margin-based loss using the most informative negative sample (Lazaridou et al., 2015). We also report results on a softmaxbased system with identical architecture except in the last layer, initializing the softmax paramete"
D19-5621,P16-1162,0,0.0189045,"embedding is driven towards its target embedding u, which can be identified since target words are available during training. This is much faster than training in the discrete-output case, since vMF densities are implicitly normalized. During inference, choosing the most likely word reduces to finding the predicted embedding’s nearest neighbour (by cosine similarity) in the L2 -normalized pretrained embedding space.2 3.1 1 The role of negative samples What is the role of the negative sample in this margin-based loss? We investigate with a geometThe discrete units may be words, sub-word units (Sennrich et al., 2016), characters (Ling et al., 2015; Kim et al., 2016) or tokens of any other granularity. We focus on the generation of words, since pretrained embeddings spaces at this granularity are interpretable and semantically coherent across languages. 2 In line with the inference mechanism, all references we make to ‘similarity’ or ‘closeness’ will be in the cosine, not Euclidean sense. In a slight change of notation, we will henceforth use u ˆ to refer to the unit vector along a predicted embedding rather than the predicted embedding itself. 200 Although uorth and udiff are functions of u ˆ, they are pl"
D19-5621,D18-1160,0,0.0269495,"imilarity between predicted embeddings and their nearest pretrained embeddings falls from vMF to SMP (0.91 to 0.86), while that between the predicted and target embeddings rises (0.39 to 0.42). This is accompanied by increases in accuracy @2, @5 and @10 (Table 2). 6 Related Work Pretrained embeddings trained in an unsupervised manner (Mikolov et al., 2013a) are used as input and intermediate representations of data for natural language processing tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016), sentiment analysis (Tang et al., 2016) and dependency parsing (He et al., 2018). We build on (Kumar and Tsvetkov, 2019), one of the first instances of using pretrained embeddings as model outputs for complex sequencegeneration tasks. Closely related work on embedding prediction includes zero-shot learning for word translation (Nakashole, 2018; Conneau et al., 2018) and image labeling (Lazaridou et al., 2015), as well as rare word prediction (Pinter et al., 2018) and classification (Card et al., 2019). Margin-based losses are commonly used to train neural networks that predict dense vectors for classification tasks, and have long been used in computer vision. Standard for"
D19-5621,P05-1044,0,0.102616,"to the increase in accuracies @2, 5 and 10 that results from the switch to syn-margin loss. lated approaches are the imposition of an angular margin constraint and the minimization of distance to the farthest intra-class example coupled with maximization of distance to the nearest interclass example (Liu et al., 2016; Deng et al., 2017). In contrast to syn-margin, many of these losses pertain to trainable target embedding spaces. The triplet loss has also been used in various NLP applications (Collobert et al., 2011). Techniques used to pick negative samples include perturbing training data (Smith and Eisner, 2005), sampling according to word frequency (Mikolov et al., 2013b), sampling until a non-zero loss is obtained (Weston et al., 2011) and searching for the negative sample that gives the largest (Rao et al., 2016) or most informative (Lazaridou et al., 2015) loss. These techniques also correspond to trainable target embedding spaces, and are all equally or less efficient than syn-margin. margin-based losses in general and syn-margin in particular. We briefly analyze the properties of embeddings predicted by vMF and SMP Fr→En systems. Among incorrect predictions (cases in which the pretrained embedd"
D19-5626,J93-2003,0,0.150604,"cs: (1) a phrase should appear at least twice in the parallel corpus; (2) it should not contain any punctuation; (3) PMI of the phrase should be positive; (4) a bigram phrase should not repeat the same word; and (5) the phrase should not contain only stopwords. We train embeddings for the resulting list of words and phrases as follows. First, we preprocess the target language’s large monolingual corpus to concatenate words to match the longest phrase in the extracted phrase list. For example, the sen2.3 Fertility module We introduce a fertility module, similar to the fertility concept in SMT (Brown et al., 1993). The fertility indicates how many target words each source word should produce in the output. The SMT models keep the fertility probabilities over fertility count, typically from zero to four, and use it to produce probability over words. We integrate this fertility concept into our PCoNMT model. Our fertility module predicts the fertility probability φe = [φe0 , ..., φeN ], where φei indicates the scalar probability of the source word at position e being translated into i words. This is predicted based on the word embedding and encoder’s output of the word: φe = FFNN(xe ; he ). FFNN is the f"
D19-5626,W17-4705,0,0.0133277,"te embeddings of sequences of varying lengths. We show that our model learns to translate phrases better, performing on par with state of the art phrase-based NMT. Since our model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline. 1 Introduction Despite the successes of neural machine translation (Wu et al., 2016; Vaswani et al., 2017; Ahmed et al., 2018), state of the art NMT systems are still challenged by translation of typologically divergent language pairs, especially when languages are morphologically rich (Burlot and Yvon, 2017). One of the reasons lies in increased sparsity of word types, which leads to the demand for (often unavailable) significantly larger training corpora (Koehn and Knowles, 2017). Another reason is an implicit assumption of sequence to sequence (seq2seq) models that input sequences are translated into a target language word-by-word or subword-by-subword (Sennrich et al., 2016). This is not the case for typologically divergent language pairs, for example when translat241 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 241–248 c Hong Kong, China, November 4,"
D19-5626,W17-3204,0,0.0179844,"model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline. 1 Introduction Despite the successes of neural machine translation (Wu et al., 2016; Vaswani et al., 2017; Ahmed et al., 2018), state of the art NMT systems are still challenged by translation of typologically divergent language pairs, especially when languages are morphologically rich (Burlot and Yvon, 2017). One of the reasons lies in increased sparsity of word types, which leads to the demand for (often unavailable) significantly larger training corpora (Koehn and Knowles, 2017). Another reason is an implicit assumption of sequence to sequence (seq2seq) models that input sequences are translated into a target language word-by-word or subword-by-subword (Sennrich et al., 2016). This is not the case for typologically divergent language pairs, for example when translat241 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 241–248 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d Figure 1: Phrase-based neural machine translation architectures generate word- and phra"
D19-5626,N03-1017,0,0.175775,"del significantly outperformed the baseline in compound word generation cases while performs worse in verb phrases generation. By looking into the instances of wrong verb phrase generation, we found that a significant amount of those errors are related to the tense of the verb. 4 Category Related Work Multi-word Expressions for NMT There have been several studies that incorporate multi-word phrases into supervised NMT (Tang et al., 2016; Wang et al., 2017; Dahlmann et al., 2017). Most approaches rely on pre-defined phrase dictionaries obtained from methods such as phrase-based Stastitical MT (Koehn et al., 2003) or wordFertility in MT Fertility (Brown et al., 1993) has been a core component in phrase-based SMT 246 German src English ref Baseline CoNMT PCoNMT German src English ref Baseline CoNMT PCoNMT German src English ref Baseline CoNMT PCoNMT und Sie sollten auch an Dinge wie Lebensqualit¨at denken and you also want to think about things like quality of life and you should think of things like life and you should think of things like quality of life . wer ein Gehirn hat , ist gef¨ahrdet . everyone with a brain is at risk . who has a brain is risk . who has a brain is at risk . ich stecke voller W"
D19-5626,D16-1137,0,0.0498988,"Missing"
D19-5626,D18-1549,0,0.0210462,"h and Quechua) or languages with productive compounding processes like German or Finnish (Matthews et al., 2016). Another ubiquitous source of one-to-many correspondences is a translation of idiomatic phrases and multi-word expressions (Rikters and Bojar, 2017). While outperformed by NMT overall, translation models in traditional statistical phrase-based approaches (Koehn, 2009, SMT) provide an inventory of phrase translations, which can be used to address the above challenges. To combine the benefits of NMT and phrase-based SMT, phrasebased NMT systems have been proposed (Huang et al., 2017; Lample et al., 2018) which combine word-based NMT with external phrase memories (Tang et al., 2016; Dahlmann et al., 2017). However, prior approaches to phrase-based NMT introduced a significant overhead of additional resources and computation. We introduce a phrase-based continuous-output NMT (PCoNMT) model built upon continuousoutput NMT (Kumar and Tsvetkov, 2019), in which the decoder generates embeddings of words or phrases (§2). The model extracts phrases in the target language from one-to-many word alignments and pre-computes word and phrase embeddings which constitute the output space of our model (§2.2)."
D19-5626,P16-1103,0,0.049341,"Missing"
D19-5626,1983.tc-1.13,0,0.308283,"Missing"
D19-5626,P02-1040,0,0.103218,"Missing"
D19-5626,N18-1202,0,0.0232128,"fertility module, and decoder), described in §2. Given an input sentence {x1 , x2 . . . xn }, our model generates the output sentence {y1 , y2 . . . ym }, where yi corresponds to words or phrases, e.g. quality of life. At each step, the decoder generates an embedding ei , then the fertility module guides it to generate a word or a phrase, via the word- or phraseembedding table, respectively. tence ‘I went to a graduate school’ will be converted into ‘I went to a graduate school’ if we have went to and graduate school in our phrase list. This concatenated corpus is then used to train fastText (Peters et al., 2018) embeddings for both phrases and words simultaneously. We use fastText because it encodes subword-level information which may provide a signal about each word in a phrase. From this training, we obtain both the word- and phrase-tables, which are of the same dimension. pounds to English, e.g., Lebensqualit¨at in German is translated as quality of life. We extract all such one-to-many word alignments from the parallel corpora using Fastalign (Dyer et al., 2013). There are several standard approaches to extract meaningful phrases from a monolingual corpus, such as using scores like pointwise mutu"
D19-5626,P16-1162,0,0.0707507,"n (Wu et al., 2016; Vaswani et al., 2017; Ahmed et al., 2018), state of the art NMT systems are still challenged by translation of typologically divergent language pairs, especially when languages are morphologically rich (Burlot and Yvon, 2017). One of the reasons lies in increased sparsity of word types, which leads to the demand for (often unavailable) significantly larger training corpora (Koehn and Knowles, 2017). Another reason is an implicit assumption of sequence to sequence (seq2seq) models that input sequences are translated into a target language word-by-word or subword-by-subword (Sennrich et al., 2016). This is not the case for typologically divergent language pairs, for example when translat241 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 241–248 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d Figure 1: Phrase-based neural machine translation architectures generate word- and phrase embeddings at each step of decoding. The PCoNMT models are guided by on the fertility prediction and the attention. computation over a huge vocabulary, it also maintains the computational efficiency"
D19-5626,P19-1355,0,0.0132137,"T 2019), pages 241–248 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d Figure 1: Phrase-based neural machine translation architectures generate word- and phrase embeddings at each step of decoding. The PCoNMT models are guided by on the fertility prediction and the attention. computation over a huge vocabulary, it also maintains the computational efficiency of continuousoutput NMT, even with additional ngram embedding tables, and is faster than the state-of-the-art baseline by 112x (§3), making our models energyefficient (Strubell et al., 2019). The key contributions of our work are twofold: (1) we develop a phrase-based NMT model that outperforms existing baselines and better translates phrases, while (2) maintaining the computational efficiency of NMT end-to-end approaches.1 2 2.1 Extending the CoNMT approach, we propose phrase-based continuous-output NMT (PCoNMT). As depicted in Figure 1, we augment the original model with (1) additional embedding tables for phrases, and (2) a fertility module that guides the choice of embedding table to look-up in (described in §2.3). Having additional large embedding tables, which significantly"
D19-5626,P16-1008,0,0.0755095,"Missing"
D19-5626,D17-1149,0,0.139793,"Missing"
E14-1065,W13-2205,1,0.897909,"Missing"
E14-1065,bojar-etal-2010-data,0,0.0134346,"the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this research. 10 Since TED translation is a voluntary effort, not all talks are available in all languages. 11 Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id 1 39 142 228 248 451 535 TED talk Al Gore, 15 Ways to Avert a Climate Crisis, 2006 Aubrey de Grey: A roadmap to end aging, 2005 Alan Russell: The potential of regenerative medicine, 2006 Alan Kay shares a powerful idea about ideas, 2007 Alisa Miller: The news about the news, 2008 Bill Gates: Mosquitos, malaria and education, 2009 Al Gore warns on latest climate trends, 2009 Table 1: Test set of TED talks. ASR errors in synthetic phrase tables corresponds to the portion of errors that ou"
E14-1065,N06-1003,0,0.0932478,"Missing"
E14-1065,2012.eamt-1.60,0,0.0130906,"cmu.edu/cgi-bin/cmudict 5 p1 ·p2 ||p1 ||·||p2 || 618 pronunciation variants. To create a phone confusion transducer T maps source to target phone sequences by performing a number of edit operations. Allowed edits are: tells the chelsea CH EH L S IY Figure 4: Pseudo-ASR output generation example for a bigram tells the. Phonetic edits are Substitute(T, CH), Substitute(Z, S), Delete(DH). • Deletion of a consonant (mapping to ). • Doubling of a vowel. • Insertion of one or two phones in the end of a sequence from the list of possible suffixes: S (-s), IX NG (-ing), D (-ed). translated TED talks (Cettolo et al., 2012b).6 English is the source language in all the experiments. In expASR we used tst2011–the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English-French language pair (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance"
E14-1065,N03-1017,0,0.00534194,"s (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this resea"
E14-1065,D13-1174,1,0.836913,". English has strong constraints on sequences of consonants; the sequence [zdr], for example, cannot be a legal EnWe adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2 We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). 3 These are the reasons why in context-dependent acoustic modeling different HMM models are trained for different contexts. 617 Source phrase tells the story tell their story tells a story tell the story tell a story tell that story tell their stories tells the stories tells her story chelsea star Target phrase raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire Original phrase translation features f1 , f 2 , f 3 , f 4 , f 5 f1 , f 2 , f 3 , f 4 , f 5 f1 ,"
E14-1065,P11-2031,1,0.862394,"Missing"
E14-1065,P07-2045,1,0.014774,"German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does"
E14-1065,P08-1115,0,0.022545,"tenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using o"
E14-1065,moore-2002-fast,0,0.0125955,"experiments the ASR outputs and lattices in standard lattice format (SLF) were produces by the participants of IWSLT 2011 evaluation campaign. 5.2 We train and test MT using the TED corpora in all five languages. For French, German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that fe"
E14-1065,W13-2234,1,0.918146,"hat can restrict allowed sequences of phones. English has strong constraints on sequences of consonants; the sequence [zdr], for example, cannot be a legal EnWe adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2 We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). 3 These are the reasons why in context-dependent acoustic modeling different HMM models are trained for different contexts. 617 Source phrase tells the story tell their story tells a story tell the story tell a story tell that story tell their stories tells the stories tells her story chelsea star Target phrase raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire Original phrase translation features f1 , f 2 , f"
E14-1065,P03-1021,0,0.0154168,"evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this research. 10 Since TED translation is a voluntary e"
E14-1065,P10-2001,0,0.017716,"ld improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language"
E14-1065,C04-1168,0,0.0360232,"ations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language pronunciation dictionary and language model (i.e., without an acoustic model), and use th"
E14-1065,P02-1040,0,0.088177,"Missing"
E14-1065,2012.iwslt-papers.18,0,0.200039,"ambiguating context and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-base"
E14-1065,P10-4002,1,\N,Missing
E14-1065,W14-3315,1,\N,Missing
E14-1065,2011.iwslt-evaluation.1,0,\N,Missing
E14-1065,2011.iwslt-evaluation.11,0,\N,Missing
J14-2007,W03-1809,0,0.0526184,"Missing"
J14-2007,bouamor-etal-2012-identifying,0,0.115291,"Missing"
J14-2007,J93-2003,0,0.0719354,"Missing"
J14-2007,N10-1029,0,0.272199,"Missing"
J14-2007,W02-1801,0,0.117347,"Missing"
J14-2007,J90-1003,0,0.272794,"ated work in the next section (borrowing from Tsvetkov and Wintner [2012]), we motivate in Section 3 the methodology we propose, and list the resources needed for implementing it. Section 4 discusses the linguistically motivated features and their implementation; the organization of the Bayesian network is described in Section 5. We explain how we generate training materials in Section 6. Section 7 provides a thorough evaluation of the results. We conclude with suggestions for future research. 2. Related Work Early approaches to MWE identification concentrated on their collocational behavior (Church and Hanks 1990). One of the first approaches was implemented as Xtract (Smadja 1993): Here, word pairs that occur with high frequency within a context of five words in a corpus are first collected, and are then ranked and filtered according to contextual considerations, including the parts of speech of their neighbors. Pecina (2008) compares 55 different association measures in ranking German Adj-N and PPVerb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang, D"
J14-2007,W07-1106,0,0.28937,"Missing"
J14-2007,W04-0412,0,0.436644,"edu. ∗∗ Department of Computer Science, University of Haifa Mount Carmel, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addre"
J14-2007,E06-1043,0,0.105949,"y fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions ´ prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moiron (2007) use lexical fixedness to extract Dutch verb-noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson 2006). Recently, Green et al. (2011) use parsing, and in particular Tree Substitution Grammars, for identifying MWEs in French. Semantic properties of MWEs can be used to distinguish between compositional and non-compositional (idiomatic) expressions. Katz and Giesbrecht (2006) and Baldwin et al. (2003) use Latent Semantic Analysis (LSA) for this purpose. They show that compositional MWEs appear in contexts more similar to their constituents than non-compositional MWEs. For example, the co-occurrence measured by LSA between the expression kick the bucket and the word die is much higher than co-occu"
J14-2007,D11-1067,0,0.216921,"Missing"
J14-2007,W10-3712,0,0.0247788,"syntactic constructions, in monolingual corpora. These include proper names, noun phrases, verb-particle pairs, and so forth. We focus on bigrams (MWEs consisting of two consecutive tokens) in this work; the methodology, however, can be extended to longer n-grams. Several properties of MWEs make this task challenging: MWEs exhibit idiosyncrasies on a variety of levels, orthographic, morphological, syntactic, and of course semantic. Such a complex task calls for a combination of multiple approaches, and much research indeed suggests “hybrid” approaches to MWE identification (Duan et al. 2009; Hazelbeck and Saito 2010; Ramisch et al. 2010; Weller and Fritzinger 2010). We believe that Bayesian networks provide an optimal architecture for expressing various pieces of knowledge aimed at MWE identification, for the following reasons (noted, e.g., by Heckerman 1995): r In contrast to many other classification methods, Bayesian networks can learn (and express) causal relationships between features. This facilitates better understanding of the problem domain. 453 Computational Linguistics r Volume 40, Number 2 Bayesian networks can encode not only statistical data, but also prior domain knowledge and human intuit"
J14-2007,W06-1203,0,0.11976,"n (2007) use lexical fixedness to extract Dutch verb-noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson 2006). Recently, Green et al. (2011) use parsing, and in particular Tree Substitution Grammars, for identifying MWEs in French. Semantic properties of MWEs can be used to distinguish between compositional and non-compositional (idiomatic) expressions. Katz and Giesbrecht (2006) and Baldwin et al. (2003) use Latent Semantic Analysis (LSA) for this purpose. They show that compositional MWEs appear in contexts more similar to their constituents than non-compositional MWEs. For example, the co-occurrence measured by LSA between the expression kick the bucket and the word die is much higher than co-occurrence of this expression and its component words. The disadvantage of this methodology is that to distinguish between idiomatic and non-idiomatic usages of the MWE it relies on the MWE’s known idiomatic meaning, and this information is usually not available. In addition,"
J14-2007,kirschenbaum-wintner-2010-general,1,0.78391,",787 804,704 218,108 30,000 895,632 27,880 865,632 225,660 English–Hebrew 19,626 271,787 14,142 252,183 128,987 19,626 280,508 12,555 280,506 149,688 to 69,912 MWEs. If the corpus surface form is not listed in the dictionary, we use the surface form in lieu of its lemma. The multiword entries of the DELA dictionaries are only used for evaluation. For some features we also need a bilingual dictionary. For English–Hebrew, we use a small dictionary consisting of 78,313 translation pairs. Some of the entries are collected manually, whereas others are produced automatically (Itai and Wintner 2008; Kirschenbaum and Wintner 2010). For English–French, because we are unable to obtain a good-quality dictionary, we use instead Giza++ (Och and Ney 2000) 1-1 word alignments computed automatically from the entire WMT-11 parallel corpus. In order to prepare training material automatically (Section 6), we use small bilingual corpora. For English–French, we use a random sample of 30,000 parallel sentences from the WMT-11 corpus. For English-Hebrew, we use the parallel corpus of Tsvetkov and Wintner (2010a). Statistics of the parallel corpora are listed in Table 2. For evaluation we need lists of MWEs, ideally augmented by lists"
J14-2007,P00-1056,0,0.801244,"280,506 149,688 to 69,912 MWEs. If the corpus surface form is not listed in the dictionary, we use the surface form in lieu of its lemma. The multiword entries of the DELA dictionaries are only used for evaluation. For some features we also need a bilingual dictionary. For English–Hebrew, we use a small dictionary consisting of 78,313 translation pairs. Some of the entries are collected manually, whereas others are produced automatically (Itai and Wintner 2008; Kirschenbaum and Wintner 2010). For English–French, because we are unable to obtain a good-quality dictionary, we use instead Giza++ (Och and Ney 2000) 1-1 word alignments computed automatically from the entire WMT-11 parallel corpus. In order to prepare training material automatically (Section 6), we use small bilingual corpora. For English–French, we use a random sample of 30,000 parallel sentences from the WMT-11 corpus. For English-Hebrew, we use the parallel corpus of Tsvetkov and Wintner (2010a). Statistics of the parallel corpora are listed in Table 2. For evaluation we need lists of MWEs, ideally augmented by lists of non-MWE bigrams. Such lists are notoriously hard to obtain. As a general method of evaluation, we run 10-fold cross-v"
J14-2007,J03-1002,0,0.0104764,"Missing"
J14-2007,P05-2003,0,0.0242544,"this feature is not language-specific, we assume that it should work best for pairs of rather distinct languages. Collocation. As a baseline, statistical association measure, we use pointwise mutual information (PMI). We define a binary feature, PMI, with two values, low and high, 458 Tsvetkov and Wintner Identification of Multiword Expressions FRZN CAPS MWE HYPHEN FOSSIL POS CNTXT TRANS HIST PMI Figure 1 The Bayesian network for MWE identification. reflecting an experimentally determined threshold. Clearly, other association measures (as well as combinations of more than one) could be used (Pecina 2005). 5. Feature Interdependencies Expressed as a Bayesian Network A Bayesian network (Jensen and Nielsen 2007) is organized as a directed acyclic graph whose nodes are random variables and whose edges represent interdependencies among those variables. We use a particular view of BN, known as causal networks, in which directed edges lead to a variable from each of its direct causes.4 This facilitates the expression of domain knowledge (and intuitions, beliefs, etc.) as structural properties of the network. We use the BN as a classification device: Training amounts to computing the joint probabilit"
J14-2007,N03-2027,0,0.0935168,"Missing"
J14-2007,N07-1051,0,0.0436039,"this section. In general, we require corpora (both monolingual and bilingual), morphological analyzers or stemmers, part-of-speech taggers, and bilingual dictionaries. No deeper processing is assumed (e.g., no parsers or lexical semantic resources are needed). The method we advocate is thus appropriate for medium-density languages (Varga et al. 2005). To compute the features discussed in Section 4, we need large monolingual corpora. For English and French, we use the 109 corpora released for WMT-11 (Callison-Burch et al. 2011); the corpora were syntactically parsed using the Berkeley parser (Petrov and Klein 2007), but we only use the POS tags in this work. For Hebrew, we use a monolingual corpus (Itai and Wintner 2008), which we pre-process as in Tsvetkov and Wintner (2012): We use a morphological analyzer (Itai and Wintner 2008) to segment word forms (separating prefixes and suffixes) and induce POS tags. Summary statistics for each corpus are listed in Table 1. For some features we need access to the lemma of word tokens. In Hebrew, the MILA morphological analyzer (Itai and Wintner 2008) provides the lemmas, but the parsed corpora we use in English and French do not. We therefore use the DELA dictio"
J14-2007,W09-2907,0,0.239822,"Missing"
J14-2007,J93-1007,0,0.715976,"motivate in Section 3 the methodology we propose, and list the resources needed for implementing it. Section 4 discusses the linguistically motivated features and their implementation; the organization of the Bayesian network is described in Section 5. We explain how we generate training materials in Section 6. Section 7 provides a thorough evaluation of the results. We conclude with suggestions for future research. 2. Related Work Early approaches to MWE identification concentrated on their collocational behavior (Church and Hanks 1990). One of the first approaches was implemented as Xtract (Smadja 1993): Here, word pairs that occur with high frequency within a context of five words in a corpus are first collected, and are then ranked and filtered according to contextual considerations, including the parts of speech of their neighbors. Pecina (2008) compares 55 different association measures in ranking German Adj-N and PPVerb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang, Danielsson, and Teubert 2002; Villavicencio et al. 2007) suggest that"
J14-2007,tsvetkov-wintner-2010-automatic,1,0.933357,"several morphological and morphosyntactic properties of such constructions. The resulting classifier performs much better than a naive baseline, reducing the error rate by over one third. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: Indeed, we demonstrate that our general methodology, trained on automatically generated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same data set (Section 7). Recently, Tsvetkov and Wintner (2010b, 2012) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; 452 Tsvetkov and Wintner Identification of Multiword Expressions but we extrapolate the results much further by extending the"
J14-2007,C10-2144,1,0.882341,"Missing"
J14-2007,D11-1077,1,0.162692,"Missing"
J14-2007,W07-1104,0,0.176476,"Missing"
J14-2007,W06-1204,0,0.0208032,".haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addressing multiple types of MWEs has its limitations: The task is less well-defined, one cannot rely on specific pr"
J14-2007,W04-0406,0,0.0113245,"of Haifa Mount Carmel, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addressing multiple types of MWEs has its limitations: The task is les"
J14-2007,D07-1110,0,0.35841,"Missing"
J14-2007,W17-1712,0,0.069263,"Missing"
L18-1445,N15-1084,0,0.0182101,"social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with comments given in response to the source texts. We collect such data from a variety of contexts, including: • Facebook (Politicians): Responses to Facebook posts from members of the U.S. House and Senate • Facebook (Public Figures): Responses to Facebook posts from other public figures, e.g., television hosts, journalists, an"
L18-1445,P06-1005,0,0.0356371,"tion are all public; however, to protect the anonymity of Facebook users in our dataset we remove all identifying user information as well as Facebook-internal information such as User IDs and Post IDs, replacing these with randomized ID numbers. Therefore users whose comments appear multiple times in our dataset may be compared, but without revealing their identity. We also only report commenter first names, since this is less identifying but still allows for running genderidentification algorithms. As a baseline for convenience we provide masculine/feminine ratios for these first names from Bergsma and Lin (2006). We collect posts and their associated top-level comments for the categories of speakers described below. In each case we find the page for the speaker with a novel method for finding gender-labeled speakers from Wikipedia. Specifically, our method takes as input a Wikipedia category page such as https://en.wikipedia.org/wiki/ Category:American_female_tennis_players, and for each name listed runs a search for public pages using Facebook’s Graph API. If an exact match for the name appears in the top three results, and the category of the page matches a relevant category (for instance, ”Public"
L18-1445,P05-1054,0,0.231484,"ource and responder may know one another and have an ongoing interaction afterwards. 2. Responses to Gender Here we aim to encourage research on responses to gender. Contrasting with language about or portraying a given gender which address abstract representations of social categories, responses to gender are directed towards an individual person. We know that social characteristics of the addressee influence linguistic behavior; existing computational work has shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often"
L18-1445,W17-3001,0,0.0230235,"nce, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Wor"
L18-1445,P16-1080,0,0.0474796,"Missing"
L18-1445,W17-2902,0,0.116166,"about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Word Count 376,114,950 123,753,913 15,549,984 6,606,087 44,537,612 Table 1: Basic statistics about the subcorpora within RtGender. Jha and Mamidi, 2017). Nevertheless, biased responses to social categories like gender can lead to marginalization (Sue, 2010) and negatively impact a person’s self-esteem and ability through mechanisms such as stereotype threat (Spencer et al., 1999). Perhaps most related to our work, Fu et al. (2016) analyze questions directed at men and women tennis players, finding that questions directed at men tend to be more about the game while questions directed at women are more likely to stray to topics about their appearance and off-court relationships. Tsou et al. (2014) similarly find comments on TED talks are more l"
L18-1445,D15-1130,0,0.0450648,"Missing"
L18-1445,D17-1247,0,0.0271873,"nstruction of identity and social categories like gender; social issues such as gender bias, in turn, often take form in language. Linguistic datasets have been used both to debunk gender-biased myths — for example, contrary to stereotype women are not actually more talkative than men (Mehl et al., 2007) — and to identify social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with"
L18-1445,D13-1170,0,0.00495629,"Missing"
L18-1445,N12-1084,0,0.027848,"shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,"
N15-1062,W12-4410,1,0.745643,"Missing"
N15-1062,N09-1067,0,0.0215923,"Missing"
N15-1062,N13-1073,1,0.86988,"Missing"
N15-1062,P97-1040,0,0.107892,"cal processes that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater a"
N15-1062,P02-1008,0,0.0358064,"that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater and Faithfulness"
N15-1062,C94-2163,0,0.419115,"d Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater and Faithfulness constraints MAX - IO - MORPH MAX - IO - C MAX - IO - V DEP - IO - MORPH DEP - IO - V IDENT- IO - C IDENT- IO - C - M IDENT- IO - C - A IDENT- IO - C - S IDENT- IO - C - P IDENT- IO - C - G IDENT- IO - C - E IDENT- IO - V IDENT- IO - V- O IDENT- IO - V- R IDENT- IO - V- F IDENT- IO - V- FIN Markedness constraints no (donor) affix deletion no consonant deletion no vowel deletion no (recipient) affix epenthesis no vowel epenthesis no consonant"
N15-1062,P09-1042,0,0.085628,"Missing"
N15-1062,P12-2027,0,0.025379,"on generation of borrowed phonemes in English–Japanese language pair (the method does not generalize from borrowed phonemes to borrowed words, and does not rely on linguistic insights), we are not aware of any prior work on computational modeling of lexical borrowing. Few papers only mention or tangentially address borrowing, we briefly list them here. Daumé III (2009) focuses on areal effects on linguistic typology, a broader phenomenon that includes borrowing and genetic relations across languages. This study is aimed at discovering language areas based on typological features of languages. Garley and Hockenmaier (2012) train a maxent classifier with character n-gram and morphological features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successf"
N15-1062,W06-1107,0,0.0761386,"Missing"
N15-1062,N01-1014,0,0.370066,"put to the borrowing model is a loanword candidate in Swahili/Romanian,8 the outputs are plausible donor words in the Arabic/French monolingual lexicon (i.e., any word in pronunciation dictionary). We train the borrowing model using a small set of training examples, and then evaluate it using a held-out test set. In the rest of this section we describe in detail our datasets, tools, and experimental results. Resources We employ Arabic–English and Swahili–English bitexts to extract a training set (corpora of sizes 5.4M and 14K sentence pairs, respectively), using a cognate discovery technique (Kondrak, 2001). Phonetically and semantically similar strings are classified as cognates; phonetic similarity is the string similarity between phonetic representations, and semantic similarly is approximated by translation.9 We thereby extract Arabic 8 Our model does not provide a mechanism for identifying loanwords in the recipient language; we only model the borrowing process. Classifying loanwords in the recipient language is an interesting but ultimately different problem: the ontological status of words in a lexicon is a difficult problem, even for human experts, however, knowledge of cross-lingual cor"
N15-1062,P13-4003,0,0.0252321,"prior work on computational modeling of lexical borrowing. Few papers only mention or tangentially address borrowing, we briefly list them here. Daumé III (2009) focuses on areal effects on linguistic typology, a broader phenomenon that includes borrowing and genetic relations across languages. This study is aimed at discovering language areas based on typological features of languages. Garley and Hockenmaier (2012) train a maxent classifier with character n-gram and morphological features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successfully applied it to Arabic–English named entity translation. Mann and Yarowsky (2001) and Kondrak (2001) identify cognate pairs, based on the learned surface 606 and phonetic similarities, respectively."
N15-1062,littell-etal-2014-morphological,0,0.0281556,"owels in loanword candidates. Morphological adaptation. Both Arabic and Swahili have significant morphological processes that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to con"
N15-1062,N01-1020,0,0.301921,"ogical features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successfully applied it to Arabic–English named entity translation. Mann and Yarowsky (2001) and Kondrak (2001) identify cognate pairs, based on the learned surface 606 and phonetic similarities, respectively. As our experiments confirm, orthographic and phonetic transliteration and string edit distance methods are not adequate models for the complex borrowing phenomena. 9 Conclusion Given a loanword, our model identifies plausible donor words in a contact language. We show that a discriminative model with Optimality Theoretic features effectively models systematic phonological changes in Arabic–Swahili loanwords. We also found that the model and methodology is generally applicable t"
N15-1062,schultz-schlippe-2014-globalphone,0,0.0656519,"Missing"
N15-1062,P14-1024,1,0.868671,"Missing"
N15-1062,Q13-1001,0,0.0761148,"Missing"
N15-1062,W02-0505,0,\N,Missing
N15-1062,J98-4003,0,\N,Missing
N16-1077,E14-1060,0,0.356302,"roach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflect"
N16-1077,N15-1107,0,0.591763,"on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected for"
N16-1077,D15-1041,1,0.717699,"out of characters. These  character vectors are parameters that are learned by our model, exactly as other character vectors. Regarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an affine transformation on the encoded vector e: e ← Wtrans e + btrans (5) where, Wtrans , btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015; Bahdanau et al., 2015). Our resultant inflection generation model is shown in Figure 3. 637 4.1 Supervised Learning The parameters of our model are the set of character vectors, the transformation parameters (Wtrans , btrans ), and the parameters of the encoder and decoder LSTMs (§3.2). We use negative loglikelihood of the output character sequence as the loss function: XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) (6) t=1 We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inflection generation and we evaluate it in two different"
N16-1077,D13-1174,1,0.807869,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,N13-1140,1,0.855557,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,E03-1009,0,0.0140071,"phology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al.,"
N16-1077,P11-1004,0,0.0372597,"ses and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed tra"
N16-1077,N15-1140,0,0.0257116,"Missing"
N16-1077,Q15-1031,0,0.0314936,", back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-"
N16-1077,N16-1080,0,0.0507045,"Missing"
N16-1077,D11-1057,0,0.0342392,"004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection"
N16-1077,N13-1138,0,0.784674,"tions. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wik"
N16-1077,P02-1001,0,0.31819,"al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms. The first step is learning character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inflection generation for different inflection types as shown in Figure 2 (b)–(d). By applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features"
N16-1077,E12-1068,0,0.158263,"cted forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impra"
N16-1077,H05-1085,0,0.0322115,"n from the back to the front vowels. 7 Related Work Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forwardbackward LSTM from a word, and use them in a traditional weighted FST to generate inflected forms. Neural encoder-decoder models of string transduction have also been used for sub-word level transformations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015). Generation of inflectional morphology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonologic"
N16-1077,D11-1125,0,0.036259,"ed corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings. Output Reranking. In the first setting, we first train the inflection generation model using the supervised setting as described in §4.1. While making predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set. Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inflection genferent (equ. 5), and observed consistently worse results. 638 root forms 2764 2027 4055 6400 7249 11200 6957 Infl. 8 27 57 28 53 9 48 Table 3: The number of root forms and types of inflections across datasets. eration model. Thus, the loss function becomes: 1 XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) t=1 Z − λlog"
N16-1077,W14-2804,0,0.0911554,"ection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table cont"
N16-1077,J94-3001,0,0.687857,"and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological process"
N16-1077,D15-1176,1,0.0464737,"Missing"
N16-1077,P07-1017,0,0.372308,"m Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they ca"
N16-1077,Q15-1012,0,0.0435906,"nization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer."
N16-1077,N15-1093,0,0.72903,"state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected form of a given root word"
N16-1077,W04-0409,0,0.110421,"Missing"
N16-1077,J96-1003,0,0.0654284,"al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ("
N16-1077,N09-1024,0,0.0607111,"vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to seq"
N16-1077,N16-1076,0,0.151754,"Missing"
N16-1077,P08-1084,0,0.0451158,"haracter vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural netw"
N16-1077,P08-1059,0,0.285332,"is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1"
N16-1077,W04-0109,0,0.0383169,"(Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large numb"
N16-1077,P00-1027,0,0.147447,"cation of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, ba"
N16-1077,D14-1179,0,\N,Missing
N16-1161,P14-2131,0,0.011684,"e tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360 words—constitute 10–"
N16-1161,P15-1033,1,0.113206,"Missing"
N16-1161,E14-1049,1,0.21174,") that polyglot phonetic feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages c"
N16-1161,D14-1012,0,0.0184753,"Missing"
N16-1161,D15-1127,0,0.0128799,"feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from"
N16-1161,D13-1196,0,0.0422633,"at (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360"
N16-1161,L16-1529,1,0.829863,"learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguistic matrix and 5 This matrix is described in Littell et al. (2016) and is available at https://github.com/dmort27/panphon/. 1364 manually examined aligned dimensions in the phone vectors from §5.3 (trained on six languages). In the maximally-correlated columns—corresponding to linguistic features long, consonant, nasalized—we examined phones with highest coefficients. These &gt; were: [5:, U:, i:, O:, E:] for long; [v, ñ, dZ, d, f, j, &gt; ts, N] for consonant; and [˜O, ˜E, A˜, œ] ˜ for nasalized. Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., &gt; /dZ/ is pres"
N16-1161,N15-1028,0,0.0136804,"ions are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventor"
N16-1161,W11-2124,0,0.0106161,"gful related groupings across languages. 6 Related Work Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still require a trained model per language, and do not allow parameter sharing at training time. Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003). Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, require adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages. Independently, Ammar et al. (2016) used a different polyglot architecture for multilingual dependency parsing. This work has also confirmed the utility of polyglot architectures in leveraging multilinguality. Multimodal neural language models. Multimodal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al."
N16-1161,qian-etal-2010-python,0,0.0593655,"Missing"
N16-1161,D13-1170,0,0.00194071,"Missing"
N16-1161,P15-2021,1,0.840243,"Missing"
N16-1161,D15-1243,1,0.588955,"with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors. 5.5 Qualitative analysis of vectors Phone vectors learned by Polyglot LMs are mere sequences of real numbers. An interesting question is whether these vectors capture linguistic (phonological) qualities of phones they are encoding. To analyze to what extent our vectors capture linguistic properties of phones, we use the QVEC—a tool to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguisti"
N16-1161,P10-1040,0,0.0233197,"nd /U/ in “bit” and “book.” Only through linguistic analysis does it become evident that (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for"
N16-1161,P15-1130,0,0.00692841,"Missing"
N16-1161,P15-1113,0,0.0360795,"Missing"
N16-1161,P13-2037,0,\N,Missing
N19-1062,W03-0419,0,\N,Missing
N19-1062,W19-3621,0,\N,Missing
N19-1062,N19-1061,0,\N,Missing
N19-1062,D17-1323,0,\N,Missing
P14-1024,W07-0103,0,0.295543,"eses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using"
P14-1024,W06-1670,0,0.0180673,"Missing"
P14-1024,levin-etal-2014-resources,1,0.819369,"selection of the training samples. Thus, we trust that annotator judgments were not biased towards the cases that the system is trained to process. Multilingual test sets We collect and annotate metaphoric and literal test sentences in four languages. Thus, we compile eight test datasets, four for SVO relations, and four for AN relations. Each dataset has an equal number of metaphors and non-metaphors, i.e., the datasets are balanced. English (EN) and Russian (RU) datasets have been compiled by our team and are publicly available. Spanish (ES) and Farsi (FA) datasets are published elsewhere (Levin et al., 2014). Table 1 lists test set sizes. EN RU ES FA SVO AN 222 240 220 44 200 200 120 320 5 5.1 Experiments English experiments Our task, as defined in Section 2, is to classify SVO and AN relations as either metaphoric or literal. We first conduct a 10-fold cross-validation experiment on the training set defined in Section 4.1. We represent each candidate relation using the features described in Section 3.2, and evaluate performance of the three feature categories and their combinations. This is done by computing an accuracy in the 10-fold cross validation. Experimental results are given in Table 2,"
P14-1024,D10-1004,0,0.013166,"ets are associated with the supersense noun.body. Therefore, the value of the feature noun.body is 4/38 ≈ 0.11. 3.3 4 Cross-lingual feature projection Datasets In this section we describe a training and testing dataset as well a data collection procedure. 4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset.17 TroFi includes 3,737 manually annotated English sentences from the Wall Street Journal (Birke and Sarkar, 2007). Each sentence contains either literal or metaphorical use for one of 50 English verbs. First, we use a dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) relations. Then, we filter extracted relations to eliminate parsing-related errors, and relations with verbs which are not in the TroFi verb list. After filtering, there are 953 metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and"
P14-1024,E14-1049,1,0.391097,"metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and culled the collected metaphors, by removing duplicates, weak metaphors, and metaphorical phrases (such as Vector space word representations. We employ 64-dimensional vector-space word representations constructed by Faruqui and Dyer (2014).14 Vector construction algorithm is a variation on traditional latent semantic analysis (Deerwester et al., 1990) that uses multilingual information to produce representations in which synonymous words have similar vectors. The vectors were curacy during cross-validation. 12 For the full taxonomy see http://www.sfs. uni-tuebingen.de/lsd/adjectives.shtml 13 http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz 14 http://www.cs.cmu.edu/˜mfaruqui/soft. html 15 http://www.statmt.org/wmt11/ http://www.babylon.com 17 http://www.cs.sfu.ca/˜anoop/students/ jbirke/ 16 251 drowning students) whose in"
P14-1024,J04-1002,0,0.0944408,"ly or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-"
P14-1024,D11-1006,0,0.0129329,"l et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in othe"
P14-1024,W06-3506,0,0.227977,"cond, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for R"
P14-1024,W97-0802,0,0.298022,"membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, for a word calm the top-2 categories (with the first and second highest degrees of membership) are adj.behavior and adj.feeling. For languages othe"
P14-1024,W13-0907,0,0.748517,"may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that"
P14-1024,P12-1092,0,0.00373356,"babilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors of (latent) semantic concepts, that preserve their “meaning” across languages. 3 Classification using Random Forests 8 See Theorem 1.2 in (Breiman, 2001) for details. In our experiments, random forests model slightly outperformed logistic regression and SV"
P14-1024,N13-1076,1,0.7392,"lkit to train our classifiers (Pedregosa et al., 2011). Supersenses are particularly attractive features for metaphor detection: coarse sense taxonomies can be viewed as semantic concepts, and since concept mapping is a process in which metaphors are born, we expect different supersense co-occurrences in metaphoric and literal combinations. In “drinks gasoline”, for example, mapping to supersenses would yield a pair &lt;verb.consumption, noun.substance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space"
P14-1024,N13-1118,0,0.140905,"tive component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors ar"
P14-1024,P13-1117,0,0.0225815,"e and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets"
P14-1024,shutova-teufel-2010-metaphor,0,0.121436,"Missing"
P14-1024,C10-1113,0,0.331036,"6 0.4 EN (area = 0.92) ES (area = 0.73) FA (area = 0.83) RU (area = 0.8) 0.2 0.0 0.0 0.2 0.4 0.6 False Positive Rate 0.8 1.0 (b) AN Figure 2: Cross-lingual experiment: ROC curves for classifiers trained on the English data using a combination of all features, and applied to SVO and AN metaphoric and literal relations in four test languages: English, Russian, Spanish, and Farsi. 6 For a historic overview and a survey of common approaches to metaphor detection, we refer the reader to recent reviews by Shutova et al. (Shutova, 2010; Shutova et al., 2013). Here we focus only on recent approaches. Shutova et al. (2010) proposed a bottom-up method: one starts from a set of seed metaphors and seeks phrases where verbs and/or nouns belong to the same cluster as verbs or nouns in seed examples. Turney et al. (2011) show how abstractness scores could be used to detect metaphorical AN phrases. Neuman et al. (2013) describe a Concrete Category Overlap algorithm, where co-occurrence statistics and Turney’s abstractness scores are used to determine WordNet supersenses that correspond to literal usage of a given adjective or verb. For example, given an adjective, we can learn that it modifies concrete nouns that usua"
P14-1024,J13-2003,0,0.144988,"ard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide"
P14-1024,P10-1071,0,0.0435314,"ine translation, dialog systems, sentiment analysis, and text analytics, etc.) would have access to a potentially useful high-level bit of information about whether something is to be understood literally or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions:"
P14-1024,W13-0909,0,0.256984,"the scope of metaphor identification by including nominal metaphoric relations as well as explore techniques for incorporating contextual features, which can play a key role in identifying certain kinds of metaphors. Second, cross-lingual model transfer can be improved with more careful cross-lingual feature projection. Broadwell et al. (2013) argue that metaphors are highly imageable words that do not belong to a discussion topic. To implement this idea, they extend MRC imageability scores to all dictionary words using links among WordNet supersenses (mostly hypernym and hyponym relations). Strzalkowski et al. (2013) carry out experiments in a specific (government-related) domain for four languages: English, Spanish, Farsi, and Russian. Strzalkowski et al. (2013) explain the algorithm only for English and say that is the same for Spanish, Farsi, and Russian. Because they heavily rely on WordNet and availability of imageability scores, their approach may not be applicable to low-resource languages. Hovy et al. (2013) applied tree kernels to metaphor detection. Their method also employs WordNet supersenses, but it is not clear from the description whether WordNet is essential or can be replaced with some ot"
P14-1024,Q13-1001,0,0.0499428,"Missing"
P14-1024,W13-0906,1,0.94438,"tance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space representations. More specifically, we calculate the degree of abstractness and imageability of all English items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcr"
P14-1024,tsvetkov-etal-2014-augmenting-english,1,0.424069,"tions. Supersenses of nouns and verbs. A lexical item can belong to several synsets, which are associated with different supersenses. Degrees of membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, f"
P14-1024,P10-1040,0,0.00568192,"sh items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors"
P14-1024,D11-1063,0,0.173781,"could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model"
P14-1024,E12-1004,0,\N,Missing
P14-1024,W07-0104,0,\N,Missing
P15-1144,J08-4004,0,0.041742,"Missing"
P15-1144,P98-1013,0,0.241048,"Missing"
P15-1144,P14-2131,0,0.0164479,"Missing"
P15-1144,E14-1049,1,0.0297678,"., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which con"
P15-1144,N15-1184,1,0.140197,"Missing"
P15-1144,P14-1046,0,0.0799848,"gularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporat"
P15-1144,N15-1004,0,0.157051,"rameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimizatio"
P15-1144,N04-1039,0,0.0582336,"rk To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V"
P15-1144,S13-1001,0,0.0655251,"Missing"
P15-1144,D14-1012,0,0.017442,"overcomplete representations A and also binarized representations B. Initial vectors are discussed in §A and tasks in §B. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L. achieves a binary, sparse vector (B) by applying:  bi,j = 1 if xi,j > 0 0 otherwise (7) The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:   1 if xi,j ≥ M + −1 if xi,j ≤ M − ai,j = (8)  0 otherwise where M + (M − ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability"
P15-1144,J15-4004,0,0.0321756,"Missing"
P15-1144,P12-1092,0,0.066485,"ia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word S"
P15-1144,W03-1018,0,0.0399485,"dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164"
P15-1144,D13-1147,0,0.0458503,"Missing"
P15-1144,D13-1196,0,0.025068,"d accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10"
P15-1144,Q13-1015,0,0.0245514,"Missing"
P15-1144,W14-2406,0,0.0387394,"Missing"
P15-1144,C02-1150,0,0.0787722,"e classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (8"
P15-1144,J93-2004,0,0.058184,"der three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validati"
P15-1144,D11-1139,1,0.497885,"Missing"
P15-1144,P14-1074,1,0.851886,"are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234"
P15-1144,P14-2089,0,0.0191967,"Missing"
P15-1144,C12-1118,0,0.72512,"is vectors. λ is a regularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can"
P15-1144,D14-1162,0,0.123387,"ymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Conte"
P15-1144,D13-1170,0,0.0032486,"/code.google.com/p/word2vec http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6 http://cs.cmu.edu/˜mfaruqui/soft.html 1497 5 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many"
P15-1144,D13-1015,0,0.0389009,"Missing"
P15-1144,C98-1013,0,\N,Missing
P15-1144,P14-1009,0,\N,Missing
P15-2021,W12-4410,1,0.853119,"anguage OOVs. We therefore run the borrowing system on OOVs and non-OOV words that occur less than 3 times in the training corpus. We list in table 4 sizes of translated lexicons that we integrate in translation tables. Loan OOVs in SW– EN Loan OOVs in RO – EN 4K 5,050 347 8K 4,219 271 14K 3,577 216 Table 4: Sizes of translated lexicons extracted using pivoting via borrowing and integrated in translation models. Transliteration-augmented setups. In addition to the standard baselines, we evaluate transliteration-augmented setups, where we replace the borrowing model by a transliteration model (Ammar et al., 2012). The model is a linear-chain CRF where we label each source character with a sequence of target characters. The features are label unigrams and bigrams, separately or conjoined with a moving window of source characters. We employ the Swahili–Arabic and Romanian–French transliteration systems that were used as baselines in (Tsvetkov et al., 2015). As in the borrowing system, transliteration outputs are filtered to contain only target language lexicons. We list in table 5 sizes of obtained translated lexicons. www.cdec-decoder.org 128 Translit. OOVs in SW– EN Translit. OOVs in RO – EN 4K 49 906"
P15-2021,N06-1003,0,0.113461,"Missing"
P15-2021,2012.eamt-1.60,0,0.0341247,"e borrowing system only minimally overgenerates the set of output candidates given an input. If the borrowing system encounters an input word that was not borrowed from the target donor language, it usually (but not always) produces an empty output. 3 We set n and k to 5, we did not experiment with other values. 127 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8 . To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4 For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules at www.omniglot.com. 5 We assume that while parallel data is limited in the recipient language, monolingual data is available. 6 code.google.com/p/word2vec 7 github.com/mfaruqui/eacl14-cca 8 sw.globalvoicesonline.org ate translation improvement on corpora of different sizes we conduct experiments with sub-sampled 4K, 8K, and"
P15-2021,D13-1174,1,0.856016,", the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hy"
P15-2021,2014.amta-researchers.24,0,0.0138589,"lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called bo"
P15-2021,P10-1048,0,0.0197761,"., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work f"
P15-2021,P10-4002,1,0.81688,"different sizes we conduct experiments with sub-sampled 4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via"
P15-2021,E14-1049,1,0.806793,"e monolingual corpora, 100-dimensional word vector representations for donor and recipient language vocabularies.5 Then, we employ canonical correlation analysis (CCA) with small donor–loanword dictionaries (training sets in the borrowing models) to project the word embeddings into 50-dimensional vectors with maximized correlation between their dimensions. The semantic feature annotating the synthetic translation candidates is cosine distance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 the language-pair and reduced only to a small set of plausible changes that the donor word can undergo in the process of assimilation in the recipient language. Thus, the borrowing system only minimally overgenerates the set of output candidates given an input. If the borrowing system encounters an input word that was not borrowed from the target donor language, it usually (but not always) produces an empty output. 3 We set n and k to 5, we did not experiment with other values. 127 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus wa"
P15-2021,P08-2015,0,0.0264291,"erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010;"
P15-2021,P08-1088,0,0.0392908,"is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an"
P15-2021,A00-1002,0,0.100731,"Missing"
P15-2021,W11-2123,0,0.0148201,". Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly transla"
P15-2021,P08-1045,0,0.030263,"rce language, producing erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani"
P15-2021,N03-2016,0,0.294531,"iques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four mai"
P15-2021,N01-1020,0,0.0698545,"ing transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be"
P15-2021,D09-1040,0,0.0250133,"n low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typo"
P15-2021,P03-1021,0,0.0122395,"4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potentia"
P15-2021,P02-1040,0,0.0938134,"n in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly translate all OOVs that were borrowed from another language? What is the overall improvement that can be achieved if we correctly translate all OOVs? We answer this question by defining “upper bound” experi"
P15-2021,P95-1050,0,0.22103,"ies and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate tra"
P15-2021,P13-1109,0,0.0138213,"ios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant reso"
P15-2021,P14-1064,0,0.0586032,"rimarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. T"
P15-2021,W13-2234,1,0.847929,"anslation. For each OOV, the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stabl"
P15-2021,N15-1062,1,0.688874,"al improvement (up to +1.6 BLEU) in Swahili–Arabic–English translation, and a small but statistically significant improvement (+0.2 BLEU) in Romanian–French–English. Figure 2: To improve a resource-poor Swahili–English SMT system, we extract translation candidates for OOV Swahili words borrowed from Arabic using the Swahili-to-Arabic borrowing system and Arabic–English resource-rich SMT. bridge between resource-rich and resource-limited languages; we use this observation in our work. Transliteration and cognate discovery models perform poorly in the task of loanword generation/identification (Tsvetkov et al., 2015). The main reason is that the recipient language, in which borrowed words are fully or partially assimilated, may have very different morpho-phonological properties from the donor language (e.g., ‘orange’ and ‘sugar’ are not perceived as foreign by native speakers, but these are English words borrowed from Arabic l. ' PAK (nArnj)1 and QºË@ (Alskr), respectively). Therefore, morpho-phonological loanword adaptation is more complex than is typically captured by transliteration or cognate models. We employ a discriminative cross-lingual model of lexical borrowing to identify plausible donors giv"
P15-2021,D12-1027,0,0.0178563,". Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- an"
P15-2021,N15-1176,0,0.0267247,"peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal,"
P15-2021,W02-0505,0,\N,Missing
P16-1013,D13-1111,1,0.823343,"and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word e"
P16-1013,N07-1058,0,0.0202033,"1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this featur"
P16-1013,W06-1670,0,0.00983607,"t); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier. We use lexical semantic databases to operationalize insights from the prototype theory in the following semantic features; the features are computed on token level and averaged over paragraphs: • Relative frequency in a supersense was computed by marginalizing the word frequencies in the training corpus over coarse semantic categories defined in the WordNet (Fellbaum, 1998; Ciaramita and Altun, 2006). There are 41 supersense types: 26 for nouns and 15 for verbs, e.g., NOUN . ANIMAL and VERB . MOTION . For example, in NOUN . ANIMAL the relative frequency of human is 0.06, of dog is 0.01, of bird is 0.01, of cattle is 0.009, and of bumblebee is 0.0002. • Relative frequency in a synset was calculated similarly to the previous feature category, but word frequencies were marginalized over WordNet synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, veteran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87. 3 Evaluation Benchmarks W"
P16-1013,P13-1004,0,0.0755168,"Missing"
P16-1013,W02-1001,0,0.118037,"ss curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for En"
P16-1013,P15-1033,1,0.791568,"scribed in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.1 treebank (Agi´c et al., 2015) with the standard development and test splits, reporting unlabeled attachment scores (UAS) on the test data. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the pretrained embeddings. • Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embeddings models, each model"
P16-1013,N16-1030,1,0.480643,"40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and spirituality as 1.07. 2 http://http://people.sutd.edu.sg/ ~yue_zhang/doc 133 # paragraphs 2,532,361 et al., 2016). The `2 -regularized logistic regression classifier is tuned on the development set and accuracy is reported on the test set. # tokens 100,872,713 # types 156,663 Table 1: Training data sizes. Named Entity Recognition (NER). Named entity recognition is the task of identifying proper names in a sentence, such as names of persons, locations etc. We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence. We use the CoNLL 2003 English NER dataset (Tjong Kim Sang and De Meulder, 2003) to train our models and present results on the test set. Setup. 100-dimensional word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).3 All training data was used, either shuffled or ordered by a curriculum. As described in §3,"
P16-1013,D13-1170,0,0.00455636,"most prototypical word, scoring 0.87. 3 Evaluation Benchmarks We evaluate the utility of the pretrained word embeddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features: • Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedoc • Concreteness ratings on the scale of 1–5 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and sp"
P16-1013,N10-1116,0,0.528653,"g♣ Brian MacWhinney♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu, lingwang@google.com, macw@cmu.edu Abstract complexity (Bengio et al., 2009; Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training inst"
P16-1013,J93-2004,0,0.0565311,"ithout additional features. All models were learned under same conditions, across curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For depend"
P16-1013,D08-1020,0,0.0314538,"SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that"
P16-1013,D13-1100,0,0.0250457,"Missing"
P16-1013,P14-1024,1,0.818283,"s the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the paragraph. • Average sentence length • Verb-token ratio • Noun-token ratio • Parse tree depth •"
P16-1013,W12-2019,0,0.0531361,"Missing"
P16-1013,P05-1065,0,0.0298158,"P • Quadratic entropy (Rao, 1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 In"
P16-1013,D15-1251,0,0.0243606,"l., 2012, GP), providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators (Bergstra et al., 2011, TPE), tailored to handle conditional spaces. Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Moˇckus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model. 2.2 are used in many contrasting fields, from ecology and biology (Rosenzweig, 1995; Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014). Let pi and pj correspond to empirical frequencies of word types ti and tj"
P16-1013,J11-1005,0,0.033983,"ories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a p"
P17-2009,D11-1145,0,0.0532572,"Missing"
P17-2009,P16-2096,0,0.0351595,". Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Be"
P17-2009,W16-6212,0,0.269445,"development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essentia"
P17-2009,P15-1120,0,0.0212281,"tudies employ off-the-shelf LID systems without considering how they were trained. We aim to create a socially-representative corpus for LID that captures the variation within a language, such as orthography, dialect, formality, topic, and spelling. Motivated by the recent language survey of Twitter (Trampus, 2016), we next describe how we construct this corpus for 70 languages along three dimensions: geography, social and topical diversity, and multilinguality. Geographic Diversity We create a large-scale dataset of geographically-diverse text by bootstrapping with a people-centric approach (Bamman, 2015) that treats location and languagesspoken as demographic attributes to be inferred for authors. By inferring both for Twitter users and then collecting documents from monolingual users, we ensure that we capture regional variation in a language, rather than focusing on a particular aspect of linguistic variety. Individuals’ locations are inferred using the method of Compton et al. (2014) as implemented by Jurgens et al. (2015). The method first identifies the individuals who have reliable ground truth locations from geotagged tweets and then infers the locations of other individuals as the geo"
P17-2009,W12-2108,0,0.0272177,"16; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and Briti"
P17-2009,D16-1120,0,0.0307458,"Missing"
P17-2009,D14-1069,0,0.044983,"enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification"
P17-2009,W16-5801,0,0.0440216,"Missing"
P17-2009,N13-1097,0,0.0695273,"Missing"
P17-2009,W14-3356,0,0.055626,"Missing"
P17-2009,W16-5806,0,0.00821164,"Missing"
P17-2009,P12-3005,0,0.0943464,"epresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language i"
P17-2009,W14-1303,0,0.0375212,"Missing"
P17-2009,D16-1217,0,0.0683736,"Missing"
P17-2009,Q14-1003,0,0.100672,"hich is roughly three epochs. Comparison Systems We compare against two broad-coverage LID systems, langid.py (Lui and Baldwin, 2012) and CLD2 (McCandless, 2010), both of which have been widely used for Twitter within in the NLP community. CLD2 is trained on web page text, while langid.py was trained on newswire, JRC-Acquis, web pages, and Wikipedia. As neither was designed for Twitter, we preprocess text to remove user mentions, hashtags, and URLs for a more fair comparison. For multilingual documents, we substitute langid.py (Lui and Baldwin, 2012) with its extension, Polyglot, described in Lui et al. (2014) and designed for that particular task. Equitable LID Classifier We introduce E QUI LID, and evaluate it on monolingual and multilingual tweet-length text. Model Character-based neural network architectures are particularly suitable for LID, as they facilitate modeling nuanced orthographic and phonological properties of languages (Jaech et al., 2016; Samih et al., 2016), e.g., capturing regular morpheme occurrences within the words of a language. Further, character-based methods significantly reduce the model complexity compared to word-based methods; the latter require separate neural represe"
P17-2009,W14-3907,0,0.0299577,"rage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioecon"
P17-2009,D13-1084,0,0.131253,"Missing"
P17-2009,W14-5307,0,0.0170451,"Missing"
P17-2009,W15-5401,0,0.0296329,"Missing"
P17-2009,P17-1180,0,0.0156764,"level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioeconomic development level ("
P17-2009,P13-1018,0,\N,Missing
P18-1080,D18-1549,0,0.0291297,"et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style from content is hard, even for humans. It depends on the task and the context of the utterance within its discourse. Ultimately we must evaluate our style transfer within some down-stream task where our style transfer has its intended use but we achieve the same task completion criteria. Acknowledgments This work was funded by a fellowship from Robert Bosch, and in part by the National Science Foundation through awar"
P18-1080,D16-1120,0,0.0208493,"Missing"
P18-1080,W15-3001,0,0.00627194,"slation quality. The BLEU scores achieved for English–French MT system is 32.52 and for French–English MT system is 31.11; these are strong translation systems. We deliberately chose a European language close to English for which massive amounts of parallel data are available and translation quality is high, to concentrate on the style generation, rather than improving a translation system. 5 Translation data. We trained an English– French neural machine translation system and a French–English back-translation system. We used data from Workshop in Statistical Machine Translation 2015 (WMT15) (Bojar et al., 2015) to train our translation models. We used the French– English data from the Europarl v7 corpus, the news commentary v10 corpus and the common crawl corpus from WMT15. Data were tokenized using the Moses tokenizer (Koehn et al., 2007). Approximately 5.4M English–French parallel sentences were used for training. A vocabulary size of 100K was used to train the translation systems. 5.1 Style Transfer Accuracy We measure the accuracy of style transfer for the generated sentences using a pre-trained style classifier (§2.2.1). The classifier is trained on data that is not used for training our style"
P18-1080,P07-2045,0,0.0111895,"Missing"
P18-1080,D17-1091,0,0.00433603,"style transfer accuracy. In the task of sentiment modification, the BST model preserved meaning worse than the baseline, on the expense of being better at style transfer. We note, however, that the sentiment modification task is not particularly well-suited for evaluating style transfer: it is particularly hard (if not impossible) to disentangle the sentiment of a sentence from its proposi873 et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style from content is hard,"
P18-1080,N18-1169,0,0.402683,") explore two models for style transfer. The first approach uses multiple decoders for each type of style. In the second approach, style embeddings are used to augment the encoded representations, so that only one decoder needs to be learned to generate outputs in different styles. Style transfer is evaluated on scientific paper titles and newspaper tiles, and sentiment in reviews. This method is different from ours in that we use machine translation to create a strong latent state from which multiple decoders can be trained for each style. We also propose a different human evaluation scheme. Li et al. (2018) first extract words or phrases associated with the original style of the sentence, delete them from the original sentence and then replace them with new phrases associated with the target style. They then use a neural model to fluently combine these into a final output. Junbo BST 2.81 2.87 3.18 2.91 3.11 2.62 Table 6: Fluency of the generated sentences. BST outperforms the baseline overall. It is interesting to note that BST generates significantly more fluent longer sentences than the baseline model. Since the average length of sentences was higher for the gender experiment, BST notably outp"
P18-1080,ganitkevitch-callison-burch-2014-multilingual,0,0.0254638,"e baseline, on the expense of being better at style transfer. We note, however, that the sentiment modification task is not particularly well-suited for evaluating style transfer: it is particularly hard (if not impossible) to disentangle the sentiment of a sentence from its proposi873 et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style from content is hard, even for humans. It depends on the task and the context of the utterance within its discourse. Ultimately we must evaluate our s"
P18-1080,P16-1094,0,0.0281791,"fer and in manual evaluation of meaning preservation and fluency. 1 Introduction Intelligent, situation-aware applications must produce naturalistic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016). In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.”. 866 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 866–876 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Style transfer pipeline: to rephrase a sentence and reduce its stylistic characteristics, the sentence is back-translated. Then, separate style-spe"
P18-1080,D11-1108,0,0.0120753,"Missing"
P18-1080,D15-1166,0,0.00638663,"Missing"
P18-1080,J10-3003,0,0.00894614,"he other to improve the style transfer accuracy. In the task of sentiment modification, the BST model preserved meaning worse than the baseline, on the expense of being better at style transfer. We note, however, that the sentiment modification task is not particularly well-suited for evaluating style transfer: it is particularly hard (if not impossible) to disentangle the sentiment of a sentence from its proposi873 et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style f"
P18-1080,E17-1083,0,0.00725164,"nsfer. We note, however, that the sentiment modification task is not particularly well-suited for evaluating style transfer: it is particularly hard (if not impossible) to disentangle the sentiment of a sentence from its proposi873 et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style from content is hard, even for humans. It depends on the task and the context of the utterance within its discourse. Ultimately we must evaluate our style transfer within some down-stream ta"
P18-1080,W15-4302,0,0.0221033,"scenarios, however, when these attributes need to be modulated or obfuscated. For example, some users may wish to preserve their anonymity online, for personal security concerns (Jardine, 2016), or to reduce stereotype threat (Spencer et al., 1999). Modulating authors’ attributes while preserving meaning of sentences can also help generate demographically-balanced training data for a variety of downstream applications. Moreover, prior work has shown that the quality of language identification and POS tagging degrades significantly on African American Vernacular English (Blodgett et al., 2016; Jørgensen et al., 2015); YouTube’s automatic captions have higher error rates for women and speakers from Scotland (Rudinger et al., 2017). Synthesizing balanced training data—using style transfer techniques—is a plausible way to alleviate bias present in existing NLP technologies. We thus focus on two tasks that have practical and social-good applications, and also accurate style classifiers. To position our method with respect to prior work, we employ a third task of sentiment transfer, which was used in two stateof-the-art approaches to style transfer (Hu et al., 2017; Shen et al., 2017). We describe the three ta"
P18-1080,J16-3007,0,0.00777154,"Missing"
P18-1080,D17-1299,0,0.0210476,"rnegie Mellon University, Pittsburgh, PA, USA {sprabhum,ytsvetko,rsalakhu,awb}@cs.cmu.edu Abstract These goals have motivated a considerable amount of recent research efforts focused at “controlled” language generation—aiming at separating the semantic content of what is said from the stylistic dimensions of how it is said. These include approaches relying on heuristic substitutions, deletions, and insertions to modulate demographic properties of a writer (Reddy and Knight, 2016), integrating stylistic and demographic speaker traits in statistical machine translation (Rabinovich et al., 2016; Niu et al., 2017), and deep generative models controlling for a particular stylistic aspect, e.g., politeness (Sennrich et al., 2016), sentiment, or tense (Hu et al., 2017; Shen et al., 2017). The latter approaches to style transfer, while more powerful and flexible than heuristic methods, have yet to show that in addition to transferring style they effectively preserve meaning of input sentences. This paper introduces a novel approach to transferring style of a sentence while better preserving its meaning. We hypothesize—relying on the study of Rabinovich et al. (2016) who showed that author characteristics a"
P18-1080,P02-1040,0,0.121219,"Missing"
P18-1080,D16-1032,0,0.0114178,"te-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency. 1 Introduction Intelligent, situation-aware applications must produce naturalistic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016). In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.”. 866 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 866–876 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Style transfer pipeline: to rephrase a"
P18-1080,W16-5603,0,0.112113,"e attention vector to replace unknown characters (UNK) using the copy mechanism in (See et al., 2017). t Lrecon (θ G ; x) = EqE (z|x) [log pgen (x|z)] ¯ s )) exp(score(ht , h at = P ¯ s0 exp(score(ht , hs0 ) (7) where Lrecon is given by Eq. (5), Lclass is given by Eq (2) and λc is a balancing parameter. 869 Style gender political sentiment social categories, types of bias, and in multi-class settings. Gender. In sociolinguistics, gender is known to be one of the most important social categories driving language choice (Eckert and McConnellGinet, 2003; Lakoff and Bucholtz, 2004; Coates, 2015). Reddy and Knight (2016) proposed a heuristic-based method to obfuscate gender of a writer. This method uses statistical association measures to identify gender-salient words and substitute them with synonyms typically of the opposite gender. This simple approach produces highly fluent, meaning-preserving sentences, but does not allow for more general rephrasing of sentence beyond single-word substitutions. In our work, we adopt this task of transferring the author’s gender and adapt it to our experimental settings. We used Reddy and Knight’s (2016) dataset of reviews from Yelp annotated for two genders corresponding"
P18-1080,D11-1054,0,0.00561696,"we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency. 1 Introduction Intelligent, situation-aware applications must produce naturalistic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016). In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.”. 866 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 866–876 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Style transfer pipeline: to rephrase a sentence and reduce its stylistic characterist"
P18-1080,W17-1609,0,0.00971283,"Missing"
P18-1080,P17-1099,0,0.0112191,"t propagation from the discriminators to the generator. Instead, following Hu et al. (2017) we use a continuous approximation based on softmax, along with the temperature parameter which anneals the softmax to the discrete case as training proceeds. To create a continuous representation of the output of the generative model which will be given as an input to the classifier, we use: z = E(x) = qE (z|x) (8) ¯ s are all where ht is the current target state and h source states. While generating sentences, we use the attention vector to replace unknown characters (UNK) using the copy mechanism in (See et al., 2017). t Lrecon (θ G ; x) = EqE (z|x) [log pgen (x|z)] ¯ s )) exp(score(ht , h at = P ¯ s0 exp(score(ht , hs0 ) (7) where Lrecon is given by Eq. (5), Lclass is given by Eq (2) and λc is a balancing parameter. 869 Style gender political sentiment social categories, types of bias, and in multi-class settings. Gender. In sociolinguistics, gender is known to be one of the most important social categories driving language choice (Eckert and McConnellGinet, 2003; Lakoff and Bucholtz, 2004; Coates, 2015). Reddy and Knight (2016) proposed a heuristic-based method to obfuscate gender of a writer. This metho"
P18-1080,N16-1005,0,0.0377154,"have motivated a considerable amount of recent research efforts focused at “controlled” language generation—aiming at separating the semantic content of what is said from the stylistic dimensions of how it is said. These include approaches relying on heuristic substitutions, deletions, and insertions to modulate demographic properties of a writer (Reddy and Knight, 2016), integrating stylistic and demographic speaker traits in statistical machine translation (Rabinovich et al., 2016; Niu et al., 2017), and deep generative models controlling for a particular stylistic aspect, e.g., politeness (Sennrich et al., 2016), sentiment, or tense (Hu et al., 2017; Shen et al., 2017). The latter approaches to style transfer, while more powerful and flexible than heuristic methods, have yet to show that in addition to transferring style they effectively preserve meaning of input sentences. This paper introduces a novel approach to transferring style of a sentence while better preserving its meaning. We hypothesize—relying on the study of Rabinovich et al. (2016) who showed that author characteristics are significantly obfuscated by both manual and automatic machine translation—that grounding in back-translation is a"
P18-1080,N15-1020,0,0.0044291,"both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency. 1 Introduction Intelligent, situation-aware applications must produce naturalistic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016). In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.”. 866 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 866–876 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Style transfer pipeline: to rephrase a sentence and reduce its stylistic characteristics, the sentence is b"
P18-1080,L18-1445,1,0.810618,"lp review, a style transfer model will generate a similar review but with an opposite sentiment. We used Shen et al.’s (2017) corpus of reviews from Yelp. They have followed the standard practice of labeling the reviews with rating of higher than three as positive and less than three as negative. They have also split the reviews to sentences and assumed that the sentence has the same sentiment as the review. Political slant. Our second dataset is comprised of top-level comments on Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages (Voigt et al., 2018).3 Only top-level comments that directly respond to the post are included. Every comment to a Congressperson is labeled with the Congressperson’s party affiliation: democratic or republican. Topic and sentiment in these comments reveal commenter’s political slant. For example, defund them all, especially when it comes to the illegal immigrants . and thank u james, praying for all the work u do . are republican, whereas on behalf of the hard-working nh public school teachers- thank you ! and we need more strong voices like yours fighting for gun control . Dataset statistics. We summarize below"
P18-1080,E17-1042,0,0.00513337,"ompared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency. 1 Introduction Intelligent, situation-aware applications must produce naturalistic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016). In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.”. 866 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 866–876 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Style transfer p"
P18-1080,D17-1026,0,0.0143475,"t the sentiment modification task is not particularly well-suited for evaluating style transfer: it is particularly hard (if not impossible) to disentangle the sentiment of a sentence from its proposi873 et al. (2017) learn a representation which is styleagnostic, using adversarial training of the autoencoder. Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation with non-parallel data. Lample et al. (2018) and Artetxe et al. (2018) have proposed sophisticated methods for unsupervised machine translation. These methods could in principle be used for style transfer as well. 7 Measuring the separation of style from content is hard, even for humans. It depends on the task and the context of the utterance within its discourse. Ultimately we must evaluate our style transfer within some down-stream task where our style transfe"
P18-1080,W11-2107,0,\N,Missing
P19-1243,W19-1909,0,0.0332838,"Missing"
P19-1243,P13-1035,0,0.142285,"Missing"
P19-1243,D16-1148,0,0.0947779,"8; Devlin et al., 2019; Radford et al., 2018) have become increasingly common in natural language processing (NLP), improving stateof-the-art results in many standard NLP tasks. However, beyond standard tasks, NLP tools are also vital to more open-ended exploratory tasks, particularly in social science. How these types of tasks can benefit from pre-trained contextualized embeddings has not yet been explored. In this work, we show how to leverage these embeddings to conduct entity-centric analyses, which broadly seek to address how entities are portrayed in narrative text (Bamman et al., 2013; Card et al., 2016). For instance, in the sentence “Batman apprehends the Joker”, a reader might infer that Batman is good, the Joker is evil, and Batman is more powerful than the Joker. Analyzing how people are portrayed in narratives is a key starting point to identifying stereotypes and bias (Joseph et al., 2017; Fast et al., 2016; Field et al., 2019). Existing methods for analyzing people portrayals take either an unsupervised approach (Bamman et al., 2013), which requires large amounts of data and can be difficult to interpret, or rely on domain-specific knowledge (Fast et al., 2016; Wagner et al., 2015), w"
P19-1243,N19-1423,0,0.323822,"unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women. 1 Introduction Pre-trained contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018) have become increasingly common in natural language processing (NLP), improving stateof-the-art results in many standard NLP tasks. However, beyond standard tasks, NLP tools are also vital to more open-ended exploratory tasks, particularly in social science. How these types of tasks can benefit from pre-trained contextualized embeddings has not yet been explored. In this work, we show how to leverage these embeddings to conduct entity-centric analyses, which broadly seek to address how entities are portrayed in narrative text (Bamman et al., 2013; Card et al., 2016). Fo"
P19-1243,N16-1180,0,0.0892581,"Missing"
P19-1243,W19-3823,1,0.851534,"contextualized word embeddings in existing NLP tasks as well as through new benchmarks, designed to distill what type of information these models encode (Tenney et al., 2019; Goldberg, 2019; Liu et al., 2019). These investigations focus on syntactic tasks, with semantic evaluations primarily limited to semantic role labeling. To the best of our knowledge, this is the first work to target affective dimensions in pre-trained contextualized word embeddings. Our findings are consistent with prior work suggesting that contextualized embeddings capture biases from training data (Zhao et al., 2019; Kurita et al., 2019) and that these models perform best when trained on in-domain data (Alsentzer et al., 2019). 7 Conclusions and Future Work We propose a method for incorporating contextualized word embeddings into entity-centric analyses, which has direct applications to numerous social science tasks. Our results are easy to interpret and readily generalize to a variety of research questions. However, we further expose several limitations to this method, specifically that contextualized word embeddings are biased towards representations from their training data, which limits their usefulness in new domains. Wh"
P19-1243,N19-1112,0,0.0202744,"like links between pages. Most affective NLP analyses of narratives focus on sentiment or specific stereotypes. Studies of power have largely been limited to a dialog setting (e.g. Danescu-Niculescu-Mizil et al. (2012), see Prabhakaran (2015) for an overview), and almost no work has examined agency, with the exception of connotation frames. Several recent works have evaluated the usefulness of pre-trained contextualized word embeddings in existing NLP tasks as well as through new benchmarks, designed to distill what type of information these models encode (Tenney et al., 2019; Goldberg, 2019; Liu et al., 2019). These investigations focus on syntactic tasks, with semantic evaluations primarily limited to semantic role labeling. To the best of our knowledge, this is the first work to target affective dimensions in pre-trained contextualized word embeddings. Our findings are consistent with prior work suggesting that contextualized embeddings capture biases from training data (Zhao et al., 2019; Kurita et al., 2019) and that these models perform best when trained on in-domain data (Alsentzer et al., 2019). 7 Conclusions and Future Work We propose a method for incorporating contextualized word embeddin"
P19-1243,P18-1017,0,0.293402,"des a framework for examining narratives that is more holistic than sentiment analyses and more generalizable than task-specific frameworks. The idea that these 3 dimensions are sufficient for capturing affect has also formed the basis of social psychological models (Heise, 2007; Alhothali and Hoey, 2015). Drawing from this theory, we combine con2550 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2550–2560 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics textualized word embeddings with affect lexicons (Mohammad, 2018) to obtain power, sentiment, and agency scores for entities in narrative text. After describing our methodology (§2), we evaluate how well these contextualized embeddings capture affect information on held-out lexicons (§4.1). We then evaluate how well our method scores entities on manually curated benchmarks (§4.2) and through qualitative examples (§4.3). Finally, we use our method to examine different portrayals of men and women (§5), focusing on the same domains as prior work (Wagner et al., 2015; Fu et al., 2016). Ultimately, our work suggests that contexualized embeddings have the potenti"
P19-1243,N18-1202,0,0.611964,"asks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women. 1 Introduction Pre-trained contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018) have become increasingly common in natural language processing (NLP), improving stateof-the-art results in many standard NLP tasks. However, beyond standard tasks, NLP tools are also vital to more open-ended exploratory tasks, particularly in social science. How these types of tasks can benefit from pre-trained contextualized embeddings has not yet been explored. In this work, we show how to leverage these embeddings to conduct entity-centric analyses, which broadly seek to address how entities are portrayed in narrative text (Bamman et al., 2013; C"
P19-1243,P16-1030,0,0.533931,". We introduce a novel approach to analyzing entities that maps contextualized embeddings to interpretable dimensions. Specifically, we propose using pre-trained embeddings to extract affect information about target entities. Social psychology research has identified 3 primary affect dimensions: Potency (strength/weakness of an identity), Valence (goodness/badness of an identity), and Activity (activeness/passiveness of an identity) (Osgood et al., 1957; Russell, 1980, 2003). We refer to these dimensions as power, sentiment, and agency for consistency with prior work in NLP (Sap et al., 2017; Rashkin et al., 2016; Field et al., 2019). Thus, in the previous example, “Batman apprehends the Joker”, we might associate Batman with high power, high sentiment, and high agency. While much literature in NLP has examined sentiment, analyses of power have largely been limited to a dialog setting (Prabhakaran, 2015), and almost no work has examined agency. We propose that mapping entities into these 3 dimensions provides a framework for examining narratives that is more holistic than sentiment analyses and more generalizable than task-specific frameworks. The idea that these 3 dimensions are sufficient for captur"
P19-1243,D17-1247,0,0.255149,"ly more expressive. We introduce a novel approach to analyzing entities that maps contextualized embeddings to interpretable dimensions. Specifically, we propose using pre-trained embeddings to extract affect information about target entities. Social psychology research has identified 3 primary affect dimensions: Potency (strength/weakness of an identity), Valence (goodness/badness of an identity), and Activity (activeness/passiveness of an identity) (Osgood et al., 1957; Russell, 1980, 2003). We refer to these dimensions as power, sentiment, and agency for consistency with prior work in NLP (Sap et al., 2017; Rashkin et al., 2016; Field et al., 2019). Thus, in the previous example, “Batman apprehends the Joker”, we might associate Batman with high power, high sentiment, and high agency. While much literature in NLP has examined sentiment, analyses of power have largely been limited to a dialog setting (Prabhakaran, 2015), and almost no work has examined agency. We propose that mapping entities into these 3 dimensions provides a framework for examining narratives that is more holistic than sentiment analyses and more generalizable than task-specific frameworks. The idea that these 3 dimensions are"
P19-1243,1983.tc-1.13,0,0.246397,"Missing"
P19-1243,N19-1064,0,0.0441308,"ness of pre-trained contextualized word embeddings in existing NLP tasks as well as through new benchmarks, designed to distill what type of information these models encode (Tenney et al., 2019; Goldberg, 2019; Liu et al., 2019). These investigations focus on syntactic tasks, with semantic evaluations primarily limited to semantic role labeling. To the best of our knowledge, this is the first work to target affective dimensions in pre-trained contextualized word embeddings. Our findings are consistent with prior work suggesting that contextualized embeddings capture biases from training data (Zhao et al., 2019; Kurita et al., 2019) and that these models perform best when trained on in-domain data (Alsentzer et al., 2019). 7 Conclusions and Future Work We propose a method for incorporating contextualized word embeddings into entity-centric analyses, which has direct applications to numerous social science tasks. Our results are easy to interpret and readily generalize to a variety of research questions. However, we further expose several limitations to this method, specifically that contextualized word embeddings are biased towards representations from their training data, which limits their usefuln"
P19-1243,N15-1178,0,\N,Missing
Q18-1024,P14-2134,0,0.0237569,"Low German: vrecht ← Proto-Germanic *fra- + *aihtiz Mid. English: wery ← Old English: w¯eri˙g ← Proto-Germanic: *w¯or¯ıgaz French: fatigue ← French: fatiguer ← Latin: fatigare Latin: exaggerare ← Latin: ex- + Latin: aggerare English: over + do Table 2: Etymological roots of example synonym sets with corresponding part-of-speech. we employed fallback to unigram probability estimation. Additionally, we replaced all non-English words with the token ‘UNK’; and all web links, subreddit (e.g., r/compling) and user (u/userid) pointers with the ‘URL’ token.10 4.3.2 Distance estimation and clustering Bamman et al. (2014) introduced a model for incorporating contextual information (such as geography) in learning vector representations. They proposed a joint model for learning word representations in a situated language, a model that “includes information about a subject (i.e., the speaker), allowing to learn the contours of a word’s meaning that are shaped by the context in which it is uttered”. Using a large corpus of tweets, their joint model learned word representations that were sensitive to geographical factors, demonstrating that the usage of wicked in the United States (meaning bad or evil ) differs fro"
Q18-1024,N12-1033,0,0.0241827,"ile our research questions are similar, we present a computational analysis of the effects of cognates on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based;"
Q18-1024,W14-1603,0,0.0231749,"cts on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic"
Q18-1024,K15-1010,0,0.239326,"language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce m"
Q18-1024,de-melo-2014-etymological,0,0.0698098,"Missing"
Q18-1024,W17-5033,0,0.0269104,"opean language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic acquisition in L2, investigating the distribution of collocations (Siyanova-Chanturia, 2015; Kochmar and Shutova, 2017) and formulaic language (Paquot and Granger, 2012) in learner corpora. We, in contrast, address highly-fluent, advanced non-natives in this work. Nastase and Strapparava (2017) presented the first attempt to leverage etymological information for the task of native language identification of English learners. They sowed the seeds for exploitation of etymological clues in the study of non-native language, but their results were very inconclusive. In contrast to the learner corpora that dominate studies in this field (Granger, 2003; Geertzen et al., 2013; Blanchard et al., 2013), our corpus conta"
Q18-1024,W17-5007,0,0.226054,"cale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognat"
Q18-1024,P13-1112,0,0.0296171,"ress syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic"
Q18-1024,D17-1286,0,0.0529633,"tic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic acquisition in L2, investigating the distribution of collocations (Siyanova-Chanturia, 2015; Kochmar and Shutova, 2017) and formulaic language (Paquot and Granger, 2012) in learner corpora. We, in contrast, address highly-fluent, advanced non-natives in this work. Nastase and Strapparava (2017) presented the first attempt to leverage etymological information for the task of native language identification of English learners. They sowed the seeds for exploitation of etymological clues in the study of non-native language, but their results were very inconclusive. In contrast to the learner corpora that dominate studies in this field (Granger, 2003; Geertzen et al., 2013; Blanchard et al., 2013), our corpus contains spontaneous productions of advanced, highly proficient non-native speakers, spanning over 80K topical threads, by 45K distinct users from 50 countries (with 46 native langu"
Q18-1024,P17-1049,1,0.909998,"Missing"
Q18-1024,W13-1706,0,0.132543,"s on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic stru"
Q18-1024,N01-1031,0,0.0243586,"mited set of languages. While our research questions are similar, we present a computational analysis of the effects of cognates on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our stud"
tsvetkov-etal-2014-augmenting-english,W10-0719,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,D08-1027,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W06-1670,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,H93-1061,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,J12-3005,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P06-2072,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P14-1024,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,P12-2050,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,N13-1132,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,I08-2105,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,peters-peters-2000-treatment,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,E14-1049,1,\N,Missing
tsvetkov-wintner-2010-automatic,resnik-1998-parallel,0,\N,Missing
tsvetkov-wintner-2010-automatic,A00-1004,0,\N,Missing
tsvetkov-wintner-2010-automatic,J03-3002,0,\N,Missing
tsvetkov-wintner-2010-automatic,C10-2144,1,\N,Missing
tsvetkov-wintner-2010-automatic,P09-1030,0,\N,Missing
tsvetkov-wintner-2010-automatic,J00-2004,0,\N,Missing
tsvetkov-wintner-2010-automatic,P99-1068,0,\N,Missing
tsvetkov-wintner-2010-automatic,2005.mtsummit-papers.11,0,\N,Missing
tsvetkov-wintner-2010-automatic,1999.mtsummit-1.79,0,\N,Missing
W13-0906,W07-0104,0,0.0943136,"computed using Vector Space Models (VSM), and (3) features based on the types of named entities, if present. Our main target language in these experiments has been Russian, but we also present preliminary experiments with Spanish. The paper is organized as follows: Section 2 contains an overview of the resources we use; Section 3 discusses the methodology; Section 4 presents the experiments; in Section 5, we discuss related work, and we conclude with suggestions for future research in Section 6. 2 Datasets We use the following English lexical resources to train our model: TroFi Example Base1 (Birke and Sarkar, 2007) of 3,737 English sentences from the Wall Street Journal. Each sentence contains one of the seed verbs and is marked L by human annotators if the verb is used in a literal sense. Otherwise, the sentence is marked N (non-literal). The model was evaluated on 25 target verbs with manually annotated 1 to 115 sentences per verb. TroFi does not define the basic meanings of these verbs, but provides examples of literal and metaphoric sentences which we use to train and evaluate our metaphor identification method. WordNet (Fellbaum, 1998) is an English lexical database where each entry contains a set"
W13-0906,W06-3506,0,0.469381,"lability of extensive manually-compiled lexical resources in target languages other than English. A metaphor detecting classifier is trained on English samples and then applied to the target language. The method includes procedures for obtaining semantic features from sentences in the target language. Our experiments with Russian and English sentences show comparable results, supporting our hypothesis that a CSF-based classifier can be applied across languages. We obtain state-ofthe-art performance in both languages. 1 Several approaches to automatic detection of metaphors have been proposed (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010), all of which rely on the availability of extensive manually crafted lexical resources such as WordNet, VerbNet, FrameNet, TreeBank, etc. Unfortunately, such resources exist only for a few resource-rich languages such as English. For most other languages, such resources either do not exist or are of a low quality. Introduction Metaphors are very powerful pervasive communication tools that help deliver complex concepts and ideas simply and effectively (Lakoff and Johnson, 1980). Automatic detection and interpretation of metaphors is critical"
W13-0906,P12-1092,0,0.0118279,"guistic and psycholinguistic attributes rated by human subjects in psycholinguistic experiments. It includes 4,295 words rated with degrees of abstractness; the ratings range from 158 (highly abstract) to 670 (highly concrete). We use these words as a seed when we calculate the values of abstractness and concreteness features for nouns and verbs in our training and test sets. Word Representations via Global Context is a collection of 100,232 words and their vector representations.4 These representations were extracted from a statistical model embedding both local and global contexts of words (Huang et al., 2012), intended to capture better the semantics of words. We use these vectors to calculate the values of abstractness and concreteness features of a word. 3 Methodology We treat the metaphor detection problem as a task of binary classification of sentences. A sentence is represented by one or more key relations such as Subject-Verb-Object triples and Adjective-Noun pairs. In this paper, we focus only on the SVO relations and we allow either the S part or the O part to be empty. If all relations representing a sentence are classified literal by our model then the whole sentence is tagged literal. O"
W13-0906,W07-0103,0,0.0302786,"anually-compiled lexical resources in target languages other than English. A metaphor detecting classifier is trained on English samples and then applied to the target language. The method includes procedures for obtaining semantic features from sentences in the target language. Our experiments with Russian and English sentences show comparable results, supporting our hypothesis that a CSF-based classifier can be applied across languages. We obtain state-ofthe-art performance in both languages. 1 Several approaches to automatic detection of metaphors have been proposed (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010), all of which rely on the availability of extensive manually crafted lexical resources such as WordNet, VerbNet, FrameNet, TreeBank, etc. Unfortunately, such resources exist only for a few resource-rich languages such as English. For most other languages, such resources either do not exist or are of a low quality. Introduction Metaphors are very powerful pervasive communication tools that help deliver complex concepts and ideas simply and effectively (Lakoff and Johnson, 1980). Automatic detection and interpretation of metaphors is critical for many practical language p"
W13-0906,D10-1004,0,0.0660215,"Missing"
W13-0906,C10-1113,0,0.175644,"Missing"
W13-0906,D11-1063,0,0.447163,"have lexF N 08 (noun.body). Therefore, the value of the feature corresponding to this lexF N is 4/38 = 0.10. This dictionary-based mapping of non-English 5 We currently exclude pronouns from the relations that we learn. 47 words into WN synsets is rather coarse. A more discriminating approach may improve the overall performance. In addition, WN synsets may not always capture all the meanings of non-English words. For example, Russian word `íîãà' refers to both the ‘foot’ and the ‘leg’. WN has synsets for foot, leg and extremity, but not for lower extremity. Degree of abstractness According to Turney et al. (2011), “Abstract words refer to ideas and concepts that are distant from immediate perception, such as economics, calculating and disputable.” Concrete words refer to physical objects and actions. Words with multiple senses can refer to both concrete and abstract concepts. Evidence from several languages suggests that concrete verbs tend to have concrete subjects and objects. If either the subject or an object of a concrete verb is abstract, then the verb is typically used in a figurative sense, indicating the presence of a metaphor. For example, when we hear that “an idea was born”, we know that t"
W13-1736,J92-4003,0,0.123486,"Missing"
W13-1736,U07-1006,0,0.0307986,"fy that language. This task has a clear empirical motivation. Nonnative speakers make different errors when they write English, depending on their native language (Lado, 1957; Swan and Smith, 2001); understanding the different types of errors is a prerequisite for correcting them (Leacock et al., 2010), and systems such as the one we describe here can shed interesting light on such errors. Tutoring applications can use our system to identify the native language of students and offer better-targeted advice. Forensic linguistic applications are sometimes required to determine the L1 of authors (Estival et al., 2007b; Estival et al., 2007a). Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language. We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted. Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into “s"
W13-1736,C90-2036,0,0.0232636,"rb and Masuda, 2008). Since English’s orthography is largely phonemic—even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse l and r relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector (Kernighan, 1990) to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay. Restored tags We focus on three important token classes defined above: punctuation marks, function words and cohesive verbs. We first remove words in these classes from the texts, and then recover the most likely hidden tokens in a sequence of words, according to an n-gram language model"
W13-1736,P08-1068,0,0.0252399,"tions). To restore hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-clu"
W13-1736,P11-1132,1,0.750772,"e markers These are 40 function words (and short phrases) that have a strong discourse function in texts (however, because, in fact, etc.). Translators tend to spell out implicit utterances and render them explicitly in the target text (Blum-Kulka, 1986). We use the list of Volansky et al. (forthcoming). Cohesive verbs This is a list of manually compiled verbs that are used, like cohesive markers, to spell out implicit utterances (indicate, imply, contain, etc.). Function words Frequent tokens, which are mostly function words, have been used successfully for various text classification tasks. Koppel and Ordan (2011) define a list of 400 such words, of which we only use 100 (using the entire list was not significantly different). Note that pronouns are included in this list. Contextual function words To further capitalize on the ability of function words to discriminate, we define pairs consisting of a function word from the list mentioned above, along with the POS tag of its adjacent word. This feature captures patterns such as verbs and the preposition or particle immediately to their right, or nouns and the determiner that precedes them. We also define 3-grams consisting of one or two function words an"
W13-1736,N13-1039,1,0.79567,"Missing"
W13-1736,W13-1706,0,0.102937,"Missing"
W13-1736,N03-1033,0,0.010714,"etreault et al., 2013). The training data consists of 1000 essays from each native language. The essays are short, consisting of 10 to 20 sentences each. We used the provided splits of 900 documents for training and 100 for development. Each document is annotated with the author’s English proficiency level (low, medium, high) and an identification (1 to 8) of the essay prompt. All essays are tokenized and split into sentences. In table 1 we provide some statistics on the training corpora, listed by the authors’ proficiency level. All essays were tagged with the Stanford part-of-speech tagger (Toutanova et al., 2003). We did not parse the dataset. # Documents # Tokens # Types Low 1,069 245,130 13,110 Medium 5,366 1,819,407 37,393 High 3,456 1,388,260 28,329 Table 1: Training set statistics. 4 Model For our classification model we used the creg regression modeling framework to train a 11-class logistic regression classifier.1 We parameterize the classifier as a multiclass logistic regression: P exp j λ j h j (x, y) pλ (y |x) = , Zλ (x) where x are documents, h j (·) are real-valued feature functions of the document being classified, λ j are the corresponding weights, and y is one of the eleven L1 class lab"
W13-1736,W07-0602,0,0.263634,"Missing"
W13-1736,P10-1040,0,0.0107663,"hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-cluster http://www.ark.c"
W13-1736,U09-1008,0,0.0944702,"Missing"
W13-1736,D11-1148,0,0.0757912,"Missing"
W13-2234,W11-2103,0,0.0339276,"Missing"
W13-2234,P13-1110,0,0.02121,"Missing"
W13-2234,2012.eamt-1.60,0,0.0414271,"Missing"
W13-2234,P05-1045,0,0.0316055,"Missing"
W13-2234,2011.iwslt-evaluation.19,0,0.221029,"including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold."
W13-2234,E12-1068,0,0.0894067,"e need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing o"
W13-2234,N12-1047,0,0.012334,"matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data. In language pairs that ar"
W13-2234,P06-1121,0,0.169489,"Missing"
W13-2234,J07-2003,0,0.343534,"Missing"
W13-2234,P11-2031,1,0.83514,"say He is in the hospital while UK English speakers might prefer He is in hospital. 7 https://github.com/redpony/creg 8 Preliminary experiments indicated that the excess of N labels resulted in poor performance. 274 et al., 2007) to train a baseline phrase-based SMT system. Each configuration we compare has a different phrase table, with synthetic phrases generated with best-first or iterative strategies, from a phrase table with- or without-determiners, with variable number of translation features. To verify that system improvement is consistent, and is not a result of optimizer instability (Clark et al., 2011), we replicate each experimental setup three times, and then estimate the translation quality of the median MT system using the MultEval toolkit.9 The corpus is the same as in Section 4.3: the training part contains 112,527 sentences from Russian-English TED corpus, randomly sampled 3K sentences are used for tuning and a disjoint set of 2K sentences is used for test. We lowercase both sides, and use Stanford CoreNLP10 tools to tokenize the corpora. We employ SRILM toolkit (Stolcke, 2002) to linearly interpolate the target side of the training corpus with the WMT English corpus, optimizing towa"
W13-2234,N12-1023,0,0.011934,"t sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data."
W13-2234,P11-1004,0,0.0666999,"billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7"
W13-2234,C08-1022,0,0.0705118,"Missing"
W13-2234,D07-1091,0,0.036584,"of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold. First, we propose a new su"
W13-2234,P10-1147,0,0.0342768,"Missing"
W13-2234,W00-1308,0,0.176747,"Missing"
W13-2234,N03-1017,0,0.0089937,"usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier. 1 Introduction Phrase-based translation works as follows. A set of candidate translations for an input sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, Au"
W13-2234,P07-2045,1,0.013907,"Missing"
W13-2234,W00-0708,0,0.154151,"him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabr"
W13-2234,W07-0704,0,0.101819,"Missing"
W13-2234,P02-1040,0,0.0861695,"Missing"
W13-2234,N10-1018,0,0.0318527,"determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrat"
W13-2234,P06-1132,0,0.0145425,"eed to feed the three billion urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiabi"
W13-2234,N07-1007,0,0.0515896,"n urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject m"
W13-2234,D08-1033,0,\N,Missing
W13-2234,I08-1059,0,\N,Missing
W14-3315,W12-4410,1,0.859453,"Hindi–English system includes improved data cleaning of development data, a sophisticated linguistically-informed tokenization scheme, a transliteration module, a synthetic phrase generator that improves handling of function words, and a synthetic phrase generator that leverages source-side paraphrases. We will discuss each of these five in turn. 4.1 4.3 We used the 12,000 Hindi–English transliteration pairs from the ACL 2012 NEWS workshop on transliteration to train a linear-chained CRF tagger1 that labels each character in the Hindi token with a sequence of zero or more English characters (Ammar et al., 2012). At decoding, unseen Hindi tokens are fed to the transliterator, which produces the 100 most probable transliterations. We add a synthetic translation option for each candidate transliteration. In addition to this sophisticated transliteration scheme, we also employ a rule-based transliterator that specifically targets acronyms. In Hindi, many acronyms are spelled out phonetically, such as NSA being rendered as enese (en.es.e). We detected such words in the input segments and generated synthetic translation options both with and without periods (e.g. N.S.A. and NSA). Development Data Cleaning"
W14-3315,W11-1011,1,0.850261,"guage pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-phase approach to translation, in which synthetic translation options supplement the default translation rule inventory that is extracted from word-aligned training data. In the German–English system, we used our compound splitter (Dyer, 2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelde"
W14-3315,J92-4003,0,0.264159,"ead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 3 Common System Improvements Both source and target surface-form LM used modified Kneser-Ney smoothing (Kneser and Ney, 1995), while the model over Brown clusters (Brown et al., 1992) used subtract-0.5 smoothing. A number of our techniques were used for both our German–English and Hindi–English primary submissions. These techniques each fall into one of three categories: those that create translation rules, those involving language models, or those that add translation features. A comparison of these techniques and their performance across the two language pairs can be found in Section 6. 3.1 3.3 In addition to the standard array of features, we added four new indicator feature templates, leading to a total of nearly 150,000 total features. The first set consists of target"
W14-3315,N13-1029,1,0.879212,"Missing"
W14-3315,D13-1174,1,0.829223,"ible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nominal system shows much mor"
W14-3315,W11-1015,1,0.855376,"ore suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are constructed in the same way, but with the added restriction that they do not violate any existing syntactic structure in the parse t"
W14-3315,J07-2003,0,0.0746686,"2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelder–Mead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA"
W14-3315,P11-2031,1,0.887607,"Missing"
W14-3315,W11-2123,0,0.0122222,"observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon"
W14-3315,W11-2107,1,0.826032,"inal test set. Another interesting result is that only one feature set, namely our rule shape features based on Brown clusters, helped on the test set in both language pairs. No feature hurt the BLEU score on the test set in both language pairs, meaning the majority of features helped in one language and hurt in the other. If we compare results on the tuning sets, however, some clearer patterns arise. Brown cluster language models, n-gram features, and our new rule shape features all helped. Furthermore, there were a few features, such as the Brown cluster language model and tuning to Meteor (Denkowski and Lavie, 2011), that helped substantially in one language pair while just barely hurting the other. In particular, the fact that tuning to Meteor instead of BLEU can actually help both BLEU and Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or m"
W14-3315,J03-1002,0,0.00414058,"nthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like"
W14-3315,W12-3131,1,0.878781,"dec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-p"
W14-3315,P02-1040,0,0.0893725,"r extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machi"
W14-3315,W13-2212,0,0.0231279,"Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or more of last year’s top performing systems as pseudo-references during tuning. We experimented with using just one pseudo-reference, taken from last year’s Spanish– English winner (Durrani et al., 2013), and with using four pseudo-references, including the output of last year’s winning Czech–English, French– English, and Russian–English systems (Pino et al., 2013). 5.2 Results 7 German–English Syntax System In addition to our primary German–English system, we also submitted a contrastive German– English system showcasing our group’s tree-totree syntax-based translation formalism. Morphological OOVs Examination of the output of our baseline systems lead us to conclude that the majority of our 145 System Baseline *Meteor Tuning Sentence Boundaries Double Aligners Manual Number Rules Brown Clus"
W14-3315,P06-1055,0,0.0580255,"must be parsed in addition to being word-aligned, we prepared separate copies of the training, tuning, and testing data that are more suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are"
W14-3315,P10-4002,1,0.866575,"2 We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, German–English and Hindi–English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012)"
W14-3315,W13-2225,0,0.0352624,"Missing"
W14-3315,N13-1073,1,0.91499,", several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginnin"
W14-3315,P14-1064,0,0.0199677,"e phrases, namely the target phrasal inventory, can also be represented in a graph form, where the distributional features can also be computed from the target monolingual data. Translation information is then propagated from the labeled phrases to the unlabeled phrases in the source graph, proportional to how similar the phrases are to each other on the source side, as well as how similar the translation candidates are to each other on the target side. The newly acquired translation distributions for the unlabeled phrases are written out to a secondary phrase table. For more information, see Saluja et al. (2014). 5 6 As we added each feature to our systems, we first ran a one-off experiment comparing our baseline system with and without each individual feature. The results of that set of experiments are shown in Table 1 for Hindi–English and Table 2 for German–English. Features marked with a * were not included in our final system submission. The most surprising result is the strength of our Hindi–English baseline system. With no extra bells or whistles, it is already half a BLEU point ahead of the second best system submitted to this shared task. We believe this is due to our filtering of the tuning"
W14-3315,W13-2234,1,0.830476,"ave many different possible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nomi"
W14-3315,W06-3119,0,0.0377712,"Missing"
W14-3315,N09-1046,1,\N,Missing
W14-3315,W12-3160,0,\N,Missing
W16-2506,N09-1003,0,0.38845,"Missing"
W16-2506,W14-1618,0,0.0531837,"ty datasets. In this paper, we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence"
W16-2506,D14-1034,0,0.0955207,"Missing"
W16-2506,Q15-1016,0,0.0805488,", we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are calle"
W16-2506,D12-1091,0,0.0128401,"Missing"
W16-2506,P12-1015,0,0.0446647,"solutions (if available) to address them. 2.1 Semantic or task-specific similarity? Subjectivity of the task The notion of word similarity is subjective and is often confused with relatedness. For example, cup, and coffee are related to each other, but not similar. Coffee refers to a plant (a living organism) or a hot brown drink, whereas cup is a manmade object, which contains liquids, often coffee. Nevertheless, cup and coffee are rated more similar than pairs such as car and train in WS-353 (Finkelstein et al., 2002). Such anomalies are also found in recently constructed datasets like MEN (Bruni et al., 2012). Thus, such datasets unfairly penalize word vector models that capture the fact that cup and coffee are dissimilar. 2.3 No standardized splits & overfitting To obtain generalizable machine learning models, it is necessary to make sure that they do not overfit to a given dataset. Thus, the datasets are usually partitioned into a training, development and test set on which the model is trained, tuned and finally evaluated, respectively (Manning and Schütze, 1999). Existing word similarity datasets are not partitioned into training, development and test sets. Therefore, optimizing the word vecto"
W16-2506,N15-1028,0,0.0179048,"y extend to the word analogy tasks. 31 itly tunes on the test set and overfits the vectors to the task. On the other hand, if researchers decide to perform their own splits of the data, the results obtained across different studies can be incomparable. Furthermore, the average number of word pairs in the word similarity datasets is small (≈ 781, cf. Table 2), and partitioning them further into smaller subsets may produce unstable results. We now present some of the solutions suggested by previous work to avoid overfitting of word vectors to word similarity tasks. Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the"
W16-2506,P11-2031,1,0.115822,"ded each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next. Rastogi et al. (2015) observed that the impro"
W16-2506,W13-3512,0,0.0510995,"r refine this hubness problem to show that there exists a power-law relationship between the frequency-rank5 of a word and the frequency-rank of its neighbors. Specifically, they showed that the average rank of the 1000 nearest neighbors of a word follows: nn-rank ≈ 1000 · word-rank0.17 Inability to account for polysemy (3) 3 This shows that pairs of words which have similar frequency will be closer in the vector-space, thus showing higher word similarity than they should according to their word meaning. Even though newer datasets of word similarity sample words from different frequency bins (Luong et al., 2013; Hill et al., 2014), this still does not solve the problem that cosine similarity in the vector-space gets polluted by frequency-based effects. Different distance normalization schemes have been proposed to downplay the frequency/hubness effect when computing nearest neighbors in the vector space (Dinu et al., 2014; Tomašev et al., 2011), Conclusion In this paper we have identified problems associated with word similarity evaluation of word vector models, and reviewed existing solutions wherever possible. Our study suggests that the use of word similarity tasks for evaluation of word vectors"
W16-2506,P14-5004,1,0.41903,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,N13-1090,0,0.0600247,"of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even"
W16-2506,E14-1049,1,0.384784,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,D14-1113,0,0.023745,"texts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of freque"
W16-2506,D14-1162,0,0.104113,"indings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even though they are not semant"
W16-2506,J15-4004,0,0.101034,"Missing"
W16-2506,N15-1058,1,0.07718,"Missing"
W16-2506,N10-1013,0,0.0397994,"mpute similarity between two words given the contexts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by se"
W16-2506,P15-1173,0,0.0267298,"k and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through t"
W16-2506,D15-1036,0,0.739851,"n with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothesis of no difference has"
W16-2506,W14-1601,0,0.0150773,"electing the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation"
W16-2506,P15-1124,0,0.00829903,"on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the baseline vectors on most tasks.2 By selecting the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferen"
W16-2506,D15-1243,1,0.825403,"llowing: Low correlation with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothe"
W16-2506,P10-1040,0,0.060644,"iple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through the space, while large number of rare words should cluster around related, but more frequent words. However, it has been shown that vector-spaces contain hubs, which are vectors that are close to a large number of other vectors in the space (Radovanovi´c et al., 2010). This problem manifests in word vector-spaces in the form of words that have high cosine similarity with a large number of other words (Dinu et al., 2014). Schnabel et"
W16-2506,P12-1092,0,\N,Missing
W16-2506,N15-1070,1,\N,Missing
W16-2520,P14-2131,0,0.0265954,"ks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with extrinsic scores is"
W16-2520,P12-1015,0,0.0445806,"ector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extri"
W16-2520,P15-1033,1,0.816876,"in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the linguistic matrix which already contains POS-annotated columns does not affect correlations of QV"
W16-2520,P14-5004,1,0.866932,"Missing"
W16-2520,N15-1184,1,0.835181,"vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaph"
W16-2520,N16-1030,1,0.460377,"h are a concatenation of the semantic and syntactic matrices described in §3 for words that occur in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the l"
W16-2520,D13-1196,0,0.0549103,"range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation"
W16-2520,D15-1161,1,0.815675,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,N15-1142,1,0.831815,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,J93-2004,0,0.0550028,"then describe QVEC - CCA. Both QVEC and QVEC - CCA rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. Linguistic resources are invaluable as they capture generalizations made by domain experts. However, resource construction is expensive, therefore it is not always possible to find an existing resource that captures exactly the set of optimal lexical properties for a downstream task. Resources that capture more coarse-grained, general properties can be used instead, for example, WordNet for semantic evaluation (Fellbaum, 1998), or Penn Treebank (Marcus et al., 1993, PTB) for syntactic evaluation. Since these properties are not an exact match to the task, the intrinsic evaluation tests for a necessary (but possibly not sufficient) set of generalizations. QVEC . The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N . To quantify the semantic content of embeddings, a semantic/syntactic linguistic matrix S ∈ RP ×N is constructed from a semantic/syntac"
W16-2520,H93-1061,0,0.0541212,"the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two matrices. Specifically, let A ∈ {0, 1}D×P be a matrix of alignments such that aij = 1 iff xi is aligned to sj , otherwise aij = 0. If r(xi , sj ) is the Pearson’s correlation between vectors xi and sj , then QVEC is defined as: QVEC = A: max P j X X S X aij ≤1 Linguistic Dimension Word Vectors Semantic vectors. To evaluate the semantic content of word vectors, Tsvetkov et al. (2015) exploit supersense annotations in a WordNetannotated corpus—SemCor (Miller et al., 1993). The resulting supersense-dimension matrix has 4,199 rows (supersense-annotated nouns and verbs that occur in SemCor at least 5 times2 ), and 41 columns: 26 for nouns and 15 for verbs. Example vectors are shown in table 1. r(xi , sj ) × aij WORD fish duck chicken i=1 j=1 NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· ··· ··· VB . MOTION 0.00 0.69 0.00 P The constraint j aij ≤ 1, warrants that one distributional dimension is aligned to at most one linguistic dimension. Table 1: Linguistic dimension word vector matrix with semantic vectors, constructed using SemCor. QVEC - CCA . To mea"
W16-2520,D14-1162,0,0.0959606,"relations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the"
W16-2520,D13-1170,0,0.035608,"tic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with"
W16-2520,P14-1024,1,0.80936,"ectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. WS-353 MEN SimLex QVEC QVEC - CCA 20NG 0.55 0.76 0.56 0.74 0.77 Metaphor 0.25 0.49 0.44 0.75 0.73 Senti 0.46 0.55 0.51 0.88 0.93 Table 3: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream text classification tasks. In table 4, we evaluate QVEC and QVEC - CCA on syntactic benchmarks. We f"
W16-2520,D15-1243,1,0.582936,"ations Yulia Tsvetkov♠ Manaal Faruqui♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu Abstract dimensions is an auxiliary mechanism for analyzing how these properties affect the target downstream task. It thus facilitates refinement of word vector models and, consequently, improvement of the target task. Finally, an intrinsic evaluation that approximates a range of related downstream tasks (e.g., semantic text-classification tasks) allows to assess generality (or specificity) of a word vector model, without actually implementing all the tasks. Tsvetkov et al. (2015) proposed an evaluation measure—QVEC—that was shown to correlate well with downstream semantic tasks. Additionally, it helps shed new light on how vector spaces encode meaning thus facilitating the interpretation of word vectors. The crux of the method is to correlate distributional word vectors with linguistic word vectors constructed from rich linguistic resources, annotated by domain experts. Q VEC can easily be adjusted to specific downstream tasks (e.g., part-of-speech tagging) by selecting task-specific linguistic resources (e.g., part-of-speech annotations). However, QVEC suffers from t"
W16-2520,P10-1040,0,0.0694995,"ffective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally effici"
W16-2520,J90-1003,0,\N,Missing
W16-2520,D14-1012,0,\N,Missing
W19-3823,P17-2009,1,0.888237,"Missing"
W19-3823,N18-1021,0,0.0771121,"Missing"
W19-3823,W19-3805,0,0.151803,"Missing"
W19-3823,D14-1162,0,0.0899417,"el using various stimuli presented in Caliskan et al. (2017). Next, we investigate the effect of a specific type of bias in a specific downstream task: gender bias in BERT and its effect on the task of Gendered Pronoun Resolution (GPR) (Webster et al., 2018). We show that the bias in GPR is highly correlated with our measure of bias (§4). Finally, we highlight the potential negative impacts of using BERT in downstream real world applications (§5). The code and data used in this work are publicly Introduction Type-level word embedding models, including word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014), have been shown to exhibit social biases present in human-generated training data (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Manzini et al., 2019). These embeddings are then used in a plethora of downstream applications, which perpetuate and further amplify stereotypes (Zhao et al., 2017; Leino et al., 2019). To reveal and quantify corpuslevel biases is word embeddings, Bolukbasi et al. (2016) used the word analogy task (Mikolov et al., 2013). For example, they showed that gendered male word embeddings like he, man are associated with higher-status jobs like computer"
W19-3823,N18-1202,0,0.445347,"al., 2019). These embeddings are then used in a plethora of downstream applications, which perpetuate and further amplify stereotypes (Zhao et al., 2017; Leino et al., 2019). To reveal and quantify corpuslevel biases is word embeddings, Bolukbasi et al. (2016) used the word analogy task (Mikolov et al., 2013). For example, they showed that gendered male word embeddings like he, man are associated with higher-status jobs like computer programmer and doctor, whereas gendered words like she or woman are associated with homemaker and nurse. Contextual word embedding models, such as ELMo and BERT (Peters et al., 2018; Devlin et al., 2019) have become increasingly common, replacing traditional type-level embeddings and attaining new state of the art results in the majority of 166 Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 166–172 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics available.1 2 We refer to this normalized measure of association as the increased log probability score and the difference between the increased log probability scores for two targets (e.g. he/she) as log probability bias score which we use as measure of bias"
W19-3823,N18-2002,0,0.15874,"Missing"
W19-3823,N19-1423,0,0.31247,"eddings are then used in a plethora of downstream applications, which perpetuate and further amplify stereotypes (Zhao et al., 2017; Leino et al., 2019). To reveal and quantify corpuslevel biases is word embeddings, Bolukbasi et al. (2016) used the word analogy task (Mikolov et al., 2013). For example, they showed that gendered male word embeddings like he, man are associated with higher-status jobs like computer programmer and doctor, whereas gendered words like she or woman are associated with homemaker and nurse. Contextual word embedding models, such as ELMo and BERT (Peters et al., 2018; Devlin et al., 2019) have become increasingly common, replacing traditional type-level embeddings and attaining new state of the art results in the majority of 166 Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 166–172 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics available.1 2 We refer to this normalized measure of association as the increased log probability score and the difference between the increased log probability scores for two targets (e.g. he/she) as log probability bias score which we use as measure of bias. Although this approa"
W19-3823,P19-1243,1,0.850794,"and BERT, but there was no clear indication of bias. Rather, they observed counterintuitive behavior like vastly different p-values for results concerning gender. Along similar lines, Basta et al. (2019) noted that contextual word-embeddings are less biased than traditional word-embeddings. Yet, biases like gender are propagated heavily in downstream tasks. For instance, Zhao et al. (2019) showed that ELMo exhibits gender bias for certain professions. As a result, female entities are predicted less accurately than male entities for certain occupation words, in the coreference resolution task. Field and Tsvetkov (2019) revealed biases in ELMo embeddings that limit their applicability across data domains. Motivated by these recent findings, our work proposes a new method to expose and measure bias in contextualized word embeddings, specifically BERT. As opposed to previ• Positive and Negative Traits Dataset8 - Contains a collection of 234 and 292 adjectives considered “positive” and “negative” traits, respectively. • O*NET 23.2 technology skills9 Contains 17649 unique skills for 27660 jobs, which are posted online Discussion We used the following two templates to measure gender bias: • “TARGET is ATTRIBUTE”,"
W19-3823,D17-1323,0,0.363156,"asure of bias (§4). Finally, we highlight the potential negative impacts of using BERT in downstream real world applications (§5). The code and data used in this work are publicly Introduction Type-level word embedding models, including word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014), have been shown to exhibit social biases present in human-generated training data (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Manzini et al., 2019). These embeddings are then used in a plethora of downstream applications, which perpetuate and further amplify stereotypes (Zhao et al., 2017; Leino et al., 2019). To reveal and quantify corpuslevel biases is word embeddings, Bolukbasi et al. (2016) used the word analogy task (Mikolov et al., 2013). For example, they showed that gendered male word embeddings like he, man are associated with higher-status jobs like computer programmer and doctor, whereas gendered words like she or woman are associated with homemaker and nurse. Contextual word embedding models, such as ELMo and BERT (Peters et al., 2018; Devlin et al., 2019) have become increasingly common, replacing traditional type-level embeddings and attaining new state of the ar"
W19-3823,D18-1521,0,0.0750372,"e if the pronoun belongs to A, B or neither. The MLP layer uses a single hidden layer with 31 dimensions, a dropout of 0.6 and L2 regularization with weight 0.1. 5 Real World Implications In previous sections, we discussed that BERT has human-like biases, which are propagated to downstream tasks. In this section, we discuss another potential negative impact of using BERT in a downstream model. Given that three quarters of US employers now use social media for recruiting job candidates (Segal, 2014), many applications are filtered using job recommendation systems and other AI-powered services. Zhao et al. (2018) Results Although the number of male pronouns associated with no entities in the training data is 5 https://github.com/ google-research-datasets/gap-coreference 6 https://www.kaggle.com/mateiionita/ taming-the-bert-a-baseline 169 discussed that resume filtering systems are biased when the model has strong association between gender and certain professions. Similarly, certain gender-stereotyped attributes have been strongly associated with occupational salary and prestige (Glick, 1991). Using our proposed method, we investigate the gender bias in BERT embeddingss for certain occupation and skil"
W19-3823,N19-1064,0,\N,Missing
W19-4208,A94-1024,0,\N,Missing
W19-4208,P98-1080,0,\N,Missing
W19-4208,C98-1077,0,\N,Missing
W19-4208,N15-1055,0,\N,Missing
W19-4208,N16-1030,0,\N,Missing
W19-4208,N16-1161,1,\N,Missing
W19-4208,P16-1184,0,\N,Missing
W19-4208,E17-5001,0,\N,Missing
W19-4208,E17-2002,1,\N,Missing
W19-4208,E17-1048,0,\N,Missing
W19-4208,D18-1039,0,\N,Missing
W19-4208,W18-6011,0,\N,Missing
W19-4208,K18-1036,0,\N,Missing
W19-4208,N19-1155,0,\N,Missing
W19-4208,D19-1279,0,\N,Missing
W19-4208,W17-4115,0,\N,Missing
W19-4208,N19-1423,0,\N,Missing
W19-5943,D16-1127,0,0.0608071,"Missing"
W19-5943,P13-1025,0,0.0750882,"Missing"
W19-5943,W14-3348,0,0.00909931,"ays to operationalize and quantify these abstract principles. In Table 1, we list our actionable tactics motivated by various negotiation principles. To detect these tactics from turns, we use a mix of learned classifiers2 for turn-level tactics (e.g., propose prices) and regular expression rules for lexical tactics (e.g., use polite words). To create the training set for learning tactic predictors, we randomly selected 200 dialogs and annotated them with tactics.3 The detectors use the following features: (1) the number of words overlapping with the product description; (2) the METEOR score (Denkowski and Lavie, 2014) of the turn given the product description as reference; (3) the cosine distance between the turn embedding and the product description embedding.4 For “Address buyer’s concerns”, we additionally include lexical features indicating a question (e.g.,“why”, “how”, “does”) from the immediate previous buyer’s turns. Table 2 summarizes the number pf training examples and prediction accuracies for each learned classifier. For lexical tactics, we have the following rules: Product Listing: Product Description Listing Price 2 3 Coach: Predict Strategies Select Optimal Strategies 3 2 4 5 Realize Strateg"
W19-5943,W15-4621,0,0.0605219,"Missing"
W19-5943,P17-1162,1,0.900787,"Missing"
W19-5943,D18-1256,1,0.672956,"nd predicts the best tactics to use in the next turn to achieve a higher final price. The seller has the freedom to choose whether to use the recommended tactics. 3 Approach We define a set of diverse tactics S from past study on negotiation in behavioral economics, including both high-level dialog acts (e.g., hpropose a pricei, hdescribe the producti) and low-level lexical features (e.g. huse hedge wordsi). Given the negotiation scenario and the dialog history, the coach takes the following steps (Figure 3) to generate suggestions: Problem Statement We follow the CraigslistBargain setting of He et al. (2018), where a buyer and a seller negotiate the price of an item for sale. The negotiation scenario is based on listings scraped from craigslist. com, including product description, product photos (if available), and the listing price. In addi1. The tactics detectors map each turn to a set of tactics in S. 2. The tactics predictor predicts the set of possible tactics in the next turn given the dia368 offer free delivery). Therefore, we develop datadriven ways to operationalize and quantify these abstract principles. In Table 1, we list our actionable tactics motivated by various negotiation princip"
W19-5943,D17-1259,0,0.0863509,"Missing"
W19-5943,P16-1094,0,0.0288899,"Missing"
W19-5943,W13-4016,0,\N,Missing
