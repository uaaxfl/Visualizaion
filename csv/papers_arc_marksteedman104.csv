2021.textgraphs-1.14,Fine-grained General Entity Typing in {G}erman using {G}erma{N}et,2021,-1,-1,2,1,747,sabine weber,Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15),0,"Fine-grained entity typing is important to tasks like relation extraction and knowledge base construction. We find however, that fine-grained entity typing systems perform poorly on general entities (e.g. {``}ex-president{''}) as compared to named entities (e.g. {``}Barack Obama{''}). This is due to a lack of general entities in existing training data sets. We show that this problem can be mitigated by automatically generating training data from WordNets. We use a German WordNet equivalent, GermaNet, to automatically generate training data for German general entity typing. We use this data to supplement named entity data to train a neural fine-grained entity typing system. This leads to a 10{\%} improvement in accuracy of the prediction of level 1 FIGER types for German general entities, while decreasing named entity type prediction accuracy by only 1{\%}."
2021.iwpt-1.4,Semi-Automatic Construction of Text-to-{SQL} Data for Domain Transfer,2021,-1,-1,3,0,5813,tianyi li,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"Strong and affordable in-domain data is a desirable asset when transferring trained semantic parsers to novel domains. As previous methods for semi-automatically constructing such data cannot handle the complexity of realistic SQL queries, we propose to construct SQL queries via context-dependent sampling, and introduce the concept of topic. Along with our SQL query construction method, we propose a novel pipeline of semi-automatic Text-to-SQL dataset construction that covers the broad space of SQL queries. We show that the created dataset is comparable with expert annotation along multiple dimensions, and is capable of improving domain transfer performance for SOTA semantic parsers."
2021.insights-1.7,Zero-Shot Cross-Lingual Transfer is a Hard Baseline to Beat in {G}erman Fine-Grained Entity Typing,2021,-1,-1,2,1,747,sabine weber,Proceedings of the Second Workshop on Insights from Negative Results in NLP,0,"The training of NLP models often requires large amounts of labelled training data, which makes it difficult to expand existing models to new languages. While zero-shot cross-lingual transfer relies on multilingual word embeddings to apply a model trained on one language to another, Yarowski and Ngai (2001) propose the method of annotation projection to generate training data without manual annotation. This method was successfully used for the tasks of named entity recognition and coarse-grained entity typing, but we show that it is outperformed by zero-shot cross-lingual transfer when applied to the similar task of fine-grained entity typing. In our study of fine-grained entity typing with the FIGER type ontology for German, we show that annotation projection amplifies the English model{'}s tendency to underpredict level 2 labels and is beaten by zero-shot cross-lingual transfer on three novel test sets."
2021.insights-1.16,Blindness to Modality Helps Entailment Graph Mining,2021,-1,-1,4,0.905534,5894,liane guillou,Proceedings of the Second Workshop on Insights from Negative Results in NLP,0,"Understanding linguistic modality is widely seen as important for downstream tasks such as Question Answering and Knowledge Graph Population. Entailment Graph learning might also be expected to benefit from attention to modality. We build Entailment Graphs using a news corpus filtered with a modality parser, and show that stripping modal modifiers from predicates in fact increases performance. This suggests that for some tasks, the pragmatics of modal modification of predicates allows them to contribute as evidence of entailment."
2021.findings-emnlp.238,Open-Domain Contextual Link Prediction and its Complementarity with Entailment Graphs,2021,-1,-1,4,1,1070,mohammad hosseini,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"An open-domain knowledge graph (KG) has entities as nodes and natural language relations as edges, and is constructed by extracting (subject, relation, object) triples from text. The task of open-domain link prediction is to infer missing relations in the KG. Previous work has used standard link prediction for the task. Since triples are extracted from text, we can ground them in the larger textual context in which they were originally found. However, standard link prediction methods only rely on the KG structure and ignore the textual context that each triple was extracted from. In this paper, we introduce the new task of open-domain contextual link prediction which has access to both the textual context and the KG structure to perform link prediction. We build a dataset for the task and propose a model for it. Our experiments show that context is crucial in predicting missing relations. We also demonstrate the utility of contextual link prediction in discovering context-independent entailments between relations, in the form of entailment graphs (EG), in which the nodes are the relations. The reverse holds too: context-independent EGs assist in predicting relations in context."
2021.emnlp-main.87,Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking,2021,-1,-1,2,0,8806,nikita moghe,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -{\textgreater} Chinese, Chinese -{\textgreater} English) and Multilingual WoZ (English -{\textgreater} German, English -{\textgreater} Italian) datasets. We achieve impressive improvements ({\textgreater} 20{\%} on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10{\%} of the target language task data and zero-shot setup respectively."
2021.emnlp-main.840,Multivalent Entailment Graphs for Question Answering,2021,-1,-1,6,0,10271,nick mckenna,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) entails WIN(Biden); (2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence."
2021.cmcl-1.3,Modeling Incremental Language Comprehension in the Brain with {C}ombinatory {C}ategorial {G}rammar,2021,-1,-1,5,0.656545,10251,milovs stanojevic,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Hierarchical sentence structure plays a role in word-by-word human sentence comprehension, but it remains unclear how best to characterize this structure and unknown how exactly it would be recognized in a step-by-step process model. With a view towards sharpening this picture, we model the time course of hemodynamic activity within the brain during an extended episode of naturalistic language comprehension using Combinatory Categorial Grammar (CCG). CCG has well-defined incremental parsing algorithms, surface compositional semantics, and can explain long-range dependencies as well as complicated cases of coordination. We find that CCG-derived predictors improve a regression model of fMRI time course in six language-relevant brain regions, over and above predictors derived from context-free phrase structure. Adding a special Revealing operator to CCG parsing, one designed to handle right-adjunction, improves the fit in three of these regions. This evidence for CCG from neuroimaging bolsters the more general case for mildly context-sensitive grammars in the cognitive science of language."
2021.case-1.6,Modality and Negation in Event Extraction,2021,-1,-1,5,1,5895,sander vroe,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),0,"Language provides speakers with a rich system of modality for expressing thoughts about events, without being committed to their actual occurrence. Modality is commonly used in the political news domain, where both actual and possible courses of events are discussed. NLP systems struggle with these semantic phenomena, often incorrectly extracting events which did not happen, which can lead to issues in downstream applications. We present an open-domain, lexicon-based event extraction system that captures various types of modality. This information is valuable for Question Answering, Knowledge Graph construction and Fact-checking tasks, and our evaluation shows that the system is sufficiently strong to be used in downstream applications."
2021.acl-long.79,Prosodic segmentation for parsing spoken dialogue,2021,-1,-1,2,0,12812,elizabeth nielsen,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn{'}t true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency {--} a distinction that the model otherwise struggles to make."
2020.textgraphs-1.7,Incorporating Temporal Information in Entailment Graph Mining,2020,-1,-1,5,0.905534,5894,liane guillou,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),0,"We present a novel method for injecting temporality into entailment graphs to address the problem of spurious entailments, which may arise from similar but temporally distinct events involving the same pair of entities. We focus on the sports domain in which the same pairs of teams play on different occasions, with different outcomes. We present an unsupervised model that aims to learn entailments such as win/lose â play, while avoiding the pitfall of learning non-entailments such as win Ì¸â lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies."
2020.starsem-1.15,Learning Negation Scope from Syntactic Structure,2020,-1,-1,2,0,10271,nick mckenna,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"We present a semi-supervised model which learns the semantics of negation purely through analysis of syntactic structure. Linguistic theory posits that the semantics of negation can be understood purely syntactically, though recent research relies on combining a variety of features including part-of-speech tags, word embeddings, and semantic representations to achieve high task performance. Our simplified model returns to syntactic theory and achieves state-of-the-art performance on the task of Negation Scope Detection while demonstrating the tight relationship between the syntax and semantics of negation."
2020.iwpt-1.12,Span-Based {LCFRS}-2 Parsing,2020,-1,-1,2,0.829013,10251,milovs stanojevic,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"The earliest models for discontinuous constituency parsers used mildly context-sensitive grammars, but the fashion has changed in recent years to grammar-less transition-based parsers that use strong neural probabilistic models to greedily predict transitions. We argue that grammar-based approaches still have something to contribute on top of what is offered by transition-based parsers. Concretely, by using a grammar formalism to restrict the space of possible trees we can use dynamic programming parsing algorithms for exact search for the most probable tree. Previous chart-based parsers for discontinuous formalisms used probabilistically weak generative models. We instead use a span-based discriminative neural model that preserves the dynamic programming properties of the chart parsers. Our parser does not use an explicit grammar, but it does use explicit grammar formalism constraints: we generate only trees that are within the LCFRS-2 formalism. These properties allow us to construct a new parsing algorithm that runs in lower worst-case time complexity of O(l n{\^{}}4 +n{\^{}}6), where $n$ is the sentence length and $l$ is the number of unique non-terminal labels. This parser is efficient in practice, provides best results among chart-based parsers, and is competitive with the best transition based parsers. We also show that the main bottleneck for further improvement in performance is in the restriction of fan-out to degree 2. We show that well-nestedness is helpful in speeding up parsing, but lowers accuracy."
2020.findings-emnlp.199,The Role of Reentrancies in {A}bstract {M}eaning {R}epresentation Parsing,2020,-1,-1,4,1,19680,ida szubert,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Abstract Meaning Representation (AMR) parsing aims at converting sentences into AMR representations. These are graphs and not trees because AMR supports reentrancies (nodes with more than one parent). Following previous findings on the importance of reen- trancies for AMR, we empirically find and discuss several linguistic phenomena respon- sible for reentrancies in AMR, some of which have not received attention before. We cate- gorize the types of errors AMR parsers make with respect to reentrancies. Furthermore, we find that correcting these errors provides an in- crease of up to 5{\%} Smatch in parsing perfor- mance and 20{\%} in reentrancy prediction"
2020.emnlp-main.642,The role of context in neural pitch accent detection in {E}nglish,2020,9,0,2,0,12812,elizabeth nielsen,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5{\%} to 88.7{\%} accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2{\%} accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus."
2020.coling-main.401,Aspectuality Across Genre: A Distributional Semantics Approach,2020,-1,-1,4,1,10629,thomas kober,Proceedings of the 28th International Conference on Computational Linguistics,0,"The interpretation of the lexical aspect of verbs in English plays a crucial role in tasks such as recognizing textual entailment and learning discourse-level inferences. We show that two elementary dimensions of aspectual class, states vs. events, and telic vs. atelic events, can be modelled effectively with distributional semantics. We find that a verb{'}s local context is most indicative of its aspectual class, and we demonstrate that closed class words tend to be stronger discriminating contexts than content words. Our approach outperforms previous work on three datasets. Further, we present a new dataset of human-human conversations annotated with lexical aspects and present experiments that show the correlation of telicity with genre and discourse goals."
2020.acl-main.378,Max-Margin Incremental {CCG} Parsing,2020,-1,-1,2,0.829013,10251,milovs stanojevic,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers."
W19-3625,Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference,2019,-1,-1,2,1,747,sabine weber,Proceedings of the 2019 Workshop on Widening NLP,0,"This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018)."
W19-0409,Temporal and Aspectual Entailment,2019,58,0,3,1,10629,thomas kober,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Inferences regarding {``}Jane{'}s arrival in London{''} from predications such as {``}Jane is going to London{''} or {``}Jane has gone to London{''} depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of {``}going to London{''} is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern embedding models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform inference on predications across aspectual types and tenses. We show that they encode a substantial amount of information relating to tense and aspect, but fail to consistently model inferences that require reasoning with these semantic properties."
P19-1238,Wide-Coverage Neural {A}* Parsing for {M}inimalist {G}rammars,2019,0,1,3,0,25676,john torr,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky{'}s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is cubic in the length of the sentence. The parser is publicly available."
P19-1468,Duality of Link Prediction and Entailment Graph Induction,2019,0,0,4,1,1070,mohammad hosseini,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two problems are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned entailments to predict improved link prediction scores. Our results show that the two tasks can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores."
N19-1020,{CCG} Parsing Algorithm with Incremental Tree Rotation,2019,0,1,2,0.829013,10251,milovs stanojevic,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as optional constituents that adjoin on the right, such as right adjuncts and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent left-branching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the higher-order unification over lambda terms (Pareschi and Steedman,1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This algorithm can cover the whole CCGbank, with greater incrementality and accuracy than previous proposals."
D19-5321,Node Embeddings for Graph Merging: Case of Knowledge Graph Construction,2019,0,0,2,1,19680,ida szubert,Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),0,"Combining two graphs requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an algorithm for building Knowledge Graphs from text corpora. AskNET node matching method uses string similarity, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall error reduction of from 56{\%} to 23.6{\%}, with a reduction of over a half in both types of incorrect node matching."
Q18-1048,Learning Typed Entailment Graphs with Global Soft Constraints,2018,0,1,7,1,1070,mohammad hosseini,Transactions of the Association for Computational Linguistics,0,"This paper presents a new method for learning typed entailment graphs from text. We extract predicate-argument structures from multiple-source news corpora, and compute local distributional similarity scores to learn entailments between predicates with typed arguments (e.g., person contracted disease). Previous work has used transitivity constraints to improve local decisions, but these constraints are intractable on large graphs. We instead propose a scalable method that learns globally consistent similarity scores based on new soft constraints that consider both the structures across typed entailment graphs and inside each graph. Learning takes only a few hours to run over 100K predicates and our results show large improvements over local similarity scores on two entailment data sets. We further show improvements over paraphrases and entailments from the Paraphrase Database, and prior state-of-the-art entailment graphs. We show that the entailment graphs improve performance in a downstream task."
P18-2072,Predicting accuracy on large datasets from smaller pilot data,2018,0,4,4,0,4047,mark johnson,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from models trained on a small pilot training dataset. We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy. We introduce a new performance extrapolation task to evaluate how well different extrapolations predict accuracy on larger training sets. We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets."
P18-1036,Character-Level Models versus Morphology in Semantic Role Labeling,2018,0,4,2,0,21981,gozde csahin,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability."
J18-4001,The Lost Combinator,2018,48,0,1,1,748,mark steedman,Computational Linguistics,0,
D18-1545,Data Augmentation via Dependency Tree Morphing for Low-Resource Languages,2018,0,1,2,0,21981,gozde csahin,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such datasets leads to poor system performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from image processing. We {``}crop{''} sentences by removing dependency links, and we {``}rotate{''} sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that crop and rotate provides improvements over the models trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems."
D17-1009,Universal Semantic Parsing,2017,0,23,4,0.851106,3549,siva reddy,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions."
Q16-1010,Transforming Dependency Structures to Logical Forms for Semantic Parsing,2016,68,58,6,1,3549,siva reddy,Transactions of the Association for Computational Linguistics,0,"The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures and semantic logical forms. In contrast{---}partly due to the lack of a strong type system{---}dependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages. However, the lack of a type system makes a formal mechanism for deriving logical forms from dependency structures challenging. We address this by introducing a robust system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees. These logical forms are then used for semantic parsing of natural language to Freebase. Experiments on the Free917 and Web-Questions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions."
N16-1052,Shift-Reduce {CCG} Parsing using Neural Network Models,2016,29,5,3,1,34684,bharat ambati,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a neural network based shift- reduce CCG parser, the first neural-network based parser for CCG. We also study the im- pact of neural network based tagging mod- els, and greedy versus beam-search parsing, by using a structured neural network model. Our greedy parser obtains a labeled F-score of 83.27%, the best reported result for greedy CCG parsing in the literature (an improve- ment of 2.5% over a perceptron based greedy parser) and is more than three times faster. With a beam, our structured neural network model gives a labeled F-score of 85.57% which is 0.6% better than the perceptron based counterpart."
N16-1120,Assessing Relative Sentence Complexity using an Incremental {CCG} Parser,2016,27,6,3,1,34684,bharat ambati,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Given a pair of sentences, we present computational models to assess if one sentence is simpler to read than the other. While existing models explored the usage of phrase structure features using a non-incremental parser, experimental evidence suggests that the human language processor works incrementally. We empirically evaluate if syntactic features from an incremental CCG parser are more useful than features from a non-incremental phrase structure parser. Our evaluation on Simple and Standard Wikipedia sentence pairs suggests that incremental CCG features are indeed more useful than phrase structure features achieving 0.44 points gain in performance. Incremental CCG parser also gives significant improvements in speed (12 times faster) in comparison to the phrase structure parser. Furthermore, with the addition of psycholinguistic features, we achieve the strongest result to date reported on this task. Our code and data can be downloaded from https://github. com/bharatambati/sent-compl."
D16-1214,Evaluating Induced {CCG} Parsers on Grounded Semantic Parsing,2016,12,1,5,0,8387,yonatan bisk,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-2610,Parser Adaptation to the Biomedical Domain without Re-Training,2015,41,0,2,0,21545,jeff mitchell,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"We present a distributional approach to the problem of inducing parameters for unseen words in probabilistic parsers. Our KNN-based algorithm uses distributional similarity over an unlabelled corpus to match unseen words to the most similar seen words, and can induce parameters for those unseen words without retraining the parser. We apply this to domain adaptation for three different parsers that employ fine-grained syntactic categories, which allows us to focus on modifying the lexicon, while leaving the structure of the parser itself intact. We demonstrate uplifts for dependency recovery of 2%-6% on novel vocabulary in biomedical text."
P15-1126,Orthogonality of Syntax and Semantics within Distributional Spaces,2015,15,11,2,0,21545,jeff mitchell,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"A recent distributional approach to wordanalogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, posinduction and word-analogy tasks."
P15-1141,A Computationally Efficient Algorithm for Learning Topical Collocation Models,2015,30,2,6,0,37533,zhendong zhao,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Most existing topic models make the bagof-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bigrams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model."
N15-1006,An Incremental Algorithm for Transition-based {CCG} Parsing,2015,28,8,4,1,34684,bharat ambati,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Incremental parsers have potential advantages for applications like language modeling for machine translation and speech recognition. We describe a new algorithm for incremental transition-based Combinatory Categorial Grammar parsing. As English CCGbank derivations are mostly right branching and non-incremental, we design our algorithm based on the dependencies resolved rather than the derivation. We introduce two new actions in the shift-reduce paradigm based on the idea of xe2x80x98revealingxe2x80x99 (Pareschi and Steedman, 1987) the required information during parsing. On the standard CCGbank test data, our algorithm achieved improvements of 0.88% in labeled and 2.0% in unlabeled F-score over a greedy non-incremental shift-reduce parser."
N15-1122,Lexical Event Ordering with an Edge-Factored Model,2015,42,6,3,0.949107,3239,omri abend,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Extensive lexical knowledge is necessary for temporal analysis and planning tasks. We address in this paper a lexical setting that allows for the straightforward incorporation of rich features and structural constraints. We explore a lexical event ordering task, namely determining the likely temporal order of events based solely on the identity of their predicates and arguments. We propose an xe2x80x9cedgefactoredxe2x80x9d model for the task that decomposes over the edges of the event graph. We learn it using the structured perceptron. As lexical tasks require large amounts of text, we do not attempt manual annotation and instead use the textual order of events in a domain where this order is aligned with their temporal order, namely cooking recipes."
Y14-1001,Robust Semantics for Semantic Parsing,2014,35,0,1,1,748,mark steedman,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,None
W14-2406,Combining Formal and Distributional Models of Temporal and Intensional Semantics,2014,12,6,2,1,4489,mike lewis,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"We outline a vision for computational semantics in which formal compositional semantics is combined with a powerful, structured lexical semantics derived from distributional statistics. We consider how existing work (Lewis and Steedman, 2013) could be extended with a much richer lexical semantics using recent techniques for modelling processes (Scaria et al., 2013)xe2x80x94for example, learning that visiting events start with arriving and end with leaving. We show how to closely integrate this information with theories of formal semantics, allowing complex compositional inferences such as is visiting!has arrived in but will leave, which requires interpreting both the function and content words. This will allow machine reading systems to understand not just what has happened, but when."
Q14-1026,Improved {CCG} Parsing with Semi-supervised Supertagging,2014,38,28,2,1,4489,mike lewis,Transactions of the Association for Computational Linguistics,0,"Current supervised parsers are limited by the size of their labelled training data, making improving them with unlabelled data an important goal. We show how a state-of-the-art CCG parser can be enhanced, by predicting lexical categories using unsupervised vector-space embeddings of words. The use of word embeddings enables our model to better generalize from the labelled data, and allows us to accurately assign lexical categories without depending on a POS-tagger. Our approach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8{\%}), Wikipedia (1.8{\%}) and biomedical (3.4{\%}) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy."
Q14-1030,Large-scale Semantic Parsing without Question-Answer Pairs,2014,35,94,3,1,3549,siva reddy,Transactions of the Association for Computational Linguistics,0,"In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art."
P14-1061,Lexical Inference over Multi-Word Predicates: A Distributional Approach,2014,51,2,3,0.949107,3239,omri abend,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Representing predicates in terms of their argument distribution is common practice in NLP. Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions. The latter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously difficult, (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts. We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWPxe2x80x99s words. We assume a latent distribution over sub-sets of the MWP, and estimate it relative to a downstream prediction task. Focusing on the supervised identification of lexical inference relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics."
E14-4031,Improving Dependency Parsers using {C}ombinatory {C}ategorial {G}rammar,2014,20,8,3,1,34684,bharat ambati,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Subcategorization information is a useful feature in dependency parsing. In this paper, we explore a method of incorporating this information via Combinatory Categorial Grammar (CCG) categories from a supertagger. We experiment with two popular dependency parsers (Malt and MST) for two languages: English and Hindi. For both languages, CCG categories improve the overall accuracy of both parsers by around 0.3-0.5% in all experiments. For both parsers, we see larger improvements specifically on dependencies at which they are known to be weak: long distance dependencies for Malt, and verbal arguments for MST. The result is particularly interesting in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web."
E14-1014,Generalizing a Strongly Lexicalized Parser using Unlabeled Data,2014,26,5,4,1,5203,tejaswini deoskar,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical. For parsers based on strongly lexicalized grammar formalisms (such as CCG, which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon. In this paper, we show that semi-supervised Viterbi-EM can be used to extend the lexicon of a generative CCG parser. By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both indomain (WSJ) and out-of-domain (questions and Wikipedia) data. Our learnt lexicons when used with a discriminative parser such as C&C also significantly improve its performance on unseen words."
E14-1066,A Generative Model for User Simulation in a Spatial Navigation Domain,2014,29,3,4,0,40090,aciel eshky,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose the use of a generative model to simulate user behaviour in a novel taskoriented dialog domain, where user goals are spatial routes across artificial landscapes. We show how to derive an efficient feature-based representation of spatial goals, admitting exact inference and generalising to new routes. The use of a generative model allows us to capture a range of plausible behaviour given the same underlying goal. We evaluate intrinsically using held-out probability and perplexity, and find a substantial reduction in uncertainty brought by our spatial representation. We evaluate extrinsically in a human judgement task and find that our modelxe2x80x99s behaviour does not differ significantly from the behaviour of real users."
D14-1107,{A}* {CCG} Parsing with a Supertag-factored Model,2014,40,59,2,1,4489,mike lewis,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We introduce a new CCG parsing model which is factored on lexical category assignments. Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation. The parser is extremely simple, with a tiny feature set, no POS tagger, and no statistical model of the derivation or dependencies. Formulating the model in this way allows a highly effective heuristic for A parsing, which makes parsing extremely fast. Compared to the standard C&C CCG parser, our model is more accurate out-of-domain, is four times faster, has higher coverage, and is greatly simplified. We also show that using our parser improves the performance of a state-ofthe-art question answering system."
U13-1001,Robust Computational Semantics,2013,0,0,1,1,748,mark steedman,Proceedings of the Australasian Language Technology Association Workshop 2013 ({ALTA} 2013),0,"Practical tasks like question answering and machine translational ultimately require computing meaning representations that support inference. Standard linguistic accounts of meaning are impracticable for such purposes, both because they assume nonmonotonic operations such as quantifier movement, and because they lack a representation for the meaning of content words that supports efficient computation of entailment. Ixe2x80x99ll discuss practical solutions to some of these problems within a near-context free grammar formalism for a working wide-coverage parser, in current work with Mike Lewis, and show how these solutions can be usefully applied in NLP tasks. Mark Steedman. 2013. Robust Computational Semantics. In Proceedings of Australasian Language Technology Association Workshop, page 2."
Q13-1015,Combined Distributional and Logical Semantics,2013,53,103,2,1,4489,mike lewis,Transactions of the Association for Computational Linguistics,0,"We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite."
P13-2107,Using {CCG} categories to improve {H}indi dependency parsing,2013,15,19,3,1,34684,bharat ambati,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery."
P13-2108,The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing,2013,36,4,2,0,37816,greg coppola,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Higher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding phrase-structure parser. We find considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material."
D13-1064,Unsupervised Induction of Cross-Lingual Semantic Relations,2013,45,20,2,1,4489,mike lewis,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Creating a language-independent meaning representation would benefit many crosslingual NLP tasks. We introduce the first unsupervised approach to this problem, learning clusters of semantically equivalent English and French relations between referring expressions, based on their named-entity arguments in large monolingual corpora. The clusters can be used as language-independent semantic relations, by mapping clustered expressions in different languages onto the same relation. Our approach needs no parallel text for training, but outperforms a baseline that uses machine translation on a cross-lingual question answering task. We also show how to use the semantics to improve the accuracy of machine translation, by using it in a simple reranker."
W12-1913,Turning the pipeline into a loop: Iterated unsupervised dependency parsing and {P}o{S} induction,2012,12,8,3,1,8555,christos christodoulopoulos,Proceedings of the {NAACL}-{HLT} Workshop on the Induction of Linguistic Structure,0,"Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009)."
W12-0903,Probabilistic Models of Grammar Acquisition,2012,5,0,1,1,748,mark steedman,Proceedings of the Workshop on Computational Models of Language Acquisition and Loss,0,"The most convincing models of human grammar acquisition to date are supervised, in the sense that they learn from pairs of strings and meaning representations (Siskind, 1996; Villavicencio, 2002; Villavicencio, 2011; Buttery, 2004; Buttery, 2006; Kwiatkowski et al., 2012). Although the principles by which such models learn are quite general, the datasets they have been applied to have unavoidably been somewhat target-language-specific, and are also limited to discourse-external world-state-related content, contrary to the observations of (Tomasello, 2001) concerning the central role of common ground and grounding in interpersonal interaction."
E12-1024,A Probabilistic Model of Syntactic and Semantic Acquisition from Child-Directed Utterances and their Meanings,2012,43,53,4,1,12529,tom kwiatkowski,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper presents an incremental probabilistic learner that models the acquistion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings. These meaning representations approximate the contextual input available to the child; they do not specify the meanings of individual words or syntactic derivations. The learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model. We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian expectation maximization. When tested on utterances from the CHILDES corpus, our learner outperforms a state-of-the-art semantic parser. In addition, it models such aspects of child acquisition as fast mapping, while also countering previous criticisms of statistical syntactic learners."
D12-1007,Generative Goal-Driven User Simulation for Dialog Management,2012,26,4,3,0,40090,aciel eshky,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let's Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs."
W11-2901,Computing Scope in a {CCG} Parser,2011,0,0,1,1,748,mark steedman,Proceedings of the 12th International Conference on Parsing Technologies,0,"Ambiguities arising from alternations of scope in interpretations for multiply quantified sentences appear to require grammatical operations that compromise the strong assumptions of syntactic/semantic transparency and monotonicity underlying the Frege-Montague approach to the theory of grammar. Examples that have been proposed include covert movement at the level of logical form, abstraction or storage mechanisms, and proliferating type-changing operations. The paper examines some interactions of scope alternation with syntactic phenomena including coordination, binding, and relativization. Starting from the assumption of Fodor and Sag, and others, that many expressions that have been treated as generalized quantifiers are in fact referential expressions, and using Combinatory Categorial Grammar (CCG) as a grammatical framework, the paper presents an account of quantifier scope ambiguities according to which the available readings are projected directly from the lexicon by the combinatorics of the syntactic derivation, without any independent manipulation of logical form and without recourse to otherwise unmotivated type-changing operations. As a direct result, scope ambiguity can be efficiently processed using packed representations from which the available readings can be simply enumerated."
W11-2916,Simple Semi-Supervised Learning for Prepositional Phrase Attachment,2011,26,3,4,0,44136,gregory coppola,Proceedings of the 12th International Conference on Parsing Technologies,0,"Prepositional phrase attachment is an important subproblem of parsing, performance on which suffers from limited availability of labelled data. We present a semi-supervised approach. We show that a discriminative lexical model trained from labelled data, and a generative lexical model learned via Expectation Maximization from unlabelled data can be combined in a product model to yield a PP-attachment model which is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a significant margin. We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM."
I11-1049,Grammar Induction from Text Using Small Syntactic Prototypes,2011,30,20,2,0,38047,prachya boonkwan,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present an efficient technique to incorporate a small number of cross-linguistic parameter settings defining default word orders to otherwise unsupervised grammar induction. A syntactic prototype, represented by the integrated model between Categorial Grammar and dependency structure, generated from the language parameters, is used to prune the search space. We also propose heuristics which prefer less complex syntactic categories to more complex ones in parse decoding. The system reduces errors generated by the state-of-the-art baselines for WSJ10 (1% error reduction of F1 score for the model trained on Sections 2xe2x80x9022 and tested on Section 23), Chinese10 (26% error reduction of F1), German10 (9% error reduction of F1), and Japanese10 (8% error reduction of F1), and is not significantly different from the baseline for Czech10."
D11-1059,A {B}ayesian Mixture Model for {P}o{S} Induction Using Multiple Features,2011,30,9,3,1,8555,christos christodoulopoulos,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested."
D11-1115,Semi-supervised {CCG} Lexicon Extension,2011,21,9,2,0,44836,emily thomforde,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"This paper introduces Chart Inference (CI), an algorithm for deriving a CCG category for an unknown word from a partial parse chart. It is shown to be faster and more precise than a baseline brute-force method, and to achieve wider coverage than a rule-based system. In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank. When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subject-extraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts."
D11-1140,Lexical Generalization in {CCG} Grammar Induction for Semantic Parsing,2011,38,149,4,1,12529,tom kwiatkowski,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring."
Y10-1058,A Multi-Dimensional Analysis of {J}apanese Benefactives: The Case of the Yaru-Construction,2010,12,1,2,1,45031,akira otani,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"This paper discusses the semantic and pragmatic properties of Japanese benefac- tives with the main focus on the yaru-construction. The benefactive sentence is judged to be acceptable if the transitive verb complement falls into a certain semantic class in which the meaning of transfer of possession is expressed. Hence, the distribution of the recipient role rather than the beneficiary role is crucial for determining the acceptability of the construction. To capture such a multi-dimensional linguistic information, HPSG account will be given."
D10-1056,Two Decades of Unsupervised {POS} Induction: How Far Have We Come?,2010,22,110,3,1,8555,christos christodoulopoulos,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on non-English corpora."
D10-1119,Inducing Probabilistic {CCG} Grammars from Logical Form with Higher-Order Unification,2010,26,153,4,0,46417,tom kwiatkowksi,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations."
Y09-1042,Note on {J}apanese Epistemic Verb Constructions: A Surface-Compositional Analysis,2009,12,0,2,1,31083,akira ohtani,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"This paper offers a new analysis of the raising to object construction in Japanese. This has been extensively discussed following Kuno (1976) for the case where the ma- trix predicate is an epistemic verb. Under CCG analysis an o-marked phrase is a surface- compositional object rather than a raised argument. This new approach correctly predicts the thetic and categorical judgments of epistemic verb constructions, which have hitherto only been accounted for by the studies which emphasize only the syntactic aspects of the construction."
D09-1085,Unbounded Dependency Recovery for Parser Evaluation,2009,27,4,3,0,8964,laura rimell,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper introduces a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions. We run a series of off-the-shelf parsers on the corpus to evaluate how well state-of-the-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers."
Y08-1029,On {J}apanese Desiderative Constructions,2008,7,0,2,1,31083,akira ohtani,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"This paper describes desiderative constructions in Japanese with the main focus on ta(i) xe2x80x98wantxe2x80x99 desideratives. In spite of the morphological one-word status, desiderative constructions have been claimed to have a complex structure at some abstract level of representation. We claim that there are two types of desideratives, and that their predicates have different lexical representations within the framework of Combinatory Categorial Grammar. Building on the proposed analysis, we also discuss the difference between the two types of desideratives in terms of adverbial modification and passivizability."
J08-1008,Last Words: On Becoming a Discipline,2008,-1,-1,1,1,748,mark steedman,Computational Linguistics,0,None
Y07-1038,"Case, Coordination, and Information Structure in {J}apanese",2007,14,1,2,1,45031,akira otani,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,None
J07-3004,{CCG}bank: A Corpus of {CCG} Derivations and Dependency Structures Extracted from the {P}enn {T}reebank,2007,87,285,2,0.744681,8351,julia hockenmaier,Computational Linguistics,0,"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-coverage statistical parsers that obtain state-of-the-art rates of dependency recovery.n n In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks."
2007.sigdial-1.47,Planning Dialog Actions,2007,32,27,1,1,748,mark steedman,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"The problem of planning dialog moves can be viewed as an instance of the more general AI problem of planning with incomplete information and sensing. Sensing actions complicate the planning process since such actions engender potentially infinite state spaces. We adapt the Linear Dynamic Event Calculus (LDEC) to the representation of dialog acts using insights from the PKS planner, and show how this formalism can be applied to the problem of planning mixedinitiative collaborative discourse."
W05-0307,A Framework for Annotating Information Structure in Discourse,2005,27,39,3,0,49326,sasha calhoun,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"We present a framework for the integrated analysis of the textual and prosodic characteristics of information structure in the Switchboard corpus of conversational English. Information structure describes the availability, organisation and salience of entities in a discourse model. We present standards for the annotation of information status (old, mediated and new), and give guidelines for annotating information structure, i.e. theme/rheme and back-ground/kontrast. We show that information structure in English can only be analysed concurrently with prosodic prominence and phrasing. This annotation, using stand-off XML in NXT, can help establish standards for the annotation of information structure in discourse."
W04-3215,Object-Extraction and Question-Parsing using {CCG},2004,17,33,2,0.423729,20968,stephen clark,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG). However, overall figures give no indication of a parserxe2x80x99s performance on specific constructions, nor how suitable a parser is for specific applications. In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text. We also show how the parser can be used to parse questions for Question Answering. The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only. Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy."
nissim-etal-2004-annotation,An Annotation Scheme for Information Status in Dialogue,2004,15,55,4,1,29,malvina nissim,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We present an annotation scheme for information status (IS) in dialogue, and validate it on three Switchboard dialogues. We show that our scheme has good reproducibility, and compare it with previous attempts to code IS and related features. We eventually apply the scheme to 147 dialogues, thus producing a corpus that contains nearly 70,000 NPs annotated for IS and over 15,000 coreference links."
C04-1180,Wide-Coverage Semantic Representations from a {CCG} Parser,2004,21,172,3,0,6245,johan bos,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
N03-1031,Example Selection for Bootstrapping Statistical Parsers,2003,19,237,1,1,748,mark steedman,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both a mostly-unsupervised approach, cotraining, in which two parsers are iteratively re-trained on each other's output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser's output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks."
E03-1008,Bootstrapping statistical parsers from small datasets,2003,14,118,1,1,748,mark steedman,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of boot-strapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that boot-strapping continues to be useful, even though no manually produced parses from the target domain are used."
P02-1042,Building Deep Dependency Structures using a Wide-Coverage {CCG} Parser,2002,15,86,3,0.423729,20968,stephen clark,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
P02-1043,Generative Models for Statistical Parsing with {C}ombinatory {C}ategorial {G}rammar,2002,14,157,2,0.744681,8351,julia hockenmaier,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies."
hockenmaier-steedman-2002-acquiring,Acquiring Compact Lexicalized Grammars from a Cleaner Treebank,2002,11,92,2,0.744681,8351,julia hockenmaier,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,Abstract We present an algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. To do this we have needed to make several systematic changes to the Treebank which have to effect of cleaning up a number of errors and inconsistencies. This process has yielded a cleaner treebank that can potentially be used in any framework. We also show how unary type-changing rules for certain types of modifiers can be introduced in a CCG grammar to ensure a compact lexicon without augmenting the generative power of the system. We demonstrate how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem.
P99-1039,Alternating Quantifier Scope in {CCG},1999,44,19,1,1,748,mark steedman,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"The paper shows that movement or equivalent computational structure-changing operations of any kind at the level of logical form can be dispensed with entirely in capturing quantifier scope ambiguity. It offers a new semantics whereby the effects of quantifier scope alternation can be obtained by an entirely monotonic derivation, without type-changing rules. The paper follows Fodor (1982), Fodor and Sag (1982), and Park (1995, 1996) in viewing many apparent scope ambiguities as arising from referential categories rather than true generalized quantifiers."
1997.iwpt-1.4,Making Use of Intonation in Interactive Dialogue Translation,1997,-1,-1,1,1,748,mark steedman,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"Intonational information is frequently discarded in speech recognition, and assigned by default heuristics in text-to-speech generation. However, in many applications involving dialogue and interactive discourse, intonation conveys significant information, and we ignore it at our peril. Translating telephones and personal assistants are an interesting test case, in which the salience of rapidly shifting discourse topics and the fact that sentences are machine-generated, rather than written by humans, combine to make the application particularly vulnerable to our poor theoretical grasp of intonation and its functions. I will discuss a number of approaches to the problem for such applications, ranging from cheap tricks to a combinatory grammar-based theory of the semantics involved and a syntax-phonology interface for building and generating from interpretations."
H94-1035,Information Based Intonation Synthesis,1994,21,4,2,1,55516,scott prevost,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"This paper presents a model for generating prosodically appropriate synthesized responses to database queries using Combinatory Categorial Grammar (CCG - cf. [22]), a formalism which easily integrates the notions of syntactic constituency, prosodic phrasing and information structure. The model determines accent locations within phrases on the basis of contrastive sets derived from the discourse structure and a domain-independent knowledge base."
H93-1113,Natural Language Research,1993,-1,-1,3,0,33217,aravind joshi,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
E93-1039,Generating Contextually Appropriate Intonation,1993,11,35,2,1,55516,scott prevost,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"One source of unnaturalness in the output of text-to-speech systems from the involvement of algorithmically generated default intonation contours, applied under minimal control from syntax and semantics. It is a tribute both to the resilience of human language understanding and to the ingenuity of the inventors of these algorithms that the results are as intelligible as they are. However, the result is very frequently unnatural, and may on occasion mislead the hearer. This paper extends earlier work on the relation between syntax and intonation in language understanding in Combinatory Categorial Grammar (CCG). A generator with a simple and domain-independent discourse model can be used to direct synthesis of intonation contours for responses to data-base queries, to convey distinctions of contrast and emphasis determined by the discourse model."
H92-1123,Natural Language Research,1992,-1,-1,3,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
P91-1010,Type-Raising and Directionality in Combinatory Grammar,1991,5,20,1,1,748,mark steedman,29th Annual Meeting of the Association for Computational Linguistics,1,"The form of rules in combinatory categorial grammars (CCG) is constrained by three principles, called adjacency, consistency and inheritance. These principles have been claimed elsewhere to constrain the combinatory rules of composition and type raising in such a way as to make certain linguistic universals concerning word order under coordination follow immediately. The present paper shows that the three principles have a natural expression in a unification-based interpretation of CCG in which directional information is an attribute of the arguments of functions grounded in string position. The universals can thereby be derived as consequences of elementary assumptions. Some desirable results for grammars and parsers follow, concerning type-raising rules."
H91-1103,Natural Language Research,1991,0,0,3,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
W90-0125,Narrated Animation: A Case for Generation,1990,5,1,2,0,25231,norman badler,Proceedings of the Fifth International Workshop on Natural Language Generation,0,"Abstract : Our project rests on the belief that computer animation in the form of narrated animated simulations can provide an engaging, effective and flexible medium for instructing agents of varying capabilities to perform tasks that make varying demands in workplaces of varying layout. To this end, we have been designing and implementing an integrated system which combines: animated agents which can demonstrate the behavior to be emulated and automatic generation of appropriate Natural Language narration which can explain what is being done and why."
P90-1002,Structure and Intonation in Spoken Language Understanding,1990,16,24,1,1,748,mark steedman,28th Annual Meeting of the Association for Computational Linguistics,1,"The structure imposed upon spoken sentences by intonation seems frequently to be orthogonal to their traditional surface-syntactic structure. However, the notion of intonational structure as formulated by Pierrehumbert, Selkirk, and others, can be subsumed under a rather different notion of syntactic surface structure that emerges from a theory of grammar based on a Combinatory extension to Categorial Grammar. Interpretations of constituents at this level are in turn directly related to information structure, or discourse-related notions of theme, rheme, focus and presupposition. Some simplifications appear to follow for the problem of integrating syntax and other high-level modules in spoken language systems."
H90-1099,Natural Language Research,1990,0,0,3,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The main objective is basic research and system development leading to (1) characterization of information carried by (a) syntax, semantics, and discourse structure, (b) their relation to information carried by intonation, and (c) development of methods for using this information for generation and understanding; (2) development of architectures for integration of utterance planning with lexical, syntactic and intonational choice; (3) development of incremental strategies for using syntactic, semantic, and pragmatic knowledge in understanding and generating language."
W89-0217,Parsing Spoken Language Using Combinatory Grammars,1989,-1,-1,1,1,748,mark steedman,Proceedings of the First International Workshop on Parsing Technologies,0,
H89-1035,Natural Language Research,1989,-1,-1,3,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,None
H89-1038,Intonation and Syntax in Spoken Language Systems,1989,16,37,1,1,748,mark steedman,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"The present paper argues that the notion of intonational structure as formulated by Pierrehumbert, Selkirk, and others, can be subsumed under the generalised notion of syntactic surface structure that emerges from a theory of grammar based on a Combinatory extension to Categorial Grammar. According to this theory, the syntactic structures and the intonation structures of English are identical, and have the same grammar. Some simplifications appear to follow for the problem of integrating syntax and other high-level modules in spoken language systems."
J88-2003,Temporal Ontology and Temporal Reference,1988,30,715,2,1,53226,marc moens,Computational Linguistics,0,"A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a nucleus. A nucleus can be thought of as an association of a goal event, or culmination, with a preparatory process by which it is accomplished, and a consequent state, which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category. We claim that any manageable formalism for natural-language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language."
P87-1001,Temporal Ontology in Natural Language,1987,19,91,2,0,53226,marc moens,25th Annual Meeting of the Association for Computational Linguistics,1,"A semantics of linguistic categories like tense, aspect, and certain temporal adverbials, and a theory of their use in defining the temporal relations of events, both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. The paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. We claim that any manageable logic or other formal system for natural language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language."
P87-1012,A Lazy way to Chart-Parse with Categorial Grammars,1987,11,55,2,0,57850,remo pareschi,25th Annual Meeting of the Association for Computational Linguistics,1,"There has recently been a revival of interest in Categorial Grammars (CG) among computational linguists. The various versions noted below which extend pure CG by including operations such as functional composition have been claimed to offer simple and uniform accounts of a wide range of natural language (NL) constructions involving bounded and unbounded movement and coordination reduction in a number of languages. Such grammars have obvious advantages for computational applications, provided that they can be parsed efficiently. However, many of the proposed extensions engender proliferating semantically equivalent surface syntactic analyses. These spurious analyses have been claimed to compromise their efficient parseability.The present paper describes a simple parsing algorithm for our own combinatory extension of CG. This algorithm offers a uniform treatment for spurious syntactic ambiguities and the genuine structural ambiguities which any processor must cope with, by exploiting the associativity of functional composition and the procedural neutrality of the combinatory rules of grammar in a bottom-up, left-to-right parser which delivers all semantically distinct analyses via a novel unification-based extension of chart-parsing."
