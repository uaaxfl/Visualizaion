2012.iwslt-evaluation.11,2011.iwslt-evaluation.1,1,0.866959,"Missing"
2012.iwslt-evaluation.11,P07-1085,0,0.0270438,"Missing"
2012.iwslt-evaluation.11,N10-1103,0,0.0548812,"Missing"
2012.iwslt-evaluation.5,2012.iwslt-evaluation.1,0,0.0315298,"D task, exploring issues such as out-of-domain data ﬁltering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology. Decoding Baseline NAIST Submission dev2010 26.02 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using"
2012.iwslt-evaluation.5,P07-2045,0,0.0162664,"2 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using the web data with ﬁltering to remove noisy sentences, phrase table smoothing, language model interpolation, and minimum Bayes risk decoding. This led to a score of 31.81 BLEU on the tst2010 data set, a"
2012.iwslt-evaluation.5,N03-1017,0,0.111297,"urposes, in Section 3, we also present additional experiments that gave negative results, which were not included in our ofﬁcial submission. For the 10 translation tasks into English, we focused on techniques that could be used widely across all languages. In particular, we experimented with unsupervised approaches to handling source-side morphology, minimum Bayes risk decoding, and large language models. In the end, most of our systems used a combination of unsupervised morpholThe NAIST English-French translation system for IWSLT 2012 was based on phrase-based statistical machine translation [3] using the Moses decoder [2] and its corresponding training regimen. Overall, we made four enhancements over the standard Moses setup to improve the translation accuracy: Large-scale Data with Filtering: In order to use the large, but noisy parallel training data in the English-French Giga Corpus, we implemented a technique to ﬁlter out noisy translated text. Phrase Table Smoothing: We performed phrase table smoothing to improve the probability estimates of low-frequency phrases. Language Model Interpolation: In order to adapt to the domain of the task, we interpolated language models trained"
2012.iwslt-evaluation.5,J05-4003,0,0.0924584,"Missing"
2012.iwslt-evaluation.5,2011.iwslt-evaluation.9,0,0.0502184,"Missing"
2012.iwslt-evaluation.5,J93-2003,0,0.0294894,"Missing"
2012.iwslt-evaluation.5,W99-0604,0,0.275234,"Missing"
2012.iwslt-evaluation.5,P02-1038,0,0.0218946,"s comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by [14], who deﬁned MBR over lattices. We tested both of these approaches (as implemented in the Moses decoder). Finally, one ﬁne point about MBR is that it requires a good estimate of the probability P (E  |F ) of hypotheses. In the discriminative training framework of [15], which is used in most modern SMT systems, scores of machine translation hypotheses are generally deﬁned as a log-linear combination of feature functions such as language model or translation model probabilities P (E  |F ) = 2.3. Language Model Interpolation One of the characteristics of the IWSLT TED task is that, as shown in Table 2, we have several heterogeneous corpora. In addition, the in-domain TED data is relatively small, so it can be expected that we will beneﬁt from using data outside of the TED domain. In order to effectively utilize out-ofdomain data in language modeling, we buil"
2012.iwslt-evaluation.5,P03-1021,0,0.0174134,"ns. 1 i wi φi (E  ,F ) e Z (4) where φi indicates feature functions such as the language model, translation model, and reordering model log probabilities, wi is the weight measuring the relative importance of this feature, and Z is a partition function that ensures that the probabilities add to 1. Choosing the weights wi for each feature such that the answer with highest probability ˆ = argmax P (E|F ) E E (5) is the best possible translation is a process called “tuning,” and essential to modern SMT systems. However, in most tuning methods, including the standard minimum error rate training [16] that was used in the proposed system, while the relative weight of each feature wi is adjusted, the overall sum of the weights i wi is generally set ﬁxed at 1. While this is not a problem when ﬁnding the highest probability hypothesis in 5, it will affect the probability estimates P (E  |F ), with 56 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Decoding Viterbi MBR (λ = 1) Lattice MBR (λ = 1) Lattice MBR (λ = 5) dev2010 27.59 27.29 26.70 27.05 tst2010 31.01 31.24 31.25 31.81 Table 6: BLEU Results using Minimum Bayes Risk decoding. larger s"
2012.iwslt-evaluation.5,2010.iwslt-papers.5,1,0.897262,"Missing"
2012.iwslt-evaluation.5,W06-1607,0,0.01659,"pothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by ["
2012.iwslt-evaluation.5,P12-1018,1,0.830308,"ch TED task. 3.2. Word Alignment & Phrase Table Combination We investigated different alignment tools and ways to combine them, as shown in Table 8. Observations are as follows: • GIZA++ and BerkeleyAligner achieve similar BLEU on this task. • Concatenating GIZA++ and BerkeleyAligner word alignment results, prior to phrase extraction, achieves a small boost (29.57 to 29.89 BLEU). We experimented with the simplest approach to exploiting out-of-domain bitext in translation models: data concatenation. This can be seen as adaptation at the earliest stage of the • We also experimented with pilaign [19], a Bayesian phrasal alignment toolkit. This tool directly extracts phrases without resorting to the preliminary step of word alignments, and achieves extremely compact phrase table sizes (0.8M entries) without signiﬁcantly sacriﬁcing BLEU (29.24). 3 It should be noted that due to constraints in the available data for these MBR experiments we are both tuning on testing on tst2010, but the tuning of λ also demonstrated gains in accuracy on the ofﬁcial blind test on tst2011 and tst2012 (37.33→37.90 and 38.92→39.47 respectively). • Combining the GIZA++ and pialign phrase tables by Moses’ multiple"
2012.iwslt-evaluation.5,D11-1125,0,0.0529106,", the number of random restarts was set to 20. 3.3. Lexical Reordering Models Several reordering models available in the Moses decoder were tried. In general, we found the full “msd-bidir-fe” option to perform best, despite the small number of word order differences between English and French. Results are shown in Table 9. Reordering model msd-bidir-fe msd-bidir-f monotonicity-bidir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated t"
2012.iwslt-evaluation.5,C08-1074,0,0.0166288,"ir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated the weights with previously learned weights to improve the stability (henceforth “PRO-interpolated”)6 , and 4 Currently, Moses’s default setting is 20. ˜hal/megam/ 6 We set the same interpolation coefﬁcient value of 0.1 as [20] noted. 5 http://www.cs.utah.edu/ the version that do not use such a interpolation (henceforth “PRO-basic”). We ﬁrst investigate the effect of the number of"
2012.iwslt-evaluation.5,N04-1022,0,0.0390465,"ing. LM TED Only Without Interp. With Interp. dev2010 24.80 26.30 27.05 tst2010 29.44 31.15 31.81 Table 5: Results training the language model on only TED data, and when other data is used without and with language model interpolation. results are shown in Table 3. As a result, we can see that using the data from the Giga corpus has a positive effect on the results, but ﬁltering does not have a clear signiﬁcant effect on the results. 2.4. Minimum Bayes Risk Decoding Finally, we experimented with improved decoding strategies for translation, particularly using minimum Bayes risk decoding (MBR, [12]). In normal translation, the decoder attempts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we"
2012.iwslt-evaluation.5,C04-1072,0,0.0264753,"empts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smooth"
2012.iwslt-evaluation.5,E03-1076,0,0.149639,"e baseline results. First, adding additional out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt mo"
2012.iwslt-evaluation.5,W02-0603,0,0.0311216,"onal out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt most from CompoundSplit, while Arabic, Rus"
2012.iwslt-evaluation.5,D08-1065,0,\N,Missing
2012.iwslt-papers.2,2011.mtsummit-papers.7,0,0.0422007,"etween duration translation and duration and power translation. Particularly, the former method was often labeled with a score of 2 indicating that the duration is not sufﬁcient to represent emphasis clearly. However, duration+power almost always scored 3 and can be recognized as the position of emphasis. This means that in English-Japanese speech translation, speech’s power is an important factor to convey emphasis. 5. Related Works There have been several studies demonstrating improved speech translation performance by utilizing paralinguistic information of source side speech. For example, [8] focuses on using the input speech’s acoustic information to improve translation accuracy. They try to explore a tight coupling of ASR and MT for speech translation, sharing information on the phone level to boost translation accuracy as measured by BLEU score. Other related works focus on using speech intonation to reduce translation ambiguity on the target side [9, 10]. While the above methods consider paralinguistic information to boost translation accuracy, as we mentioned before, there is more to speech translation than just the accuracy of the target sentence. It is also necessary to con"
2012.iwslt-papers.2,P07-2045,0,0.00260194,"vely, and speech synthesis with TTS. While this is the same general architecture as traditional speech translation systems, we add an additional model to translate not only lexical information but also two types of paralinguistic information: duration and power. In this paper, in order to focus speciﬁcally on paralinguistic translation we chose a simple, small-vocabulary lexical MT task: number-to-number translation. (6) where J indicates the target language sentence and E indicates the recognized source language sentence. Generally we can use a statistical machine translation tool like Moses [4], to obtain this translation in standard translation tasks. However in this paper we have chosen a simple number-tonumber translation task so we can simply write one-to-one lexical translation rules with no loss in accuracy. 3.3. Paralinguistic Translation Paralinguistic translation converts the source-side duration and mean power vector X into the target-side duration and mean power vector Y according to the following equation 159 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 ˆ = arg max P (Y|X). Y Y (7) training data by minimize root mean squ"
2013.iwslt-evaluation.23,N10-1103,0,0.0377182,"Missing"
2013.iwslt-papers.3,D10-1092,0,0.0188586,"nslators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the relationship betwe"
2013.iwslt-papers.3,2011.iwslt-evaluation.18,0,0.0461329,"Missing"
2013.iwslt-papers.3,P11-2093,1,0.744327,"Missing"
2013.iwslt-papers.3,N12-1048,0,0.119056,"hen looking at the source and the translation, the word order is quite different, reversing two long clauses: A and B. In contrast, when looking at the source and the simultaneous interpretation, the word order is similar. If a simultaneous ST system attempts to reproduce the first word order, it will only be able to start translation after it has received the full “A because B.” On the other hand, if the system is able to choose the word order closer to human interpreters, it can begin translation after “A,” resulting in a lower delay. There are several related works about simultaneous ST [2][3][4] that automatically divide longer sentences up into a number of shorter ones similarly to the salami technique employed by simultaneous interpreters. While these related works aim to segment sentences in a similar fashion to simultaneous interpreters, all previous works concerned with sentence segmentation have used translation data (made by translators) for learning of the machine translation system. In addition, while there are other related works about collecting simultaneous interpretation data [5][6][7], all previous works did not compare simultaneous interpreters of multiple experienc"
2013.iwslt-papers.3,shimizu-etal-2014-collection,1,0.823939,"ranslation results closer to a highly experienced simultaneous interpreter than when translation data alone is used in training. We also find that according to automatic evaluation metrics, our system achieves performance similar to that of a simultaneous interpreter that has 1 year of experience. 2. Simultaneous interpretation data As the first step to performing our research, we first must collect simultaneous interpretation data. In this section, we describe how we did so with the cooperation of professional simultaneous interpreters. A fuller description of the corpus will be published in [8]. 2.1. Materials As materials for the simultaneous interpreters to translate, we used TED1 talks, and had the interpreters translate in real time from English to Japanese while watching and listening to the TED videos. We have several reasons for using TED talks. The first is that for many of the TED talks there are already Japanese subtitles available. This makes it possible to compare data created by translators (i.e. the subtitles) with simultaneous interpretation data. TED is also an attractive testbed for machine translation systems, as it covers a wide variety of topics of interest to a"
2013.iwslt-papers.3,P02-1040,0,0.0899235,"reters and translators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the rel"
2013.iwslt-papers.3,ma-2006-champollion,0,0.0318601,"Missing"
2013.iwslt-papers.3,P07-2045,0,0.0252123,"Missing"
2013.iwslt-papers.3,J03-1002,0,0.00584698,"Missing"
2013.iwslt-papers.3,P03-1021,0,0.0154201,"Missing"
2013.iwslt-papers.8,stuker-etal-2012-kit,1,0.825719,"where new data for retraining comes from the same speaker, channel and related conversation topics. Following the implications of [8] we add low confidence score data to the training, but unlike in other work we apply wordbased weighting in order to compensate for errors, as it was done by [9] for acoustic model adaptation. The assumption is that erroneous data is helpful to improve system generalization. Unlike other work, e.g. [10], we refrained from a lattice-based approach. 2. Data The experiments in this paper were conducted with the help of the KIT Lecture Corpus for Speech Translation [11]. The corpus consists of recorded scientific lectures that were held at the Karlsruhe Institute of Technology (KIT). Currently the corpus mainly contains computer science lectures, and a small amount of lectures from other departments and ceremonial talks. 2.1. Training Data The speaker-independent system that we used in our experiments was trained on about 94 hours of speech from the lecture corpus. Our experiments were constrained to two distinct speakers. As training data we had 7.4 hours for speaker A and 8.3 hours for speaker B respectively, which had not been used for training the speake"
2015.iwslt-evaluation.17,H92-1073,0,\N,Missing
2015.iwslt-evaluation.17,rousseau-etal-2014-enhancing,0,\N,Missing
2020.lrec-1.62,N19-1423,0,0.00517172,"xpressiveness of the emotion by the system. We recorded speeches from a student of a voice actor school, who is training to be a professional voice actor. It is challenging to record the whole of the collected system’s responses (7,356 responses); thus, we ranked each dialogue context. We used BERT to convert a dialogue context to a vector in a latent space and used K-means clustering to select points in the latent space. We selected a variety of dialogue contexts for building a robust dialogue system. 3.1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT (Devlin et al., 2019) is a language representation model, which is trained from large-scale text corpus with its transformer architecture (Vaswani et al., 2017). The pretrained model of BERT is trained to predict masked next or previous sentence in language modeling task for making representation of sentences. It is reported that the model well represents a sentence meaning and outperformed existing word or sentence representation methods in several language understanding benchmark tasks. We used a pretrained model that is trained from Japanese texts on social network services (Sakaki et al., 2019). We input a sen"
2020.lrec-1.62,W13-4016,0,0.0199182,"combinations of broad dialogue context and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system’s emotion to users. Keywords: dialogue corpus, emotional expression, speech corpus, persuasive dialogue 1. Introduction Persuasion or negotiation is an important dialogue style, which has been widely researched recently (Mazzotta et al., 2007; Georgila, 2013; Hiraoka et al., 2016; Wang et al., 2019). Emotional expressions have an important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogu"
2020.lrec-1.62,P17-1059,0,0.0664608,"Missing"
2020.lrec-1.62,P17-2017,0,0.0125703,"ollected in this work, which have the same meaning as the original “target response” in different emotional expressions. The original corpus was collected in Japanese; thus, the English is a translation. sultant utterances that are classified as “target responses” is 1,839, including 774 neutral, 320 anger, 392 sadness, and 353 happy labeled utterances. We used crowd-sourcing to collect response variations in different emotions. Crowdsourcing is a widely used approach for collecting paraphrasing expressions in existing works (Burrows et al., 2013) to cover lexical divergence (Xu et al., 2014; Jiang et al., 2017). In this work, we focus on collecting emotional variations of system utterances. We showed the dialogue context, target response, and a new emotion label, and requested the crowd-sourcing participants to paraphrase the target response with the given new emotion label. In the example in Table 2, the dialogue context and target response are “system-1”, “user-1”, and “system-2”, and the target emotion label is one of the emotions except “neutral”: “angry”, “sad”, and “happy”. During the annotation, we showed the participants a figure of Russell ’s circumplex model (Figure 1) and the following in"
2020.lrec-1.62,P19-1566,0,0.0284738,"and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system’s emotion to users. Keywords: dialogue corpus, emotional expression, speech corpus, persuasive dialogue 1. Introduction Persuasion or negotiation is an important dialogue style, which has been widely researched recently (Mazzotta et al., 2007; Georgila, 2013; Hiraoka et al., 2016; Wang et al., 2019). Emotional expressions have an important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogue systems that can use emotional expressio"
2020.lrec-1.62,Q14-1034,0,0.0220673,"onse variations collected in this work, which have the same meaning as the original “target response” in different emotional expressions. The original corpus was collected in Japanese; thus, the English is a translation. sultant utterances that are classified as “target responses” is 1,839, including 774 neutral, 320 anger, 392 sadness, and 353 happy labeled utterances. We used crowd-sourcing to collect response variations in different emotions. Crowdsourcing is a widely used approach for collecting paraphrasing expressions in existing works (Burrows et al., 2013) to cover lexical divergence (Xu et al., 2014; Jiang et al., 2017). In this work, we focus on collecting emotional variations of system utterances. We showed the dialogue context, target response, and a new emotion label, and requested the crowd-sourcing participants to paraphrase the target response with the given new emotion label. In the example in Table 2, the dialogue context and target response are “system-1”, “user-1”, and “system-2”, and the target emotion label is one of the emotions except “neutral”: “angry”, “sad”, and “happy”. During the annotation, we showed the participants a figure of Russell ’s circumplex model (Figure 1)"
2020.lrec-1.62,L18-1194,1,0.88301,"important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogue systems that can use emotional expressions to improve its success rate (Yoshino et al., 2018). When the system uses emotional expression in a dialogue, it is important to correctly express the emotion that the system intended. Expressing actual emotion to the users with only textual information is sometimes difficult because textual information has limited expressiveness. In contrast, emotional speech or gesture has the potential to improve the expressiveness for expressing the intended emotional state to the user. In this paper, we collected emotional expressions for the persuasive scenario and recorded their audio by expressing the emotional state to be indicated to the dialogue par"
2020.sltu-1.18,I08-3018,0,0.083296,"Missing"
2020.sltu-1.18,H92-1073,0,0.831266,"Missing"
2021.iwslt-1.3,P19-1126,0,0.0284687,"Missing"
2021.iwslt-1.3,N12-1048,0,0.010074,"pon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automa"
2021.iwslt-1.3,N19-1423,0,0.0161391,"が 起き て も 何 が 起き て も 何 が 起き て も 何 が 起き て 人間 は 何 が 間違っ て いる の か を 考える の が 得意 です 新しい こと を 試し て み て も いい です よ ね 昇給 を 求める という よう な 何 か 新しい こと を 試みよ う という とき 人 は どう まずい こと に なり 得る か 考える こと に 長け て い ます Table 3: Translation examples by wait-k baseline and wait-k with chunk shuffling (pr = 0.02). System wait-10 + CShuflow wait-20 + SKDmedium wait-30high BLEU 14.41 16.20 16.19 train 2,762,408 AL 7.21 11.54 13.83 translation for partially-observed input, using a multi-label classifier based on linear SVMs (Fan et al., 2008). Motivated by this study, we used a neural network-based classifier using BERT (Devlin et al., 2019) for NCLP. The problem of NCLP is defined as the label prediction of a syntactic constituent coming next to a given word subsequence in the pre-order tree traversal. In this work, we used 1-lookahead prediction, so the problem was relaxed into the prediction of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the"
2021.iwslt-1.3,2020.emnlp-demos.19,0,0.0245443,"mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sentences from IWSLT 2017. During fine-tuning, usually generates literal translati"
2021.iwslt-1.3,J93-2004,0,0.0770268,"on of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime"
2021.iwslt-1.3,E17-1099,0,0.0202073,"refix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automatic text-to-text simultaneous translation system for this shared task. We applied some extensions to a standard wait-k NMT i"
2021.iwslt-1.3,2014.iwslt-papers.16,0,0.0132413,". A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime (wait-20 + SKD) worked relatively well; it achived a comparable BLEU result with wait-30. However"
2021.iwslt-1.3,P15-1020,1,0.837341,"tempt: Incremental Next Constituent Label Prediction We tried another technique described below in the shared task, but it was not included in our primary submission because it did not outperform the baseline. Here, we also describe this for further investigation in future. For simultaneous machine translation, deciding how long to wait for input before translation is important. Predicting what kind of phrase comes next is a part of useful information in determining the timing. In this study, we tried incremental Next Constituent Label Prediction (NCLP). In SMT-based simultaneous translation, Oda et al. (2015) proposed a method to predict unseen syntactic constituents to determine when to start 42 natural than baseline like these examples. However, many sentences are not informative and missing details compared to the baseline. We’ll investigate a more effective way to use NCLP in our future work. 18 16 wait-k wait-k+NCLP BLEU 14 32 12 k=10 56 52 40 60 64 48 28 44 7 In this paper, we described our English-to-Japanese text-to-text simultaneous translation system. We extended the baseline wait-k with the knowledge distillation to encourage literal translation and targetside chunk shuffling to relax t"
2021.iwslt-1.3,D16-1139,0,0.0183701,"o the length of T . Then, we choose to shuffle or keep the chunks in T¯ with a probability pr , defined as a hyperparameter. We tried only the random shuffling with the fixed chunk size of k in this time; More linguistically-motivated chunk reordering would be worth trying as future work. q(y = k|x; θT ) × k=1 log p(y = k|x; θ) (3) where θT parameterizes the teacher distribution. Sequence-level Knowledge Distillation (SKD), which gives the student model the output of the teacher model as knowledge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X"
2021.iwslt-1.3,N19-4009,0,0.0238387,"ge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sent"
2021.iwslt-1.3,P06-2088,0,0.0507834,"slate X to Y incrementally. In other words, each output prediction of Y is made upon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due"
2021.iwslt-1.3,P16-1162,0,0.0452263,"IWSLT 2017. During fine-tuning, usually generates literal translations. Here, we we examined the effectiveness of knowledge distilfirstly train a teacher, Transformer-based offline lation and chunk shuffling with several hyperparamNMT model using the training corpus and use it to eter settings and reported the results by the models obtain pseudo-reference translations in the target that resulted in the higher BLEU on IWSLT 2021 language. Then, we train a student, Transformer- development set. The text was preprocessed by based simultaneous NMT model using the pseudo- Byte Pair Encoding (BPE) (Sennrich et al., 2016) parallel corpus with the original source language 1 https://github.com/pytorch/fairseq/ sentences and the corresponding translation re- blob/master/examples/simultaneous_ sults by the teacher model. The pseudo-references translation/docs/enja-waitk.md 40 BLEU 16.8 AL - 18 16 18 11.8 14.69 15.57 7.27 11.47 13.7 14 BLEU System offline Baseline wait-10 wait-20 wait-30high Proposed wait-10 + CShuflow wait-10 + SKD wait-20 + SKDmedium wait-30 + SKD 16 12 12 k=10 12 18 14 14 20 24 22 26 28 30 32 28 30 24 22 32 26 16 10 13.77 13.5 15.22 15.21 7.29 7.28 11.48 13.71 wait-k wait-k+SKD 8 6 6 8 10 12 14"
C14-1106,D10-1092,0,0.0603452,"big, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram features. 2 3 http://code.google.com/p/nile/ http://code.google.com/p/egret-parser/ 1128 System pbmt hiero f2s BLEU(dev) Original LM applied 0"
C14-1106,P07-2045,0,0.0192459,"effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as descri"
C14-1106,C04-1072,0,0.0264719,"2 Discriminative Language Models In this section, we first introduce the discriminative LM used in our method. As a target for our analysis, ˆ1, . . . , E ˆ K } of an MT system, and we have input sentences F = {F1 , . . . , FK }, n-best outputs Eˆ = {E reference translations R = {R1 , . . . , RK }. Discriminative LMs define feature vectors ϕ(Ei ) for each ˆ k = {E1 , E2 , . . . , EI }, and calculate inner products w · ϕ(Ei ) as scores. candidate in E To train the weight vector w, we first calculate evaluation scores of all candidates using a sentencelevel evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence Rk . 1125 We choose the sentence with the highest evaluation EV as an oracle Ek∗ . Oracles are chosen for each n-best, and we train w so that the oracle’s score becomes higher than the other candidates. 2.1 Structured Perceptron While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007) in using the structured perceptron as a simple and effective method for LM training. The structured perceptron is a widely used on-line learning method that examines one training instance and updates the weight vector using the difference bet"
C14-1106,P13-4016,1,0.814008,"tion systems, and choose representative n-grams using the proposed method. Then we examine the selected n-grams in context and then compare the result of this analysis. 4 Experiments We evaluate the effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et"
C14-1106,J03-1002,0,0.00792232,"Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-b"
C14-1106,P03-1021,0,0.0624679,", 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We us"
C14-1106,P02-1040,0,0.091993,"he size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram fea"
C14-1106,J11-4002,0,0.124693,"analyze a large number of translations to get an overall grasp of the system’s error trends. In addition, many sentences will contain no errors, or only errors from the long tail that are not representative of the system as a whole. On the other hand, if we are able to detect and rank important errors automatically, we will likely be able to find representative errors of the SMT system more efficiently. Previous work has proposed methods for automatic error analysis of MT systems based on automatically separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis is to identify errors to make some concrete improvement to the system, it is often necessary to perform a more focused analysis, looking at actual errors made by a particular language pair or system. We show examples of errors types that are informative, but are language- or task-specific, and not covered by previous methods in Figure 1. In this example, the type given by more standard error typologie"
C14-1106,P09-1054,0,0.0258248,"ge or we reach a fixed iteration limit N . We show the above procedure in Algorithm 1. Algorithm 1 Structured perceptron training of the discriminative LM for n = 1 to N do ˆ ∈ Eˆ do for all E ∗ E ← arg max EV (E) ˆ E∈E ˆ ← arg max w · ϕ(E) E ˆ E∈E ˆ w ← w + ϕ(E ∗ ) − ϕ(E) end for end for 2.2 Learning Sparse Discriminative LMs While the structured perceptron is a simple and effective method for learning discriminative LMs, it also has no bias towards reducing the number of features used in the model. However, if we add a bias towards learning smaller models, we can keep only salient features (Tsuruoka et al., 2009). In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro∑ portional to the L1 norm ∥w∥1 = i |wi |, pushing a large number of elements in w to 0, so ineffective features are removed from the model. To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algorithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calculates the regularization upon using the weight to improve efficiency. 2.3 Features of Discriminative LMs In the LM, we used the following three features: 1. System sc"
C14-1106,vilar-etal-2006-error,0,0.0742926,"Missing"
C14-1161,W13-4016,0,0.324973,"f framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing. 1 Introduction With the basic technology supporting dialogue systems maturing, there has been more interest in recent years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation (Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user and system goals. For these types of systems, creating a system policy that both has persuasive power and is able to ensure that the user is satisfied is the key to the system’s success. In recent years, reinforcement learning has gained much attention in the dialogue research community as an approach for automatically learning optimal d"
C14-1161,C10-1086,0,0.0268802,"a et al., 2014). In this paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and"
C14-1161,W12-1611,0,0.0952795,"his paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and a grocer are assumed"
C14-1161,W12-1639,0,0.024996,"dialogue acts is added at each turn. 3 Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we introduce this variable to the user simulator. 4 We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However, the convergence of the learning was much longer, and the performance was relatively bad. 5 Originally, there are more dialogue features for the predictive model. However as in previous research, we choose significant dialogue features by step-wise feature selection (Terrell and Bilge, 2012). 1710 Table 4: Features for calculating reward. These features are also used as the system belief state. Satuser P Ssys N Table 5: System framing. Pos represents positive framing and Neg represents negative framing. A, B, C, D, E represent camera names. Pos A Neg A Frequency of system commisive Frequency of system question Total time Calt (for each 6 cameras) Salt (for each 6 cameras) System and user current GPF System and user previous GPF System framing Pos B Neg B Pos C Neg C Pos D Neg D Pos E Neg E None Table 6: System action. <None, ReleaseTurn&gt; <Pos A, Inform&gt; <Neg A, Inform&gt; <Pos B, An"
E14-4025,strapparava-valitutti-2004-wordnet,0,0.156175,"s that answer this very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational"
E14-4025,N10-1119,0,0.0291115,"mally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happiness sadness anger fear sur"
E14-4025,esuli-sebastiani-2006-sentiwordnet,0,0.0164046,"s very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happine"
E14-4025,P11-1038,0,0.0114375,"his section, we describe an experimental evaluation of the accuracy of automatic extraction of emotion-provoking events. 5.1 Experimental Setup We use Twitter3 as a source of data, as it is it provides a massive amount of information, and also because users tend to write about what they are doing as well as their thoughts, feelings and emotions. We use a data set that contains more than 30M English tweets posted during the course of six weeks in June and July of 2012. To remove noise, we perform a variety of preprocessing, removing emoticons and tags, normalizing using the scripts provided by Han and Baldwin (2011), and Han et al. (2012). CoreNLP4 was used to get the information about part-of-speech, syntactic parses, and lemmas. We prepared four systems for comparison. As a baseline, we use a method that only uses the original seed pattern mentioned in Section 3 to acquire emotion-provoking events. We also evaluate expansions to this method with clustering, with pattern expansion, and with both. We set a 10 iteration limit on the Espresso algorithm and after each iteration, we add the 20 3 2 http://www.twitter.com http://nlp.stanford.edu/software/ corenlp.shtml In the current work we did not allow anno"
E14-4025,D12-1039,0,0.0389339,"Missing"
E14-4025,P06-1015,0,0.0314114,"e shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously mentioned restriction of one verb phrase and one noun phrase"
E14-4025,P02-1006,0,0.0415296,"ly, for each emotion we extract all the events that are shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously men"
E14-4025,W03-0404,0,0.109919,"Missing"
E14-4025,C08-1111,0,0.219614,"ecognition. In this paper, we describe work on creating prevalence-ranked dictionaries of emotionprovoking events through both manual labor and automatic information extraction. To create a manual dictionary of events, we perform a survey asking 30 participants to describe events that caused them to feel a particular emotion, and manually cleaned and aggregated the results into a ranked list. Next, we propose several methods for extracting events automatically from large data from the Web, which will allow us to increase the coverage over the smaller manually created dictionary. We start with Tokuhisa et al. (2008)’s patterns as a baseline, and examine methods for improving precision and coverage through the use of seed expansion and clustering. Finally, we discuss evaluation measures for the proposed task, and perform an evaluation of the automatically extracted emotion-provoking events. The acquired events will be provided publicly upon acceptance of the paper. This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of"
I17-1044,P07-2045,0,0.0041814,"015) Proposed Att Enc-Dec + Unconst (exp) 40.9 (2σ = 4) Att Enc-Dec + Unconst (exp) 41.8 (2σ = 6) generation process usually follows the source sentence structure without many reordering process. 7.1 Dataset We used BTEC dataset (Kikui et al., 2003) and chose English-to-France and Indonesian-toEnglish parallel corpus. From BTEC dataset, we extracted 162318 sentences for training and 510 sentences for test data. Because there are no default development set, we randomly sampled 1000 sentences from training data for validation set. For all language pairs, we preprocessed our dataset using Moses (Koehn et al., 2007) tokenizer. For training, we replaced any word that appear less then twice with unknown (unk) symbol. In details, we keep 10105 words for French corpus, 8265 words for English corpus and 9577 words for Indonesian corpus. We only used sentence pairs where the source is no longer than 60 words in training phase. 7.2 7.3 Result Discussion Table 3 summarizes our experiment on proposed local attention models compared to baseline global attention model and local-m attention model (Luong et al., 2015). Generally, local monotonic attention had better result compared to global attention on both English"
I17-1044,D15-1166,0,0.143094,"chieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture. 1 Introduction End-to-end training is a newly emerging approach to sequence-to-sequence mapping tasks, that allows the model to directly learn the mapping between variable-length representation of different 431 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 431–440, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP used today has a “global” property (Bahdanau et al., 2014; Luong et al., 2015). Every time the decoder needs to predict the output given the previous output, it must compute a weighted summarization of the whole input sequence generated by the encoder states. This global property allows the decoder to address any parts of the source sequence at each step of the output generation and provides advantages in some cases like machine translation tasks. Specifically, when the source and the target languages have different sentence structures and the last part of the target sequence may depend on the first part of the source sequence. However, although the global attention mec"
I17-1044,D14-1179,0,0.0296084,"Missing"
I17-1044,1983.tc-1.13,0,0.568509,"Missing"
L18-1194,P12-3007,0,0.0305924,"ogue systems investigated efficient dialogue strategy on dialogue management to persuade users. Hiraoka et al., (2016) introduced actions of framing and logical explanations of advantages and disadvantages of products for The major problem of implementing a dialogue system is data, in any domains or tasks of systems, because most methods of dialogue modeling are based on statistical methods that require large-scale data-sets. However, collecting new dialogue data in accordance with a defined new task is costly. Some approaches enable easy data collection in a new domain by utilizing Web data (Banchs and Li, 2012) or by extracting dialogue parts from chat-like conversations (Nio et al., 2014). However, collecting large-scale dialogue data that contain emotional expressions are still difficult. Emotional expressions tend to be observed in communications between people who have close relationships. However, it is hard to record dialogues in such closed situations. It is also difficult to extract such conversations from Web because expressing emotions in public space is somewhat suppressed. Crowdsourcing has attracted attention as an efficient way for collecting or expanding dialogue data (Yu et al., 2016"
L18-1194,W04-3230,0,0.0318912,"Vector Regression (SVR). SVR is an expansion of Support Vector Machine (SVM) for the re#annotations Fleiss’ Kappa Mean square error emotion 22,008 0.411 — acceptance 10,980 0.370 0.850 gression problem. SVR has high generalizing capability because the learning of SVR minimizes the upper bound of generalization error. We used the corpus that is annotated with the degree of user’s acceptance as described in Section 3.4.. To make feature vectors from the user’s utterances for the regression, we extracted words as linguistic features from the user utterance by using morphological analyzer Mecab (Kudo et al., 2004). Synonyms of words in the user utterance are extracted by using WordNet (Bond et al., 2012) to extend the word feature vector. We also used positive/negative score by using pre-defined dictionary of positive/negative words (Takamura et al., 2005). Each extracted vector is concatenated as a single vector to be used as the input of SVR. The regression learns the annotated degree of user’s acceptance for each utterance. The system calculates the cosine similarity cos(ut , qj ) for each pair of the user utterance ut and a user-query in the 1 example database qj (Figure 2- ). The example database"
L18-1194,P05-1017,0,0.08217,"cause the learning of SVR minimizes the upper bound of generalization error. We used the corpus that is annotated with the degree of user’s acceptance as described in Section 3.4.. To make feature vectors from the user’s utterances for the regression, we extracted words as linguistic features from the user utterance by using morphological analyzer Mecab (Kudo et al., 2004). Synonyms of words in the user utterance are extracted by using WordNet (Bond et al., 2012) to extend the word feature vector. We also used positive/negative score by using pre-defined dictionary of positive/negative words (Takamura et al., 2005). Each extracted vector is concatenated as a single vector to be used as the input of SVR. The regression learns the annotated degree of user’s acceptance for each utterance. The system calculates the cosine similarity cos(ut , qj ) for each pair of the user utterance ut and a user-query in the 1 example database qj (Figure 2- ). The example database consists of pairs of a user query qj and its response rj with annotations of the user’s acceptance of the query bj and the emotional state of the response ej . The database consist of query-response pairs that are extracted from the collected corp"
L18-1468,W16-3210,0,0.066685,"Missing"
L18-1468,O04-3001,0,0.0630407,"based only on monolingual transcription. In multilingual corpora, the ATR basic travel expression corpus (BTEC) has served as the primary source for developing broad-coverage speech translation systems (Kikui et al., 2006). Its sentences were collected by bilingual travel experts from Japanese/English sentence pairs in travel domain phrasebooks. The ATR-BTEC has been translated into 18 languages, including French, German, Italian, Chinese, Korean, and Indonesian. Each language is comprised of 160,000 sentences. This corpus contains only text-based data. The Formosa Speech Database (Formosa) (Lyu et al., 2004), a multilingual corpus for TaiwaneseHakka-Mandarin, was created by recording 49 hours of speech. Its corpus construction project took over one year to collect recordings from thousands of speakers. The constructed corpus consists of speech and text data. Recently, the Multi30K Database (Elliott et al., 2016), which is from Multilingual English-German Image Descriptions, was created for a WMT Shared Task of Multimodal Machine Translation. It is based on the Flickr30K Entities dataset (Plummer et al., 2015) that was selected and manually translated into German and French by human translators. H"
N15-3009,J93-2004,0,0.0526534,"e of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existing tools produce a certain number of failed parses when run on large data sets. In this paper, we introduce Ckylark, a new PCFG-LA parser specifically designed for robustness. Specifically, Ckylark"
N15-3009,P05-1010,0,0.0533528,"ortant from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our"
N15-3009,P08-1023,0,0.0266681,"id underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser to train a PCFG-LA grammar with the Penn Treebank WSJ dataset section 2 to 22 (WSJ-train/dev). Egret and Ckylark can use the same model as the Berkeley Parser so we can evaluate only the performance of the parsers using the same grammar. Each parser is run on a Debian 7.1 machine with an Intel Core i7 CPU (3.40GHz, 4 cores, 8MB caches) and 4GB RAM. We chose 2 datasets to evaluate the performances of each parser. First, WSJ"
N15-3009,P14-2024,1,0.692936,"failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1 1 Introduction Parsing accuracy is important. Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment (Yuret et al., 2010) and machine translation (Neubig and Duh, 2014), and most work on parsing evaluates accuracy to some extent. However, one element that is equally, or perhaps even more, important from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among"
N15-3009,N07-1051,0,0.0743748,",Z (1) where X is any pre-terminal (part-of-speech) symbol in the grammar, w is any word, and wunk is the unknown word. λ is an interpolation factor between w and wunk , and should be small enough to cause no effect when the parser can generate the result without interpolation. Our implementation uses λ = 10−10 . 3.3 Probability Scaling To solve the problem of underflow, we modify model probabilities as Equations (2) to (4) to avoid underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser"
N15-3009,P06-1055,0,0.47607,"downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existin"
N15-3009,S10-1009,0,0.0488826,"Missing"
P14-2090,N03-1033,0,0.00952551,"introduce regularization into the Greedy+DP algorithm, with the evaluation function ω rewrit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy sear"
P14-2090,2012.eamt-1.60,0,0.00455234,"completely described in Algorithms 1 and 2. However, these algorithms require a large amount of computation and simple implementations of them are too slow to finish in realistic time. Because the heaviest parts of the algorithm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proporti"
P14-2090,2013.iwslt-evaluation.1,0,0.0607953,"Missing"
P14-2090,P13-2119,1,0.641725,"hm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proportional to the delay experienced during simultaneous speech translation (Rangarajan Sridhar et al., 2013) and thus a smaller value is desirable. RP, Greedy, and Greedy+DP methods have multiple results in these graphs because t"
P14-2090,D10-1092,0,0.0248496,"segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by POS bigrams. Punct-Predict is the method using predicted positions of punctuation (Rangarajan Sridhar et al., 2013). RP is the method using right probability (Fujita et al., 2013). ten as belo"
P14-2090,P07-2045,0,0.00281304,"j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by P"
P14-2090,C04-1072,0,0.0577816,"n this work, we define ω as the sum of the evaluation measure for each parallel sentence pair ⟨fj , ej ⟩: ω(S) := N ∑ EV (M T (fj , S), ej ), (3) j=1 where M T (f , S) represents the concatenation of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. T"
P14-2090,2006.iwslt-papers.1,0,0.0144169,"is one example of such an application. When translating dialogue, the length of each utterance will usually be short, so the system can simply start the translation process when it detects the end of an utterance. However, in the case of lectures, for example, there is often no obvious boundary between utterances. Thus, translation systems require a method of deciding the timing at which to start the translation process. Using estimated ends of sentences as the timing with which to start translation, in the same way as a normal text translation, is a straightforward solution to this problem (Matusov et al., 2006). However, this approach 2 The method using RP can decide relative frequency of segmentation by changing a parameter, but guessing the length of a translation unit from this parameter is not trivial. 1 The implementation is available at http://odaemon.com/docs/codes/greedyseg.html. 551 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551–556, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics lation accuracy as measured by BLEU or another evaluation measure. We evaluate our methods on a speech"
P14-2090,P11-2093,1,0.45417,"rit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn"
P14-2090,P02-1040,0,0.0982654,"tion of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. This classifier is used to detect segmentation boundaries at test time. Steps 1. and 3. of the above procedure are trivial. In contrast, choosing a good segmentation according to Equation (2) is di"
P14-2090,N13-1023,0,0.679115,"s reason, segmentation strategies, which separate the input at appropriate positions other than end of the sentence, have been studied. A number of segmentation strategies for simultaneous speech translation have been proposed in recent years. F¨ugen et al. (2007) and Bangalore et al. (2012) propose using prosodic pauses in speech recognition to denote segmentation boundaries, but this method strongly depends on characteristics of the speech, such as the speed of speaking. There is also research on methods that depend on linguistic or non-linguistic heuristics over recognized text (Rangarajan Sridhar et al., 2013), and it was found that a method that predicts the location of commas or periods achieves the highest performance. Methods have also been proposed using the phrase table (Yarmohammadi et al., 2013) or the right probability (RP) of phrases (Fujita et al., 2013), which indicates whether a phrase reordering occurs or not. However, each of the previously mentioned methods decides the segmentation on the basis of heuristics, so the impact of each segmentation strategy on translation performance is not directly considered. In addition, the mean number of words in the translation unit, which strongly"
P14-2090,N12-1048,0,\N,Missing
P14-2090,I13-1141,0,\N,Missing
P14-2090,federico-etal-2012-iwslt,0,\N,Missing
P15-1020,P13-2121,0,0.0185406,"and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved h"
P15-1020,2006.amta-papers.8,0,0.0443221,"II et al. (2014), who describe a method that predicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we obse"
P15-1020,D10-1092,0,0.0151267,"y, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method is the state-of-the-art segmentation strategy proposed by Oda et al. (2014), which"
P15-1020,P07-2045,0,0.00788058,"I et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syn"
P15-1020,C04-1072,0,0.0585426,"Missing"
P15-1020,J93-2004,0,0.0498969,"tisfy these conditions. As shown in the Figure 3, there is ambiguity regarding syntactic constituents to be predicted (e.g. we can choose either [ NP ] or [ DT , NN ] as R for w = [ “this”, “is” ]). These conditions avoid ambiguity of which syntactic constituents should predicted for partial sentences in the training data. Looking at the example, Figures 3(d1) and 3(e1) satisfy these conditions, but 3(d2) and 3(e2) do not. Figure 4 shows the statistics of the lengths of L and R sequences extracted according to these criteria for all substrings of the WSJ datasets 2 to 23 of the Penn Treebank (Marcus et al., 1993), a standard training set for English syntactic parsers. From the figure we can see that lengths of up to 2 constituents cover the majority of cases for both L and R, but a significant number of cases require longer strings. Thus methods that predict a fixed number of constituents are not appropriate here. In Algorithm 1, we show the method we propose to Algorithmically, parsing with predicted syntactic constituents can be achieved by simply treating each syntactic constituent as another word in the input sequence and using a standard parsing algorithm such as the CKY algorithm. In this proces"
P15-1020,2006.iwslt-papers.1,0,0.0292458,"mation needed 1 Introduction Speech translation is an application of machine translation (MT) that converts utterances from the speaker’s language into the listener’s language. One of the most identifying features of speech translation is the fact that it must be performed in real time while the speaker is speaking, and thus it is necessary to split a constant stream of words into translatable segments before starting the translation process. Traditionally, speech translation assumes that each segment corresponds to a sentence, and thus performs sentence boundary detection before translation (Matusov et al., 2006). However, full sentences can be long, particularly in formal speech such as lectures, and if translation does not start until explicit ends of 198 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 198–207, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Process of English-Japanese simultaneous translation with sentence segmentation. tic prediction to MT, including the proposal of a heuristic method that examines whether a future co"
P15-1020,P11-2093,1,0.840277,"these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to"
P15-1020,2012.eamt-1.60,0,0.0111743,"uents. Creating a language model that contains probabilities for these tags in the appropriate places is not trivial, so for simplicity, we simply assume that every syntactic constituent tag is an unknown word, and that the output of translation consists of both translated normal words and non-translated tags as shown in Figure 5. We relegate a more complete handling of these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we"
P15-1020,P13-4016,1,0.829485,"redictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al."
P15-1020,J03-1002,0,0.00408816,"of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki"
P15-1020,2014.iwslt-papers.8,0,0.0146061,"are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentatio"
P15-1020,P03-1021,0,0.0381705,"203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor seg"
P15-1020,P14-2090,1,0.854392,"rovide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translation accuracy. This is a problem in phrase-based MT, the framework used in the majority of previous research on simultaneous translati"
P15-1020,N15-3009,1,0.857129,"Missing"
P15-1020,D14-1140,0,0.315517,"Missing"
P15-1020,P02-1040,0,0.0940091,"nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method i"
P15-1020,N13-1023,0,0.362073,"internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentation methods (Figure 2) provide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translat"
P15-1020,P06-2088,0,0.113368,"Figure 3(e2) by appending only NN after the unit. 3 Parsing Incomplete Sentences 3.1 Difficulties in Incomplete Parsing In standard phrase structure parsing, the parser assumes that each input string is a complete sentence, or at least a complete phrase. For example, Figure 3 (a) shows the phrase structure of the complete sentence “this is a pen.” However, in the case of simultaneous translation, each translation unit 3.2 Formulation of Incomplete Parsing 1 There is also one previous rule-based system that uses syntax in incremental translation, but it is language specific and limited domain (Ryu et al., 2006), and thus difficult to compare with our SMT-based system. It also does not predict unseen constituents, relying only on the observed segment. A typical model for phrase structure parsing is the probabilistic context-free grammar (PCFG). Parsing is performed by finding the parse tree T that 200 maximizes the PCFG probability given a sequence of words w ≡ [w1 , w2 , · · · , wn ] as shown by Eq. (2): T ∗ ≡ arg max Pr(T |w) T It should be noted that here L refers to syntactic constituents that have already been seen in the past. Thus, it is theoretically possible to store past parse trees as hist"
P15-1020,W10-1733,0,0.0784773,"on In simultaneous translation, we assume that we are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard de"
P15-1020,C04-1073,0,0.0449431,"redicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we observe the following SBAR. Our method attempts to pr"
P15-1020,N12-1048,0,\N,Missing
P15-1020,I13-1141,0,\N,Missing
P15-2094,N15-1033,1,0.878686,"Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up t"
P15-2094,P13-4016,1,0.853226,"MSCFG +PivotLM 2M † 25.75 † 24.58 ‡ 22.29 † 19.40 † 29.95 ‡ 25.64 † 19.19 ‡ 31.00 ‡ 26.22 ‡ 18.52 † 29.31 † 29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method con"
P15-2094,P03-1021,0,0.0312701,"score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with larger pivot LMs improves the BLEU scores. For all languages, the"
P15-2094,P02-1040,0,0.0927681,"29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with large"
P15-2094,N07-1061,0,0.884546,"e translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally w"
P15-2094,D14-1174,0,0.0648612,"et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich m"
P15-2094,J07-2003,0,0.870535,"Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up to 1.2 BLEU points), in all combinations of 4 languages with English"
P15-2094,P07-1092,0,0.805068,"n In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this a"
P15-2094,W08-0333,0,0.349773,"ional triangulation method, information about pivot phrases that behave as bridges between source and target phrases is lost after learning phrase pairs, as shown in Figure 1 (b). To overcome these problems, we propose a novel triangulation method that remembers the pivot phrase connecting source and target in the records of phrase/rule table, and estimates a joint translation probability from the source to target Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate"
P15-2094,J93-1004,0,\N,Missing
P15-2094,J93-2003,0,\N,Missing
P15-2094,2005.mtsummit-papers.11,0,\N,Missing
Q15-1041,D11-1039,0,0.0373913,"Missing"
Q15-1041,P14-1133,0,0.0472303,"semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the canonical NL questions being trivially convertible to an MR. Berant and Liang (2014) extract entities from a full-text question, map these entities into a set of candidate MRs, and generate canonical utterances accordingly. Then the canonical utterance that best paraphrases the input is chosen, thereby outputting the corresponding MR. Our approach is the similar but orthogonal to these works in that we focus on situations where the original user input is underspecified, and try to generate a natural language paraphrase that more explicitly states the user intention for disambiguation purposes. A second difference is that we do not use separate model to do paraphrasing, instea"
Q15-1041,W11-2103,0,0.0148728,"ig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM"
Q15-1041,J07-2003,0,0.754042,"rse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the source and target side of the grammar. SCFGs have recently come into favor as a tool for statistical machine translation (SMT). In SMT, a synchronous rule could, for example, take the form of: X → ⟨X0 eats X1 , X0 wa X1 wo taberu⟩ (2) where γs is an English string and γt is a Japanese string. Each non-terminal on the right side is indexed, with non-terminals with identical indices corresponding to each-other. Given the SCFG grammar, we can addit"
Q15-1041,P13-1158,0,0.217481,"nguage model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM Size 5.5G 1.5G ∼96K Table 2: Details of the data used to build LMs. In addition, because the Geoquery data is useful but small, for all 3-SCFG systems, we perfo"
Q15-1041,N04-1035,0,0.0247175,"as the natural language query and γt as an MR based on λ calculus. SCFG rules are automatically learned from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First"
Q15-1041,P06-1121,0,0.0351764,"ed from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First, we add a deletion rule which allows us to delete any arbitrary word w with any head symbol X, formall"
Q15-1041,D11-1108,0,0.0328147,"Missing"
Q15-1041,N13-1092,0,0.0757262,"Missing"
Q15-1041,P09-1069,0,0.299465,"ng et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of the"
Q15-1041,N13-1116,0,0.0241653,"Missing"
Q15-1041,W11-2123,0,0.0140013,"the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely"
Q15-1041,W04-3250,0,0.158833,"Missing"
Q15-1041,D13-1161,0,0.0432772,"Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphra"
Q15-1041,P11-1060,0,0.0494075,"iguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the or"
Q15-1041,W07-0716,0,0.0308294,"te model to do paraphrasing, instead using the same model to do paraphrasing and semantic parsing synchronously. This has the advantage of being able to scale more easily to complicated and highly compositional questions such as the ones found in Geoquery. In addition to being useful for semantic parsing, SCFGs have also been used for paraphrasing. A variety of research has used SCFG-based paraphrases for text-to-text generation tasks like sentence compression (Cohn and Lapata, 2009; Ganitkevitch et al., 2011), or expanding the set of reference translations for machine translation evaluation (Madnani et al., 2007). In this paper we have introduced a novel use of 3-way SCFGs that allows us to simultaneously do semantic parsing and text-to-text generation. To our knowledge, this is the first method to parse an underspecified input by trying to reconstruct a more explicit paraphrase of the input and validate 582 the naturalness of the paraphrase to disambiguate the meaning of the original input. 8 Conclusion and Future Work In this paper we introduced a method for constructing a semantic parser for ambiguous input that paraphrases the ambiguous input into a more explicit form, and verifies the correctness"
Q15-1041,J83-1001,0,0.767026,"In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more"
Q15-1041,P94-1004,0,0.166442,"hat doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the ca"
Q15-1041,P11-1064,1,0.832921,"880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether it is a keyword or full question. We also perform stemming on all natural language text, both in the keyword and question queries. Rule Extraction: Alignment is performed by pialign (Neubig et al., 2011) with the setting forcing one-to-many alignments. The algorithm to extract the tri-synchronous grammar is as discussed in Section 4.2 and maximum size of the rules for composition is 4. Decoding: To query the database, we use prolog queries fired against the Geoquery database. The parsing problem can thus be considered the task of decoding from underspecified natural language 3 We also tried gradient-based optimization methods and large feature sets as in Wong and Mooney (2007) and Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 que"
Q15-1041,N15-1033,1,0.821749,"rd inputs and question paraphrases were available, it is theoretically possible for our proposed method to learn from this data as well. 4 Joint Semantic Parsing and Paraphrasing using Tri-Synchronous Grammars In this section we describe our proposed method to parse underspecified and ungrammatical input while jointly generating a paraphrase that can be used to disambiguate the meaning of the original query. 4.1 Generalized Synchronous Context Free Grammars Before defining the actual parsing framework, we first present a generalization of SCFGs, the nsynchronous context free grammar (n-SCFG) (Neubig et al., 2015). In an n-SCFG, the elementary structures are rewrite rules of n − 1 target sides: X → ⟨γ1 , γ2 , ..., γn ⟩ (4) Grammar r0 QUERY → ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 CONJ → ⟨FORM0 STATE1 , FORM0 in STATE1 , (FORM0 , loc(x1 , x2 ), const(x2 , stateid(STATE1 )))⟩ r2 FORM → ⟨cities, cities, city(x1 )⟩ r3 STATE → ⟨virginia, virginia, virginia⟩ Derivations ⟨QUERY0 , QUERY0 , QUERY0 ⟩ r0 ⇒ ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 ⇒ ⟨FORM2 STATE3 , give me the FORM2 in STATE3 , answer(x1 , (FORM2 , loc(x1 , x2 ), const(x2 , stateid(STATE3 ))) ⟩ r2 ⇒ ⟨cities STATE3 , give"
Q15-1041,P13-4016,1,0.853123,"Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 queries into prolog queries. This is done by performing decoding of the SCFG-based parsing model to translate the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-"
Q15-1041,J03-1002,0,0.00411021,"the generated paraphrase. • Language Model: Counts the log language model probability of the paraphrase. • Unknown: Counts the number of tokens in the paraphrase that are unknown in the language model. • Paraphrase Length: Counts the number of words in the paraphrase, and can be calculated for each rule as the number of terminals in the paraphrase. This feature helps compensate for the fact that language models prefer shorter sentences. 5.3 Learning Feature Weights Now that we have defined the feature space, we need to optimize the weights. For this we use minimum error rate training (MERT) (Och and Ney, 2003), maximizing the number of correct answers over the entire corpus.3 6 Experiment and Analysis We evaluate our system using the Geoquery corpus (Zelle and Mooney, 1996), which contains 880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether"
Q15-1041,D09-1001,0,0.242932,"Missing"
Q15-1041,C12-1143,0,0.293503,"aning representation via verification using a language model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a cit"
Q15-1041,P13-2008,0,0.0256986,"s. Underspecified queries are commonly entered into search engines, leading to large result sets that are difficult for users to navigate (Sajjad et al., 2012). Studies have shown that there are several ways to deal with this problem, including query reformulation, which can fall in the categories of query expansion or query substitution (Shokouhi et al., 2014; Xue and Croft, 2013). Leveling (2010) proposed a paraphrasing method that tries to reconstruct original questions given keyword inputs in the IR context, but did not model this reformulation together with semantic parsing. In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2"
Q15-1041,N06-1056,0,0.145141,"uery-logic pairs. First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly. On the other hand, when incorporating the proposed tri-synchronous 572 grammar to generate paraphrases and verify them with a language model, we find that it is possible to recover the loss of accuracy, resulting in a model that is able to parse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the sou"
Q15-1041,P07-1121,0,0.424701,"Semantic Parsing of Ambiguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less a"
Q15-1041,D07-1071,0,0.183538,"age model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a city in Japan; or an NBA basketball 1 Tools to replicate our exp"
sakti-etal-2014-towards,W13-4604,1,\N,Missing
sakti-etal-2014-towards,maekawa-etal-2000-spontaneous,0,\N,Missing
sakti-etal-2014-towards,P11-2093,1,\N,Missing
shimizu-etal-2014-collection,N12-1048,0,\N,Missing
shimizu-etal-2014-collection,E09-1040,0,\N,Missing
shimizu-etal-2014-collection,N13-1023,0,\N,Missing
W13-4604,P11-2093,1,0.605576,"Missing"
W13-4604,2012.eamt-1.60,0,0.0168747,"in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or info"
W13-4604,J07-2003,0,0.120881,"Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better results, and also if any difference in the effectivenes"
W13-4604,C04-1114,0,0.0639568,"Missing"
W13-4604,P13-4016,1,0.882079,"Missing"
W13-4604,I11-1087,1,0.830624,"Missing"
W13-4604,J03-1002,0,0.0085147,"Missing"
W13-4604,D10-1092,0,0.0662551,"Missing"
W13-4604,P02-1040,0,0.0879458,"Missing"
W13-4604,N03-1017,0,0.0066382,"us(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better r"
W13-4604,P07-2045,0,0.0126736,"Missing"
W13-4604,P10-1017,0,0.13341,"Missing"
W13-4604,takezawa-etal-2002-toward,0,0.0605347,"edical communication as well. As a result, it is likely that adapting to medical terminology of the domain is somewhat less important than adapting to the conversational speaking style of the speech. 4 4.1 Experimental Setup For the tuning and test data for our translation system, we use the data described in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hie"
W13-4604,I05-3027,0,0.0659246,"Missing"
W13-4604,A00-2028,0,0.0297292,"challenging for a number of reasons. The first reason is that communication of incomplete or incorrect information could lead to a mistaken diagnosis with severe consequences, and thus extremely high levels of accuracy and reliability are 22 International Joint Conference on Natural Language Processing Workshop on Natural Language Processing for Medical and Healthcare Fields, pages 22–29, Nagoya, Japan, 14-18 October 2013. man interpreters, either based on a manual request of one of the users, or through automatic detection of when the dialogue is going poorly, such as the method described by Walker et al. (2000). Even with this fall-back to human interpreters, it is still desirable that the automatic translation system is effective as possible. In order to ensure this, we must be certain that the ASR, MT, and TTS models are all tuned to work as well as possible in medical situations. Some potential problems that we have identified so far based on our analysis of data are as follows: Figure 1: An overview of the use scenario for the medical translation system. Specialized Vocabulary: Perhaps the most obvious problem is that the ASR, MT, and TTS systems must all be able to handle the specialized vocabu"
W13-4604,C04-1168,0,0.0303816,"r. However, as the cost of hiring and maintaining medical interpreters is quite high, we would also like to reduce our reliance on human effort as much as possible. Thus, each device will use automatic translation by default, but also have functionality to connect to huTranslation/Synthesis of Erroneous Input: As we can expect ASR not to be perfect, it will be necessary to be able to translate input that contains errors. This problem can potentially be ameliorated by passing multiple speech recognition hypotheses to translation (Ney, 1999), and jointly optimizing the parameters of ASR and MT (Zhang et al., 2004; Ohgushi et al., 2013). In addition, it will also be necessary to resolve difficulties in TTS due to grammatical errors, lack of punctuation, and unknown words (Parlikar et al., 2010). While all of these problems need to be solved to provide high-reliability speech translation systems, in this paper as a first step we focus mainly on the MT system, and relegate the last problem of integration with ASR to future work. 23 3 Medical Translation Corpus Construction and Analysis In this section, we describe our collection of a tri-lingual (Japanese, English, Chinese) corpus to serve as an initial"
W13-4604,W12-4213,0,\N,Missing
W13-4604,P08-1023,0,\N,Missing
W14-3211,N13-1084,0,0.0485056,"Missing"
W14-3211,W10-4346,0,0.245775,"Missing"
W14-4004,P08-1087,0,0.0211581,"phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label"
W14-4004,P06-1077,0,0.155526,"e syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper,"
W14-4004,D12-1079,0,0.0187559,"of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute"
W14-4004,P11-2031,0,0.0194023,"(test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexic"
W14-4004,P14-2024,1,0.800601,"be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs that have very different word order, but is sensitive to alignment accuracy. On the other hand, we know that in most cases Japanese word order tends to be head final, and thus any rules that do not obey h"
W14-4004,P05-1066,0,0.118261,"Missing"
W14-4004,P11-2093,1,0.796984,"of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation"
W14-4004,P13-4016,1,0.791971,"eriment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluatio"
W14-4004,C00-2162,0,0.0855186,"translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), method"
W14-4004,W11-1011,0,0.0190313,"tic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether the reordering and lexical processing of Head Finalization contributes to the improvement of syntax-based mach"
W14-4004,J03-1002,0,0.0134154,", because translation patterns of T2S are expressed by using source sentence subtrees, the effect of reordering problems are relatively small, and the majority of reordering rules specified by hand can be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs"
W14-4004,D10-1092,0,0.046309,"Missing"
W14-4004,P03-1021,0,0.140083,"(such as sentences that contain the determiner 37 “no,” or situations where non-literal translations are necessary) and a hard constraint to obey headfinal word order could be detrimental. In order to incorporate this intuition, we add a feature (HF-feature) to translation patterns that conform to the reordering rules of Head Finalization. This gives the decoder ability to discern translation patterns that follow the canonical reordering patterns in English-Japanese translation, and has the potential to improve translation quality in the T2S translation model. We use the log-linear approach (Och, 2003) to add the Head Finalization feature (HF-feature). As in the standard log-linear model, a source sentence f is translated into a target language sentence e, by searching for the sentence maximizing the score: ˆ = arg max wT · h(f , e). e e VP Source side of translation pattern VBD NP hit x0:NP Word alignment Target side of translation pattern x0 wo utta 1. Apply Reordering to source translation pattern VP Reordered translation pattern NP VBD x0:NP hit Target side of translation pattern (1) where h(f , e) is a feature function vector. w is a weight vector that scales the contribution from each"
W14-4004,W10-1736,0,0.239358,"PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalizati"
W14-4004,P02-1040,0,0.0902419,"s, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indi"
W14-4004,N03-1017,0,0.0192216,"ttle work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized. 1 Introduction In the widely-studied framework of phrase-based machine translation (PBMT) (Koehn et al., 2003), translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs"
W14-4004,P07-2045,0,0.0091836,"2.11 37.99 5 Experiment In our experiment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki"
W14-4004,W04-3250,0,0.0397255,"ing information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05). For PBMT, we can see that reordering plays an extremely important role, with the highest BLEU and RIBES scores being achieved when using Reordering preprocessing (line 3, 4). Lexical Processing also provided a slight performance gain for PBMT. When we applied Lexical Processing to PBMT, BLEU and RIBES scores were improved (line 1 vs 2), although this gain was not significant when Reordering was performed as well. Overall T2S without any preprocessing achieved better translation quality than all conditions of PBMT (line 1 of T2S vs line 1-4 of PBMT). In addition, BLEU and RIBES sco"
W14-4004,C04-1073,0,0.0946405,"Missing"
W14-4004,N09-1028,0,0.0214648,"ethods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sent"
W14-4004,P01-1067,0,0.415385,"simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based S"
W14-4004,N06-1033,0,0.0175714,"n and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether th"
W14-4004,P13-1083,0,\N,Missing
W14-4004,J08-3004,0,\N,Missing
W15-3057,P07-2045,0,0.00583319,"rds has an important role in CLQA tasks using knowledge bases. In addition, as a result of fine-grained manual analysis, we identify a number of factors of translation results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set ca"
W15-3057,P13-2009,0,0.0233688,"anslation results that affect CLQA accuracy. 1 Introduction Question answering (QA) is the task of searching for an answer to question sentences using some variety of information resource. Generally, documents, web pages, or knowledge bases are used as these information resources. When the language of the question differs from the language of the information resource, the task is called cross-lingual question answering (CLQA) (Magnini et al., 2004; 2 MT is also used in mono-lingual QA tasks when question sentences are translated into the formal language used to query the information resource (Andreas et al., 2013). 1 All data used in the experiments will be released upon publishing of the paper. 442 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 442–449, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. questions from Japanese to English (the HT set). languages. Correspondingly, it is of interest to investigate which factors of translation output affect CLQA accuracy, which is the first step towards designing MT systems that achieve better accuracy on the task. In this paper, to investigate the influence of translation on CLQA using k"
W15-3057,P14-1133,0,0.0141859,"ld be noted ric putting a weight an content words should be that these words are frequent, and thus even NIST used. 2) References that are actually answerable score will not be able to perform adequate evaluaby the QA system should be used. tion, indicating that other measures may be necesWe should qualify this result, however, noting sary. the fact that the results are based on the use solely of the SEMPRE parsing system. While SEMPRE has shown highly competitive results on standard Table 4: Examples of translations with mistaken QA tasks, we also plan to examine other methods syntax such as Berant and Liang (2014)’s semantic pars◦ OR what library system is the sunset branch library in - JA サンセット・ブランチ図書館はどの図書館システムに所属しますか ing through paraphrasing, which may be less sen◦ HT to what library system does sunset branch library belong ◦ GT sunset branch library do you belong to any library system sitive to superficial differences in surface forms of the translation results. We also plan to to optimize ◦ YT which library system does the sunset branch library belong to ◦ Mo sunset branch library, which belongs to the library system machine translation systems using this analysis, ◦ Tra sunset branch library, bel"
W15-3057,2003.mtsummit-papers.32,0,0.0579481,"Ě KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1"
W15-3057,D13-1160,0,0.0102613,"for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research (Cai and Yates, 2013; Berant et al., 2013). It consists of 917 pairs of question sentences and “logical forms” which are computer-processable expressions of the meaning of the question that can be fired against the Freebase database to return the correct answer. Following Cai and Yates (2013), we divide this data into a training set (512 pairs), dev set (129 pairs) and test set (276 pairs). In the remainder of the paper, we refer to the questions in the test set before translation as the original (OR) set. Next, to investigate the influence of translation quality on the accuracy of QA, we created a question set with five different var"
W15-3057,C04-1072,0,0.0495265,"e QA system. ĂůŝŐŶŵĞŶƚ ĚŝĚ KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, a"
W15-3057,W14-3336,0,0.0532295,"Missing"
W15-3057,P13-1042,0,0.0393717,"a Graduate School of Information Science Nara Institute of Science and Technology Takayamacho 8916-5, Ikoma, Nara {sugiyama.kyoshiro.sc7, neubig}@is.naist.jp Abstract Sasaki et al., 2007). Machine translation (MT) is one of the most widely used tools to achieve CLQA (Mori and Kawagishi, 2005; Fujii et al., 2009; Kettunen, 2009).2 In the realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these featu"
W15-3057,P15-2024,0,0.0128817,"realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full"
W15-3057,P13-4016,1,0.836387,"tion results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research"
W15-3057,P02-1040,0,0.105224,"l forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To create the query for the knowledge base, SEMPRE merges neighboring logical forms in a binary tree structure. Bridging is an operation that generates predicates compatible with neighboring predicates. WER Word error rate (WER) is the edit distance between the translation and reference normalized by the sentence length. The formula"
W15-3057,N15-1149,0,0.0167969,"t these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accur"
W15-3057,P14-1083,0,0.0412693,"ion, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT"
W15-3057,D10-1092,0,0.175745,"c parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To crea"
W18-5017,N16-1014,0,0.14454,"ories to emotionally color the response via the internal state of the decoder. However, this study has not yet considered user’s emotion in the response generation process, nor attempted improve emotional experience of user. Towards positive emotion elicitation, Lubis et al. (2018) have recently proposed a model that encodes emotion information from user input and utilizes it in generating response. However, the resulting system is still limited to short and generic responses with positive affect, echoing the long standing lack-of-diversity problem in neural network based response generation (Li et al., 2016). Furthermore, the reported system has not learn about positive emotion elicitation strategies from an expert as the corpus construction relied on crowd-sourcing workers. This points to another problem: the lack of data that shows positive emotion elicitation or emotion recovery in everyday situations. Learning from expert responses and actions are essential in such a scenario as these potentially differ from standard chat-based scenarios. With scarcity of large-scale data, additional knowledge from higher level abstraction, such as dialogue action labels, may be highly beneficial. However, su"
W18-5017,P12-3007,0,0.0347229,"the model, in form of response cluster labels (Section 4.1), to aid its re165 … ?2,?2 ?2,1 ?3,1 … ?3,?3 Pretraining SubTle Counseling data Counselor dialogue clustering Unsupervised action label Finetuning ℎ??? ℎ??? ℎ??? ℎ??? Testing Figure 3: The flow of the experiment. ?1,1 … ?1,?1 ?2,1 … ?2,?2 5.1 Figure 2: MC-HRED architecture. Emotion encoder is shown in dark blue, and action encoder in dark yellow. Blue NNs are relating to input, and yellow NNs to response. Previous works have demonstrated the effectiveness of large scale conversational data in improving the quality of dialogue systems (Banchs and Li, 2012; Ameixa et al., 2014; Serban et al., 2016). In this study, we make use of SubTle (Ameixa et al., 2014), a large scale conversational corpus collected from movie subtitles, to learn the syntactic and semantic knowledge for response generation. The use of movie subtitles is particularly suitable as they are available in large amounts and reflecting natural human communication. In our experiments, we utilize the HRED trained on the SubTle corpus as our starting model. We follow the data pre-processing method in (Serban et al., 2016). The processed SubTle corpus contained 5,503,741 query-answer p"
W18-5017,W12-1630,0,0.0746901,"Missing"
W18-5017,gratch-etal-2014-distress,0,0.300446,"ty and the induced emotion (sadness or anger). Finally, we selected 20 videos, 10 of each emotion with varied intensity level where the two human ratings agree. Corpus Construction: Positive Emotion Elicitation by an Expert 2.2 Data Collection We arrange for the dyad to consist of an expert and a participant, each with a distinct role. The roles are based on the “social sharing of emotion” scenario, which argues that after an emotional event, a person is inclined to initiate an interaction which Even though various affective conversational scenarios have been considered (McKeown et al., 2012; Gratch et al., 2014), there is still a lack of resources that show common emotional problems in everyday social settings. Furthermore, a great ma162 yses of validation experiments have confirmed the reliability and indicated the precision of the FEELtrace system (Cowie et al., 2000). centers on the event and their reactions to it (Rime et al., 1991; Luminet IV et al., 2000). This form of social sharing is argued to be integral in processing the emotional event (Rime et al., 1991). In the interactions, the expert plays the part of the external party who helps facilitate this process following the emotional respons"
W18-5017,P13-1095,0,0.0320912,"04). Acosta and Ward (2011) have attempted to connect the two competences to build rapport, by recognizing user’s emotion and reflecting it in the system response. Although these competences address some of the user’s emotional needs (Picard and Klein, 2002), they are not sufficient to provide emotional support in an interaction. Recently, there has been an increasing interest in eliciting user’s emotional response via dialogue system interaction, i.e. emotion elicitation. Skowron et al. (2013) have studied the impact of different affective personalities in a text-based dialogue system, while Hasegawa et al. (2013) constructed translation-based response generators with various emotion targets. Despite the positive results, these approaches have not yet paid attention to the emotional benefit for the users. Our work aims to draw on an important overlooked potential of emotion elicitation: its application to improve emotional states, similar to that of emotional support between humans. This can be achieved by actively eliciting a more positive emotional valence throughout the interaction, i.e. positive emotion elicitation. This takes form as a chat-oriented dialogue system interaction that is layered with"
W18-5017,N15-1020,0,0.0713517,"Missing"
