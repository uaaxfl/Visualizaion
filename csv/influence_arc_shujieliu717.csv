2011.mtsummit-papers.19,P10-1147,0,0.0155607,"zed for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs as training objective (not BLEU-oriented). Such models may still suffer from over-fitting, as the features used may not be powerful enough to separate highly probable translation pairs from unlikely ones. yu bei han you bangjiao (a) have diplomatic relation with North Korea (b) (c) yu bei han have diplomatic relation you bangjiao with North Korea 0.05 yu X1 you X2 have X2 with X1 0.5 bei han North Korea 0.4 bangjiao diplomatic relation 0.1 0.02 Figure 1. An example for"
2011.mtsummit-papers.19,P08-1010,0,0.124702,"sed a phrase-based, joint probability model with EM training to do direct phrase alignment of training data. Since this method suffers from over-fitting, subsequent researchers introduced prior into the generative process: Blunsom et al. (2008) and DeNero et al. (2008) proposed to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs a"
2011.mtsummit-papers.19,C10-1056,0,0.0166382,"to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs as training objective (not BLEU-oriented). Such models may still suffer from over-fitting, as the features used may not be powerful enough to separate highly probable translation pairs from unlikely ones. yu bei han you bangjiao (a) have diplomatic relation with North Korea (b) (c) y"
2011.mtsummit-papers.19,W04-3250,0,0.0662409,"1O improves the performance on Nist’08 but not significantly on Nist’06. Since both UTMPERC and UTMMIRA get almost the same performance on training data, and UTMMIRA outperforms UTMPERC on the two test datasets a lot, it is confirmed that UTMMIRA is better in generalization capacity 4 (to be deeply analyzed in the next section). UTMOWL with logistic loss is not very good on Nist’06, while it significantly improves the performance on Nist’08. UTMMIRA gets larger improvement than other methods. Note that the BLEU differences between UTMMIRA and the three baselines are statistically significant (Koehn, 2004). Compared with PDM, UTMMIRA achieves a better improve- 7 Conclusion ment, and also the training time of our method (34 In order to handle the inconsistence between transhours, for UTMMIRA, UTMPERC and UTMPERC)5 is lation modeling and decoding, we propose a new 6 much shorter than that of PDM (119 hours) . discriminative translation sub-model, which is optimized with the same criterion as the translation 4 output is evaluated (BLEU). The new translation This observation is contrary to that in Arun and Koehn sub-model uses all translation pairs in the transla(2007), which shows perceptron and"
2011.mtsummit-papers.19,P06-1096,0,0.173483,"ation. Moreover, this method of translation modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable training (MIRA) of the new sub-model and the standard training (MERT) of conventional framework are unified into an interactive training process. In the following, the previous a"
2011.mtsummit-papers.19,P05-1012,0,0.152347,"Missing"
2011.mtsummit-papers.19,D08-1023,0,0.0419415,"Missing"
2011.mtsummit-papers.19,J03-1002,0,0.00412002,"tion performance is evaluated by case-insensitive BLEU4. Our decoder is a state-of-the-art implementation of hierarchical phrase-based model (HPM) (Chiang, 2007) with standard features, including language sub-model, translation sub-model, etc. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. FBIS newswire corpus is our training data, which is used to extract translation pairs and train the scalable feature weights ݓ in equation (2). The translation pairs are extracted as in Chiang (2007) from word alignment matrixes, which are generated by running GIZA++ (Och and Ney, 2003) in two directions, and then symmetrized using the grow-diag-final method (Koehn et al., 2003). The idea of unification of MERT and MIRA in UTM is evaluated against PDM, which uses the same features as in UTM. The generalization capacity of UTM using MIRA (UTMMIRA) is evaluated against three baselines, viz. leaving-oneout (L1O), UTM using Perceptron (UTMPERC) and UTM using OWL-QN (UTMOWL). The NIST’05 test set is used as our development dataset to tune the sub-model weights ߣ in equation (1) and the NIST’06 and NIST’08 test sets are used as our test sets. 6.1 Explanatory Capacity As mentioned"
2011.mtsummit-papers.19,P09-1088,0,0.0136108,"the translation pairs are usually assigned by maximum likelihood estimation (MLE). That is, conventional translation modeling never takes translation evaluation metric into consideration. Moreover, this method of translation modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable"
2011.mtsummit-papers.19,P03-1021,0,0.0181234,"pectively. Experiment result and analysis are given in Section 6. 2 Translation Modeling The task of translation modeling can be defined as, given a bilingual training corpus, the search of the optimal set of translation pairs with two goals: 1) Explanatory capacity: i.e. the training data can be fully represented by the translation pairs. 2) Generalization capacity: i.e. the translation pairs can also predict the correct translation given unseen source input. In other words, the translation pairs can avoid over-fitting. The conventional approach to translation modeling comprises three steps (Och, 2003): Firstly, the sentence pairs in training corpus are aligned at word level. Secondly, translation pairs are extracted using a heuristic method. Lastly, MLE is used to compute translation probabilities. There are a few shortcomings of this method: 1) Inconsistent format of translation knowledge: word alignment in training vs. translation pairs (phrase pairs) in decoding. 2) The training process is not oriented towards translation evaluation metric: BLEU is not considered in the scoring of translation pairs. 3) This method may cause over-fitting: it is not considered whether the phrases are extr"
2011.mtsummit-papers.19,J07-2003,0,0.0601155,"o ܰܧܩሺܨሻ, so that more feature weights could be updated. We adopt the oracle ranking method to rank ܰܧܩሺܨሻ, and the top half candidates in ܰܧܩሺܨሻare used as the positive samples and the left as the negative samples (line 5). At last, we use OWL-QN to optimize ݓusing the positive and negative samples (line 6). 6 Experiment The experiments evaluate the performance of our model and training methods in a Chinese-English setting. Translation performance is evaluated by case-insensitive BLEU4. Our decoder is a state-of-the-art implementation of hierarchical phrase-based model (HPM) (Chiang, 2007) with standard features, including language sub-model, translation sub-model, etc. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. FBIS newswire corpus is our training data, which is used to extract translation pairs and train the scalable feature weights ݓ in equation (2). The translation pairs are extracted as in Chiang (2007) from word alignment matrixes, which are generated by running GIZA++ (Och and Ney, 2003) in two directions, and then symmetrized using the grow-diag-final method (Koehn et al., 2003). The idea of unification of MERT and MIRA in UTM"
2011.mtsummit-papers.19,D08-1024,0,0.393931,"n modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable training (MIRA) of the new sub-model and the standard training (MERT) of conventional framework are unified into an interactive training process. In the following, the previous approaches to translation modeling are review"
2011.mtsummit-papers.19,W02-1001,0,0.0483533,"ussed in section 5. MERT and scalable training are unified into an interactive training process, as shown in Figure 2. Algorithm Interactive Training INPUT: Training dataሺܨǡ ܧሻ; Develop dataሺܨௗ ǡ ܧௗ ሻ OUTPUT: ߣ and ݓ 1: while(BLEU onሺܨௗ ǡ ܧௗ ሻ is improved) 2: fix ߣ, and train  ݓusing scalable method onሺܨǡ ܧሻ 3: fix ݓ, and train ߣ using MERT on ሺܨௗ ǡ ܧௗ ሻ 4: Return ߣ and ݓ Perceptron (Rosenblatt, 1962) is an incremental training procedure (i.e. stochastic approximation) which optimizes a minimum square error (MSE) loss function. Here, we use the average perceptron (Collins, 2002) for our scalable training, which is shown to be more effective than the standard one. The update rule on an example ሺ ǡ  ሻ is:  ݓ՚  ݓ  ᇱ ሺ ǡ  ሻ െ  ᇱ ሺ ǡ  ሻ Figure 2. Training method for UTM Given initial ߣ and ݓ, we first fix ߣ and train ݓ using scalable method on training data ሺܨǡ ܧሻ, then we fix the new  ݓand update ߣ using MERT on development data ሺܨௗ ǡ ܧௗ ሻ . MERT and scalable training continue interactively until translation performance is not improved on development data ሺܨௗ ǡ ܧௗ ሻ. We use 0 as initial  ݓand the trained submodel weights (using"
2011.mtsummit-papers.19,D08-1033,0,0.0185821,"ess is not oriented towards translation evaluation metric: BLEU is not considered in the scoring of translation pairs. 3) This method may cause over-fitting: it is not considered whether the phrases are extracted from a highly probable phrase alignment or from an unlikely one. To unify the format of translation knowledge, Marcu and Wong (2002) proposed a phrase-based, joint probability model with EM training to do direct phrase alignment of training data. Since this method suffers from over-fitting, subsequent researchers introduced prior into the generative process: Blunsom et al. (2008) and DeNero et al. (2008) proposed to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system per"
2011.mtsummit-papers.19,2005.mtsummit-papers.33,0,0.103624,"Missing"
2011.mtsummit-papers.19,P10-1049,0,0.0148023,"ion to over-fitting for nearly all discriminative translation modeling approaches is the length constraint to the source and/or target side of translation pairs. That is, the long translation pairs, which have weak generalization capacity, are simply filtered away. This solution, however, also discards long but useful phrases like &quot;I would like to have&quot;. Blunsom et al. (2008) used a discriminative latent variable model with each translation pair as a feature and a  ʹܮregularization to deal with overfitting, and in order to incorporate language model, sampling method is adopted for training. Wuebker et al. (2010) used leaving-one-out (L1O) to deal with over-fitting and forced alignment to deal with the errors introduced by incorrect word alignment. The basic idea is to use the trained SMT decoder to re-decode the training data, and then use the decoded result to re-calculate translation pair probabilities. Since the correct target sentence (i.e. the target side of training data) is not guaranteed to be generated by SMT decoder, forced alignment is used to generate the correct target sentence by discarding all phrase translation candidates which do not match any sequence in the correct target sentence."
2011.mtsummit-papers.19,P08-1012,0,0.033463,"Missing"
2011.mtsummit-papers.19,J93-2003,0,\N,Missing
2011.mtsummit-papers.6,W08-0301,1,0.906283,"skeletonenhanced re-ranking, which re-ranks the n-best output of a conventional SMT decoder with respect 73 to translated skeleton; another is skeletonenhanced decoding, which re-ranks the translation hypotheses of not only the entire sentence but any span of the sentence. In the following, we will elaborate the related works in Section 2, followed by the modules of skeleton-enhanced translation framework in Section 3, and the experiments results, which show significant improvements over a state-of-the-art phrase-based baseline system, in Section 4. Section 5 is our conclusion. 2 Related Work Li et al. (2008) proposed three source spurious word deletion models2 to calculate the translation probability for any source word to be translated into the special empty symbol, ε. The first model uses a uniform probability ሺߝሻ , calculated by MLE from the word-aligned training corpus. The second one is word-type sensitive probability ሺߝȁݓሻ, where  ݓis the type of a source word, estimated in similar way as the first model. The third one is a word-token sensitive model, which uses Conditional Random Fields (CRF) (Lafferty et al., 2001) to calculate how likely a word token should be spurious given it"
2011.mtsummit-papers.6,P09-1065,0,0.0164394,"quires a mapping between (nonspurious) source word positions during source word deletion. During each decoding step, where the translation candidates of a particular source sentence span are under consideration, SED fetches the corresponding source skeleton span and its best partial translated skeleton, and then calculates several kinds of similarity between the (partial) translation candidates and the partial translated skeleton. The scoring of the translation candidates is thus enhanced by the skeleton-related features. At a glance, SED is similar to collaborative decoding (Li et al., 2009, Liu et al., 2009). There is, however, a major difference. While every decoder involved in collaborative decoding selects its own best translation candidate by considering the candidates from other decoders, the SED decoder considers the candidates from skeleton translation, but not vice versa. 4 In this section, after elaborating the experiment settings in Section 4.1, we will explain how the thresholds for CIM and CSM are chosen empirically in Section 4.2 and Section 4.3 respectively. The improvement in SMT performance by the skeletonenhanced methods will be shown in Section 4.4, followed by a detailed featur"
2011.mtsummit-papers.6,D08-1077,0,0.0165919,"ranslation probability for any source word to be translated into the special empty symbol, ε. The first model uses a uniform probability ሺߝሻ , calculated by MLE from the word-aligned training corpus. The second one is word-type sensitive probability ሺߝȁݓሻ, where  ݓis the type of a source word, estimated in similar way as the first model. The third one is a word-token sensitive model, which uses Conditional Random Fields (CRF) (Lafferty et al., 2001) to calculate how likely a word token should be spurious given its context. All of them can improve a phrase-based system significantly. Menezes and Quirk (2008) improved both phrase-based and treelet systems by introducing structural word insertion and deletion without requiring lexical anchor. The insertion/deletion order templates are based on syntactic cues and two additional feature functions (a count of structurally inserted words and a count of structurally deleted words). The probabilities of the templates are estimated by MLE. 2 However, the problem of word insertion is discussed but not addressed. In these two approaches, spurious words are treated the same as non-spurious words; they are simply assigned with a special translation probabilit"
2011.mtsummit-papers.6,J03-1002,0,0.00511041,"rther improvement. 76 Experiment Setting We conduct our experiments on the test data from NIST 2006 and NIST 2008 Chinese-to-English machine translation tasks. To tune the model parameters, NIST 2005 test data is used as our development data. The bilingual training dataset is NIST 2008 training set excluding the Hong Kong Law and Hong Kong Hansard (contains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performe"
2011.mtsummit-papers.6,P03-1021,0,0.0231374,"on skeleton which is computed by the language model trained with spurious-word-deleted language model training data (in section 3.2). The features of unigram/bigram recall/precision measure the similarity with respect to faithfulness while the feature of skeleton language model measures the similarity with respect to fluency 4 . The values of these five features are calculated on the fly during decoding. The five features are used alongside the conventional features in SMT, and the weights of all these features can be trained by any conventional method like Minimum Error Rate Training (MERT) (Och, 2003). In sum, SED requires a mapping between (nonspurious) source word positions during source word deletion. During each decoding step, where the translation candidates of a particular source sentence span are under consideration, SED fetches the corresponding source skeleton span and its best partial translated skeleton, and then calculates several kinds of similarity between the (partial) translation candidates and the partial translated skeleton. The scoring of the translation candidates is thus enhanced by the skeleton-related features. At a glance, SED is similar to collaborative decoding (L"
2011.mtsummit-papers.6,P02-1040,0,0.0813556,"y as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS23-23 TNS-123 TDS123-123 TNS-1234 TDS1234-1234 Sourc"
2011.mtsummit-papers.6,J07-2003,0,0.0472828,"common between ܶௌ and ܵ 4 Trigram and 4-gram features were attempted but found to give no further improvement. 76 Experiment Setting We conduct our experiments on the test data from NIST 2006 and NIST 2008 Chinese-to-English machine translation tasks. To tune the model parameters, NIST 2005 test data is used as our development data. The bilingual training dataset is NIST 2008 training set excluding the Hong Kong Law and Hong Kong Hansard (contains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2"
2011.mtsummit-papers.6,J97-3002,0,0.0211816,"ontains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences a"
2011.mtsummit-papers.6,D07-1006,0,0.0158878,"s, and conjunctions like &quot;because&quot;, &quot;therefore&quot;, etc. refer to objects, actions, events, or logical relationships. Their meaning is language-neutral and they usually have counterparts in another language. In contrast, some words serve to express (language-specific) grammatical relations only, and thus they may 72 To deal with the spurious words in sentence pairs, IBM models 3, 4 and 5 (Brown et al., 1993) introduce a special token null, which can align to a source/target word. Hence there are two types of words: spurious words (null-aligned) and nonspurious words. Similar with the IBM models, Fraser and Marcu (2007) proposed a new generative model called LEAF, in which words are classified into three types instead of two: spurious words, head words (which are the key words of a sentence) and non-head words (modifiers of head words). These two methods are generative models for word alignment, which cannot be directly used in the conventional log-linear model of statistic machine translation (SMT). The conventional phrasebased SMT captures spurious words within the phrase pairs in the translation table. For example, in the phrase pair (&quot;yong (spend) na (that) bi qian(money)&quot;, &quot;to spend that money&quot;), it imp"
2011.mtsummit-papers.6,N03-1017,0,0.108226,"Missing"
2011.mtsummit-papers.6,W04-3250,0,0.722002,"inhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS23-23 TNS-123 TDS123-123 TNS-1234 TDS1234-1234 Source Target Word Frequency Word Frequency de 127,226 the 237,160 le 11,991 of 128,443 zai 6,577 to 41,217 zhong 6,250 in 39,146 shang 4,287 for 35,570 Table 1"
2011.mtsummit-papers.6,P06-1066,0,0.0211885,"acted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS"
2011.mtsummit-papers.6,J93-2003,0,\N,Missing
2011.mtsummit-papers.6,P09-1066,1,\N,Missing
2011.mtsummit-papers.6,J04-4002,0,\N,Missing
2011.mtsummit-papers.6,D07-1080,0,\N,Missing
2011.mtsummit-systems.2,J07-2003,0,0.040477,"Missing"
2011.mtsummit-systems.2,P06-1097,0,0.0570349,"Missing"
2011.mtsummit-systems.2,P06-1065,0,0.0513002,"Missing"
2020.acl-main.130,P17-1152,0,0.0695968,"Missing"
2020.acl-main.130,W19-4828,0,0.0695299,"Missing"
2020.acl-main.130,N19-1423,0,0.142582,"similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Giv"
2020.acl-main.130,N19-1246,0,0.0402022,"Missing"
2020.acl-main.130,D19-1243,0,0.0342445,"Missing"
2020.acl-main.130,P19-1356,0,0.0569446,"Missing"
2020.acl-main.130,N18-1023,0,0.0279937,"alogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for instance, sampled from the test set and categorize them into six groups. The definition and ratio of each group are shown as follows. Attitude Reasoning: This type of instance tests if a model knows the speaker’s attitude towards an object. Algebrai"
2020.acl-main.130,D19-1051,0,0.0296336,"instance when inspectors doubt the uniqueness or correctness of the answer. 3.2 Analysis The detailed statistics of MuTual are summarized in Table 2. MuTual has an average of 4.73 turns. The vocabulary size is 11,343, which is smaller than other dialogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for in"
2020.acl-main.130,D17-1082,0,0.0723066,"Missing"
2020.acl-main.130,W15-4640,0,0.493103,"we will be late for the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly"
2020.acl-main.130,D19-1191,0,0.0373074,"Missing"
2020.acl-main.130,Q19-1016,0,0.113805,"sense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $"
2020.acl-main.130,P15-1152,0,0.0210348,"et al., 2018a) considers consistent personality in dialogue. Crowd workers are required to act the part of a given provided persona, and chat naturally. Dialogue NLI (Welleck et al., 2019) is a natural language inference dataset modified from PERSONA-CHAT. It demonstrates that NLI can be used to improve the consistency of dialogue models. CoQA (Reddy et al., 2019) is collected by pairing two annotators to chat about a passage in the form of questions and answers. Each question is dependent on the conversation history. There are also several large-scale datasets in Chinese, such as Sina Weibo (Shang et al., 2015), Douban Conversation Corpus (Wu et al., 2017) and E-commerce Dialogue Corpus (Zhang et al., 2018b). As shown in Table 1, most of the existing conversation benchmarks do not focus on testing reasoning ability. One exception is CoQA, which considers pragmatic reasoning. The difference is that CoQA is a machine comprehension dataset, in which conversations are based on a given passage. Another related reading comprehension dataset is DREAM (Sun et al., 2019), which is designed specifically for challenging dialogue-based reading 1407 Listening Comprehension M Ma'am, you forgot your phone. Dialogu"
2020.acl-main.130,Q19-1014,0,0.172647,"et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Per"
2020.acl-main.130,N19-1421,0,0.465732,"in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open Narrative Open Manually $ "" $ "" $ """
2020.acl-main.130,P19-1001,0,0.279625,"Missing"
2020.acl-main.130,voorhees-tice-2000-trec,0,0.41351,"Missing"
2020.acl-main.130,P19-1363,0,0.130911,"soning capability and commonsense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances P"
2020.acl-main.130,J19-1005,1,0.900814,"Missing"
2020.acl-main.130,P17-1046,1,0.758434,"the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive resul"
2020.acl-main.130,N19-1242,0,0.0248812,"e context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Given a context, the positive response has a higher probability compared with negative responses. Motivated by this, we concatenate co"
2020.acl-main.130,D18-1009,0,0.269438,"ow we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open"
2020.acl-main.130,P18-1205,0,0.49499,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,C18-1317,0,0.432694,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,P18-1103,0,0.0685413,"date response as the model output. The “IDF” is calculated only on the training set. Dual LSTM (Lowe et al., 2015): Two LSTMs are used to encode context and response, respectively. The relevance between context and response is calculated by the similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representa"
2020.acl-main.318,D18-1214,0,0.10454,"Missing"
2020.acl-main.318,P17-1042,0,0.500515,"om which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding near"
2020.acl-main.318,P18-1073,0,0.0579027,"es clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial s"
2020.acl-main.318,D18-1399,0,0.0702327,"Missing"
2020.acl-main.318,P19-1019,0,0.0119731,"ut also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping"
2020.acl-main.318,Q17-1010,0,0.0911767,"the embeddings of the source language as X := WX. Divide {Cix }m 9 Initialize i=0 into K groups via clustering. {Wi }K i=0 with W. Fine-tune each Wi according to Eq. (6) and do the refinement. return {Wi }K i=0 4.2.3 we retrieve the translation of x by calculating the CSLS score of Ws x and each target embedding y, similar to Eq. (2) introduced in §2.2. 4 4.3.1 Dataset Bilingual lexicon induction (BLI) measures the word translation accuracy in comparison to a gold standard. We report results on the widely used MUSE dataset (Conneau et al., 2017). This dataset consists of monolingual fastText (Bojanowski et al., 2017) embeddings of many languages and dictionaries for many language pairs divided into training and test sets. The evaluation follows the setups of Conneau et al. (2017). 4.2 4.2.1 Implementation Details Pre-processing We choose the top 10,000 word embeddings to build word graph because the monolingual embeddings of low-frequency words may be trained insufficiently. The embeddings are normalized following Artetxe et al. (2018b). Specifically, we first apply length normalization to the embeddings, and then mean center each dimension. After that, we do length normalization again to ensure the word"
2020.acl-main.318,D18-1043,0,0.111745,"he performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding nearest neighbors in the target embedding space. With the new dictionary, the mapping fun"
2020.acl-main.318,Q19-1007,0,0.0634208,"e method proposed by Artetxe et al. (2018b). We use their public code to finish this step. We initialized W with a random orthogonal matrix. After building the seed dictionary, we first solve the Procrustes problem (Eq. (1)), followed by the refinement process. 4.3 Experiment 4.1 Clique Extraction Main Results Baselines We choose several supervised and unsupervised methods to be our baselines. The supervised baselines include: (1) The iterative Procrustes method proposed by Smith et al. (2017); (2) The multi-step framework proposed by Artetxe et al. (2018a); (3) a geometric method proposed by Jawanpuria et al. (2019). The unsupervised baselines include (1) MUSE proposed by Conneau et al. (2017), which is a GAN based method followed by a refinement process; (2) a Wasserstein GAN based method combined with distribution matching and back translation, proposed by Xu et al. (2018); (3) a method proposed by Alvarez-Melis and Jaakkola (2018) that views the mapping problem as optimal transportation and optimize the Gromov-Wasserstein distance between embedding spaces; (4) A robust self-learning method proposed by Artetxe et al. (2018b), which leverages the intra-linguistic word similarity information to infer ini"
2020.acl-main.318,D18-1330,0,0.0646003,"Missing"
2020.acl-main.318,D18-1549,0,0.0751542,"oach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary ("
2020.acl-main.318,W15-1521,0,0.0323991,"2 Background Unsupervised bilingual lexicon induction (BLI) is the task of inducing word translations from monolingual corpora of two languages. Recently proposed methods follow the same procedure, i.e., first learning cross-lingual embeddings in an unsupervised way (§2.1) and then inducing bilingual lexicons from the embedding spaces (§2.2). 2.1 Unsupervised Cross-lingual Embeddings Previous methods for learning cross-lingual embeddings can be roughly divided into two categories (Ormazabal et al., 2019), i.e., mapping methods and joint learning methods. As the second category, the skip-gram (Luong et al., 2015) for example, requires bilingual corpus during training, current methods for unsupervised cross-lingual embeddings mainly fall into the first category. Given pretrained monolingual embeddings of two languages, the mapping methods try to map the source and target embedding spaces through a linear transformation (Mikolov et al., 2013) W ∈ Md×d (R), where Md×d (R) is the space of d × d matrices of real numbers and d is the dimension of the embeddings. Based on that, Xing et al. (2015) propose to constrain W to be orthogonal, i.e., W&gt; W = I, and Conneau et al. (2017) find this is a Procrustes prob"
2020.acl-main.318,P19-1492,0,0.0998402,"ngs; (3) We improve the BLI performance on the MUSE dataset with our method, even compared with strong baselines. 2 Background Unsupervised bilingual lexicon induction (BLI) is the task of inducing word translations from monolingual corpora of two languages. Recently proposed methods follow the same procedure, i.e., first learning cross-lingual embeddings in an unsupervised way (§2.1) and then inducing bilingual lexicons from the embedding spaces (§2.2). 2.1 Unsupervised Cross-lingual Embeddings Previous methods for learning cross-lingual embeddings can be roughly divided into two categories (Ormazabal et al., 2019), i.e., mapping methods and joint learning methods. As the second category, the skip-gram (Luong et al., 2015) for example, requires bilingual corpus during training, current methods for unsupervised cross-lingual embeddings mainly fall into the first category. Given pretrained monolingual embeddings of two languages, the mapping methods try to map the source and target embedding spaces through a linear transformation (Mikolov et al., 2013) W ∈ Md×d (R), where Md×d (R) is the space of d × d matrices of real numbers and d is the dimension of the embeddings. Based on that, Xing et al. (2015) pro"
2020.acl-main.318,P18-1072,0,0.123342,"Missing"
2020.acl-main.318,N15-1104,0,0.134887,"Missing"
2020.acl-main.318,D18-1268,0,0.322829,"proach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding nearest neighbors in the target embedding space. With the new dict"
2020.acl-main.318,P17-1179,0,0.591623,"ngual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embe"
2020.acl-main.320,D18-1549,0,0.236248,"mple et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe et al., 2018a), and then use them to build two initial Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) models with two language models. However, there are two problems with their initialization methods. (1) Some complex sentence structures of original training sentences are hard to be recovered with the"
2020.acl-main.320,P19-1019,0,0.019551,"e and taren as target 59.87 48.52 fr as target 58.71 47.63 Table 3: Test BLEU scores of the rewriting models. 4 Related Work Unsupervised machine translation becomes a hot research topic in recent years. The pioneering methods are based on NMT models (Transformer) (Artetxe et al., 2017; Lample et al., 2017; Yang et al., 2018) trained with denoising auto-encoder (Vincent et al., 2010) and iterative back-translation. The following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b; Lample et al., 2018; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). They build the initial PBSMT models with language models and phrase tables inferred from unsupervised cross-lingual n-gram embeddings. Recently, Lample and Conneau (2019) propose a pre-training method and achieve state-of-theart performance on unsupervised en-f r and en-de translation tasks. But they use much more monolingual data from Wikipedia than previous work and this paper. We must also mention the work of Wu et al. (2019). They similarly use retrieval and rewriting framework for unsupervised MT. However, ours is different from theirs in two aspects. First, we efficiently calculate the"
2020.acl-main.320,D18-1399,0,0.0828926,"o minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Re"
2020.acl-main.320,J03-1002,0,0.0445331,"eir initialization methods. (1) Some complex sentence structures of original training sentences are hard to be recovered with the n-gram translation tables. (2) The initial translation tables inevitably contain much noise, which will be amplified in the subsequent process. In this paper, we propose a novel retrieve-andrewrite initialization method for UMT. Specifically, we first retrieve semantically similar sentence pairs from monolingual corpora of two languages with the help of unsupervised cross-lingual sentence embeddings. Next, with those retrieved similar sentence pairs, we run GIZA++ (Och and Ney, 2003) to get word alignments which are used to delete unaligned words in the target side of the retrieved sentences. The modified target sentences are then rewritten with a designed sequence-to-sequence rewriting model to minimize the semantic gap between the source and target sides. Taking the pairs of the source sentences and corresponding rewritten targets as pseudo parallel data, we then build two initial PBSMT models (source-to-target and targetto-source), which are used to generate pseudo parallel data to warm up NMT models, followed by an iterative back-translation training process. Our code"
2020.acl-main.320,Q17-1010,0,0.0129359,"emantic gap between the source and retrieved targets, we do target sentences rewriting (§2.2) by deleting unaligned words in the target side, and generate complete and better-aligned targets via our rewriting model with the help of missing information provided by the source. After that, we treat the rewritten pairs as the pseudo parallel data for translation models initialization and training (§2.3). 2.1 Similar Sentences Retrieval Given two monolingual corpora Dx and Dy of two languages X and Y respectively, we first build unsupervised cross-lingual word embeddings of X and Y using fastText (Bojanowski et al., 2017) and vecmap (Artetxe et al., 2018a), and then we obtain cross-lingual sentence embeddings based on the cross-lingual word embeddings via SIF (Arora et al., 2017). After that, we use the marginal-based scoring (Artetxe and Schwenk, 2018) to retrieve Figure 2: Example of rewriting. The unaligned words, i.e., 250 and 建议(suggestion), proposed by GIZA++ have been removed in y 0 , which is then rewritten by the model to the right target yˆ (40 and 反馈(responses)). More examples of the sentences before and after rewriting are shown in Appendix B. similar sentences from two corpora1 . Examples retrieve"
2020.acl-main.320,P16-1009,0,0.0220579,"els, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe"
2020.acl-main.320,D07-1103,0,0.101406,"Missing"
2020.acl-main.320,P16-1162,0,0.00987909,"els, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe"
2020.acl-main.344,D16-1133,0,0.0207171,"ich makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speec"
2020.acl-main.344,N19-1006,0,0.126859,"chained together, or an end-to-end approach, where a single model converts the source language audio sequence to the target language text sequence directly (Berard et al., 2016). Due to the alleviation of error propagation and lower latency, the end-to-end ST model has been a hot topic in recent years. However, large paired data of source audios and target sentences are required to train such a model, which is not easy to satisfy for most language pairs. To address this ∗ Works are done during internship at Microsoft issue, previous works resort to pre-training technique (Berard et al., 2018; Bansal et al., 2019), where they leverage the available ASR and MT data to pre-train an ASR model and an MT model respectively, and then initialize the ST model with the ASR encoder and the MT decoder. This strategy can bring faster convergence and better results. The end-to-end ST encoder has three essential roles: transcribe the speech, extract the syntactic and semantic knowledge of the source sentence and then map it to a semantic space, based on which the decoder can generate the correct target sentence. These pose a heavy burden to the encoder, which can be alleviated by pre-training. However, we argue that"
2020.acl-main.344,2004.iwslt-evaluation.13,0,0.0588685,"sing sentencepiece (Kudo and Richardson, 2018) with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance (Berard et al., 2018). Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin, 2010). For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit (Bender et al., 2004). 4 3732 https://github.com/espnet/espnet 2) We perform sentence-level force-alignment between audio and transcription using aeneas5 tool and segment the audio according to alignment results. 4.2 Baselines Experiments are conducted in two settings: base setting and expanded setting. In base setting, only the corpus described in Section 4.1 is used for each task. In the expanded setting, additional ASR and/or MT data can be used. All results are reported on case-insensitive BLEU with the multibleu.perl script unless noted. 4.2.1 End-to-End ST Baselines We mainly compare our method with the conv"
2020.acl-main.344,N19-1423,0,0.0284924,"s the CTC loss Lctc and the cross-entropy loss LCE : LASR = αLCT C + (1 − α)LCE = −α log Pctc (y s |x) − (1 − α) log Ps2s (y s |x) (1) In this work, we set α to 0.3. The CTC loss works on the encoder output and it pushes the encoder to learn frame-wise alignment between speech with words. 3.3 Advanced Courses: Understanding and Word Mapping With the ability of transcription, we further propose two new tasks for the advanced courses. 3.3.1 Frame-based Masked Language Model The design of the Frame-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT (Devlin et al., 2019) and semantic mask for ASR task (Wang et al., 2019a). This task enables the encoder to understand the inner meaning of a segment of speech. As shown in Figure 2, we first perform forcealignment between the speech and the transcript sentence to determine where in time particular words occur in the speech segment. For each word yis , we obtain its corresponding start position si and the end position ei in the sequence x according to force alignment results. At each training iteration, we randomly sample some percentage of the words in the y s and denote the selected word set as y ˜s . s s Next,"
2020.acl-main.344,N16-1109,0,0.241841,"to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speech translation, where"
2020.acl-main.344,L18-1001,0,0.23356,"Missing"
2020.acl-main.344,1998.amta-tutorials.1,0,0.75262,"Missing"
2020.acl-main.344,rousseau-etal-2014-enhancing,0,0.0514647,"Missing"
2020.acl-main.344,Q19-1020,0,0.126466,"2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speech translation, where they use a new decoder to replace the ASR decoder and to learn the output from the MT dec"
2020.acl-main.344,D18-2012,0,0.0163649,"filterbanks stacked with 3-dimensional pitch features extracted with a step size of 10ms and window size of 25ms. The features are normalized by the mean and the standard deviation for each training set. Utterances of more than 3000 frames are discarded. We perform speed perturbation with factors 0.9 and 1.1. The alignment results between speech and transcriptions are obtained by Montreal Forced Aligner (McAuliffe et al., 2017). For references pre-processing, we tokenize and lowercase all the text with the Moses scripts. For pre-training tasks, the vocabulary is generated using sentencepiece (Kudo and Richardson, 2018) with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance (Berard et al., 2018). Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin, 2010). For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit (Bender et al., 2004). 4 3732 https://github.co"
2020.acl-main.344,D15-1166,0,0.0303917,"features, FMLM and FBLT, which explicitly teach the encoder to do source language understanding and target language meaning mapping. (3) Experiments show that both the proposed courses are helpful for speech translation, and our proposed curriculum pre-training leads to significant improvements. 2 2.1 Related Work Speech Translation Early work on speech translation used a cascade of an ASR model and an MT model (Ney, 1999; Matusov et al., 2005; Mathias and Byrne, 2006), which makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Li"
2021.acl-long.348,P18-1073,0,0.162705,"g steps of pre-training encoder and decoder are separated, therefore the training samples of them are not necesarrily the same. (In the figure, the training sample for pre-training the encoder is x1 = x11 x21 ..x61 ) and the training sample for pre-training the decoder is x2 = x12 x22 ..x62 ). For MT fine-tuning, we use the parallel training sample {x1 , y1 } from the parallel corpus or generated from back-translation. connected for MT fine-tuning. We propose two types of semantic interfaces, namely CL-SemFace and VQ-SemFace. The former takes the trained unsupervised cross-lingual embeddings (Artetxe et al., 2018) as the interface for encoder and decoder pretraining. Inspired by the success of neural discrete representation learning (Van Den Oord et al., 2017), the latter uses language-independent vector quantized (VQ) embeddings (semantic unites) as the interface to map encoder outputs and decoder inputs into the shared VQ space. Experiments conducted on both supervised and unsupervised translation tasks demonstrate that SemFace effectively connects the pre-trained encoder and decoder, and achieves a significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively. Our contributions ar"
2021.acl-long.348,N19-1423,0,0.0204389,"the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits little in their results. The potential reason is that the cross-attention between the encoder and decoder is not pre-trained, which is randomly initialized when they are connected for fine-tuning, resulting in a lack of semantic interfaces between the pre-trained encoder and decoder. Another line of work attempts to pre-train a sequence-to-sequence model directly, e.g., MASS (Song et al., 2019) and BART (Lewis et al., 2020). But these methods usually"
2021.acl-long.348,2020.acl-main.703,0,0.419451,"d decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-trainin"
2021.acl-long.348,2020.emnlp-main.210,0,0.0184456,"wis et al., 2020) adopted a similar framework and trained the model as a denoising auto-encoder. mBART (Liu et al., 2020) trained BART model on large-scale monolingual corpora in many languages. Although the above work can pre-train the cross-attention of decoder, they are learned on monolingual denoising auto-encoding and cannot learn the corss-lingual transformation between source and target languages. There is also some work trying to explicitly introduce cross-lingual information in a code-switch way during the sequence-to-sequence pre-training, such as CSP (Yang et al., 2020b) and mRASP (Lin et al., 2020). However, their methods need a lexicon or phrase translation table, which is inferred from unsupervised cross-lingual embeddings. Therefore, they depend on the quality of the dictionary. The most similar work to ours is probably the one of DALL·E and CLIP (Radford et al., 2020). DALL·E is a transformer language model that receives both the text and the image as a single stream of data. The core idea is to define the cross-modality interface of image and text, which can generate images from text descriptions. In this paper, to address the above limitations of pretraining methods for NMT, we at"
2021.acl-long.348,2020.tacl-1.47,0,0.349601,"s significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits li"
2021.acl-long.348,N18-1202,0,0.11426,"eddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages:"
2021.acl-long.348,D19-1071,1,0.849776,"(Devlin et al., 2018). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2"
2021.acl-long.348,W18-5446,0,0.0462228,"Missing"
2021.acl-long.348,2020.emnlp-main.208,0,0.0575371,"18). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2020). The former de"
2021.emnlp-main.179,2020.emnlp-main.275,0,0.472754,"ce a more consistent and engaging response. ternal knowledge base during training, guessing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikipedia. Li so as to better understand the input utterance and et al. (2019) use two-step decoding, which first gen2329 erate a response based on context, and then take the generated response and relative knowledge as input to generate a new response. Kim et al. (2020) focus on knowledge selection in dialogue generation by utilizing a sequential latent variable model. Chen et al. (2020) further enhance the selection module with the posterior information. Zhao et al. (2020b) use reinforcement learning to optimize knowledge selection with unlabeled data. Different from their work, our KE-Blender does not take knowledge as input, because knowledge is only used to enhance our model during training. 3 3.1 Method Task Suppose that we have a training set DS = |L| {USi , KSi , RSi }|i=1 , where USi , KSi and RSi are the dialogue context, the external knowledge retrieved from the knowledge base and the response, respectively. In addition to DS , we have a test dataset DP = {UP , RP }"
2021.emnlp-main.179,W19-2310,0,0.0278137,"e dialogue pre-training model. We fine-tune the Blender on Wizard training set without utilizing external knowledge. KG-Blender. We fine-tune Blender on the Wizard training set by concatenating the context and the associated knowledge as the input. In the setting where external knowledge is unavailable, only context is used to generate response. 4.4 Metrics Automatic evaluation metrics: Following Dinan et al. (2019) and Kim et al. (2020), models are measured using the perplexity of the ground4.3 Baselines truth response (PPL) and unigram F1-score (F1). We compare KE-Blender with Blender and KGGhazarian et al. (2019) show that BERT can Blender, also drawing the following state-of-the-art be used to evaluate the generated response. We methods as reference: employ BERT-based evaluation metrics to evaluTransformer (Vaswani et al., 2017) is a stan- ate whether the generated response is knowledgedard transformer model for dialogue generation. It able as supplements to PPL and F1. As shown takes the concatenation of context utterances and in Table 3, the dialogue generation model is first ˆ based on the dithe associated knowledge as input. required to generate response R 2332 Test Seen Model PPL F1 Test Unseen"
2021.emnlp-main.179,2020.acl-main.423,0,0.0156439,"t 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledge to improve the generation ability of seq2seq models given the unseen word. To achieve this, we take Blender (Roller et al., 2020) as our backbone model, and propose two auxiliary training objectives (Figure 1(c)) in"
2021.emnlp-main.179,2020.acl-main.703,0,0.0189288,"he generation ability of seq2seq models given the unseen word. To achieve this, we take Blender (Roller et al., 2020) as our backbone model, and propose two auxiliary training objectives (Figure 1(c)) in fine2.2 Knowledge Grounded Dialogue tuning, dubbed as Knowledge Enhanced Blender Generation (KE-Blender). The first objective is Interpret Masked Word, which predicts the word’s defini- With advances in deep learning, pre-trained lantion based on the context, where the definition is guage models have shown promising results in diaobtained from a knowledge base. The second is logue generation (Lewis et al., 2020; Zhang et al., Hypernym Generation, which predicts the corre- 2020; Roller et al., 2020). To equip the models sponding hypernym of the word given by WordNet. with external knowledge, Zhang et al. (2018) first These two introduced training objectives force the show that adding user profile information is able to model to learn semantic information from the ex- produce a more consistent and engaging response. ternal knowledge base during training, guessing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikip"
2021.emnlp-main.179,P19-1002,0,0.0638107,"Missing"
2021.emnlp-main.179,D19-1005,0,0.0514127,"Missing"
2021.emnlp-main.179,D11-1054,0,0.151504,"Missing"
2021.emnlp-main.179,2020.acl-main.183,0,0.0126138,"of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge can help dialogue generation for different methods. does not appear in the training data of Blender 1 (Roller et al., 2020), leading to poor performance when a user mentions “COVID-19”. As shown"
2021.emnlp-main.179,2020.emnlp-main.523,0,0.0319752,"ender baselines (16.3 and 19.9 PPL). To further verify the effectiveness of our method in real-world scenarios, we collect 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledge to improve the generation ability of seq2seq models given the unseen word. To achieve this, we"
2021.emnlp-main.179,P18-1205,0,0.139303,"dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge can help dialogue generation for different methods. does not appear in the training data of Blender 1 (Roller et al., 2020), leading to poor performance w"
2021.emnlp-main.179,2020.acl-demos.30,0,0.0383602,"f the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge c"
2021.emnlp-main.179,P19-1139,0,0.0221808,"the knowledge available setting and unavailable setting, respectively, which outperforms the Blender baselines (16.3 and 19.9 PPL). To further verify the effectiveness of our method in real-world scenarios, we collect 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledg"
2021.emnlp-main.179,2020.emnlp-main.272,0,0.565196,"ing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikipedia. Li so as to better understand the input utterance and et al. (2019) use two-step decoding, which first gen2329 erate a response based on context, and then take the generated response and relative knowledge as input to generate a new response. Kim et al. (2020) focus on knowledge selection in dialogue generation by utilizing a sequential latent variable model. Chen et al. (2020) further enhance the selection module with the posterior information. Zhao et al. (2020b) use reinforcement learning to optimize knowledge selection with unlabeled data. Different from their work, our KE-Blender does not take knowledge as input, because knowledge is only used to enhance our model during training. 3 3.1 Method Task Suppose that we have a training set DS = |L| {USi , KSi , RSi }|i=1 , where USi , KSi and RSi are the dialogue context, the external knowledge retrieved from the knowledge base and the response, respectively. In addition to DS , we have a test dataset DP = {UP , RP }. Unlike DS , DP does not contain external knowledge, because associated background kno"
2021.emnlp-main.771,P17-1176,0,0.0166798,"ode. To facilitate the study of this task, we create a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the b"
2021.emnlp-main.771,2020.emnlp-main.728,0,0.0161259,"an appropriate message, while the cascaded model fails may due to the error propagation. More examples on multilingual dataset are shown in Appendix D. 6 Related Work Our work is enlightened from two research lines of studies, which are automated code repair and commit message generation. We discuss these topics in the following. projects with numerous buggy-fixes pairs (Tufano et al., 2018; Chen et al., 2019; Vasic et al., 2019; Yasunaga and Liang, 2020). Tufano et al. (2018) first proposed using end-to-end neural machine translation model for learning bug-fixing patches. Besides, Guo et al. (2020) demonstrated that appropriately incorporating the natural language descriptions into the pre-train model could further improve the performance of code repair. Commit Message Generation Early work on automatic commit message generation translates source code changes (such as feature additions and bug repairs) into natural language based on predefined rules and templates (Buse and Weimer, 2010; Cortés-Coy et al., 2014). To overcome the limitation of high complexity and difficult extensibility, some researchers employ information retrieval methods to generate commit messages, which attempts to r"
2021.emnlp-main.771,D17-1158,0,0.0168867,"a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method"
2021.emnlp-main.771,P84-1044,0,0.302985,"Missing"
2021.emnlp-main.771,Q17-1024,0,0.0151993,"ination between 9787 cφ = sof tmax zc zTφ √ H ! zφ (9) the output state of commit message decoder and the gated fusion of context representations, which can be calculated as:  T oc = zc + Wo gδ δ + gζ ζ T  Joint Training We jointly train our model in an end-to-end manner, the overall loss is defined as (13) where LR (θ), LC (θ)and LT (θ) are used to optimize the repaired code generation, commit message generation, and binary sequence classification, respectively. When training multilingual model of fixing code and predicting commit message, following multilingual neural machine translation (Johnson et al., 2017), we mix the training corpus and add a special token (e.g., <java>) at the beginning of each input sequence to distinguish from different programming languages. 4 Train Valid Test Total Multi. Python Java Javascript C-sharp Cpp 36682 11129 21446 5424 8510 4585 1391 2680 678 1063 4586 1392 2681 678 1064 45853 13912 26807 6780 10637 Mono. Java 47775 3000 3000 53775 (12) where Wo ∈ RH×H is the learnable weights. denotes the element-wise product. LJ (θ) = LR (θ) + LC (θ) + LT (θ) Languages Data In this section, we describe the creation of the dataset in detail. We first describe how we collect the"
2021.emnlp-main.771,P17-2045,0,0.116157,"ly reduce debugging costs in software development and helps 1 Introduction programmers to understand the high-level rationale Deep learning has been demonstrated remarkably of changes, a lot of great work has been proposed to adept at numerous natural language processing deal with automated program repair (Tufano et al., (NLP) tasks, such as machine translation (Bah- 2018; Chen et al., 2019; Dinella et al., 2020; Yadanau et al., 2014), relation extraction (Zhang et al., sunaga and Liang, 2020; Tang et al., 2021) and 2017), grammar error correction (Ge et al., 2018), commit message generation (Loyola et al., 2017; and so on. The success of deep learning in NLP Liu et al., 2020; Nie et al., 2020), respectively. also promotes the development of which in pro- However, existing work tackles the two tasks ingramming languages (Clement et al., 2020; Lu dependently, ignoring the underlying relationship et al., 2021). Recently, researchers have exploited between these two closely related tasks, e.g., afdeep learning to programming-language related ter fixing the bug, commit message can record the tasks, such as code completion (Svyatkovskiy et al., process of code repair. Therefore it is crucial to 2020), aut"
2021.emnlp-main.771,P16-1162,0,0.561635,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
2021.emnlp-main.771,2021.findings-acl.111,1,0.811582,"Missing"
2021.emnlp-main.771,P02-1040,0,0.109484,"e, and commit message. The statistics of the dataset used in this paper are summarized in Table 1. More processing details and statistics can be found in Appendix A and Appendix B. We release the datasets at https: //github.com/jqbemnlp/BFCsData. 5 5.1 Experiments Experimental Settings Evaluation Metrics We conduct evaluations on both code repair and commit message generation. For the code repair, we use exact match accuracy (Chen et al., 2018) to measure the percentage of the predicted fixed code that are exactly matching the truth fixed code. In addition, we also introduce the BLEU-4 score (Papineni et al., 2002) as a supplementary metric to evaluate their partial match. For the commit message generation, we use BLEU-4 and Rouge-L (Lin, 2004) to evaluate our model. 3 https://www.githubarchive.org https://docs.github.com/en/ free-pro-team@latest/rest 4 5 https://sites.google.com/view/ learning-fixes/data 9788 Models Naive Method Oracle Method Cascaded Model + Teacher-student + Multitask + Back-translation Joint Model Automated Code Repair BLEU-4 xMatch 87.45 0.00 85.07 3.21 88.23 6.16 87.94 8.33 87.73 5.26 87.61 8.01 Commit Message Generation BLEU-4 ROUGE-L 8.40 7.98 12.64 11.59 9.69 9.41 10.58 10.19 1"
2021.emnlp-main.771,D17-1182,0,0.0279627,"Missing"
2021.emnlp-main.771,W16-2323,0,0.311742,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
C08-1141,E06-1032,0,0.273045,"phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems. 1 Introduction Automatic MT evaluation is a crucial issue for MT system developers. The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Papineni et al., 2002) and its variants. Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation. Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006). For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice. Furthermore, BL"
C08-1141,A00-2019,0,0.0155641,"Missing"
C08-1141,W04-3250,0,0.149378,"Missing"
C08-1141,P04-1077,0,0.0226721,"uding 3,200 pairs of sentences containing 6 classes of check-points. Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically. Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching. There are many recent work motivated by ngram based approach. (Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level. (Lin and Och, 2004) used longest common subsequence and skipbigram statistics. (Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses. (Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment. (Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings. (Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greate"
C08-1141,W05-0904,0,0.0285694,"Missing"
C08-1141,J03-1002,0,0.00842467,"Missing"
C08-1141,P02-1040,0,0.0750984,"Missing"
C08-1141,W00-1212,1,0.735942,"Missing"
C08-1141,P03-1054,0,0.00524886,"Missing"
C08-1141,W07-0738,0,\N,Missing
C08-1141,W05-0909,0,\N,Missing
C10-2084,W06-3123,0,0.0189229,"n phrase. As an example, if there is a simple phrase pair &lt;white house, 白 宫&gt;, then it is transformed into the ITG rule  white house 白宫 . An important question is how these phrase pairs can be formulated. Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. This method suffers from computational complexity because it considers all possible phrases and all their possible alignments. Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence. Our PITG collects all phrase pairs which are consistent with a word alignment matrix produced by a simpler word alignment model. 2.3 HP-ITG : P-ITG with H-Phrase pairs P-ITG is the first enhancement of ITG to capture the linguistic phenomenon that more than one word of a language may function as a single unit, so that these words should be aligned to a single unit of another language. But P-ITG can only treat contiguous words as a sing"
C10-2084,P06-2014,0,0.0363234,"Missing"
C10-2084,W07-0403,0,0.0690132,"ng compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-ba"
C10-2084,J07-2003,0,0.819709,"i et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-based SMT (Chiang, 2007) is proved to be superior to simple phrase-based SMT, it is natural to ask, why don‟t we further incorporate hierarchical phrase pairs (henceforth h-phrase pairs) into ITG? In this paper we propose a ITG formalism and parsing algorithm using h-phrase pairs. The ITG model involves much more parameters. On the one hand, each phrase/h-phrase pair has its own probability or score. It is not feasible to learn these parameters through discriminative/supervised learning since the repertoire of phrase pairs is much larger than the size of human-annotated alignment set. On the other hand, there are als"
C10-2084,P08-1010,0,0.0439096,"Missing"
C10-2084,P05-1057,0,0.196987,"Missing"
C10-2084,W02-1018,0,0.0261841,"irs of phrases (or blocks). That is, a sequence of source language word can be aligned, as a whole, to one (or a sequence of more than one) target language word. These methods can be subsumed under the term phrase-based ITG (P-ITG), which enhances W-ITG by altering the definition of a terminal production to include phrases:  (c.f. Figure 1(c)). stands for English phrase and stands for foreign phrase. As an example, if there is a simple phrase pair &lt;white house, 白 宫&gt;, then it is transformed into the ITG rule  white house 白宫 . An important question is how these phrase pairs can be formulated. Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. This method suffers from computational complexity because it considers all possible phrases and all their possible alignments. Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence. Our PITG collects all phrase pairs which are c"
C10-2084,P06-1065,0,0.0766653,"n initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approx735 imate EM learning process are alternatively iterated until there is no more improvement. The sketch of the semi-supervised training is shown in Figure 5. 4.5 Features for word pairs The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993). 2) Conditional link probability (Moore, 2006). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 4.6 Features for phrase/h-phrase pairs For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-toEnglish directions are estimated and taken as features, in addition to those features in WDITG, in the discriminative model of alignment hypothesis selection: 1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase. 2) : The conditional probability of foreign phrase/h-phrase given English phrase/h-phrase. The features are calculated as described in section 4.3. 5 Evaluatio"
C10-2084,P03-1021,0,0.00775187,"rs in (1) to be learned. The first is the values of the features Ψ. Most features are indeed about the probabilities of the phrase/h-phrase pairs and there are too many of them to be trained from a labeled data set of limited size. Thus the feature values are trained by approximate EM. The other kind of parameters is feature weights λ, which are 734 trained by an error minimization method. The discriminative training of λ and the approximate EM training of Ψ are integrated into a semisupervised training framework similar to EMD3 (Fraser and Marcu, 2006). 4.2 Discriminative Training of λ MERT (Och, 2003) is used to train feature weights λ. MERT estimates model parameters with the objective of minimizing certain measure of translation errors (or maximizing certain performance measure of translation quality) for a development corpus. Given an SMT system which produces, with model parameters , the K-best candidate translations for a source sentence , and an error measure of a particular candidate with respect to the reference translation , the optimal parameter values will be: MERT for DITG applies the same equation for parameter tuning, with different interpretation of the components in the equ"
C10-2084,J04-4002,0,0.405349,"Missing"
C10-2084,C96-2141,0,0.40965,"roximate EM learning of feature values are integrated in a single semi-supervised framework. Given an initial estimation of (estimated from an initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approx735 imate EM learning process are alternatively iterated until there is no more improvement. The sketch of the semi-supervised training is shown in Figure 5. 4.5 Features for word pairs The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993). 2) Conditional link probability (Moore, 2006). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 4.6 Features for phrase/h-phrase pairs For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-toEnglish directions are estimated and taken as features, in addition to those features in WDITG, in the discriminative model of alignment hypothesis selection: 1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase. 2) : The condition"
C10-2084,J97-3002,0,0.87575,"properties for word alignment, it still suffers from the limitation of one-to-one matching. While existing approaches relax this limitation using phrase pairs, we propose a ITG formalism, which even handles units of non-contiguous words, using both simple and hierarchical phrase pairs. We also propose a parameter estimation method, which combines the merits of both supervised and unsupervised learning, for the ITG formalism. The ITG alignment system achieves significant improvement in both word alignment quality and translation performance. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns"
C10-2084,P06-1066,0,0.0698234,"ese word segmentation standard. 250 sentence pairs are used as training data and the other 241 are test data. The large, un-annotated bilingual corpus for approximate EM learning of feature values is FBIS, which is also the training set for our SMT systems. In SMT experiments, our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. The NIST‟03 test set is used as our development corpus and the NIST‟05 and NIST‟08 test sets are our test sets. We use two kinds of state-of-the-art SMT systems. One is a phrase-based decoder (PBSMT) with a MaxEntbased distortion model (Xiong, et al., 2006), and the other is an implementation of hierarchical phrase-based model (HPBSMT) (Chiang, 2007). The phrase/rule table for these two systems is not generated from the terminal node of HPDITG tree directly, but extracted from word alignment matrix (HP-DITG generated) using the same criterion as most phrase-based systems (Chiang, 2007). 5.2 roughly the same in all cases, while W-ITG has the lowest recall, due to the limitation of one-toone matching. The improvement by (simple) phrase pairs is roughly the same as that by hphrase pairs. And it is not surprising that the combination of both kinds o"
C10-2084,J07-3002,0,0.0771218,"0 24.90 34.77 25.25 25.43 35.04 34.82 25.56 Appendix A. The Normal Form Grammar Table 4 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null. 1 2 3 Table 3. Semi-supervised Training Task on BLEU. It can be observed that EMD improves SMT performance in most iterations in most cases. EMD does not always improve BLEU score because the objective function of the discriminative training in EMD is about alignment FMeasure rather than BLEU. And it is well known that the correlation between F-Measure and BLEU (Fraser and Marcu, 2007) is itself an intriguing problem. The best HP-DITG leads to more than 1 BLEU point gain compared with GIZA++ on all datasets/MT models. Compared with BERK, EMD3 improves SMT performance significantly on NIST05 and slightly on NIST08. 6 ity but also machine translation performance significantly. Combining the formalism and semi-supervised training, we obtain better alignment and translation than the baselines of GIZA++ and BERK. A fundamental problem of our current framework is that we fail to obtain monotonic increment of BLEU score during the course of semisupervised training. In the future,"
C10-2084,P05-1059,0,0.113012,"ption. Figure 4(c) shows how the span pair [e1,e3]/[f1,f3] can be generated in two ways: one is combining a phrase pair and a word pair directly, and the other way is replacing the X in the h-phrase pair with a word pair. Here we only show how hphrase pairs with one variable be used during the parsing, and h-phrase pairs with more than one variable can be used in a similar way. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993) probabilities of the word pairs inside and outside a span pair are useful. Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O(n4). Tic-tac-toe pruning method is adopted in this paper. 4 Semi-supervised Training The original formulation of ITG (W-ITG) is a generative model in which the ITG tree of a sentence pair is produced by a set of rules. The parameters of these rules are trained by EM. Certainly it is difficult to add more non-independent features in such a genera"
C10-2084,P09-1104,0,0.47586,"ITG alignment system achieves significant improvement in both word alignment quality and translation performance. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clau"
C10-2084,P08-1012,0,0.0176898,"such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-based SMT (Chiang, 2007) is proved to be super"
C10-2084,W01-1812,0,0.084175,"Missing"
C10-2084,J93-2003,0,\N,Missing
C10-2084,2006.amta-papers.2,0,\N,Missing
C10-2084,P06-1097,0,\N,Missing
C16-1290,J93-2003,0,0.0754472,"a more flexible way, the attention mechanism (Bahdanau et al., 2014) was introduced into the encoderdecoder model. In an attention-based encoder-decoder model, matching scores between the source and target words are calculated based on their corresponding encoder and decoder states. These scores are then normalized and used as weights for the source words given each target word. This can be seen as a soft alignment and the attention mechanism here plays similar role to that of a traditional alignment model. In alignment models used in traditional machine translation models such as IBM Models (Brown et al., 1993), distortion and fertility are modeled explicitly. By comparison, in the attention mechanism, alignment is computed by matching the previous decoder hidden state with all the encoder hidden states, without modeling distortion and fertility. Since the translation of target words is guided by the attention † Work done while author was an undergraduate student of Shanghai Jiao Tong University and intern at Microsoft Research. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3082 Proceedings of COLIN"
C16-1290,J07-2003,0,0.0314165,"Missing"
C16-1290,W14-4012,0,0.00869674,"Missing"
C16-1290,D14-1179,0,0.0670752,"Missing"
C16-1290,P14-1129,0,0.0219979,"Missing"
C16-1290,P09-1104,0,0.0314005,"+ F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figure 2 we can see, R EC ATT generated the correct alignment on the example sentence shown"
C16-1290,D13-1176,0,0.25225,"Missing"
C16-1290,koen-2004-pharaoh,0,0.0197588,"ss the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h0i + zi hi−1 + tanh(V h di ) where di is the coverage vector, ei is the new added extract gate, which is used to update di based on the words that has been translated. di is desig"
C16-1290,D15-1166,0,0.635888,"t the election to be held on January 30th next year would not be an end to serious violence in Iraq.” Figure 2. Our proposed model R EC ATT produced the correct reordering of the source words, and based on that generated a better translation. At position i in the target sentence, the attention model computes a matching score eij with match function α, for the previous decoder state hi−1 and each encoder state sj . eij = v > tanh (α(hi−1 , sj )) exp(eij ) wij = P k exp(eik ) We wrap this computation of weights as ALIGN: wi = ALIGN(hi−1 , sJ1 ) There are various match functions, as analyzed in (Luong et al., 2015). In our paper we use the sum match function α(hi−1 , sj ) = P W α hi−1 + U α sj . The weighted average of the encoder states sJ1 is calculated as the context ci = j wij sj . It is added to the input of each gate in the decoder, together with previous state hi−1 and previous target word embedding yi−1 : hi = RNN(hi−1 , yi−1 , ci ) 3 Problems of the Attention Mechanism Although attention modeling works well in finding translation correspondence between source and target words, there are still some issues that can be systematically identified, which fall into three categories: incorrect reorderi"
C16-1290,J03-1002,0,0.0421655,"e 1. BLEU scores w/o UNK replacement and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test"
C16-1290,P02-1040,0,0.0972389,"Missing"
C16-1290,D15-1044,0,0.0274475,"Missing"
C16-1290,P16-5005,0,0.00482736,"and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figur"
C16-1290,D15-1199,0,0.0707399,"always have full information about the previous alignments even when a longdistance jump happens, which makes the implicit distortion model much more flexible. 4.2 F ERT D EC To address the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h"
D12-1078,2008.amta-srw.1,0,0.406236,"LT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our m"
D12-1078,D08-1092,0,0.169702,"pending on the order of application. Table 7 and 8 show the experiment results of combining FA-PR with IDSG. It can be seen that either way of the combination is better than using FA-PR or IDSG alone. Yet there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. Th"
D12-1078,P11-1003,0,0.0136755,"SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to improve machine translation performance, many methods were proposed to modify word alignment by taking syntactic tree into consideration, including deleting incorrect word alignment links by a discriminative model (Fossum et al., 2008), re-aligning sentence pairs using EM method with the rules extracted with initial alignment (Wang et al., 2010), and removing ambiguous alignment of functional words with constraint from chunk-level information during rule extraction (Wu et al., 2011). Unlike all these pursuits, to generate a consistent word alignment, our method modifies the popularly used IDG symmetrization method to make it suitable for string-to-tree rule extraction, and our method is much simpler and faster than the previous works. 6 Conclusion In this paper we have attempted to improve SSMT by reducing the errors introduced by the mutual independence between monolingual parser and word aligner. Our major contribution is the strategies of re-training parser with the bilingual information in alignment matrices. Either of our proposals of targeted self-training with fro"
D12-1078,P03-1021,0,0.185049,"Missing"
D12-1078,W08-0306,0,0.0913119,"ent and syntactic tree for the target sentence. All the nodes in gray are frontier nodes. Example (a) contains two error links (in dash line), and the syntactic tree for the target sentence of example (b) is wrong. structure by incorrect alignment links, as shown by the two dashed links in Figure 1(a). These two incorrect links hinder the extraction of a good minimal rule “毡房 ” and that of a good composed rule “牧民, 的 NP(DT(the), NN(herdsmen), POS(&apos;s)) ”. By and large, incorrect alignment links lead to translation rules that are large in size, few in number, and poor in generalization ability (Fossum et al, 2008). The second problem is parsing error, as shown in Figure 1(b). The incorrect POS tagging of the word “lectures"" causes a series of parsing errors, including the absence of the noun phrase “NP(NN(propaganda), NN(lectures))”. These parsing errors hinder the extraction of good rules, such as “ 宣 讲 NP(NN(propaganda), NN(lectures)) ”. Note that in Figure 1(a), the parse tree is correct, and the incorrect alignment links might be fixed if the aligner takes the parse tree into consideration. Similarly, in Figure 1(b) some parsing errors might be fixed if the parser takes into consideration the corre"
D12-1078,P06-1121,0,0.207786,"aligner does not consider the syntax information of both languages, and the output links may violate syntactic correspondence. That is, some SL words yielded by a SL parse tree node may not be traced to, via alignment links, some TL words with legitimate syntactic structure. On the other hand, parser design is a monolingual activity and its impact on MT is not well studied (Ambati, 2008). Many good translation rules may thus be filtered by a good monolingual parser. In this paper we will focus on the translation task from Chinese to English, and the string-to-tree SSMT model as elaborated in (Galley et al., 2006). There are two kinds of translation rules in this model, minimal rules, and composed rules, which are composition of minimal rules. The minimal rules are extracted from a special kind of nodes, known as frontier nodes, on TL parse tree. The concept of frontier node can be illustrated by Figure 1, which shows two partial bilingual sentences with the corresponding TL sub-trees and word alignment links. The TL words yielded by a TL parse node can be traced to the corresponding SL words through alignment links. In the diagram, each parse node is represented by a rectangle, showing the phrase labe"
D12-1078,N06-1031,0,0.168264,"there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as"
D12-1078,W04-3250,0,0.317444,"Missing"
D12-1078,J10-2004,0,0.205983,"; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our methods try to learn a suitable grammar for string-to-tree SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to"
D12-1078,J03-1002,0,0.00402198,"p2: Select the one which can generate the biggest frontier set:  step3: Add to , and repeat step 1, until no new link can be added. Like IDG, IDSG starts with all the links in and its main task is to add links selected from . IDSG is also subject to the constraints of IDG. The new criterion in link selection in IDSG is specified in Step 2. Given a parse tree of the TL side of the bilingual sentence, in each iteration IDSG considers the change of frontier set size caused by 858 PP 1 2-6 PP 3-6 1-2 Word Alignment Symmetrization The most widely used word aligners in MT, like HMM and IBM Models (Och and Ney, 2003), are directional aligners. Such aligner produces one set of alignment matrices for the SL-to-TL direction and another set for the TL-to-SL direction. Symmetrization refers to the combination of these two sets of alignment matrices. The most popular method of symmetrization is intersect-diag-grow (IDG). Given a bilingual sentence and its two alignment matrices and IDG starts with all the links in . Then IDG considers each link in in turn. A link is added if its addition does not make some phrase pairs overlap. Although IDG is simple and efficient, and has been shown to be effective in phrase-b"
D12-1078,P10-1049,0,0.118082,"ht be fixed if word aligner and parser are not mutually independent. In this paper, we emphasize more on the correction of parsing errors by exploiting alignment information. The general approach is to re-train a parser with parse trees which are the most consistent with alignment matrices. Our first strategy is to apply the idea of targeted selftraining (Katz-Brown et al., 2011) with the simple evaluation function of frontier set size. That is to re-train the parser with the parse trees which give rise to the largest number of frontier nodes. The second strategy is to apply forced alignment (Wuebker et al., 2010) to bilingual data and select the parse trees generated by our SSMT system for re-training the parser. Besides, although we do not invent a new word aligner exploiting syntactic information, we propose a new method to symmetrize the alignment matrices of two directions by taking parse tree into consideration. 2 Parser Re-training Strategies Most monolingual parsers used in SSMT are trained upon certain tree bank. That is, a parser is trained with the target of maximizing the agreement between its decision on syntactic structure and that decision in the human-annotated parse trees. As mentioned"
D12-1078,P02-1040,0,0.0863916,"Missing"
D12-1078,N07-1051,0,0.0726623,"Missing"
D12-1078,D11-1017,0,\N,Missing
D12-1078,P07-1003,0,\N,Missing
D12-1078,P06-2014,0,\N,Missing
D12-1078,D09-1024,0,\N,Missing
D13-1041,E06-1002,0,0.851221,"tion,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, t"
D13-1041,D07-1074,0,0.971336,"e false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives the model enough flexibility and expressivity. It can include any features that describe the similarity or dissimilarity of context d and candidate entity e. They often perform well even on small training set, with carefullydesigned features. This category falls into the local approach as the decision processes for each mention are made independently (Ratinov et al., 2011). (Cucerzan, 2007) first suggests to optimize an objective function that is similar to the collective ap427 Wikipedians annotate entries in Wikipedia with category network. This valuable information generalizes entity-context correlation to category-context correlation. (Bunescu and Pasca, 2006) utilize category-word as features in their ranking model. (Kataria et al., 2011) employ a hierarchical topic model where each inner node in the hierarchy is a category. Both approaches must rely on pruned categories because the large number of noisy categories. We try to address this problem with recent advances of repr"
D13-1041,D11-1072,0,0.338593,"Missing"
D13-1041,D08-1017,0,0.0165384,"global ranker. The differences are that we use stacking to train the local ranker to handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009;"
D13-1041,P11-1138,0,0.060973,"machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to. The task of linking names to knowledge base is known as entity linking or disambiguation (Ji et al., 2011). The resulting text is populated with semantic rich links to knowledge base like Wikipedia, and ready for various downstream NLP applications. ∗ Corresponding author Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously (Han et al., 2011; Hoffart et al., 2011; Kulkarni et al., 2009; Ratinov et al., 2011). Collective approaches can improve performance when local evidence is not confident enough. They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches (Ratinov et al., 2011). However, collective inference processes are often expensive and involve an exponential search space. We propose a collective entity linking method based on stacking. Stacked generalization (Wolpert, 1992) is a powerful meta learning algorithm that uses two levels of learners. The predictions of the first learner are taken as"
D13-1041,P05-1044,0,0.0476092,"the mention “Romney” as an examFinally, the semantic relatedness measure of two entities ei ,ej is defined as the common in-links of ei and ej in Wikipedia (Milne and Witten, 2008; Han et al., 2011): 430 where W is learned with supervision like clickthrough data. Given training data {(qi , di )}, training is done by randomly sampling a negative target d− . The model optimizes W such that f (q, d+ ) > f (q, d− ). Thus, the training objective is to minimize the following margin-based loss function: ∑ max(0, 1 − f (q, d+ ) + f (q, d− )) (7) q,d+ ,d− which is also known as contrastive estimation (Smith and Eisner, 2005). W can become very large and inefficient when we have a big vocabulary size. This is addressed by replacing W with its low rank approximation: W = UT V + I (8) here, the identity term I is a trade-off between the latent space model and a vector space model. The gradient step is performed with Stochastic Gradient Descent (SGD): U ←U + λV (d+ − d− )q T , if 1 − f (q, d+ ) + f (q, d− ) > 0 (9) − T V ←V + λU q(d − d ) , + if 1 − f (q, d+ ) + f (q, d− ) > 0. (10) where λ is the learning rate. The query and document are not necessary real query and document. In our case, we treat our problem as: gi"
D13-1041,P11-1139,0,0.022453,"handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives"
D13-1041,N10-1072,0,0.661752,"aboratory of Computational Linguistics (Peking University) Ministry of Education,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning"
D13-1107,D11-1033,0,0.0801174,"Missing"
D13-1107,W06-1615,0,0.0632491,"ation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each 1063 task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking o"
D13-1107,J07-2003,0,0.0972138,"nces for testing in each domain. The details are shown in Table 2. Domain Business Ent. Health Sci&Tech Sports Politics Train En Ch 30M 28M 25M 22M 23M 20M 28M 26M 19M 16M 28M 24M Dev En Ch 36K 35K 21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation"
D13-1107,P13-2061,1,0.817113,"Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). ”M” refers to million and"
D13-1107,D08-1072,0,0.0445467,"omain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods"
D13-1107,W10-1757,0,0.0555944,"e Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general"
D13-1107,eck-etal-2004-language,0,0.18163,"g domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply the"
D13-1107,W07-0717,0,0.372942,"dels may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To levera"
D13-1107,W06-1607,0,0.0717034,"efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 R"
D13-1107,D10-1044,0,0.132836,"Missing"
D13-1107,P09-1098,1,0.83532,"addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled mono"
D13-1107,W07-0733,0,0.520345,"riety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we"
D13-1107,W04-3250,0,0.0374174,"++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison."
D13-1107,P06-1096,0,0.264664,"Missing"
D13-1107,D07-1036,0,0.13075,"Missing"
D13-1107,P10-2041,0,0.30299,"hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult t"
D13-1107,J03-1002,0,0.00587202,"21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We h"
D13-1107,P03-1021,0,0.0335982,"ser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to G"
D13-1107,P02-1040,0,0.0869864,"rithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-t"
D13-1107,P12-1099,0,0.144964,"s often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptati"
D13-1107,E12-1055,0,0.187281,"lation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise"
D13-1107,P12-1002,0,0.446041,"ased on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual 1056 adaptation approach towards each domain. The rest of the paper is organized as follows: The proposed approach is explain"
D13-1107,P07-1004,0,0.348402,"nd second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adapt"
D15-1106,J08-1001,0,0.0939993,"Missing"
D15-1106,J97-3002,0,0.0402534,"of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we"
D15-1106,D14-1218,0,0.181084,"the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural network"
D15-1106,P06-1066,0,0.0156601,"3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we can find that the rerank system improves SMT performance consistently. For a single sentence without the"
D15-1106,P14-1140,1,0.746216,"Missing"
D15-1106,P13-1017,1,0.460566,"olov et al., 2010) uses a hidden layer which employs a real-valued vector recurrently as network’s input to keep as many history as possible. This makes RNNLM be able to extend for capturing history beyond a sentence. To prevent the potential exponential decay of the history, the history length in RNN can not be too long. Here we approximate the history information of previous sentences, p(Sk |S1 , S2 , ..., Sk−1 ), by the following: For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2 NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying"
D15-1106,P03-1021,0,0.0435395,"two Chinese sentence in one document together with their correct translation: 我 拍摄 过 的 冰山, 有些 冰 是 非常 年 轻 - - 几千 年 年龄 Some of the ice in the icebergs that I photograph is very young - - a couple thousand years old. 有些 冰 超过 十万 年 And some of the ice is over 100,000 years old. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document. Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank905 Chinese word “ 有 些” means “some” in English. But when it is used in parallelism sentences, it means “some of” instead of “some”. The traditional SMT system translates the italics part without considering the context. The translation result for this kind"
D15-1106,P02-1040,0,0.10845,"Missing"
D15-1106,D13-1170,0,0.00405147,"dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. Related work An attempt of introducing RNN into convolutional neural networ"
D15-1106,2012.eamt-1.60,0,\N,Missing
D17-1175,P16-1231,0,0.218022,"ies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsi"
D17-1175,D14-1082,0,0.83352,"ntroduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq mo"
D17-1175,D14-1179,0,0.0403433,"Missing"
D17-1175,P16-2006,0,0.024372,". We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve compa"
D17-1175,D16-1137,0,0.0605328,"Missing"
D17-1175,P81-1022,0,0.754027,"Missing"
D17-1175,P15-1033,0,0.472757,"arning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information,"
D17-1175,E17-1063,0,0.0894503,"been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key r"
D17-1175,P11-2033,0,0.106709,"LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.53 87.28 85.30 87.41 85.40 88.06 86.30 87.97 86.18 Table 1: Results of various state-of-the-art parsing systems on English dataset (PTB with Stanford Dependencies) and Chinese dataset (CTB). The numbers reported from different systems are taken from: Z&N11 (Zhang and Nivre, 2011); C&M14 (Chen and Manning, 2014); ConBSO (Wiseman and Rush, 2016); Dyer15 (Dyer et al., 2015); Weiss15 (Weiss et al., 2015); K&G16 (Kiperwasser and Goldberg, 2016); DENSE (Zhang et al., 2017). p 6/(drow + dcol ), where drow and dcol are the number of rows and columns (Glorot and Bengio, 2010). Our models are trained on a Tesla K40m GPU and optimized with vanilla SGD algorithm with mini-batch size 64 for English dataset and 32 for Chinese dataset. The initial learning rate is set to 2 and will be halved when unlabeled attachment scores (UAS) on the development set do not increase for 900 batche"
D17-1175,Q16-1023,0,0.291671,"s on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-"
D17-1175,J93-2004,0,0.0581776,"Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |y<i , h) = P k exp (gk ) ∗ I(yk ) (9) where gi is the ith element of g(yi−1 , zi , ci ). 4 Experiments In this section, we evaluate our parsing model on the English and Chinese datasets. Following Dyer et al. (2015), Stanford Dependencies (de Marneffe and Manning, 2008) conversion of the Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vector"
D17-1175,W04-0308,0,0.0771375,"Ua ht + Sa st ) i,t = va tanh(Wa [zi−1 ; ci (7) where W m is the weight matrix. With this network structure, we obtain different context vectors (c1i , c2i , . . . , cli ), and the final context vector ci , which is considered as complex context information, is replaced by the concatenation of those vectors: ci = [c1i ; c2i ; . . . ; cli ]. Decoder: Unlike machine translation and text summarization in which seq2seq model is widely applied, a sequence of action in dependency parsing must satisfy some constraints so that they can generate a dependency tree. Following the arcstandard algorithm (Nivre, 2004), the precondition can be categorized as 1) SHIFT(SH): There exists at least one word that is not pushed into the stack; 2) LEFT-ARC(LR(d)) and RIGHT-ARC(RR(d)): There are at least two words in the stack. These two constraints can be defined as indicator functions  yi = SH, Wc ≤ 0  0 0 yi = LR(d) or RR(d), Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |"
D17-1175,D14-1162,0,0.087559,"d Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vectors (Pennington et al., 2014) to initialize our word embedding matrix. Other model parameters are initialized using a normal distribution with a mean of 0 and a variance of 1679 Parser Z&N11 C&M14 ConBSO Dyer15 Weiss15 K&G16 DENSE seq2seq Our model Ensemble PTB-SD Dev Test UAS LAS UAS LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.5"
D17-1175,D15-1044,0,0.0506612,"ributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key role in classic transition-based or graph-based dependency parsing models, cannot be explicitly employed. For example, classic transition-based parsing algorithm utilizes a stack to manage the heads of partial s"
D17-1175,P15-1032,0,0.458119,"tiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-ba"
D19-1071,P18-1073,0,0.146796,"e translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pr"
D19-1071,D18-1399,0,0.0526068,"e translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pr"
D19-1071,D18-1549,0,0.210576,"l unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. The"
D19-1071,W03-0301,0,0.0610666,"model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with Lample and Conneau (2019) (with the significance level of p<0.01). 4.3 4.4 Evaluation of Cross-lingual Pre-trained Encoder Word Alignment To evaluate the cross-lingual modeling capacity of our pre-trained models, we first conduct experiments on the English-French (en-fr) dataset of the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003). Given two parallel sentences in English and French respectively, we feed each sentence into the pre-trained cross-lingual encoder and get its respective outputs. Then, we calculate the similarities between the outputs of the two sentences and choose target words with max similarity scores as the alignments of correUsage of Pre-trained Decoder As mentioned in Section 3.4, it is interesting to explore the different usage of pre-trained decoders in the MT task. According to our intuition, directly using the pre-trained model as the decoder may not work well because parameters of the decoder nee"
D19-1071,Q17-1010,0,0.0324462,"ge Model (CMLM) which is to predict the translation candidates of randomly masked n-grams. The last step is to leverage the pre-trained cross-lingual language models as the encoder and decoder of a neural machine translation model and fine-tune the translation model iteratively. 3 3.1 Method 3.2 N-gram Translation Table Inferring Following previous work of unsupervised machine translation (Artetxe et al., 2018b; Lample et al., 2018; Ren et al., 2019), given two languages X and Y , we build our n-gram translation tables as follows. First, we obtain monolingual n-gram embeddings using fastText (Bojanowski et al., 2017) and then get cross-lingual n-gram embeddings using vecmap (Artetxe et al., 2018a) in a fully unsupervised way. Based on that, we calculate the similarity score of n-grams x and y in two languages respectively with the marginal-based scoring method (Conneau et al., 2017; Artetxe and Schwenk, 2018). Specifically, given the crosslingual embeddings of x and y, denoted as ex and ey , the similarity score is calculated as: Overview Our method can be divided into three steps as shown in Figure 2. Given two languages X and Y , we first get unsupervised cross-lingual n-gram embeddings of them, from wh"
D19-1071,P12-1017,0,0.0214834,"ed BPE space during their pre-training method, which is inexplicit and limited. Therefore, we figure out a new pre-training method that gives the model more explicit and stronger cross-lingual information. our model to learn both monolingual and crosslingual information during pre-training. Besides, we find the two modifications(translation prediction and n-gram mask) made by CMLM have nearly equal contributions to the translation performance, except for en2de, where the “n-gram mask” has little influence. 5 Related Work Unsupervised machine translation dates back to Klementiev et al. (2012); Nuhn et al. (2012), but becomes a hot research topic in recent years. As the pioneering methods, Artetxe et al. (2017); Lample et al. (2017); Yang et al. (2018) are mainly the modifications of the encoder-decoder structure. The core idea is to constrain outputs of encoders of two languages into a same latent space with a weight sharing mechanism such as using a shared encoder. Denoising auto-encoder (Vincent et al., 2010) and adversarial training methods are also leveraged. Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervis"
D19-1071,J93-2003,0,0.135968,"training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pre-training objective called Cross-lingual Masked Language Model (CMLM), which masks the input n-grams randomly and predicts their corresponding n-gram translation candidates inferred above. To solve the mismatch between different lengths of the masked source and predicted target n-grams, IBM models are introduced (Brown et al., 1993) to derive the training loss at each Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the crosslingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pa"
D19-1071,J03-1002,0,0.0130581,"Unsupervised Machine Translation Taking two cross-lingual language models pretrained with the above method as the encoder and decoder, we build an initial unsupervised neural machine translation model. Then, we train the model with monolingual data until convergence via denoising auto-encoder and iterative backtranslation, as described in Artetxe et al. (2017); Lample et al. (2017, 2018). Different from them, we step further and make another iteration with updated n-gram translation tables. Specifically, we translate the monolingual sentences with our latest translation model and run GIZA++ (Och and Ney, 2003) on the generated pseudo parallel data to extract updated n-gram translation pairs, which are used to tune the encoder as Section 3.3, together with the back-translation within a multi-task learning framework. Experimental results show that running another iteration can further improve the translation performance. It is also interesting to explore the usage of pre-trained decoders in the translation model. It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders (Lample and Conneau, 2019), one reason for which could be that the encoder-to-dec"
D19-1071,D18-1269,0,0.343657,"that our method can produce better cross-lingual representations and significantly improve the performance of unsupervised machine translation. Our contributions are listed as follows. 2.2 • We propose a novel cross-lingual pre-training method to incorporate explicit cross-lingual information into pre-trained models, which significantly improves the performance of unsupervised machine translation. Based on BERT, Lample and Conneau (2019) propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification (Conneau et al., 2018), machine translation, and unsupervised cross-lingual word embedding. The basic points of XLM are mainly two folds. The first one is to use a shared vocabulary of BPE (Sennrich et al., 2016b) to provide potential crosslingual information between two languages just as mentioned in Lample et al. (2018), in an inexplicit way though. The second point is a newly proposed training task called Translation Language Modeling (TLM), which extends MLM by concatenating parallel sentences into a single training text stream. In this way, the model can leverage the cross-lingual information provided by paral"
D19-1071,P16-1009,0,0.717545,"ting, China § Microsoft Research Asia, Beijing, China † {shuoren,mashuai}@buaa.edu.cn § {Wu.Yu,shujliu,mingzhou}@microsoft.com Abstract performance as pointed in Lample et al. (2018), Artetxe et al. (2018b) and Ren et al. (2019). Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are"
D19-1071,P16-1162,0,0.79878,"ting, China § Microsoft Research Asia, Beijing, China † {shuoren,mashuai}@buaa.edu.cn § {Wu.Yu,shujliu,mingzhou}@microsoft.com Abstract performance as pointed in Lample et al. (2018), Artetxe et al. (2018b) and Ren et al. (2019). Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are"
D19-1071,P18-1005,0,0.0186214,"the model more explicit and stronger cross-lingual information. our model to learn both monolingual and crosslingual information during pre-training. Besides, we find the two modifications(translation prediction and n-gram mask) made by CMLM have nearly equal contributions to the translation performance, except for en2de, where the “n-gram mask” has little influence. 5 Related Work Unsupervised machine translation dates back to Klementiev et al. (2012); Nuhn et al. (2012), but becomes a hot research topic in recent years. As the pioneering methods, Artetxe et al. (2017); Lample et al. (2017); Yang et al. (2018) are mainly the modifications of the encoder-decoder structure. The core idea is to constrain outputs of encoders of two languages into a same latent space with a weight sharing mechanism such as using a shared encoder. Denoising auto-encoder (Vincent et al., 2010) and adversarial training methods are also leveraged. Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervised machine translation, some following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b;"
D19-1192,W15-4640,0,0.0649058,"response selection, and obtained better results in a comparison with STC. A common practice for multi-turn retrieval-based chatbots ﬁrst retrieve candidates from a large index with a heuristic context rewriting method. For example, (Wu et al., 2017) and (Yan et al., 2016) reﬁne the last utterance by appending keywords in history, and retrieve candidates with the reﬁned utterance. Then, response selection methods are applied to measure the relevance between history and candidates. A number of studies about generation-based chatbots have considered multi-turn response generation. Sordoni et al. (2015) is the pioneer of this type of research, it encodes history information into a vector and feeds to the decoder. Shang et al. (2015) propose three types of attention to utilize the context information. In addition, Serban et al. (2016) propose a Hierarchical Recurrent EncoderDecoder model (HRED), which employs a hierarchical structure to represent the context. After that, latent variables (Serban et al., 2017b) and hierarchical attention mechanism (Xing et al., 2018) have been introduced to modify the architecture of HRED. Compared to previous work, the originality of this study is that it pro"
D19-1192,P16-1154,0,0.0364321,"le us to employ a single-turn framework to solve the multi-turn conversation task. The singleturn conversation technology is more mature than the multi-turn conversation technology, which is able to achieve higher responding accuracy. To this end, we propose a context rewriting network (CRN) to integrate the key information of the context and the original last utterance to build a rewritten one, so as to improve the answer performance. Our CRN model is a sequenceto-sequence network (Ilya Sutskever, 2014) with a bidirectional GRU-based encoder, and a GRUbased decoder enhanced with the CopyNet (Gu et al., 2016), which helps the CRN to directly copy words from the context. Due to the absence of the real written last utterance, unsupervised methods are used with two training stages, a pre-training stage with pseudo rewritten data, and a ﬁne-tuning stage using reinforcement learning (RL) (Sutton et al., 1998) to maximize the reward of the ﬁnal answer. Without the pre-training part, RL is unstable and slow to converge, since the randomly initialized CRN model cannot generate reasonable rewritten last utterance. On the other hand, only the pre-training part is not enough, since the pseudo data may contai"
D19-1192,N16-1014,0,0.522815,"ntext modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots. 1 Rewritten-Utterance: Why hate drinking coffee? It`s tasty. Response Generation/Retrieval Model Response: I think coffee is so bitter. Figure 1: An Example of Context Rewriting. Introduction Recent years have witnessed remarkable progress in open domain conversation (non-task oriented dialogue system) (Ji et al., 2014; Li et al., 2016a) due to the easy-accessible conversational data and the development of deep learning techniques (Bahdanau et al., 2014). One of the most difﬁcult problems for open domain conversation is how to model the conversation context. A conversation context is composed of multiple utterances, which raises some challenges not existing in the sentence modeling, including: 1) topic transition; 2) plenty of coreferences (he, him, she, it, they); and 3) long term dependency. To tackle these problems, existing works either reﬁne the context by appending keywords to the last turn utterance (Yan et al., 2016"
D19-1192,P16-1094,0,0.218345,"ntext modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots. 1 Rewritten-Utterance: Why hate drinking coffee? It`s tasty. Response Generation/Retrieval Model Response: I think coffee is so bitter. Figure 1: An Example of Context Rewriting. Introduction Recent years have witnessed remarkable progress in open domain conversation (non-task oriented dialogue system) (Ji et al., 2014; Li et al., 2016a) due to the easy-accessible conversational data and the development of deep learning techniques (Bahdanau et al., 2014). One of the most difﬁcult problems for open domain conversation is how to model the conversation context. A conversation context is composed of multiple utterances, which raises some challenges not existing in the sentence modeling, including: 1) topic transition; 2) plenty of coreferences (he, him, she, it, they); and 3) long term dependency. To tackle these problems, existing works either reﬁne the context by appending keywords to the last turn utterance (Yan et al., 2016"
D19-1192,D16-1127,0,0.237527,"ntext modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots. 1 Rewritten-Utterance: Why hate drinking coffee? It`s tasty. Response Generation/Retrieval Model Response: I think coffee is so bitter. Figure 1: An Example of Context Rewriting. Introduction Recent years have witnessed remarkable progress in open domain conversation (non-task oriented dialogue system) (Ji et al., 2014; Li et al., 2016a) due to the easy-accessible conversational data and the development of deep learning techniques (Bahdanau et al., 2014). One of the most difﬁcult problems for open domain conversation is how to model the conversation context. A conversation context is composed of multiple utterances, which raises some challenges not existing in the sentence modeling, including: 1) topic transition; 2) plenty of coreferences (he, him, she, it, they); and 3) long term dependency. To tackle these problems, existing works either reﬁne the context by appending keywords to the last turn utterance (Yan et al., 2016"
D19-1192,D16-1230,0,0.0474865,"uct the training dataset (5,591,794 utterance-response pairs) for the single-turn generation model. We do not use the rewritten context as the input in the training phase, since we would like to guarantee the gain only comes from the rewriting mechanism at the inference stage. 4.2.2 Evaluation Metrics We regard the human response as the ground truth, and use the following metrics: Word overlap based metrics: We report BLEU score (Papineni et al., 2002) between model outputs and human references. Embedding based metrics: As BLEU is not correlated with the human annotation perfectly, following (Liu et al., 2016), we employ embedding based metrics, Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) to evaluate results. The word2vec is trained on the training data set, whose dimension is 200. Diversity: We evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct1 and Distinct-2 (Li et al., 2016a). Human Annotation: We ask three native speakers to annotate the quality of generated responses. We compare the quality of our model with HRED and S2SA. We conduct 5-scale rating: +3, +2, +1, 0 and -1. +"
D19-1192,P02-1040,0,0.106341,"can be found in Supplementary and we set the same sizes of hidden states and embedding in all models. We regard two adjacent utterances in our training data to construct the training dataset (5,591,794 utterance-response pairs) for the single-turn generation model. We do not use the rewritten context as the input in the training phase, since we would like to guarantee the gain only comes from the rewriting mechanism at the inference stage. 4.2.2 Evaluation Metrics We regard the human response as the ground truth, and use the following metrics: Word overlap based metrics: We report BLEU score (Papineni et al., 2002) between model outputs and human references. Embedding based metrics: As BLEU is not correlated with the human annotation perfectly, following (Liu et al., 2016), we employ embedding based metrics, Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) to evaluate results. The word2vec is trained on the training data set, whose dimension is 200. Diversity: We evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct1 and Distinct-2 (Li et al., 2016a). Human Annotation: We ask three native sp"
D19-1192,D11-1054,0,0.0424878,"s and noise, which restricts the performance of our CRN. We evaluate our method with four tasks, including the rewriting quality, the multi-turn response generation, the multi-turn response selection, and the end-to-end retrieval-based chatbots. Empirical results show that the outputs of our method are closer to human references than baselines. Besides, the rewriting process is beneﬁcial to the endto-end retrieval-based chatbots and the multi-turn response generation, and it shows slightly positive effect on the response selection. 2 Related Work Recently, data-driven approaches for chatbots (Ritter et al., 2011; Ji et al., 2014) has achieved promising results. Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Zhou et al., 2016) and generation-based methods (Shang et al., 2015; Serban et al., 2016; Vinyals and Le, 2015; Li et al., 2016a,b; Xing et al., 2017; Serban et al., 2017a). Early research into retrieval-based chatbots (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015) only considers the last utterances and ignores previous ones, which is also called Short Text Conversation (STC). Recently, several studies"
D19-1192,P15-1152,0,0.057275,"response selection, and obtained better results in a comparison with STC. A common practice for multi-turn retrieval-based chatbots ﬁrst retrieve candidates from a large index with a heuristic context rewriting method. For example, (Wu et al., 2017) and (Yan et al., 2016) reﬁne the last utterance by appending keywords in history, and retrieve candidates with the reﬁned utterance. Then, response selection methods are applied to measure the relevance between history and candidates. A number of studies about generation-based chatbots have considered multi-turn response generation. Sordoni et al. (2015) is the pioneer of this type of research, it encodes history information into a vector and feeds to the decoder. Shang et al. (2015) propose three types of attention to utilize the context information. In addition, Serban et al. (2016) propose a Hierarchical Recurrent EncoderDecoder model (HRED), which employs a hierarchical structure to represent the context. After that, latent variables (Serban et al., 2017b) and hierarchical attention mechanism (Xing et al., 2018) have been introduced to modify the architecture of HRED. Compared to previous work, the originality of this study is that it pro"
D19-1192,N15-1020,0,0.0215662,"response selection, and obtained better results in a comparison with STC. A common practice for multi-turn retrieval-based chatbots ﬁrst retrieve candidates from a large index with a heuristic context rewriting method. For example, (Wu et al., 2017) and (Yan et al., 2016) reﬁne the last utterance by appending keywords in history, and retrieve candidates with the reﬁned utterance. Then, response selection methods are applied to measure the relevance between history and candidates. A number of studies about generation-based chatbots have considered multi-turn response generation. Sordoni et al. (2015) is the pioneer of this type of research, it encodes history information into a vector and feeds to the decoder. Shang et al. (2015) propose three types of attention to utilize the context information. In addition, Serban et al. (2016) propose a Hierarchical Recurrent EncoderDecoder model (HRED), which employs a hierarchical structure to represent the context. After that, latent variables (Serban et al., 2017b) and hierarchical attention mechanism (Xing et al., 2018) have been introduced to modify the architecture of HRED. Compared to previous work, the originality of this study is that it pro"
D19-1192,D13-1096,0,0.0258771,"hatbots and the multi-turn response generation, and it shows slightly positive effect on the response selection. 2 Related Work Recently, data-driven approaches for chatbots (Ritter et al., 2011; Ji et al., 2014) has achieved promising results. Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Zhou et al., 2016) and generation-based methods (Shang et al., 2015; Serban et al., 2016; Vinyals and Le, 2015; Li et al., 2016a,b; Xing et al., 2017; Serban et al., 2017a). Early research into retrieval-based chatbots (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015) only considers the last utterances and ignores previous ones, which is also called Short Text Conversation (STC). Recently, several studies (Lowe et al., 2015; Yan et al., 2016; Wu et al., 2017, 2018b) have investigated multi-turn response selection, and obtained better results in a comparison with STC. A common practice for multi-turn retrieval-based chatbots ﬁrst retrieve candidates from a large index with a heuristic context rewriting method. For example, (Wu et al., 2017) and (Yan et al., 2016) reﬁne the last utterance by appending keywords in history,"
D19-1192,D18-1397,0,0.0208566,"d model, we leverage the reinforcement learning method to build the direct connection between the context rewrite model CRN and different tasks. We ﬁrst generate rewritten utterance candidates qr with our pre-trained model, and calculate the reward R(qr ) which will be maximized to optimize the network parameters of our CRN. Due to the discrete choices of words in sequential generation, the policy gradient is used to calculate the gradient. ∇θ J(θ) = E[R · ∇ log(P (yt |x))] (12) For reinforcement learning in sequential generation task, instability is a serious problem. Similar to other works (Wu et al., 2018a), we combine MLE training objective with RL objective as Lcom = L∗rl + λLM LE (13) where λ is a harmonic weight. By directly maximizing the reward from end tasks (response generation and selection), we hope that our CRN can correct the errors in the pseudo data and generate better rewritten last utterance. Two different rewards are used to ﬁne-tune our CRN for the tasks of response generation and selection respectively. We will introduce them in detail in the following. 3.3.1 End-to-end response generation reward Similar as we do in Section 3.2.2, for end-to-end response generation task, we"
D19-1192,C18-1206,0,0.121634,"Missing"
D19-1192,D16-1036,0,0.076017,"the end-to-end retrieval-based chatbots. Empirical results show that the outputs of our method are closer to human references than baselines. Besides, the rewriting process is beneﬁcial to the endto-end retrieval-based chatbots and the multi-turn response generation, and it shows slightly positive effect on the response selection. 2 Related Work Recently, data-driven approaches for chatbots (Ritter et al., 2011; Ji et al., 2014) has achieved promising results. Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Zhou et al., 2016) and generation-based methods (Shang et al., 2015; Serban et al., 2016; Vinyals and Le, 2015; Li et al., 2016a,b; Xing et al., 2017; Serban et al., 2017a). Early research into retrieval-based chatbots (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015) only considers the last utterances and ignores previous ones, which is also called Short Text Conversation (STC). Recently, several studies (Lowe et al., 2015; Yan et al., 2016; Wu et al., 2017, 2018b) have investigated multi-turn response selection, and obtained better results in a comparison with STC. A common practice for multi-turn retri"
D19-1192,P18-1103,0,0.0134652,"which is created by crawling a popular Chinese forum, the Douban Group 5 , covering various topics. Its training set contains 0.5 million conversational sessions, and the validation set contains 50,000 sessions. The negative instances in both sets are randomly sampling with a 1:1 positivenegative ratio. The test set contains 1000 conversation contexts, and each context has 10 response candidates with human annotations. We split the last utterance from each context in the training data, and forms 0.5 million of (q, r) pairs. Subsequently, we train a single-turn Deep Attention Matching Network (Zhou et al., 2018) consuming the pair as an input, which is denoted as DAMsingle . The DAMsingle model is treated as a rank model in Section 3.2.2 and a reward function in Section 3.3.2. In the testing stage, we use the CRN and the DAMsingle to assign a score for each candidate. Notably, the original DAM takes a context-response pair as an input, which is set as a baseline method. The parameters of the DAM is the same as its original paper. 1840 5 https://www.douban.com/group/explore MAP MRR P@1 R10 @1 R10 @2 R10 @5 TF-IDF (Lowe et al., 2015) RNN (Lowe et al., 2015) CNN (Lowe et al., 2015) LSTM (Lowe et al., 20"
D19-1192,P17-1046,1,0.277364,"r idea. The last utterance contains the word “it” which refers to the coffee in context. Moreover, “Why?” is an elliptical interrogative sentence, which is a shorter form of “Why hate drinking coffee?”. We rewrite the context and yield a self-contained utterance “Why hate drinking coffee? It’s tasty.” Compared to previous methods, our method enjoys the following advantages: 1) The rewriting process is friendly to the retrieval stage of retrievalbased chatbots. Retrieval-based chatbots consists of two components: candidate retrieval and candidate reranking. Traditional works (Yan et al., 2016; Wu et al., 2017) pay little attention to the 1834 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1834–1844, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics retrieval stage, which regards the entire context or context rewritten with heuristic rules as queries so noise is likely to be introduced; 2) It makes a step toward explainable and controllable context modeling, because the explicit context rewriting results are easy to debug and analyze. 3) Rewrit"
I11-1072,P07-2026,0,0.0161257,"not guarantee that the translations obtained in each iteration are good e643 T-MERT, we observe that the performance is always the best when the inner-MERT terminates as Hi reaches peak1 . 3.4 Table 1: Statistics on development and test data sets. DATA S ET MT03 MT05 MT06 MT08 Hypothesis Selection with Minimum Bayes Risk(MBR) From T-MERT algorithm, we can get M different results from M outer-translation rounds. Due to intrinsic property and the randomness in MERT, the results from outer-translation step of T-MERT are not quite stable, making the hypotheses selection a necessity. According to (Ehling et al., 2007), for each source sentence with N different translations, we could select the final translation based on the following Minimum Bayes Risk principal: eˆ = arg min{ e (P r(e0 |f ) · (1 − BLEU (e0 , e)))} (6) Here P r(e0 |f ) denotes the posterior probability for translation e0 and BLEU (e0 , e) represents the sentence-level BLEU score for e0 using e as reference. However, since the translation hypotheses are generated under different groups of weights, the corresponding posterior probability is no longer comparable. Here we simplify this problem under the assumption that all available translatio"
I11-1072,P07-1007,0,0.0299923,", since overtraining is serious due to the limited size of development data. In this work, we use hyper-parameter to indicate the overtraining in the estimation step. Finally, our method is more efficient than adaptation on the translation & language model. In Ueffing et al.(2007), training model building is necessary for each round, which is time consuming. By comparison, the running time is much shorter for our method, since no model building is required, although it is still longer than simple onepass translation under baseline. the previous works of transductive learning(Liu et al., 2010; Chan and Ng, 2007), the unlabeled data can be used to improve the model training so as to tackle the bias-estimation problem. Under such framework, the weight learned on both development and test dataset, in which the test dataset is constructed using n-best translations as pseudo references, moves towards the test data with regularization of development data, which alleviates the overtraining in normal MERT and matches the test data better. The remainder of this paper is structured as follows: Related works on model adaptation in SMT are presented in Section 2, and our transductive MERT is proposed in Section"
I11-1072,P05-1033,0,0.0256378,". We build the 5-gram language model on the English section of all bilingual training data together with the Xinhua portion of the English Gigaword corpus. The development and test dataset pairs are selected from NIST2003 (MT03), NIST2005 (MT05), NIST2006 (MT06) and NIST2008 (MT08). The data statistics are shown in Table 1. In the experiments, all translation results are measured in case-insensitive BLEU scores (Papineni et al., 2002). X e0 #S ENTENCE 919 1,082 1,664 1,357 Experimental Results Experimental Setup In the experiments, we re-implement a hierarchical phrase-based decoder based on (Chiang, 2005). The word alignment is trained by GIZA++ under an intersect-diag-grow heuristics refinement. The plain phrases are extracted from all bilingual training data available from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, 1 And we find that in experiments, hyper-parameter Hi of the second round is always maximal. 644 vor those incorrectly generated translation references, which makes the overtraining more serious and hurts the final performance. By applying the hyper-parameter as the stop metric, we could control the learning procedure to avoid the overtraining. We can also review the roles"
I11-1072,W07-0717,0,0.041765,"Missing"
I11-1072,2005.eamt-1.19,0,0.0290747,"tion 2, and our transductive MERT is proposed in Section 3. Experimental results are shown in Section 4, followed by conclusions and future work in the last section. 2 Related Work Model adaptation in SMT has attracted increasing attentions in recent years. As mentioned in the previous section, corresponding to the two steps in SMT pipeline, there are two directions for adaptations. The first one is feature adaptation, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010"
I11-1072,W04-3250,0,0.128511,"Missing"
I11-1072,C10-1075,1,0.782011,"et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing sour"
I11-1072,D07-1036,0,0.0469544,"Missing"
I11-1072,D09-1074,0,0.0149196,"ntal results are shown in Section 4, followed by conclusions and future work in the last section. 2 Related Work Model adaptation in SMT has attracted increasing attentions in recent years. As mentioned in the previous section, corresponding to the two steps in SMT pipeline, there are two directions for adaptations. The first one is feature adaptation, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given t"
I11-1072,2009.eamt-1.22,0,0.0146165,"on, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also pro"
I11-1072,P03-1021,0,0.110377,"t dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing source sentences F1S with corresponding reference translations R1S , the purpose of MERT (Och, 2003) is to find a set of parameters λM 1 which optimizes an automated evaluation metric (e.g., BLEU) under a log-linear model: ˆ M = arg min λ 1 S X ˆ s ; λM ))) (3) (Err(Rs , E(F 1 λM 1 s=1 in which the number of errors in sentence E is obtained by comparing it with a reference sentence R using function Err(R, E) and ˆ s ; λM ) = arg max E(F 1 E S X (λm hm (E, Fs )) (4) s=1 As shown in Algorithm 1, the decoder translates development dataset under current weight(default weight for first round), and generates N-best translation hypotheses for each sentence. The weight is then updated according to e"
I11-1072,P02-1038,0,0.0748791,"ures and their corresponding weights are both important issues in SMT. In this article we focus on the latter issue, i.e., the model parameter adaptation. From the viewpoint of machine learning, development data is labeled data used for parameter learning, while test data is unlabeled and applied for evaluation. In Introduction Machine translation (MT) is the automatic translation from one natural language into another using computer, while SMT is an approach to MT that is characterized by the use of machine learning methods (Lopez, 2008). Nowadays, SMT is usually built on a log-linear model (Och and Ney, 2002), which can be abstracted into two steps: the ∗ Part of this work is done during the first author’s internship at Microsoft Research Asia. 641 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 641–648, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP method is different from their in the following three aspects: Firstly, our method focuses on model parameter adaptation, while Ueffing et al (2007) pays attention to feature adaptation. In their work, the training model is rebuilt by combining original training data with n-best translation output"
I11-1072,P02-1040,0,0.0979398,"archical rules are only extracted from selected data sets, including LDC2003E14, LDC2003E07, LDC2005T10, LDC2006E34, LDC2006E85, and LDC2006E92, which contain about 467K sentence pairs. We build the 5-gram language model on the English section of all bilingual training data together with the Xinhua portion of the English Gigaword corpus. The development and test dataset pairs are selected from NIST2003 (MT03), NIST2005 (MT05), NIST2006 (MT06) and NIST2008 (MT08). The data statistics are shown in Table 1. In the experiments, all translation results are measured in case-insensitive BLEU scores (Papineni et al., 2002). X e0 #S ENTENCE 919 1,082 1,664 1,357 Experimental Results Experimental Setup In the experiments, we re-implement a hierarchical phrase-based decoder based on (Chiang, 2005). The word alignment is trained by GIZA++ under an intersect-diag-grow heuristics refinement. The plain phrases are extracted from all bilingual training data available from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, 1 And we find that in experiments, hyper-parameter Hi of the second round is always maximal. 644 vor those incorrectly generated translation references, which makes the overtraining more serious and h"
I11-1072,C10-2124,0,0.191458,"ce model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing source sentences F1S with corresponding reference translations R1S , the purpose of MERT (Och, 2003) is to find a set of parameters λM 1 which optimizes an automated evaluation metric (e.g., BLEU) under"
I11-1072,P07-1004,0,0.296145,"is an approach to MT that is characterized by the use of machine learning methods (Lopez, 2008). Nowadays, SMT is usually built on a log-linear model (Och and Ney, 2002), which can be abstracted into two steps: the ∗ Part of this work is done during the first author’s internship at Microsoft Research Asia. 641 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 641–648, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP method is different from their in the following three aspects: Firstly, our method focuses on model parameter adaptation, while Ueffing et al (2007) pays attention to feature adaptation. In their work, the training model is rebuilt by combining original training data with n-best translation outputs of development and test data, in order to overcome the data sparseness problem. In contrast, we try to solve the parameter bias-estimated problem using the information of both development and test data. Secondly, the parameter adaptation problem is more complicated in SMT, since overtraining is serious due to the limited size of development data. In this work, we use hyper-parameter to indicate the overtraining in the estimation step. Finally,"
J15-2004,W11-0705,0,0.0607269,"structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Na¨ıve Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Na¨ıve Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to t"
J15-2004,Q13-1005,0,0.00541006,"al of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the"
J15-2004,P14-1091,1,0.77232,"h iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the CYK algorithm that parses sentences in a bottom–up fashion. We use the log-linear model to score candidates generated by beam search. Instead of using question-answ"
J15-2004,P05-1022,0,0.023269,"ssification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; ¨ Kubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our goal is to compute correct"
J15-2004,J07-2003,0,0.0181793,"represent the form of an inference rule as: (r) H1 . . . [i, X, j] HK (8) where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we employ the word terms. Theoretically, we can convert the sentiment rules to CNF versions, and then use the CYK algorithm to conduct parsing. Because the maximum number of non-terminal symbols in a rule is already restricted to two, we formulate the statistical sentiment parsing based on a customized CYK algorithm that is similar to the work of Chiang (2007). Let X, X1 , X2 represent the non-terminals N or P; the inference rules for the statistical sentiment parsing are summarized in Figure 3. 3.3 Ranking Model The parsing model generates many candidate parse trees T(s) for a sentence s. The goal of the ranking model is to score and rank these parse trees. The sentiment tree with the highest score is treated as the best representation for sentence s. We extract a feature vector φ(s, t) ∈ Rd for the specific sentence-tree pair (s, t), where t ∈ T(s) is the parse tree. Let ψ ∈ Rd be the parameter vector for the features. We use the log-linear model"
J15-2004,D08-1083,0,0.669511,"contrast] The negation expressions, intensification modifiers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the P"
J15-2004,D09-1062,0,0.0171966,"Missing"
J15-2004,P10-2050,0,0.0145972,"tect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calculate the sentiment of all the parsed elements in the depe"
J15-2004,W10-2903,0,0.0205801,"parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without a"
J15-2004,W10-3110,0,0.0535975,"Missing"
J15-2004,C10-2028,0,0.0707075,"Missing"
J15-2004,P10-1018,0,0.0204095,"Missing"
J15-2004,J99-4004,0,0.0826633,"In order to tackle the OOV problem, we treat a text span that consists of OOV words as empty text span, and derive them to E. The OOV text spans are combined with other text spans without considering their sentiment information. Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations. We can use the sentiment grammar to compactly describe the derivation process of a sentence. 3.2 Parsing Model We present the formal description of the statistical sentiment parsing model following deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in traditional syntactic parsing. For a concrete example, (A → BC) 274 [i, B, k] [i, A, j] [k, C, j] (6) Dong et al. A Statistical Parsing Framework for Sentiment Classification ∗ ∗ j ∗ which represents if we have the rule A → BC and B ⇒ wki and C ⇒ wk (⇒ is used to represent the reflexive and transitive closure of immediate derivation), then we can ∗ j obtain A ⇒ wi . By adding a unary rule j (A → w i ) [i, A, j] (7) with the binary rule in Equation (6), we can express the standard CYK algorithm for CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start"
J15-2004,P14-1022,0,0.0365465,"Missing"
J15-2004,P97-1023,0,0.045005,"rage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polar"
J15-2004,D07-1115,0,0.0357653,"akamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity stren"
J15-2004,kamps-etal-2004-using,0,0.0896409,"Missing"
J15-2004,W06-1642,0,0.0313257,"adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas"
J15-2004,P06-1115,0,0.0399416,"latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) pr"
J15-2004,P03-1054,0,0.0210265,"ey regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing synt"
J15-2004,R09-1034,0,0.0605772,"Missing"
J15-2004,D12-1069,0,0.0126019,"ount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a n"
J15-2004,J13-2005,0,0.0371933,"Missing"
J15-2004,D09-1017,0,0.0285956,"Missing"
J15-2004,C12-2072,0,0.0397133,"Missing"
J15-2004,P11-1015,0,0.111172,"en widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and th"
J15-2004,J93-2004,0,0.0523085,"Missing"
J15-2004,P05-1012,0,0.100479,"Missing"
J15-2004,P07-1055,0,0.014092,"ated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually cra"
J15-2004,N10-1120,0,0.417582,"Missing"
J15-2004,P04-1035,0,0.1445,"r of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Na¨ıve Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina,"
J15-2004,P05-1015,0,0.976359,"over, the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe these data sets as follows. RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative and 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,"
J15-2004,W02-1011,0,0.0223195,"Missing"
J15-2004,P06-2034,0,0.0277218,"s. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration,"
J15-2004,D12-1110,0,0.0833857,"Missing"
J15-2004,D11-1014,0,0.2693,"Missing"
J15-2004,D13-1170,0,0.308991,"egation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most pre"
J15-2004,J11-2001,0,0.865765,"28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. I"
J15-2004,P11-2100,0,0.0312767,"Missing"
J15-2004,P05-1017,0,0.0398978,"Missing"
J15-2004,P12-2066,0,0.204703,"Missing"
J15-2004,P02-1053,0,0.0173161,"publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created"
J15-2004,J09-3003,0,0.0514213,"Missing"
J15-2004,D10-1102,0,0.041237,"Missing"
J15-2004,W03-1017,0,0.152178,"n this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classif"
J15-2004,D07-1071,0,0.0408997,"t, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the"
J15-2004,P09-1110,0,0.026688,"Missing"
J15-2004,N10-1119,0,\N,Missing
J15-2004,P12-2018,0,\N,Missing
J15-2004,P11-1060,0,\N,Missing
K18-1019,P16-1185,0,0.0373358,"Missing"
K18-1019,J07-2003,0,0.136275,"e whole training process. It confirms that our proposed approach not only stabilizes GAN training but also achieves better results. 31 30 29 BLEU 28 27 26 25 24 23 22 10000 20000 30000 40000 50000 60000 70000 80000 90000 Mini-Batches Figure 2: The BLEU score changes on IWSLT2014 German-English validation set for RNNSearch, Adversarial-NMT* and BGAN-NMT as training progresses. 4.3 We also conduct experiments on Chinese-English translation task with strong SMT and NMT baselines: HPSMT, RNNSearch and AdversarialNMT*. HPSMT is an in-house implementation of the hierarchical phrase-based MT system (Chiang, 2007), where a 4-gram language model is trained using the modified Kneser-Ney smoothing algorithm over the target data from bilingual data. Table 2 shows the evaluation results of different models on NIST datasets. All the results are reported based on case-insensitive BLEU. We can observe that RNNSearch significantly outperforms HPSMT by 4.78 BLEU points on average, and BGAN-NMT can further improve the performances, with 2.33 BLEU points on average. Additionally, our BGAN-NMT gains better performances than Adversarial-NMT* with 1.03 BLEU points on average. These experimental results confirm the ef"
K18-1019,N03-1017,0,0.124311,"Missing"
K18-1019,J82-2005,0,0.733933,"Missing"
K18-1019,D17-1230,0,0.0285585,"ndidates are used as positive and negative examples of discriminator training respectively. Due to the computation cost, we cannot generate many negative examples, so that the discriminator is easy to overfit. The overfitted discriminator will give biased signals to the generator and make it update incorrectly, leading to the instability of the generator training. Wu et al. (2017) found that combining adversarial training objective with MLE can significantly improve the stability of generator training, which is also reported in language model and neural dialogue generation (Lamb et al., 2016; Li et al., 2017). Actually, although this method leverages real translation signal to guide the generator and alleviate the effect of overfitted discriminator, it cannot deal with the inadequate training problem of the discriminator, which essentially plays a more important role in GAN training. cients α1 , α2 , ... , αT calculated with exp (a(ht , zi−1 )) αt = P k exp (a(hk , zi−1 )) (3) where a is a feed-forward neural network with a single hidden layer. 2.1.2 MLE Training NMT systems are usually trained to maximize the conditional log-probability of the correct translation given a source sentence with resp"
K18-1019,P15-1002,0,0.0274615,"optimized using the vanilla SGD algorithm with mini-batch 32 for De-En and 128 for Zh-En. We re-normalize gradients if their norm exceeds 2.0. The initial learning rate is set as 0.2 for De-En and 0.02 for Zh-En, and it is halved when BLEU scores on the validation set do not increase for 20,000 batches. To generate the synthetic bilingual data, beam search strategy with beam size 4 is adopted for both De-En and Zh-En. At test time, beam search is employed to find the best translation with beam size 8 and translation probabilities normalized by the length of the candidate translations. Follow Luong et al. (2015), <unk&gt; is replaced with the corresponding target word in a post processing step. Model Architecture RNNSearch model proposed by Bahdanau et al. (2014) is leveraged to be the translation model, but it should be noted that our BGAN-NMT is independent of the NMT network structure. We use a single layer GRU for encoder and decoder. For Zh-En, the size of word embedding (for both source and target words) is 256 and the size of hidden layer is set to 1024. For De-En, in order to compare with previous work (Ranzato et al., 2015; Bahdanau et al., 2016), the size of word embedding and GRU hidden state"
K18-1019,P02-1040,0,0.103721,"ess: the generator G can be improved with the discriminator D in GAN 1, and then the enhanced G serves as a better discriminator to guide 4 4.1 Experiments Setup To examine the effectiveness of our proposed approach, we conduct experiments on translation 194 Methods MIXER (Ranzato et al., 2015) MRT (Shen et al., 2016) BSO (Wiseman and Rush, 2016) Adversarial-NMT (Wu et al., 2017) A-C (Bahdanau et al., 2016) Softmax-Q (Ma et al., 2017) Adversarial-NMT* BGAN-NMT tasks with two language pairs: German-English (De-En for in short) and Chinese-English (Zh-En for in short). In all experiments, BLEU (Papineni et al., 2002) is adopted as the automatic metric for translation quality evaluation and computed using Moses multi-bleu.perl script. 4.1.1 Dataset Model 21.81 25.84 26.36 27.94 28.53 28.77 28.03 29.17 Table 1: Comparison with previous work on IWSLT2014 German-English translation task. The “Baseline” means the performance of pretrained model used to warmly start training. For German-English translation task, following previous work (Ranzato et al., 2015; Bahdanau et al., 2016), we select data from German-English machine translation track of IWSLT2014 evaluation tasks, which consists of sentence-aligned subt"
K18-1019,P16-1009,0,0.10016,"Missing"
K18-1019,P16-1159,0,0.384953,"the conditional log-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence p"
K18-1019,D16-1137,0,0.33289,"g-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence pair is real or generated."
K18-1019,N18-1122,0,0.04082,"Missing"
N18-1141,D13-1160,0,0.066233,"ning when to regard generated questions as positive instances (collaborative) could improve the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks"
N18-1141,D17-1091,0,0.0227649,", question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algor"
N18-1141,P17-1123,0,0.0582539,"arning of question answering and question generation. Question answering (QA) and question generation (QG) are closely related natural language processing tasks. The goal of QA is to obtain an answer given a question. The goal of QG is almost reverse which is to generate a question from the answer. In this work, we consider answer selection (Yang et al., 2015; Balakrishnan et al., 2015) as the QA task, which assigns a numeric score to each candidate answer, and selects the top ranked one as the answer. We consider QG as a generation problem and exploit sequence-to-sequence learning (Seq2Seq) (Du et al., 2017; Zhou et al., 2017) as the backbone of the QG model. The key idea of this work is that QA and QG are two closely tasks and we seek to leverage the connection between these two tasks to improve both QA and QG. Our primary motivations are twofolds. On one hand, the Seq2Seq based QG model is trained by maximizing the literal similarity between the generated sentence and the ground truth sentence with maximum-likelihood estimation objective function (Du et al., 2017). However, there is no signal indicating whether or not the generated sentence could be correctly answered by the input. This proble"
N18-1141,D17-1090,1,0.740288,"g (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algorithm learns when to"
N18-1141,P16-1154,0,0.0765019,"Missing"
N18-1141,P16-1014,0,0.026262,"vector, header vector, and the cell vector. The backbone of the decoder is an attention based GRU RNN, which generates a word at each time step and repeats the process until generating the end-of-sentence symbol. We made two modifications to adapt the decoder to the table structure. The first modification is that the attention model is calculated over the headers, cells and the caption of a table. Ideally, the decoder should learn to focus on a region of the table when generating a word. The second modification is a table based copying mechanism. It has been proven that the copying mechanism (Gulcehre et al., 2016; Gu 1568 et al., 2016) is an effective way to replicate lowfrequent words from the source to the target sequence in sequence-to-sequence learning. In the decoding process, a word is generated either from the target vocabulary via standard sof tmax or from a table via the copy mechanism. A neural gate gt is used to trade-off between generating from the target vocabulary and copying from the table. The probability of generating a word y calculated as follows, where αt (y) is the attention probability of the word y from the table at time step t and βt (y) is the probability of predicting the wor"
N18-1141,P17-1019,0,0.0168286,"ion-answer pairs are correct and some are wrong. However, this kind of dataset is hard to obtain in most situations because of the lack of manual annotation efforts. From this perspective, the QA model could exactly benefit from the QG model through incorporating additional questionanswer pairs whose questions are automatically generated by the QG model1 . To achieve this goal, we present a training algorithm that improves the QA model and the 1 An alternative way is to automatically generate answers for each question. Solving the problem in this condition requires an answer generation model (He et al., 2017), which is out of the focus of this work. Our algorithm could also be adapted to this scenario. 1564 Proceedings of NAACL-HLT 2018, pages 1564–1574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics QG model in a loop. The QA model improves QG through introducing an additional QA-specific loss function, the objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating"
N18-1141,N03-1017,0,0.0990867,"A and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25 algorithm. WordCnt uses the number of co-occurred words in query-caption pair, query-header pair, and query-cell pair, respectively. MT based PP is a phrase-level feature. The features come from a phrase table which is extracted from bilingual corpus via statistical machine translation approach (Koehn et al., 2003). LambdaMART (Burges, 2010) is used to train the ranker. CNN uses convolutional neural network to measure the similarity between the query and table caption, table headers, and table cells, respectively. TQNN is the table-based QA model implemented in this work, which is regard as the baseline for the joint learning algorithm. Results of single systems are given in Table 1. We can see that BM25 is a simple yet very effective baseline method. Our basic model performs better than all the single models in terms of MAP. Method BM25 WordCnt MT based PP CNN TQNN (baseline) Seq2SeqPara GCN (competiti"
N18-1141,D16-1127,0,0.264999,"rks on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary tas"
N18-1141,P17-1103,0,0.0185062,"49M query-table pairs. An example of the data is given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity be"
N18-1141,P16-1170,0,0.0518732,"simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when t"
N18-1141,P02-1040,0,0.10243,"given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25"
N18-1141,D16-1264,0,0.0851505,"e the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from differe"
N18-1141,P17-1096,0,0.474587,"objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating additional training instances. Here the key problem is how to label the generated question-answer pair. The application of Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 2017) in this scenario regards every generated question-answer pair as a negative instance. On the contrary, Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) regards every generated question-answer pair appended with special domain tag as a positive instance. However, it is non-trivial to label the generated question-answer pairs because some of which are good paraphrases of the ground truth yet some might be negative instances with similar utterances. To address this, we bring in a collaboration detector, which takes two question-answer pairs as the input and determines their relation as collaborative or competitive. The output of the collaboration detector is regarded as the label of the generated questionanswer pair. We conduct experiments on b"
N18-1154,W12-3018,0,0.100628,"Missing"
N18-1154,P16-1185,0,0.0153548,"tion models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both"
N18-1154,P07-2045,0,0.00570763,"follows: 3.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 32.2 32.1 32 BLEU S(Y, Y ⇤ ) r log p⌘ (Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the"
N18-1154,W04-1013,0,0.0147779,"h (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN can yield a further improvement. We also observe that LM GBN and coaching GBN have both achieved better performance than Uniform GBN, which confirms that better regularization effects are achieved, and the generators become more robust and generalize better. We draw the learning curve of both the bridge and the generator in Figure 5 to demonstrate how they coo"
N18-1154,P02-1040,0,0.103934,"(Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the coaching bridge. During decoding, a beam size of 8 is used to approximate the full search space. An important hyper-para"
N18-1154,D15-1044,0,0.198506,"rized in Table 1. We can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems an"
N18-1154,P16-1009,0,0.0691058,"Missing"
N18-1154,D16-1137,0,0.0544207,"Missing"
N18-1154,P17-1139,0,0.0205442,"ity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both of the benefits from synthetic data and regularization. Within the architecture, the bridge module (bridge) first transforms the point-wise ground truth into a bridge distribution, which can be viewed as a target proposer from whom more target examples are drawn to train the generator. By introducing different constraints, the bridge can be set or trained to possess specific property, with which the drawn samples can augment target-side data (allevi"
N18-1154,D16-1160,0,0.0716227,"neural sequence prediction models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (G"
N18-1154,P17-1101,1,0.942479,"can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN c"
P10-1033,H05-1011,0,0.0925595,"econd stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel,"
P10-1033,P06-1065,0,0.0535147,"ntence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by includin"
P10-1033,P06-2014,0,0.0182417,"that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these tec"
P10-1033,W07-0403,0,0.0529252,"features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al. (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. It is interesting to see if DPDI can benefit the parsing of a more realistic ITG. HP-DITG extends Cherry and Lin‟s approach by not only employing simple phrase pairs but also hierarchical phrase pairs (Chiang, 2007). The grammar is enriched with rules of the format: ?  ?? /?? where ?? and ?? refer to the English and foreign side of the i-th (simple/hierarchical) phrase pair respectively. As example, if there is a simple phrase p"
P10-1033,C96-2141,0,0.52026,"for HP-DITG ID 8 Table 5 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null. 1 ? → ?|?|? 2 ? → ?? |?? |?? 3 ? → ?? |?? |?? ? → ?? |?? 4 ? → ?? |??? |??? 5 ? → ??? ??? 6 ?? → ?/? 7 ?? → ?/?; ?? → ?/? 8 ??? → ?? |??? ?? ; ??? 9 ??? → ??? ?? ; ??? → Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning"
P10-1033,2005.mtsummit-papers.33,0,0.0984013,"Missing"
P10-1033,J97-3002,0,0.682813,"ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et"
P10-1033,J07-2003,0,0.825163,"istent link ratio with the sum of the link‟s posterior probability. The DITG Models The discriminative ITG alignment can be conceived as a two-staged process. In the first stage DPDI selects good span pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which de"
P10-1033,P09-1104,0,0.474924,"all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these techniques have certain con"
P10-1033,W05-1506,0,0.0332463,"y span pairs. It is empirically found to be alignment is done in a similar way to chart pars- highly harmful to alignment performance and ing (Wu, 1997). The base step applies all relevant therefore not adopted in this paper. The third type of pruning is equivalent to miterminal unary rules to establish the links of word pairs. The word pairs are then combined into nimizing the beam size of alignment hypotheses span pairs in all possible ways. Larger and larger in each hypernode. It is found to be well handled span pairs are recursively built until the sentence by the K-Best parsing method in Huang and Chiang (2005). That is, during the bottom-up pair is built. Figure 1(a) shows one possible derivation for a construction of the span pair repertoire, each span toy example sentence pair with three words in pair keeps only the best alignment hypothesis. each sentence. Each node (rectangle) represents a Once the complete parse tree is built, the k-best pair, marked with certain phrase category, of for- list of the topmost span is obtained by minimally eign span (F-span) and English span (E-span) expanding the list of alignment hypotheses of (the upper half of the rectangle) and the asso- minimal number of sp"
P10-1033,P00-1056,0,0.439865,"the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic e"
P10-1033,W01-1812,0,0.0499944,"Missing"
P10-1033,P05-1057,0,0.0363119,"an pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM"
P10-1033,J93-2003,0,\N,Missing
P10-1033,P05-1059,0,\N,Missing
P10-1033,P06-1097,0,\N,Missing
P10-1033,N09-1026,0,\N,Missing
P10-1033,P03-1021,0,\N,Missing
P10-1033,P08-1012,0,\N,Missing
P10-1033,W04-3250,0,\N,Missing
P12-1032,N09-1014,0,0.194275,"ve hundred dollars tea ? Src 我 想要 五百 元 以下 的 茶 . Ref I would like some tea under five hundred dollars . Best1 I would like tea under five hundred dollars . Figure 1. Two sentences from IWSLT (Chinese to English) data set. ""Src"" stands for the source sentence, and ""Ref"" means the reference sentence. ""Best1"" is the final output of the decoder. the most similar translation examples from translation memory (TM) systems (Ma et al., 2011). A classifier is applied to re-rank the n-best output of a decoder, taking as features the information about the agreement with those similar translation examples. Alexandrescu and Kirchhoff (2009) proposed a graph-based semi-supervised model to re-rank n-best translation output. Note that these two attempts are about translation consensus for similar sentences, and about reranking of n-best output. It is still an open question whether translation consensus for similar sentences/spans can be applied to the decoding process. Moreover, the method in Alexandrescu and Kirchhoff (2009) is formulated as a typical and simple label propagation, which leads to very large graph, thus making learning and search inefficient. (c.f. Section 3.) In this paper, we attempt to leverage translation consen"
P12-1032,P09-1064,0,0.0405537,"Missing"
P12-1032,N10-1141,0,0.0239482,"Missing"
P12-1032,C10-1036,1,0.814223,"Missing"
P12-1032,W04-3250,0,0.137364,"Missing"
P12-1032,N04-1022,0,0.0389813,"just a span of it, whether the candidate is the same as or similar to the supporting candidates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multip"
P12-1032,P09-1019,0,0.0161674,"upporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source"
P12-1032,P09-1066,1,0.837826,"Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source sentence. It should be noted that consensus among translations of similar source sentences/spans is also helpful for good candidate selection. Consider the examples in Figure 1. For the source (Chine"
P12-1032,P06-1096,0,0.0822772,"Missing"
P12-1032,P03-1021,0,0.104259,"eatures and feature weights in the log-linear model. Algorithm 1 Semi-Supervised Learning 0; λ = , , ; while not converged do , , , . λ , , end while return last ( , λ ) e1 a1 m n Algorithm 1 outlines our semi-supervised method for such alternative training. The entire process starts with a decoder without consensus features. Then a graph is constructed out of all training, dev, and test data. The subsequent structured label propagation provides feature values to the MT decoder. The decoder then adds the new features and re-trains all the feature weights by Minimum Error Rate Training (MERT) (Och, 2003). The decoder with new feature weights then provides new n-best candidates and their posteriors for constructing another consensus graph, which in turn gives rise to next round of 306 0.75 1, f1 b c d1 2, f1 d1 b c 3, f2 d1 b c 0.5 e1 a1 b n e1 d 1 b n Figure 2. A toy graph constructed for re-ranking. MERT. This alternation of structured label propagation and MERT stops when the BLEU score on dev data converges, or a pre-set limit (10 rounds) is reached. 5 Graph Construction A technical detail is still needed to complete the description of graph-based consensus, namely, how the actual consensu"
P12-1032,P02-1040,0,0.0822517,"Missing"
P12-1032,D08-1065,0,0.0157186,"didates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for th"
P12-1032,J97-3002,0,0.317878,"Missing"
P12-1032,P10-1049,0,0.0144735,"urce span. It is not difficult to handle test nodes, since the purpose of MT decoder is to get all possible segmentations of a source sentence in dev/test data, search for the translation candidates of each source span, and calculate the probabilities of the candidates. Therefore, the cells in the search space of a decoder can be directly mapped as test nodes in the graph. Training nodes can be handled similarly, by applying forced alignment. Forced alignment performs phrase segmentation and alignment of each sentence pair of the training data using the full translation system as in decoding (Wuebker et al., 2010). In simpler term, for each sentence pair in training data, a decoder is applied to the source side, and all the translation candidates that do not match any substring of the target side are deleted. The cells of in such a reduced search space of the decoder can be directly mapped as training nodes in the graph, just as in the case of test nodes. Note that, due to pruning in both decoding and translation model training, forced alignment may fail, i.e. the decoder may not be able to produce target side of a sentence pair. In such case we still map the cells in the search space as training nodes"
P12-1032,P06-1066,0,0.0697401,"Missing"
P12-1032,P11-1124,0,\N,Missing
P13-1017,N03-1017,0,0.0812093,"ify the word embeddings in subsequent steps according to bilingual data. our model from raw sentence pairs, they are too computational demanding as the lexical translation probabilities must be computed from neural networks. Hence, we opt for a simpler supervised approach, which learns the model from sentence pairs with word alignment. As we do not have a large manually word aligned corpus, we use traditional word alignment models such as HMM and IBM model 4 to generate word alignment on a large parallel corpus. We obtain bidirectional alignment by running the usual growdiag-final heuristics (Koehn et al., 2003) on unidirectional results from both directions, and use the results as our training data. Similar approach has been taken in speech recognition task (Dahl et al., 2012), where training data for neural network model is generated by forced decoding with traditional Gaussian mixture models. Tunable parameters in neural network alignment model include: word embeddings in lookup table LT , parameters W l , bl for linear transformations in the hidden layers of the neural network, and distortion parameters sd of jump distance. We take the following ranking loss with margin as our training criteria:"
P13-1017,J96-1002,0,0.112467,"and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation function. For the distortion td , we could use a lexicalized distortion model: td (ai , ai−1 |e, f) = td (ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compu"
P13-1017,J93-2003,0,0.0874815,"where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the simi"
P13-1017,P10-1033,1,0.843062,"proved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “huge”, it would be easier to find the corresponding word in the Chinese sentence. As we mentioned in the last paragraph, word embeddi"
P13-1017,P11-1043,0,0.018762,"test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations have been made in previous work as (DeNero and Macherey, 2011) that better alignment quality does not necessarily lead to better end-to-end result. 6.4 0.86 0.84 0.82 0.8 0.78 0.76 0.74 1 3 5 7 9 11 13 Figure 3: Effect of different window sizes on word alignment F-score. fitting problem. This is not surprising considering that larger window size only requires slightly more parameters in the linear layers. Lastly, it is worth noticing that our model with no context (window size 1) performs much worse than settings with larger window size and baseline IBM4. Our explanation is as follows. Our model uses the simple jump distance based distortion, which is we"
P13-1017,P11-1042,0,0.0684704,") (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We"
P13-1017,H05-1011,0,0.0188895,"For word pair (ei , fj ), we take fixed length windows surrounding both ei and fj , . . . , ei+ sw , fj− tw , . . . , fj+ tw ), as input: (ei− sw 2 2 2 2 where sw, tw stand window sizes on source and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation functio"
P13-1017,2012.iwslt-papers.3,0,0.0275577,"dent Deep Neural Network with HMM (CD-DNN-HMM) to speech recognition task, which significantly outperforms traditional models. Most methods using DNN in NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted"
P13-1017,P09-1104,0,0.0091929,"obtained by classic HMM and IBM4 model. The second row and fourth row show results of the proposed model trained from HMM and IBM4 respectively. Experiments and Results results of our model also depends on the quality of baseline results, which is used as training data of our model. In future we would like to explore whether our method can improve other word alignment models. We also conduct experiment to see the effect on end-to-end SMT performance. We train hierWe conduct our experiment on Chinese-to-English word alignment task. We use the manually aligned Chinese-English alignment corpus (Haghighi et al., 2009) which contains 491 sentence pairs as test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations"
P13-1017,P05-1044,0,0.0167083,"(ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated"
P13-1017,D12-1110,0,0.0460558,"Missing"
P13-1017,N12-1005,0,0.0718261,"n NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a"
P13-1017,C12-1089,0,0.0108582,") proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features,"
P13-1017,P10-1040,0,0.0173855,"ch smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We set word embedding length to 20, window size to 5, and the length of the only hidden layer to 40. Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. Note that embedding for null word in either Ve and Vf cannot be trained from monolingual corpus, and we simply leave them at the initial value untouched. Word embeddings from monolingual corpus learn strong syntactic knowledge of each word, which is not always desirable for word alignment between some language pairs like English and Chinese. For example, many Chinese words can act as a verb, noun and adjective without any change, while their"
P13-1017,C96-2141,0,0.954535,"n better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “h"
P13-1017,J97-3002,0,0.1441,"l data is used to pre-train wordembeddings. Experiments on large-scale Chineseto-English task show that the proposed method produces better word alignment results, compared with both classic HMM model and IBM model 4. For future work, we will investigate more settings of different hyper-parameters in our model. Secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models. Furthermore, our current model use rather simple distortions; it might be helpful to use more sophisticated model such as ITG (Wu, 1997), which can be modeled by Recursive Neural Networks (Socher et al., 2011). improvement. Due to time constraint, we have not tuned the hyper-parameters such as length of hidden layers in 1 and 3-hidden-layer settings, nor have we tested settings with more hidden-layers. It would be wise to test more settings to verify whether more layers would help. 6.4.4 Word Embedding Following (Collobert et al., 2011), we show some words together with its nearest neighbors using the Euclidean distance between their embeddings. As we can see from Table 2, after bilingual training, “bad” is no longer in the ne"
P13-1017,J07-2003,0,\N,Missing
P13-2006,P11-1138,0,0.511698,"muli,mingzhou}@microsoft.com zhlongk@qq.com wanghf@pku.edu.cn Abstract d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the"
P13-2006,E06-1002,0,0.0494954,"ity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straight"
P13-2006,D07-1074,0,0.185206,"Missing"
P13-2006,P05-1044,0,0.00959837,"es are f (d) and f (e). In Figure 3, each row shares forward path of f (d) while each column shares forward path of f (e). At backpropagation stage, gradient is summed over each row of score nodes for f (d) and over each column for f (e). Till now, our input simply consists of bag-ofwords binary vector. We can incorporate any handcrafted feature f (d, e) as: exp sim(d, e) (3) ei ∈C(m) exp sim(d, ei ) L(d, e) = − log P Finally, we seek to minimize the following training objective across all training instances: X L= L(d, e) (4) d,e The loss function is closely related to contrastive estimation (Smith and Eisner, 2005), which defines where the positive example takes probability mass from. We find that by penalizing more negative examples, convergence speed can be greatly accelerated. In our experiments, the sof tmax loss function consistently outperforms pairwise ranking loss function, which is taken as our default setting. sim(d, e) = Dot(f (d), f (e)) + ~λf~(d, e) (5) In fact, we find that with only Dot(f (d), f (e)) as ranking score, the performance is sufficiently good. So we leave this as our future work. 32 3 Experiments and Analysis different decisions. To our surprise, our method with only local evi"
P13-2006,D11-1072,0,0.84346,"Missing"
P13-2006,N10-1072,0,0.0590816,"Missing"
P13-2006,P11-1115,0,0.0101432,"d pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness"
P13-2006,C10-1142,0,\N,Missing
P13-2006,P14-2013,0,\N,Missing
P13-2006,P14-1062,0,\N,Missing
P13-2006,Q14-1019,0,\N,Missing
P13-2006,W12-6324,0,\N,Missing
P13-2006,P14-1146,1,\N,Missing
P13-2006,W12-6322,0,\N,Missing
P13-2006,W12-6325,0,\N,Missing
P13-2006,W12-6323,0,\N,Missing
P13-2061,W06-1607,0,0.246555,"methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance s"
P13-2061,2012.amta-papers.7,0,0.0835577,"Missing"
P13-2061,P09-1098,1,0.833027,"test web data web data web data Table 1: Development and testing data used in the experiments. are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation. 2.5 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. based on the iterative computation in the Section 2.3. Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , r"
P13-2061,N03-1017,0,0.122322,"ften lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Senten"
P13-2061,W04-3250,0,0.270408,"Missing"
P13-2061,J05-4003,0,0.126612,"Missing"
P13-2061,J04-4002,0,0.0798821,"phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Sentence Pair Vertices Gra"
P13-2061,P03-1021,0,0.159372,"Missing"
P13-2061,P02-1040,0,0.0949147,"Missing"
P13-2061,J03-3002,0,0.192071,"Missing"
P13-2061,P06-1062,1,0.888295,"Missing"
P13-2061,P07-1070,0,0.162098,"s are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. The quality of bilingual data is a key factor in"
P13-2061,J97-3002,0,0.279256,"utlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , rij i and hpj , rij i. The pairs with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generr ated in the format hsi , P ij rkj · v(pj )i and hpj , P Experiments A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: Integration into translation modeling After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(si )) are obtained. Instead of simple filtering, we use the 1 Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 baseline (Wuebker et al., 2010) -0.25M -"
P13-2061,P10-1049,0,0.0134939,"me low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence p"
P13-2061,P06-1066,0,0.0731442,"Missing"
P13-2061,W04-3252,0,\N,Missing
P13-2061,J03-1002,0,\N,Missing
P14-1011,D13-1106,0,0.00699973,"ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic mea"
P14-1011,W04-3250,0,0.0534067,"sh translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans"
P14-1011,D13-1054,0,0.266933,"ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the"
P14-1011,D12-1088,0,0.0249865,"Missing"
P14-1011,P13-1078,0,0.0249237,"ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase emb"
P14-1011,P13-2119,0,0.0122932,"resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase e"
P14-1011,2007.mtsummit-papers.22,0,0.123108,"Missing"
P14-1011,D11-1014,0,0.55051,"sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or"
P14-1011,D07-1103,0,0.030171,"Missing"
P14-1011,P13-1045,0,0.574395,"syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully"
P14-1011,D13-1176,0,0.788727,"ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that"
P14-1011,2009.mtsummit-papers.17,0,0.0492238,"Missing"
P14-1011,D13-1140,0,0.00759691,"(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, n"
P14-1011,J97-3002,0,0.185251,"Missing"
P14-1011,P06-1066,0,0.0963857,"Missing"
P14-1011,P13-1017,1,0.245382,"ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilingu"
P14-1011,D12-1089,0,0.0382796,"Missing"
P14-1011,D13-1141,0,0.206555,"didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core i"
P14-1011,D13-1170,0,\N,Missing
P14-1013,W06-1607,0,0.0295023,"The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. We i"
P14-1013,P05-1048,0,0.0187263,"vel. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Balt"
P14-1013,C08-1041,0,0.130559,"ent level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the"
P14-1013,2007.mtsummit-papers.11,0,0.0175661,"uation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, J"
P14-1013,P13-1126,0,0.0442344,"Missing"
P14-1013,W04-3250,0,0.0266605,"language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method"
P14-1013,J07-2003,0,0.732936,"tributions while topic-specific rules have sharper distributions. A standard entropy metric is used to measure the sensitivity of the source-side of hα, γi as: The model minimizes the pairwise ranking loss across all training instances: X L(f, e) (6) L= Sen(α) = − hf,ei We incorporate the learned topic similarity scores into the standard log-linear framework for SMT. When a synchronous rule hα, γi is extracted from a sentence pair hf, ei, a triple instance I = (hα, γi, hf, ei, c) is collected for inferring the topic representation of hα, γi, where c is the count of rule occurrence. Following (Chiang, 2007), we give a count of one for each phrase pair occurrence and a fractional count for each hierarchical phrase pair. The topic representation of hα, γi is then calculated as the weighted average: P (hα,γi,hf,ei,c)∈T {c × zf } zα = P (7) (hα,γi,hf,ei,c)∈T {c} zγ = (hα,γi,hf,ei,c)∈T P {c × ze } (hα,γi,hf,ei,c)∈T {c} eˆ = arg max P (e|f ) where the translation probability is given by: X P (e|f ) ∝ wi · log φi (f, e) i = Sim(zs , zγ ) = cos(zs , zγ ) (10) X | wj · log φj (f, e) + j {z Standard } X | k wk · log φk (f, e) {z Topic related (13) where φj (f, e) is the standard feature function and wj is"
P14-1013,D13-1107,1,0.837725,"hieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Related Work Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topic140 Sim(Src) Sim(Trg) Sim(Src+Trg) Sim(Src+Trg)+Sen(Src) Sim(Src+Trg)+Sen(Trg) Sim(Src+Trg)+Sen(Src+Trg) 42.51 42.43 42"
P14-1013,D08-1010,0,0.211625,"the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This ma"
P14-1013,P08-1114,0,0.0702404,"though we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches i"
P14-1013,P03-1021,0,0.0503972,"tions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the"
P14-1013,P12-1079,0,0.66379,"s are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Mar"
P14-1013,P02-1040,0,0.0916175,"News Weblog Total Chinese Docs Words 5.7M 5.4B 2.1M 8B 7.8M 13.4B using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method i"
P14-1013,P09-1036,0,0.0149151,"y LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches inefficient when appli"
P14-1013,P05-1044,0,0.0214738,"ss consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜ x) to a V -dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜ x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜ x)), x) = kh(g(˜ x)) − xk2 Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence. Inspired by the contrastive estimation method (Smith and Eisner, 2005), for each parallel sentence pair hf, ei as a positive instance, we select another sentence pair hf 0 , e0 i from the training data and treat hf, e0 i as a negative instance. To make the similarity of the positive instance larger than the negative instance by some margin η, we utilize the following pairwise ranking loss: (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function is calculated and back-propagated to the previous layer to update its parameters. Training neural networks involves many factors"
P14-1013,P06-2124,0,0.0322586,"a. By associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling t"
P14-1013,D11-1014,0,0.0446933,"Missing"
P14-1013,P12-1048,0,0.225478,"T Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of pa"
P14-1140,D13-1106,0,0.373491,"mension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier"
P14-1140,W04-3250,0,0.143032,"Missing"
P14-1140,D13-1054,0,0.251501,"bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neura"
P14-1140,P06-1096,0,0.0191709,"Missing"
P14-1140,P13-1078,0,0.283964,"atures of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for"
P14-1140,J04-4002,0,0.0450753,"er combination, according to their plausible scores. coming from France and Russia R2NN coming from France and Russia R2NN coming from Rule Match 来自 laizi France and Russia Rule Match 法国 和 俄罗斯 faguo he eluosi NULL Rule Match 的 de Figure 4: R2 NN for SMT decoding x ˆ[l,n] = x[l,m] ./ s[l,m] ./ x[m,n] ./ s[m,n] [l,n] sj y [l,n] = = f( X X [l,n] i x ˆi wji ) (s[l,n] ./ x[l,n] )j vj (1) (2) (3) j where ./ is a concatenation operator in Equation 1 and Equation 3, and f is a non-linear function, here we use HT anh function, which is defined as: We extract phrase pairs using the conventional method (Och and Ney, 2004). The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x . During decoding, recurrent input vectors x for internal nodes are calculated accordingly. The difference between our model and the conventional log-linear model includes: • R2 NN is not linear, while the conventional model is a linear combination. (4) • Representations of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. Figure 4 illustrates the architecture for SMT"
P14-1140,P02-1040,0,0.0905836,"Missing"
P14-1140,P13-1045,0,0.0239661,"ti-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). In this paper, we propose a novel recursive recurrent neural network (R2 NN) to model the end-to-end decoding process for statistical machine translation. R2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A"
P14-1140,J97-3002,0,0.035851,"the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2 NN. 6.2 Translation Results As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to ver1497 ify it. We first train the source and target wo"
P14-1140,P10-1049,0,0.0100658,"parameters W and V . The loss function is the commonly used ranking loss with a margin, and it is defined as follows: [l,n] [l,n] LSLT (W, V, s[l,n] ) = max(0, 1 − yoracle + yt ) (6) [l,n] where s[l,n] is the source span. yoracle is the plausible score of a oracle translation result. [l,n] yt is the plausible score for the best translation candidate given the model parameters W and V . The loss function aims to learn a model which assigns the good translation candidate (the oracle candidate) higher score than the bad ones, with a margin 1. Translation candidates generated by forced decoding (Wuebker et al., 2010) are used as oracle translations, which are the positive samples. Forced decoding performs sentence pair segmentation using the same translation system as decoding. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. From the forced decoding result, we can get the ideal derivation tree in the decoder’s search space, and extract positive/oracle translation candidates. 1495 4.3 Supervised Global Training The supervised local training us"
P14-1140,P06-1066,0,0.231824,"propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neural network and recurrent neural network. In R2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure ca"
P14-1140,P13-1017,1,0.829763,"kthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score,"
P14-1140,D13-1112,0,0.0238646,"obal training is proposed to tune the model according to the final translation performance of the whole source sentence. Actually, we can update the model from the root of the decoding tree and perform back propagation along the tree structure. Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. To handle this problem, we use early update strategy for the supervised global training. Early update is testified to be useful for SMT training with large scale features (Yu et al., 2013). Instead of updating the model using the final translation results, early update approach optimizes the model, when the oracle translation candidate is pruned from the n-best list, meaning that, the model is updated once it performs a unrecoverable mistake. Back propagation is performed along the tree structure, and the phrase pair embeddings of the leaf nodess are updated. The loss function for supervised global training is defined as follows: words, but bilingual corpus is much more difficult to acquire, compared with monolingual corpus. Embedding Word Word Pair Phrase Pair #Data 1G 7M 7M #"
P16-1212,P14-1091,1,0.746378,"er to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM"
P16-1212,P14-1133,0,0.0178386,"variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along wit"
P16-1212,D13-1160,0,0.016404,"der framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be"
P16-1212,D14-1179,0,0.00526158,"Missing"
P16-1212,N03-1017,0,0.0311904,"Target Generation can generate a natural language sentence based on the existing semantic tuples; • Combining them, KBSE can be used to translation a source sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there"
P16-1212,P07-2045,0,0.0586397,"ce sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed"
P16-1212,P11-1060,0,0.0109095,"d on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what i"
P16-1212,P13-1078,0,0.0244095,"es for number of objects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a senten"
P16-1212,P14-1140,1,0.808281,"bjects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also c"
P16-1212,P03-1021,0,0.0453806,"periments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are 200. Adadelta (Zeiler, 2012) 1 http://www.statmt.org/moses/ ht"
P16-1212,P02-1040,0,0.100737,"Missing"
P16-1212,D15-1199,0,0.0201843,"Missing"
P16-1212,P15-1128,0,0.00689599,"ce into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along with the sentence generate"
P16-1212,J07-2003,0,\N,Missing
P17-1174,W14-4012,0,0.135364,"Missing"
P17-1174,D14-1179,0,0.139136,"Missing"
P17-1174,W16-4616,0,0.0627963,"Missing"
P17-1174,W16-4617,0,0.281753,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,P16-1078,0,0.444205,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,W08-0509,0,0.0958144,"Missing"
P17-1174,D10-1092,0,0.0822748,"Missing"
P17-1174,D13-1176,0,0.543806,"roperties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state h"
P17-1174,2010.eamt-1.27,0,0.242941,"re serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the phrase “早く (early)” and the phrase “家へ (to home)” is flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order. In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages. In this paper, we refine the original RNN decoder to consider chunk information in NMT"
P17-1174,N03-1017,0,0.138863,"Missing"
P17-1174,P15-1107,0,0.0436534,"Missing"
P17-1174,D15-1106,1,0.817883,"tructures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network. Considering that our Model 1 described in § 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multi-layer RNN s to capture hierarchical structures in data. Hierarchical RNNs are used for various NLP tasks such as machine translation (Luong and Manning, 2016), document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical encoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly ca"
P17-1174,P02-1040,0,0.098266,"Missing"
P17-1174,P16-1100,0,0.223268,"based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi−1 : hi = GRU(hi−1 , xi ). (1) The function GRU(·) is calculated as ri = σ(Wr xi +"
P17-1174,P15-1002,0,0.0264998,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,D15-1166,0,0.0747669,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,C00-1082,0,0.0179728,"Missing"
P17-1174,W16-4610,0,0.0340474,"Missing"
P17-1174,W15-5003,0,0.0366542,"Missing"
P17-1174,P11-2093,0,0.0894041,"Missing"
P17-1174,J03-1002,0,0.017893,"Missing"
P17-1174,W16-2323,0,0.0517383,"Missing"
P17-1174,P15-1150,0,0.0592716,"Missing"
P17-1174,P03-1039,0,0.041371,"Missing"
P17-1174,D09-1160,1,0.802151,"Missing"
P17-1174,C10-1140,1,0.849059,"Missing"
P17-1174,C14-1103,1,0.828074,"Missing"
P17-1174,P17-2092,0,0.0515711,"trast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes decoders for NMT that can capture plausible linguistic structures such as chunk. Finally, we noticed that (Zhou et al., 2017) (which is accepted at the same time as this paper) have also proposed a chunk-based decoder for NMT . Their good experimental result on Chinese to English translation task also indicates the effectiveness of “chunk-by-chunk” decoders. Although their architecture is similar to our Model 2, there are several differences: (1) they adopt chunk-level attention instead of word-level attention; (2) their model predicts chunk tags (such as noun phrase), while ours only predicts chunk boundaries; and (3) they employ a boundary gate to decide the chunk boundaries, while we do that by simply having the"
P17-1174,P07-2045,0,\N,Missing
P18-1006,W15-3014,0,0.0214419,"nt variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbou"
P18-1006,D13-1176,0,0.0940585,"can be rich) to improve the translation performance of lowresource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSR"
P18-1006,D16-1139,0,0.0206136,"Optimize Θz|y and Θx|z 9: Generate z 0 from p(z 0 |y) and build the training batches B3 = (y, z 0 ) ∪ (y ∗ , z ∗ ), B4 = (x, z 0 ) ∪ (x∗ , z ∗ ) 10: E-step: update Θz|y with B3 (Equation 7) 11: M-step: update Θx|z with B4 (Equation 8) 12: end while 13: return Θz|x , Θy|z , Θz|y and Θx|z Figure 2: Triangular Learning Architecture for Low-Resource NMT 2.3 Training Details A major difficulty in our unified bidirectional training is the exponential search space of the translation candidates, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo"
P18-1006,2005.mtsummit-papers.11,0,0.0832691,"xtra monolingual Z described in Table 1 to do back-translation; to train the model p(x|z), we use monolingual X taken from (X, Y ). Procedures for training p(z|y) and p(y|z) are similar. This method use extra monolingual data of Z compared with our TA-NMT method. But we can incorporate it into our method. choice. Note that in this dataset, low-resource pairs (X, Z) and (Y, Z) are severely overlapped in Z. In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair. We also use additional EnglishRomanian bilingual data from Europarlv7 dataset (Koehn, 2005). The monolingual data of Z (HE and RO) are taken from the web2 . In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data"
P18-1006,N03-1017,0,0.0227686,"0 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student a"
P18-1006,D15-1166,0,0.0211414,"nd achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Our contributions are listed as follows: of the rich-resource pair English-French. • We propose a novel triangular training architecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framewor"
P18-1006,2012.eamt-1.60,0,0.0142104,"h, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We randomly choose subsets of bilingual data of (X, Z) and (Y, Z) in the original dataset to simulate low-resource situations,"
P18-1006,W17-4739,0,0.0156186,"zed with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computa"
P18-1006,P17-1176,0,0.170018,"), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student alike method for low-resource translation. For training p(z|x) and p(x|z), we mix the true pair (y ∗ , z ∗ ) ∈ D with the pseudo pair (x0 , z ∗ ) generated by teacher model p (x0 |y ∗ ) in the same mini-batch. The training procedure of p(z|y) and p(y|z) is similar. 3.3 Overall Results Experimental results on both datasets are shown in Table 3 and 4 r"
P18-1006,P16-1009,0,0.0988588,"Missing"
P18-1006,P16-1185,0,0.357447,"jing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better capitalize on monolin"
P18-1006,W16-2323,0,0.039451,"anslation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15"
P18-1006,eisele-chen-2010-multiun,0,0.0358089,"tes, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We r"
P18-1006,1983.tc-1.13,0,0.45006,"Missing"
P18-1006,D16-1160,0,0.0545878,"ty, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better c"
P18-1006,D16-1163,0,0.272716,"uage Translation ∗ Shuo Ren1,2 , Wenhu Chen3 , Shujie Liu4 , Mu Li4 , Ming Zhou4 and Shuai Ma1,2 1 2 SKLSDE Lab, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint traini"
P18-1006,N16-1101,0,\N,Missing
