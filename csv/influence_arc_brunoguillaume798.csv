2006.jeptalnrecital-long.11,C94-1042,0,0.082995,"Missing"
2006.jeptalnrecital-long.11,J93-1002,0,\N,Missing
2007.jeptalnrecital-long.20,2006.jeptalnrecital-long.11,1,0.875034,"Missing"
2007.jeptalnrecital-long.20,sagot-etal-2006-lefff,0,0.065359,"Missing"
2007.jeptalnrecital-long.20,saint-dizier-2006-prepnet,0,0.062192,"Missing"
2008.jeptalnrecital-court.6,2006.jeptalnrecital-long.11,1,0.863432,"Missing"
2008.jeptalnrecital-court.6,2007.jeptalnrecital-poster.15,0,0.0505751,"Missing"
2008.jeptalnrecital-court.6,P07-1115,0,0.0710112,"Missing"
2008.jeptalnrecital-court.6,W04-2104,0,0.0779349,"Missing"
2008.jeptalnrecital-court.6,sagot-etal-2006-lefff,0,0.0633788,"Missing"
2008.jeptalnrecital-court.6,francopoulo-etal-2006-lexical,0,\N,Missing
2009.jeptalnrecital-court.3,P02-1042,0,0.310541,"Missing"
2009.jeptalnrecital-court.3,2005.jeptalnrecital-long.2,0,0.0352326,"Missing"
2009.jeptalnrecital-court.3,J05-2003,0,0.0517569,"Missing"
2009.jeptalnrecital-court.3,P83-1020,0,0.575706,"Missing"
2009.jeptalnrecital-court.3,J01-1004,0,0.0678827,"Missing"
2010.jeptalnrecital-demonstration.13,2009.jeptalnrecital-court.3,1,0.604863,"Missing"
2010.jeptalnrecital-demonstration.13,2010.jeptalnrecital-long.7,1,0.843165,"Missing"
2015.jeptalnrecital-demonstration.7,P13-2017,0,0.0691845,"Missing"
2017.jeptalnrecital-court.21,W14-1206,0,0.0405071,"Missing"
2017.jeptalnrecital-court.21,2010.jeptalnrecital-long.3,0,0.0433137,"Missing"
2017.jeptalnrecital-court.21,C12-1055,1,0.828073,"Missing"
2017.jeptalnrecital-court.21,C16-1286,1,0.892863,"Missing"
2017.jeptalnrecital-court.21,W15-2204,1,0.884851,"Missing"
2017.jeptalnrecital-court.21,sagot-2010-lefff,0,0.0261628,"Missing"
2017.jeptalnrecital-court.21,seretan-2012-acquisition,0,0.0371227,"Missing"
2017.jeptalnrecital-court.21,P13-4001,0,0.0623349,"Missing"
2020.lrec-1.541,W08-2230,0,0.0931812,"Missing"
2020.lrec-1.541,L18-1024,0,0.0277894,"lecting language data as we write these lines. Other GWAPs were then designed, addressing new tasks, like ZombiLingo (still running) for the annotation of dependency relations for French (Guillaume et al., 2016) or Wordrobe (no more active), for various semantic annotation tasks (Bos and Nissim, 2015). Most of the active GWAPs in the domain now appear on the LDC LingoBoingo portal1 . Apart from the already mentioned games, it presents TileAttack (Madge et al., 2017), Wormingo (Kicikoglu et al., 2019), WordClicker (Madge et al., 2019), Name That Language! (described as the Language ID game in (Cieri et al., 2018)) and Know Your Nyms?. Many of these GWAPs were quite successful, both in terms of the quantity of created data and of the obtained quality (Chamberlain et al., 2013). However, despite the wide variety of available GWAPs, there is, to our knowledge, no other active (and open) gamified platform or game dealing with MWE identification. The only related work we found is a gamified interface which was developed as part of the PARSEME COST action2 , allowing selected participants (researchers) to guess the meaning of opaque MWEs in other languages (Krstev and Savary, 2018). 3. 3.1. Rigor Mortis A g"
2020.lrec-1.541,W18-4923,1,0.779561,"Missing"
2020.lrec-1.541,C16-1286,1,0.846595,"h respect to experts. 2. tested for a while now. The first to be developed were JeuxDeMots, a game allowing the creation of a lexical network for French, which is more than ten years old now (Lafourcade, 2007; Lafourcade et al., 2018), closely followed by Phrase Detectives (Chamberlain et al., 2008), in which participants annotate co-reference relations in English corpora. Both games are still running and collecting language data as we write these lines. Other GWAPs were then designed, addressing new tasks, like ZombiLingo (still running) for the annotation of dependency relations for French (Guillaume et al., 2016) or Wordrobe (no more active), for various semantic annotation tasks (Bos and Nissim, 2015). Most of the active GWAPs in the domain now appear on the LDC LingoBoingo portal1 . Apart from the already mentioned games, it presents TileAttack (Madge et al., 2017), Wormingo (Kicikoglu et al., 2019), WordClicker (Madge et al., 2019), Name That Language! (described as the Language ID game in (Cieri et al., 2018)) and Know Your Nyms?. Many of these GWAPs were quite successful, both in terms of the quantity of created data and of the obtained quality (Chamberlain et al., 2013). However, despite the wid"
2020.lrec-1.541,W18-4925,0,0.0477251,"Missing"
2020.lrec-1.541,schneider-etal-2014-comprehensive,0,0.22839,"re used to learn state-of-the-art identification models (Schneider et al., 2016; Ramisch et al., 2018). The construction of such annotated corpora is nonetheless costly. Indeed, they are mainly annotated by experts or linguistics-aware people long-trained by experts in order to guarantee the annotation quality. Indeed, MWEs are known to be hard to identify due to the fuzzy delimitation between compositional and non-compositional combinations of words. This difficulty is demonstrated in the modest average inter-annotator agreement (0.65 in F-score) for comprehensive MWEs annotation in English (Schneider et al., 2014). In this paper, we propose a gamified platform for annotating MWEs. Experiments were carried out for French. The aim of this paper is to assess to what extent one can rely on corpora annotated in MWEs by the participants with respect to experts. 2. tested for a while now. The first to be developed were JeuxDeMots, a game allowing the creation of a lexical network for French, which is more than ten years old now (Lafourcade, 2007; Lafourcade et al., 2018), closely followed by Phrase Detectives (Chamberlain et al., 2008), in which participants annotate co-reference relations in English corpora."
2020.lrec-1.541,S16-1084,0,0.0271908,"ons include multiple linguistic phenomena, as mentioned in (Sag et al., 2001), for example idioms (e.g. add fuel to the fire), phrasal verbs (e.g. give up), complex function words (e.g. as soon as), light-verb constructions (e.g. take a bath), adverbial and nominal open compounds (e.g. by the way, dry run). Handling MWEs is a key challenge for natural language processing (Sag et al., 2001), on which researchers have long been working. Recently, significant advances have been made thanks to the availability of new MWEannotated data that are used to learn state-of-the-art identification models (Schneider et al., 2016; Ramisch et al., 2018). The construction of such annotated corpora is nonetheless costly. Indeed, they are mainly annotated by experts or linguistics-aware people long-trained by experts in order to guarantee the annotation quality. Indeed, MWEs are known to be hard to identify due to the fuzzy delimitation between compositional and non-compositional combinations of words. This difficulty is demonstrated in the modest average inter-annotator agreement (0.65 in F-score) for comprehensive MWEs annotation in English (Schneider et al., 2014). In this paper, we propose a gamified platform for anno"
2020.lrec-1.651,W19-7803,1,0.895443,"Missing"
2020.lrec-1.651,W17-6508,1,0.806338,"s can overcome, to a certain degree, rarely occurring errors in the training corpus. This is particularly true when making use of word vector representation models trained on massive amount of raw textual data (Devlin et al., 2019; Peters et al., 2018). However, in the use cases of language teaching, the query of counterexamples to syntactic claims, or typological comparative measures of (word order, syntactic relation, construction) distribution tendencies, these errors can influence the results significantly. See for example the differences between treebanks of the same language reported by Chen and Gerdes (2017). It has become essential for the UD project not only to facilitate treebank curation for the treebank maintainer but to find ways to open treebank corrections to a wider audience of linguists and language students. All the aforementioned treebank annotation errors come essentially in two flavors: occasional slips of attention of the annotator and systematic discrepancies with the desired correct analysis. The former type of problems (occasional slips of attention of the annotator) can be addressed by means of an easy access to “strange” constructions1 and myriads of annotators who look at the"
2020.lrec-1.651,W16-4011,0,0.0453495,"Missing"
2020.lrec-1.651,N19-1423,0,0.00542738,"Kanayama et al., 2018). 3. Some treebank creators do not have sufficient time or competence to provide satisfactory analyses of some of the innumerable syntactic phenomena of their language. Moreover some of the treebanks have been abandoned and do not follow the latest updates of the UD specifications. Generally speaking, dependency parsing and tagging with recent quantitative NLP tools can overcome, to a certain degree, rarely occurring errors in the training corpus. This is particularly true when making use of word vector representation models trained on massive amount of raw textual data (Devlin et al., 2019; Peters et al., 2018). However, in the use cases of language teaching, the query of counterexamples to syntactic claims, or typological comparative measures of (word order, syntactic relation, construction) distribution tendencies, these errors can influence the results significantly. See for example the differences between treebanks of the same language reported by Chen and Gerdes (2017). It has become essential for the UD project not only to facilitate treebank curation for the treebank maintainer but to find ways to open treebank corrections to a wider audience of linguists and language st"
2020.lrec-1.651,W15-0904,0,0.0114676,"l choice to actually annotate texts and to combine the tokenization and syntactic annotation step into one single task. Note however that annotator-based tokenization complicates significantly the computation of inter-annotator agreement, as we jointly observe tokenization and syntactic annotation. And most importantly, tokenization is either trivial and orthography based (i.e. a token is a sequence of letters, possible errors are expressed in the syntactic annotation) or based on lexical and semantic criteria, which makes it a challenging task to reach a satisfying inter-annotator agreement (Farahmand et al., 2015; Savary et al., 2017). Arborator takes a middle stand, making use of its hierarchy of user modes, see Section 4., and allows validators and project owners to modify (delete, add, join, split) tokens, but these changes are then carried out on all trees, whatever the user, of the modified sentence. Such a global modification of a sentence’s tokenization can cause other annotators’ trees to be disconnected or different from the desired structure. This 5293 behavior is the only exception to the basic Arborator rule which states that users can view other annotators’ trees, depending on their acces"
2020.lrec-1.651,C14-2013,0,0.0139377,"of matches, but they do not include further statistical data about the query results. Grew-match9 goes a step beyond that with the possibility to cluster on any of the nodes or edges of the query results. This includes simple clustering of the form for any lemma, thus providing a list of forms, and also the clustering of the relation between any two parts of speech, providing a list of relations that link the two parts of speech. The integration of this feature into Arborator-Grew and its usage will be explained in Section 3.2. Another remarkable tool that goes a step further is the Trameur (Fleury and Zimina, 2014). It applies corpus linguistics statistical tools to raw corpora, and, in its online version iTrameur10 also to dependency treebanks. It can therefore show significant over or under-representations of specific sub-trees in one sample compared to another. We intend to study further the possible use of these measures in Arborator-Grew’s error-mining tools. 9 http://match.grew.fr http://www.tal.univ-paris3.fr/trameur/ iTrameur/ iTrameur only has a French interface. 10 One important question in the design for multi-user annotation systems is the status of tokenization. Brat and WebAnno allow the u"
2020.lrec-1.651,W16-1715,1,0.842736,"Yet, for UD as well as for other treebank creation projects, many of the treebanks contain substantial errors and inconsistencies, which can be attributed to three main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information needed for a transfer into UD, or the converters are incomplete. 2. Some descriptions in the UD guidelines are underspecified and leave room for different analyses of the same construction, inside a language, a language group, or generally among languages, cf. the constantly active UD discussion group on GitHub and also Gerdes and Kahane (2016). Moreover, some UD rules are not well-adapted for specific languages – e.g. the discussion of the direction of coordination in Japanese (Kanayama et al., 2018). 3. Some treebank creators do not have sufficient time or competence to provide satisfactory analyses of some of the innumerable syntactic phenomena of their language. Moreover some of the treebanks have been abandoned and do not follow the latest updates of the UD specifications. Generally speaking, dependency parsing and tagging with recent quantitative NLP tools can overcome, to a certain degree, rarely occurring errors in the train"
2020.lrec-1.651,W19-7814,1,0.616106,"to develop large scale and multi-lingual treebanks. The flagship project is certainly Universal Dependencies (UD) (McDonald et al., 2013), which has served as the input to numerous parsers, text generators, and morphological taggers around various shared tasks (Zeman et al., 2018; Mille et al., 2019; McCarthy et al., 2019). The impressive project with more than a hundred treebanks in the same annotation scheme for 90+ languages, combined with great online viewers and query tools have given increased visibility to the project also inside the syntax and typology communities (Croft et al., 2017; Gerdes et al., 2019b). Yet, for UD as well as for other treebank creation projects, many of the treebanks contain substantial errors and inconsistencies, which can be attributed to three main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information needed for a transfer into UD, or the converters are incomplete. 2. Some descriptions in the UD guidelines are underspecified and leave room for different analyses of the same construction, inside a language, a language group, or generally among languages, cf. the constantly active UD discussion group on GitHub and a"
2020.lrec-1.651,W19-8015,1,0.900831,"to develop large scale and multi-lingual treebanks. The flagship project is certainly Universal Dependencies (UD) (McDonald et al., 2013), which has served as the input to numerous parsers, text generators, and morphological taggers around various shared tasks (Zeman et al., 2018; Mille et al., 2019; McCarthy et al., 2019). The impressive project with more than a hundred treebanks in the same annotation scheme for 90+ languages, combined with great online viewers and query tools have given increased visibility to the project also inside the syntax and typology communities (Croft et al., 2017; Gerdes et al., 2019b). Yet, for UD as well as for other treebank creation projects, many of the treebanks contain substantial errors and inconsistencies, which can be attributed to three main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information needed for a transfer into UD, or the converters are incomplete. 2. Some descriptions in the UD guidelines are underspecified and leave room for different analyses of the same construction, inside a language, a language group, or generally among languages, cf. the constantly active UD discussion group on GitHub and a"
2020.lrec-1.651,W13-3711,1,0.911019,"fante et al., 2018). Our new tool Arborator-Grew provides support for the whole process of treebank creation, publication, errormining, and curation. It is essentially a front-end editor to the Grew graph rewriting system3 (Guillaume et al., 2012; Bonfante et al., 2018), adding access control, predefined error mining queries, exercises for teaching and annotator training, graphical diff tools, and versioning via GitHub. Arborator-Grew is under active development and a first public release is planned during Spring 2020. It attempts to replicate the features of the current version of Arborator (Gerdes, 2013), in particular its class-sourcing tools (Zeldes, 2017), while improving and modernizing queries, error-mining, versioning, and collaborative features. It is the first tool to integrate complex graph querying and treebank annotation software. The advantages of this combination will be discussed in Section 4. 2. Related Work Quite a few tools exist for dependency treebank development, visualization, and querying, many of which share some of the features of Arborator-Grew. These tools can be divided into three mains groups: dependency visualization tools, dependency annotation tools, and treeban"
2020.lrec-1.651,F12-5001,1,0.665113,"(2011) where the authors found that annotators frequently confused direct objects and nominal modifiers, syntactically readily mistakable. But annotators also confused subjects and adjectival modifiers for the surprising reason that the annotation tool’s shortcut keys are placed next to one another on the keyboard. These types of error can easily be detected using Grew (Bonfante et al., 2018). Our new tool Arborator-Grew provides support for the whole process of treebank creation, publication, errormining, and curation. It is essentially a front-end editor to the Grew graph rewriting system3 (Guillaume et al., 2012; Bonfante et al., 2018), adding access control, predefined error mining queries, exercises for teaching and annotator training, graphical diff tools, and versioning via GitHub. Arborator-Grew is under active development and a first public release is planned during Spring 2020. It attempts to replicate the features of the current version of Arborator (Gerdes, 2013), in particular its class-sourcing tools (Zeldes, 2017), while improving and modernizing queries, error-mining, versioning, and collaborative features. It is the first tool to integrate complex graph querying and treebank annotation"
2020.lrec-1.651,C16-1286,1,0.894178,"Missing"
2020.lrec-1.651,W19-8010,0,0.0157577,"nd back-end, as well as allowing for more web-based project configurations. The visualization uses a multi-line configuration where relations can go across different lines, which can be confusing, but remains necessary as the same format is also used to annotate coreference and other long-distance relations across many sentences. Recent CoNLL-U files need to be converted first in Brat’s and WebAnno’s internal standoff formats. Single user online graphical CoNLL file editors include Arborator’s Quick online tool8 , Annotatrix (Tyers et al., 2017) (providing Latex export), and the ConlluEditor (Heinecke, 2019). The ConlluEditor is noteworthy for its easy token splitting and joining, its stemma-like horizontal visualization, its integration of UD validation scripts, and its interaction with GitHub versioning. One last annotation tool worth mentioning in this context is ZombiLingo (Guillaume et al., 2016), a tool for crowd-sourcing of the syntactic annotation process through gamification. Users have to pass rather basic proficiency tests to be allowed to play: they are presented with a sentence at a time, for which they have to determine one single relation, such as finding the subject of a verb, and"
2020.lrec-1.651,W18-6009,0,0.0188114,"e main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information needed for a transfer into UD, or the converters are incomplete. 2. Some descriptions in the UD guidelines are underspecified and leave room for different analyses of the same construction, inside a language, a language group, or generally among languages, cf. the constantly active UD discussion group on GitHub and also Gerdes and Kahane (2016). Moreover, some UD rules are not well-adapted for specific languages – e.g. the discussion of the direction of coordination in Japanese (Kanayama et al., 2018). 3. Some treebank creators do not have sufficient time or competence to provide satisfactory analyses of some of the innumerable syntactic phenomena of their language. Moreover some of the treebanks have been abandoned and do not follow the latest updates of the UD specifications. Generally speaking, dependency parsing and tagging with recent quantitative NLP tools can overcome, to a certain degree, rarely occurring errors in the training corpus. This is particularly true when making use of word vector representation models trained on massive amount of raw textual data (Devlin et al., 2019; P"
2020.lrec-1.651,W17-0233,0,0.0118395,"h can handle multi-layer annotations, a graphical query builder, chunk, phrase structure tree, and dependency visualization (the latter using Arborator’s visualization Javascript library), integration of sound files, and queries on multiple tokenizations thanks to its stand-off format (Krause et al., 2012). Yet, due to its complexity, Annis requires a non-trivial installation and data-insertion process. AQL is rather verbose, and it is not blazingly fast. Other tools are more specifically designed for queries into single-layer dependency treebanks. One of these tools is Dep Search from Turku (Luotolahti et al., 2017). It is very lightweight and fast, with a succinct and quite powerful query language based on TGrep (Rohde, 2005), though quite unfamiliar and tricky for users trained on other query languages. Most of these tools are designed to provide the matching 8 5292 https://arborator.ilpga.fr/q.cgi Figure 1: A screenshot of the user interface that gives access to the different annotations of a sentence, with one dependency tree per user. The sentence is drawn from the ongoing treebank annotation project of spoken Naija (Nigerian PidginCr´eole). The transcribed sentence has a so-called macro-syntactic m"
2020.lrec-1.651,W19-4226,0,0.0124478,"actic treebanks and semantic graph banks. Keywords: dependency treebanks, annotation tools, crowd-sourcing, class-sourcing, error-mining, graph banks 1. Introduction Dependency treebanks have become the standard resource for training syntactic parsers, and substantial efforts have been undertaken to develop large scale and multi-lingual treebanks. The flagship project is certainly Universal Dependencies (UD) (McDonald et al., 2013), which has served as the input to numerous parsers, text generators, and morphological taggers around various shared tasks (Zeman et al., 2018; Mille et al., 2019; McCarthy et al., 2019). The impressive project with more than a hundred treebanks in the same annotation scheme for 90+ languages, combined with great online viewers and query tools have given increased visibility to the project also inside the syntax and typology communities (Croft et al., 2017; Gerdes et al., 2019b). Yet, for UD as well as for other treebank creation projects, many of the treebanks contain substantial errors and inconsistencies, which can be attributed to three main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information needed for a transfer i"
2020.lrec-1.651,P13-2017,0,0.0934021,"Missing"
2020.lrec-1.651,D19-6301,0,0.0137404,"g, and curating syntactic treebanks and semantic graph banks. Keywords: dependency treebanks, annotation tools, crowd-sourcing, class-sourcing, error-mining, graph banks 1. Introduction Dependency treebanks have become the standard resource for training syntactic parsers, and substantial efforts have been undertaken to develop large scale and multi-lingual treebanks. The flagship project is certainly Universal Dependencies (UD) (McDonald et al., 2013), which has served as the input to numerous parsers, text generators, and morphological taggers around various shared tasks (Zeman et al., 2018; Mille et al., 2019; McCarthy et al., 2019). The impressive project with more than a hundred treebanks in the same annotation scheme for 90+ languages, combined with great online viewers and query tools have given increased visibility to the project also inside the syntax and typology communities (Croft et al., 2017; Gerdes et al., 2019b). Yet, for UD as well as for other treebank creation projects, many of the treebanks contain substantial errors and inconsistencies, which can be attributed to three main causes: 1. Many of the UD treebanks are converted from other formats that do not contain all the information"
2020.lrec-1.651,N18-1202,0,0.0096408,"). 3. Some treebank creators do not have sufficient time or competence to provide satisfactory analyses of some of the innumerable syntactic phenomena of their language. Moreover some of the treebanks have been abandoned and do not follow the latest updates of the UD specifications. Generally speaking, dependency parsing and tagging with recent quantitative NLP tools can overcome, to a certain degree, rarely occurring errors in the training corpus. This is particularly true when making use of word vector representation models trained on massive amount of raw textual data (Devlin et al., 2019; Peters et al., 2018). However, in the use cases of language teaching, the query of counterexamples to syntactic claims, or typological comparative measures of (word order, syntactic relation, construction) distribution tendencies, these errors can influence the results significantly. See for example the differences between treebanks of the same language reported by Chen and Gerdes (2017). It has become essential for the UD project not only to facilitate treebank curation for the treebank maintainer but to find ways to open treebank corrections to a wider audience of linguists and language students. All the aforem"
2020.lrec-1.651,W17-0412,0,0.0306576,"Missing"
2020.lrec-1.651,W19-7816,0,0.0183719,"Missing"
2020.lrec-1.651,W17-1704,0,0.0138495,"otate texts and to combine the tokenization and syntactic annotation step into one single task. Note however that annotator-based tokenization complicates significantly the computation of inter-annotator agreement, as we jointly observe tokenization and syntactic annotation. And most importantly, tokenization is either trivial and orthography based (i.e. a token is a sequence of letters, possible errors are expressed in the syntactic annotation) or based on lexical and semantic criteria, which makes it a challenging task to reach a satisfying inter-annotator agreement (Farahmand et al., 2015; Savary et al., 2017). Arborator takes a middle stand, making use of its hierarchy of user modes, see Section 4., and allows validators and project owners to modify (delete, add, join, split) tokens, but these changes are then carried out on all trees, whatever the user, of the modified sentence. Such a global modification of a sentence’s tokenization can cause other annotators’ trees to be disconnected or different from the desired structure. This 5293 behavior is the only exception to the basic Arborator rule which states that users can view other annotators’ trees, depending on their access level, but can only"
2020.lrec-1.651,E12-2021,0,0.123655,"Missing"
2020.lrec-1.651,W17-7604,0,0.0128578,"ation front-end with Brat, while modernizing keyboard interactions and back-end, as well as allowing for more web-based project configurations. The visualization uses a multi-line configuration where relations can go across different lines, which can be confusing, but remains necessary as the same format is also used to annotate coreference and other long-distance relations across many sentences. Recent CoNLL-U files need to be converted first in Brat’s and WebAnno’s internal standoff formats. Single user online graphical CoNLL file editors include Arborator’s Quick online tool8 , Annotatrix (Tyers et al., 2017) (providing Latex export), and the ConlluEditor (Heinecke, 2019). The ConlluEditor is noteworthy for its easy token splitting and joining, its stemma-like horizontal visualization, its integration of UD validation scripts, and its interaction with GitHub versioning. One last annotation tool worth mentioning in this context is ZombiLingo (Guillaume et al., 2016), a tool for crowd-sourcing of the syntactic annotation process through gamification. Users have to pass rather basic proficiency tests to be allowed to play: they are presented with a sentence at a time, for which they have to determine"
2020.lrec-1.651,K18-2001,0,0.0374439,"Missing"
2020.lrec-1.721,W19-6131,0,0.0162399,"e problems of the test suite. This provides a way of checking formal semanticists’ hypotheses against actual semantic capacity of speakers (in the present case, French speakers), and allow us to compare the results we obtained with the ones of similar experiments that have been conducted for other languages. Keywords: semantics, inference, French 1. Introduction derstanding systems with different underlying logics, for instance systems implementing natural logic (MacCartney and Manning, 2007; MacCartney and Manning, 2008), or using the Coq proof assistant (Bernardy and Chatzikyriakidis, 2017; Chatzikyriakidis and Bernardy, 2019), exemplifying that “[t]he major added value of logic as a representational framework in computational linguistics is its suitability for the development of provably correct inference procedures.” (Pinkal and Koller, 2012). For these experiments, MacCartney provided an XML version of the FraCaS test suite1 . This version has been used in different projects, in particular to create a bilingual (English and Swedish) treebank of parsing trees (Ljunglöf and Siverbo, 2011; Ljunglöf and Siverbo, 2012) using Grammatical Framework (GF, Ranta (2011)). It also has been used in the MultiFraCaS project2 t"
2020.lrec-1.721,W17-7203,0,0.451542,"in another language. For instance, an adjective might be more easily considered as non-intersective in a language than in another one. Studies have been performed in order to assess the extent to which intuitions of inference of ordinary speakers corre1 https://nlp.stanford.edu/~wcmac/ downloads/ 2 https://gu-clasp.github.io/multifracas/ 5887 spond to the theoretical notions of inference that the authors of the FraCaS test suite intended to illustrate. They demonstrate the degree to which the intuitions of the speakers may vary for some of the problems of the test suite (Cooper et al., 2016; Chatzikyriakidis et al., 2017). In this article, we introduce a French version of the FraCaS test suite. The resource is freely available3 under Creative Commons BY-NC-SA license. To the best of our knowledge, beside the Cross-lingual Natural Language Inference corpus (XNLI4 , Conneau et al. (2018)), it is the only available corpus for testing natural language inference for French. We also ran an experiment to assess both the idiosyncrasy of the translation and the extent to which it fits the expectation in terms of the resulting inference. Section 2. introduces more precisely the FraCaS test suite and the data it contains"
2020.lrec-1.721,D18-1269,0,0.0220075,"u/~wcmac/ downloads/ 2 https://gu-clasp.github.io/multifracas/ 5887 spond to the theoretical notions of inference that the authors of the FraCaS test suite intended to illustrate. They demonstrate the degree to which the intuitions of the speakers may vary for some of the problems of the test suite (Cooper et al., 2016; Chatzikyriakidis et al., 2017). In this article, we introduce a French version of the FraCaS test suite. The resource is freely available3 under Creative Commons BY-NC-SA license. To the best of our knowledge, beside the Cross-lingual Natural Language Inference corpus (XNLI4 , Conneau et al. (2018)), it is the only available corpus for testing natural language inference for French. We also ran an experiment to assess both the idiosyncrasy of the translation and the extent to which it fits the expectation in terms of the resulting inference. Section 2. introduces more precisely the FraCaS test suite and the data it contains. Section 3. describes our French version of the the FraCaS test suite and the design choices we made. Section 4. describes an experiment that we ran, and compare its results with the results that were obtained for other similar experiments. 2. the following, we only g"
2020.lrec-1.721,W07-1431,0,0.0621412,"y the translation. We also report an experiment we ran in order to test both the translation and the logical semantics underlying the problems of the test suite. This provides a way of checking formal semanticists’ hypotheses against actual semantic capacity of speakers (in the present case, French speakers), and allow us to compare the results we obtained with the ones of similar experiments that have been conducted for other languages. Keywords: semantics, inference, French 1. Introduction derstanding systems with different underlying logics, for instance systems implementing natural logic (MacCartney and Manning, 2007; MacCartney and Manning, 2008), or using the Coq proof assistant (Bernardy and Chatzikyriakidis, 2017; Chatzikyriakidis and Bernardy, 2019), exemplifying that “[t]he major added value of logic as a representational framework in computational linguistics is its suitability for the development of provably correct inference procedures.” (Pinkal and Koller, 2012). For these experiments, MacCartney provided an XML version of the FraCaS test suite1 . This version has been used in different projects, in particular to create a bilingual (English and Swedish) treebank of parsing trees (Ljunglöf and Si"
2020.lrec-1.721,C08-1066,0,0.0551257,"ort an experiment we ran in order to test both the translation and the logical semantics underlying the problems of the test suite. This provides a way of checking formal semanticists’ hypotheses against actual semantic capacity of speakers (in the present case, French speakers), and allow us to compare the results we obtained with the ones of similar experiments that have been conducted for other languages. Keywords: semantics, inference, French 1. Introduction derstanding systems with different underlying logics, for instance systems implementing natural logic (MacCartney and Manning, 2007; MacCartney and Manning, 2008), or using the Coq proof assistant (Bernardy and Chatzikyriakidis, 2017; Chatzikyriakidis and Bernardy, 2019), exemplifying that “[t]he major added value of logic as a representational framework in computational linguistics is its suitability for the development of provably correct inference procedures.” (Pinkal and Koller, 2012). For these experiments, MacCartney provided an XML version of the FraCaS test suite1 . This version has been used in different projects, in particular to create a bilingual (English and Swedish) treebank of parsing trees (Ljunglöf and Siverbo, 2011; Ljunglöf and Siver"
2020.mwe-1.14,calzolari-etal-2002-towards,0,0.0556036,"ary discovery methods. We released annotated and raw corpora in 14 languages, and this semi-supervised challenge attracted 7 teams who submitted 9 system results. This paper describes the effort of corpus creation, the task design, and the results obtained by the participating systems, especially their performance on unseen expressions. 1 Introduction Multiword expressions (MWEs) such as to throw someone under the bus ‘to cause one’s suffering to gain personal advantage’ are idiosyncratic word combinations which need to be identiﬁed prior to further semantic processing (Baldwin and Kim, 2010; Calzolari et al., 2002). The task of MWE identiﬁcation, that is, automatically locating instances of MWEs in running text (Constant et al., 2017) has received growing attention in the last 4 years. Progress on this task was especially motivated by shared tasks such as DiMSUM (Schneider et al., 2016), and two editions of the PARSEME shared tasks, edition 1.0 in 2017 (Savary et al., 2017), and edition 1.1 in 2018 (Ramisch et al., 2018). Previous editions of the PARSEME shared task focused on the identiﬁcation of verbal MWEs (VMWEs), because of their challenging traits: complex structure, discontinuities, variability,"
2020.mwe-1.14,J17-4005,1,0.888334,"Missing"
2020.mwe-1.14,W17-4418,0,0.065602,"Missing"
2021.eacl-demos.21,W13-2322,0,0.0517724,"o the head: pattern { M -[nsubj]-> N; M.Number &lt;> N.Number; } without { M -[cop]-> C } The new request returns 25 occurrences which can be manually inspected: we have found a mix of annotation errors, irregularities (institution plural name used as a singular the United Nations rates. . . ) or misspelled sentences. The same approach can be used for many aspect: searching for verbs without subjects, for unwanted multiple relation (more than one obj on the same node). 4.2 data exploration More generally, G REW- MATCH can be used for any kind of data exploration. Here, we use the example of AMR (Banarescu et al., 2013) annotations, this will allow us to show examples where the graph matching used cannot be reduced as a tree matching. Two corpora are available from the AMR website7 : the English translation of the Saint-Exupéry’s novel The Little Prince and some PubMed articles. With the pattern below, we search for a node which is the ARG0 argument of two different related concepts. pattern { P1 -> P2; P1 -[ARG0]-> N; P2 -[ARG0]-> N; } 7 172 Error mining https://amr.isi.edu/ Figure 3: G REW- MATCH main interface 211 occurrences of this pattern are found in The Little Prince. Two of them are showed below, fo"
2021.eacl-demos.21,F12-2024,0,0.0488806,"was design to let the user control these applications. When using rewriting, confluence and termination are important aspects. These questions are discussed on examples in the next section. 3 Graph rewriting in practice The goal of this section is to present through examples the usage of the rewriting part of G REW. Some important concepts like confluence and termination will be also discussed. 3.1 First rules The conversion between different formats is one the common usage of G REW. We will use the example of the conversion from one dependency annotation format (used in the Sequoia project (Candito and Seddah, 2012)) to Universal Dependencies (UD) (Nivre et al., 2016). The Figure 1 shows the annotations of a French sentence in both formats. The whole transformation is decomposed into small steps which are described by rules. When G REW is used to rewrite an input graph, a strategy describes how rules should be applied. In the first examples below, the strategy consists in just one rule. In our conversion example, we need a rule to change the POS for adjectives: A is used in Sequoia and ADJ in UD. The G REW rule for this transformation is: rule adj { pattern { N [upos=A] } commands { N.upos = ADJ } } The"
2021.eacl-demos.21,W18-6008,1,0.885933,"Missing"
2021.eacl-demos.21,W19-7814,1,0.785323,"Missing"
2021.eacl-demos.21,2020.lrec-1.651,1,0.752645,"relations and heads correspond to dedicated subset of rules. 4 Application of graph matching Graph matching is a subpart of the system used to describe left part of rewriting rules, but it is also useful alone as a way to make requests on a graph or a set of graphs. In practice, it can be used for searching examples of a given construction, for checking consistencies of annotations or for error mining. This subpart of G REW is now proposed as a separate tool, named G REW- MATCH and freely available as a web service5 . This graph matching system is also available in the A RBORATOR G REW tool (Guibon et al., 2020)6 . A screenshot of the G REW- MATCH interface is shown in Figure 3. With the top bar and the list 5 6 http://match.grew.fr https://arborator.github.io It is difficult in general to ensure consistent annotations in large corpora. G REW- MATCH can be used to detect this kind of inconsistencies by making linguistic observation on some corpus. The Figure 3 illustrates the first step of such usage with the request: find nsubj relations where there is a Number disagreement (the head and the dependant of the relation both have a Number feature but with different values). In version 2.7 of UD_E NGLIS"
2021.iwpt-1.18,2020.iwpt-1.16,0,0.0563984,"Missing"
2021.iwpt-1.18,2020.iwpt-1.20,0,0.168462,"021 Shared Task on Parsing into Enhanced Universal Dependencies (Bouma et al., 2021) is a second edition of an equivalent shared task in 2020 (Bouma et al., 2020). The goal of the shared task is to produce EUD (Schuster and Manning, 2016), with several new annotation layers expressed on top of UD annotations (Nivre et al., 2020). In the previous shared task, there were two kinds of approaches: producing EUD annotation from raw text with machine learning methods or producing EUD from UD with a rule-based approach (with or without some learning to optimize rule usage). Like (Heinecke, 2020) or (Dehouck et al., 2020), our proposal corresponds to the second approach: we used an existing tool for producing UD annotations and work only on the conversion from UD to EUD. Unlike other rule-based approaches, we used G REW, a generic Graph Rewriting tool (Bonfante et al., 2018), in order to describe the rules for enhancement. Another specificity of our work is that we primarily design our rules by following the guidelines. Even if, in a secondary step and in the context of the shared task, we adapt the system to the corpora which diverge from the guidelines (section 2.6), we can easily provide a system closer to"
2021.iwpt-1.18,2020.iwpt-1.18,0,0.397597,"roduction The IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (Bouma et al., 2021) is a second edition of an equivalent shared task in 2020 (Bouma et al., 2020). The goal of the shared task is to produce EUD (Schuster and Manning, 2016), with several new annotation layers expressed on top of UD annotations (Nivre et al., 2020). In the previous shared task, there were two kinds of approaches: producing EUD annotation from raw text with machine learning methods or producing EUD from UD with a rule-based approach (with or without some learning to optimize rule usage). Like (Heinecke, 2020) or (Dehouck et al., 2020), our proposal corresponds to the second approach: we used an existing tool for producing UD annotations and work only on the conversion from UD to EUD. Unlike other rule-based approaches, we used G REW, a generic Graph Rewriting tool (Bonfante et al., 2018), in order to describe the rules for enhancement. Another specificity of our work is that we primarily design our rules by following the guidelines. Even if, in a secondary step and in the context of the shared task, we adapt the system to the corpora which diverge from the guidelines (section 2.6), we can easily p"
2021.iwpt-1.18,2020.lrec-1.497,0,0.100517,"Missing"
2021.iwpt-1.18,L16-1376,0,0.248614,"Missing"
2021.iwpt-1.18,L16-1680,0,0.0804843,"Missing"
C04-1044,W03-3006,0,0.224279,"d keep only the most probable ones; but if we want to keep all successful taggings, we must use exact methods. Among these, one consists in abstracting information that is relevant for the filtering process, from the formalism F used for representing the concerned grammar G. In this way, we obtain a new formalism Fabs which is a simplification of F and the grammar G is translated into a grammar abs(G) in the abstract framework Fabs . From this, disambiguating with G consists in parsing with abs(G). The abstraction is relevant if parsing eliminates a maximum of bad taggings at a minimal cost. (Boullier, 2003) uses such a method for Lexicalized Tree Adjoining Grammars (LTAG) by abstracting a tree adjoining grammar into a context free grammar and further abstracting that one into a regular grammar. We also propose to apply abstraction but after a preprocessing polarization step. The notion of polarity comes from Categorial Grammars (Moortgat, 1996) which ground syntactic composition on the resource sensitivity of natural languages and it is highlighted in Interaction Grammars (Perrier, 2003), which result from refining and making Categorial Grammars more flexible. Polarization of a grammatical forma"
C04-1044,C94-1024,0,0.0219148,"tralization between polarities. Parsing with the simplified grammar in the abstract formalism can be used efficiently for filtering lexical selections. Introduction There is a complexity issue if one consider exact parsing with large scale lexicalized grammars. Indeed, the number of way of associating to each word of a sentence a corresponding elementary structure—a tagging of the sentence— is the product of the number of lexical entries for each word. The procedure may have an exponential complexity in the length of the sentence. In order to filter taggings, we can use probabilistic methods (Joshi and Srinivas, 1994) and keep only the most probable ones; but if we want to keep all successful taggings, we must use exact methods. Among these, one consists in abstracting information that is relevant for the filtering process, from the formalism F used for representing the concerned grammar G. In this way, we obtain a new formalism Fabs which is a simplification of F and the grammar G is translated into a grammar abs(G) in the abstract framework Fabs . From this, disambiguating with G consists in parsing with abs(G). The abstraction is relevant if parsing eliminates a maximum of bad taggings at a minimal cost"
C04-1044,2004.jeptalnrecital-long.23,0,0.0749889,"tep. The notion of polarity comes from Categorial Grammars (Moortgat, 1996) which ground syntactic composition on the resource sensitivity of natural languages and it is highlighted in Interaction Grammars (Perrier, 2003), which result from refining and making Categorial Grammars more flexible. Polarization of a grammatical formalism F consists in adding polarities to its syntactic structures to obtain a polarized formalism Fpol in which neutralization of polarities is used for controlling syntactic composition. In this way, the resource sensitivity of syntactic composition is made explicit. (Kahane, 2004) shows that many grammatical formalisms can be polarized by generalizing the system of polarities used in Interaction Grammars. To abstract a grammatical formalism, it is interesting to polarize it before because polarities allow original methods of abstraction. The validity of our method is based on a concept of morphism (two instances of which being polarization and abstraction) which characterizes how one should transport a formalism into another. In sections 1 and 2, we present the conceptual tools of grammatical formalism and morphism which are used in the following. In section 3, we defi"
C04-1044,J92-4004,0,0.27751,"grounds methods of lexical disambiguation, which reduce to parsing in simplified formalisms. We illustrate our purpose with an incremental and a bottom-up method. In section 6, we present some experimental results which illustrate the flexibility of the approach. 1 Characterization of a grammatical formalism The projection PhonLTAG is the canonical projection of a locally ordered tree on its leaves. Finally, RulesLTAG is made up of two rules: substitution and adjunction. To view adjunction as a superposition rule, we resort to the monotone presentation of LTAG with quasi-trees introduced by (Vijay-Shanker, 1992). Taking a slightly modified characterization of polarized unification grammars introduced by (Kahane, 2004) we define a grammatical formalism F (not necessarily polarized) as a quadruple hStructF , SatF , PhonF , RulesF i: 1. StructF is a set of syntactic structures which are graphs1 in which each edge and vertex may be associated with a label representing morpho-syntactic information; we assume that the set of labels associated with F is equipped with subsumption, a partial order denoted v, and with unification, an operation denoted t, such that, for any labels l and l0 , either l t l0 is no"
C08-3003,C04-1044,1,0.86677,"rsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global polarity balance (Bonfante"
C08-3003,W03-3006,0,0.0195939,"echanism in the parsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global po"
C08-3003,P06-1018,0,0.489469,"Missing"
C08-3003,C96-2120,0,0.582231,"Missing"
C16-1286,W04-3202,0,0.053824,"e Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are (micro)paid to perform simplified tasks (Snow et al., 2008). Apart from the ethical issues raised by these platforms (detailed in (Fort et al., 2011)), microworking platforms d"
C16-1286,D15-1075,0,0.036927,"o.org/. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 2 See (Church, 2011) for an in-depth reflection on the subject. License details: http:// 3041 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3041–3052, Osaka, Japan, December 11-17 2016. could be simplified by presenting only one pair of sentences and a binary response to a single question, such as “Would most people say that if the first sentence is true, then the second sentence must be true?” (Bowman et al., 2015). To paraphrase (Dandapat et al., 2009), it seems that there is no easy escape from the high cost of complex linguistic annotation. We present here an on-line game that enables the production of quality annotations of complex phenomena (here, dependency syntax), tested on French. The produced corpus is constantly growing and is designed to be (i) completely free and available, and (ii) of sufficient quality. We first examine the existing treebanks for French and their limitations and detail some previous experiments with Games with a Purpose (GWAPs). Then we present the game we developed and t"
C16-1286,F12-2024,0,0.0219807,"resourced, primarily due to legal issues: lexicons and annotated corpora existed but could not be used or redistributed freely. This is in particular the case for the French Treebank (FTB or corpus arbor´e de Paris 7) (Abeill´e et al., 2003), which is available for research purposes only and cannot be freely redistributed.3 Several versions are reported in the literature, with the size varying from 12,351 sentences and 350,947 words (for the FTBUC) to 18,535 sentences and 557,149 words4 (for the FTB-SPRML). To try and circumvent this restriction, Candito and Seddah created the Sequoia corpus (Candito and Seddah, 2012), which is freely available5 under a LGPL-LR license, but is limited to 67,038 tokens. The same authors developed an additional question bank with 23,236 tokens (Seddah and Candito, 2016). Both corpora use the same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain d"
C16-1286,carmen-etal-2010-tag,0,0.0255639,"wever, the creation of such resources is notoriously costly, especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are ("
C16-1286,W09-3002,0,0.113493,"vailability of manually annotated corpora of high quality (or, at least, reliability) is therefore key to the development of the field in any given language. However, the creation of such resources is notoriously costly, especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human"
C16-1286,W15-2204,1,0.858222,"Missing"
C16-1286,hana-hladka-2012-getting,0,0.0546123,"Missing"
C16-1286,Q14-1035,0,0.0388699,"Missing"
C16-1286,lacheret-etal-2014-rhapsodie,0,0.0261395,"he same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language, as with Rhapsodie (Lacheret et al., 2014) or the oral Treebank (Abeill´e and Crabb´e, 2013), or specific language types, like the Social Media Treebank (Seddah et al., 2012). This situation (in which references exist, but a large, fully available, manually annotated corpus is lacking) makes dependency syntax for French an appropriate candidate for testing a new paradigm for complex linguistic resource development: the use of on-line Games with a Purpose. 2.2 Playing to Create Language Resources Games with a Purpose are games in which participants, knowingly or not, create data by playing. They are not serious games as such, as their"
C16-1286,J93-2004,0,0.053289,"d a message revealing the right answer parvient (reaches) are displayed. An advantage of offering a separate T RAINING for each relation is that the player does not have to wait long before starting the game. Once s/he is connected to the game, only a few minutes are required before starting actual play and production of annotations. 3.2.2 Play phase In the general mode, the player chooses a relation from those available and a sequence of ten questions is proposed. The questions vary as follows: 15 Insight concerning the complexity of syntactic annotation for the Penn Treebank is provided in (Marcus et al., 1993), where the learning curve for syntax was estimated to be twice that for part-of-speech (two months vs one). 16 This is much like in an annotation guide but with simpler vocabulary and fewer details. 3044 REFEval Play phase TRAINING CONTROL (feedback) (feedback) EVAL Eval Unannotated corpus (Wikipedia) Ref corpus (Sequoia) REFTrain & Control Training phase (no feedback) Player’s confidence EXPEval Pre annotation with 2 parsers ANNOTATION Raw text (no feedback) EXPGame Figure 1: Organization of the different mechanisms and corpora • For relations where the dependent element tends to be unique f"
C16-1286,L16-1375,0,0.0362327,"or corpus arbor´e de Paris 7) (Abeill´e et al., 2003), which is available for research purposes only and cannot be freely redistributed.3 Several versions are reported in the literature, with the size varying from 12,351 sentences and 350,947 words (for the FTBUC) to 18,535 sentences and 557,149 words4 (for the FTB-SPRML). To try and circumvent this restriction, Candito and Seddah created the Sequoia corpus (Candito and Seddah, 2012), which is freely available5 under a LGPL-LR license, but is limited to 67,038 tokens. The same authors developed an additional question bank with 23,236 tokens (Seddah and Candito, 2016). Both corpora use the same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language,"
C16-1286,C12-1149,0,0.0223579,"and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language, as with Rhapsodie (Lacheret et al., 2014) or the oral Treebank (Abeill´e and Crabb´e, 2013), or specific language types, like the Social Media Treebank (Seddah et al., 2012). This situation (in which references exist, but a large, fully available, manually annotated corpus is lacking) makes dependency syntax for French an appropriate candidate for testing a new paradigm for complex linguistic resource development: the use of on-line Games with a Purpose. 2.2 Playing to Create Language Resources Games with a Purpose are games in which participants, knowingly or not, create data by playing. They are not serious games as such, as their main purpose is not to train people, but to produce data (such as annotations, lexicon entries, image labels, etc). GWAPs for NLP ar"
C16-1286,W13-2304,0,0.0226836,"especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are (micro)paid to perform simplified tasks (Snow et al., 200"
C16-1286,D08-1027,0,0.14295,"Missing"
C16-1286,W13-0215,0,0.0473477,"Missing"
C16-1286,J11-2010,1,\N,Missing
candito-etal-2014-deep,candito-etal-2010-statistical,1,\N,Missing
candito-etal-2014-deep,de-marneffe-etal-2006-generating,0,\N,Missing
candito-etal-2014-deep,J93-2004,0,\N,Missing
candito-etal-2014-deep,W09-4624,0,\N,Missing
candito-etal-2014-deep,H94-1020,0,\N,Missing
candito-etal-2014-deep,P05-1011,0,\N,Missing
candito-etal-2014-deep,P04-1041,0,\N,Missing
candito-etal-2014-deep,W00-1436,0,\N,Missing
candito-etal-2014-deep,J05-1004,0,\N,Missing
candito-etal-2014-deep,W13-3724,0,\N,Missing
candito-etal-2014-deep,abeille-barrier-2004-enriching,0,\N,Missing
F12-2022,W11-0108,1,0.896652,"Missing"
F12-2022,E09-1001,0,0.0280971,"Missing"
F13-2016,W10-1804,0,0.0378841,"Missing"
F13-2016,F12-2024,0,0.0350345,"Missing"
F13-2016,J08-3001,0,0.0561746,"Missing"
F13-2016,scott-etal-2012-corpus,0,0.0258017,"Missing"
F14-2031,candito-etal-2010-statistical,1,0.895459,"Missing"
F14-2031,candito-etal-2014-deep,1,0.70798,"Missing"
F14-2031,F12-2024,1,0.886133,"Missing"
F14-2031,J05-1004,0,0.189665,"Missing"
guillaume-etal-2014-mapping,sagot-2010-lefff,0,\N,Missing
guillaume-etal-2014-mapping,2010.jeptalnrecital-long.32,0,\N,Missing
W07-1603,J93-1002,0,0.15729,"Missing"
W07-1603,2006.jeptalnrecital-long.11,1,0.830389,"scription of some entries with the preposition avec [with] in valence dictionaries • DICOVALENCE, a valence dictionary of French, formerly known as PROTON (van den Eynde and Mertens, 2002), which has been based on the pronominal approach. In version 1.1, this dictionary details the subcategorization frames of more than 3,700 verbs (table 1 gives an example of a DICOVALENCE entry). We extracted the simple and multiword prepositions it contains (i.e. more than 40), as well as their associated semantic classes. • We completed this argument prepositions list with information gathered from SynLex (Gardent et al., 2006), a syntactic lexicon created from the LADL lexicon-grammar tables (Gross, 1975) (see table 1 for a SynLex entry). Using these sources, we conducted a systematic study of each preposition, checking its presence in each source, whether in verb subcategorization frames or not, as well as its associated semantic class(es). We then grouped the prepositions that appear both as lexical entries and in verb subcategorization frames. As multiword prepositions show specific characteristics (in particular, their number) and raise particular issues (segmentation), we processed them sepa19 rately, using th"
W07-1603,saint-dizier-2006-prepnet,0,0.0638249,"d FrenchUNL dictionary: 2.2 We then completed the list of prepositions using manually built resources, including lexicons, dictionaries and grammars: • The Grevisse (Grevisse, 1997) grammar, in its paper version, allowed us to check some intuitions concerning the obsolescence or usage of some prepositions. • The TLFi (Tr´esor de la langue franc¸aise informatis´e), that we consulted through the CNRTL2, and that offers a slightly different list of prepositions. In particular, it contains the forms voici and voil`a, that are seldom quoted in the other available resources. • Finally, the PrepNet (Saint-Dizier, 2006) prepositions database was used to check the completeness of our list as well as the semantic information provided by other sources. 2.3 • Lefff (Lexique des Formes Fl´echies du Franc¸ais/French inflected form lexicon (Sagot et al., 2006)) is a large coverage (more than 110,000 lemmas) French morphological and syntactic lexicon (see table 1 for an example of a Lefff syntactic entry). 18 Using reference sources Using verb valence dictionaries We then looked for a way to enrich the list of prepositions appearing in verb subcategorization frames in Lefff and UNL, using resources that focus more p"
W07-1603,C00-2111,0,0.0399785,"Missing"
W07-1603,W06-2106,0,\N,Missing
W07-1603,sagot-etal-2006-lefff,0,\N,Missing
W09-3416,C04-1044,1,0.835878,"erently. The notion of saturation in terms of polarity is defined as a saturated structure that has all its polarities neutral, whereas an unsaturated structure keeps positive or negative polarities which express its ability to interact with other structures. A complete syntactic tree must be saturated; that means it is without positive or negative nodes and it can not be composed with other structures: so all labels are associated with the polarity of = or <=>. The set of polarities {-> , <- , = , <=>} is equipped with the operation of compositional unification as defined in the table below (Bonfante et al, 2004): <-> <=> <-> <=> <-> = <=> = <=> <-> = <=> <=> Table 1. Polarity compositions on the nodes 2.3 Tree Description Logic in IG Another specification of IG is that syntactic structures can be underspecified: these structures are trees descriptions. It is possible, for instance, to impose that a node dominates another node without giving the length of the domination path. Guillaume and Perrier (2008) have defined four kinds of relations: - Immediate dominance relations: N > M means that M is an immediate sub-constituent of N. - Underspecified dominance relations: N >* M means that the constituent"
W09-3416,C96-1034,0,0.144384,"Missing"
W09-3416,P06-1018,0,0.0182293,"y, it seems Nasr (1995) was the first to propose a 107 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 107–114, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP formalism that explicitly uses the polarized structure in computational linguistics. Then researches such as Muskens and Krahmer (1998), Duchier and Thater (1999), and Perrier (2000) proposed grammatical formalisms in which polarity is also explicitly used. However, Categorial Grammar was the first grammatical formalism that exploited implicitly the idea of polarity (Lambek, 1958). Recently, Kahane (2006) showed that well-known formalisms such as CFG, TAG, HPSG, and LFG could be viewed as polarized formalisms. IG has highlighted the fundamental mechanism of neutralization between polarities underlying CG in such a way that polarities are attached to the features used for describing constituents and not to the constituents themselves. Polarization of a grammatical formalism consists of adding polarities to its syntactic structure to obtain a polarized formalism in which neutralization of polarities is used to control syntactic composition. In this way, the resource sensitivity of syntactic comp"
W09-3416,C08-3003,1,\N,Missing
W09-3416,C96-2120,0,\N,Missing
W09-3416,C00-2087,0,\N,Missing
W09-3840,J99-2004,0,0.0370624,"urages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensu"
W09-3840,C04-1044,1,0.803995,"Missing"
W09-3840,C96-2120,0,0.125722,"Missing"
W09-3840,W03-3006,0,0.0308806,"We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensure that the deep parsing will not fail because of an error at the disambiguation step. In wide-coverage lexicalized grammars, a word We propose a generic method to perform lexical disambiguation in lexicalized grammatical formalisms. It relies on dependency constraints between words. The soundness of the"
W09-3840,2009.jeptalnrecital-court.3,1,0.895723,"Mathieu Morey LORIA Nancy-Universit´e guillaume.bonfante@loria.fr bruno.guillaume@loria.fr mathieu.morey@loria.fr Abstract Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical e"
W09-3840,C04-1041,0,0.0249572,"e aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensure that the deep parsing will not fail because of an error at the disambiguation step. In wide-cov"
W09-3840,J94-2001,0,0.0700084,"tance. We call lexicon the function (written `) from V to subsets of G defined by: typically has about 10 corresponding lexical descriptions, which implies that for a short sentence of 10 words, we get 1010 possible taggings. It is not reasonable to treat them individually. To avoid this, it is convenient to use an automaton to represent the set of all paths. This automaton has linear size with regard to the initial lexical ambiguity. The idea of using automata is not new. In particular, methods based on Hidden Markov Models (HMM) use such a technique for part-of-speech tagging (Kupiec, 1992; Merialdo, 1994). Using automata, we benefit from dynamic programming procedures, and consequently from an exponential temporal and space speed up. 2 `(w) = {t ∈ G |anc(t) = w}. We will say that a lexical tagging L = [t1 , . . . , tn ] is a lexical tagging of the sentence [anc(t1 ), . . . , anc(tn )]. The final structures in p (L) ⊂ F are called the parsing solutions of L. Henceforth, in our examples, we will consider the ambiguous French sentence (1). (1) “La belle ferme la porte” Abstract Grammatical Framework Example 1 We consider the following toy AGF, suited for parsing our sentence: Our filtering method"
W09-3840,P02-1042,0,0.0272196,"es between words. The most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). 242 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242–253, c Paris, October 2009. 2009 Association for Com"
W09-3840,P01-1033,0,0.0378362,"Missing"
W09-3840,W06-1619,0,0.031406,"Missing"
W09-3840,W05-1605,0,0.0777061,"e LORIA INPL Bruno Guillaume LORIA INRIA Mathieu Morey LORIA Nancy-Universit´e guillaume.bonfante@loria.fr bruno.guillaume@loria.fr mathieu.morey@loria.fr Abstract Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiti"
W09-3840,P99-1061,0,0.0476369,"Missing"
W09-3840,E09-1053,0,0.0244248,"e most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). 242 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242–253, c Paris, October 2009. 2009 Association for Computational Linguistics struc"
W09-3840,P06-1018,0,\N,Missing
W11-0108,abeille-barrier-2004-enriching,0,0.0179974,"Missing"
W11-0108,W01-0807,0,0.0349587,"DMRS (Copestake (2009)) or MTT (Mel’ˇcuk (1988)) . The principles being fixed, our problem was then to choose a model of computation well suited to transforming syntactic graphs into semantic graphs. The λ-calculus, which is widely used in formal semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules. Among the pioneers in the use of graph rewriting, we mention Hyv¨onen (1984); Bohnet and Wanner (2001); Crouch (2005); Jijkoun and de Rijke (2007); B´edaride and Gardent (2009); Chaumartin and Kahane (2010). A graph rewriting system is defined as a set of graph rewrite rules and a computation is a sequence of rewrite rule applications to a given graph. The application of a rule is triggered via a mechanism of pattern matching, hence a sub-graph is isolated from its context and the result is a local modification of the input. This allows a linguistic phenomenon to be easily isolated for applying a transformation. Since each step of computation is fired by some local conditions in the whole grap"
W11-0108,candito-etal-2010-statistical,0,0.0417554,"Missing"
W11-0108,2010.jeptalnrecital-court.9,0,0.0153365,"to choose a model of computation well suited to transforming syntactic graphs into semantic graphs. The λ-calculus, which is widely used in formal semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules. Among the pioneers in the use of graph rewriting, we mention Hyv¨onen (1984); Bohnet and Wanner (2001); Crouch (2005); Jijkoun and de Rijke (2007); B´edaride and Gardent (2009); Chaumartin and Kahane (2010). A graph rewriting system is defined as a set of graph rewrite rules and a computation is a sequence of rewrite rule applications to a given graph. The application of a rule is triggered via a mechanism of pattern matching, hence a sub-graph is isolated from its context and the result is a local modification of the input. This allows a linguistic phenomenon to be easily isolated for applying a transformation. Since each step of computation is fired by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps. The more rules, the more in"
W11-0108,W07-1210,0,0.0309161,"l subordinate clauses) are marked with the usual labels of syntactic functions, • the anaphora relations that are predictable from the syntax (i.e. the antecedents of relative, reflexive and repeated pronouns) are marked with a special label ANT. This additional information can already be provided by many syntactic parsers and is particularly interesting to compute semantics. The semantic format is Dependency Minimal Recursion Semantics (DMRS) which was introduced by Copestake (2009) as a compact and easily readable equivalent to Robust Minimal Recursion Semantics (RMRS), which was defined by Copestake (2007). This underspecified semantic formalism was designed for large scale experiments without committing to fine-grained semantic choices. DMRS graphs contain the predicate-argument relations, the restriction of generalized quantifiers and the mode of combination between predicates. Predicate-argument relations are labelled ARGi, where i is an integer following a fixed order of obliqueness SUJ, OBJ, ATS, ATO, A - OBJ, DE - OBJ. . . . Naturally, the lexicon must be consistent with this ordering. The restrictions of generalized quantifiers are labelled RSTR ; their bodies are not overtly expressed b"
W11-0108,E09-1001,0,0.351652,"ve chosen dependency graphs, because syntactic dependencies are closely related to predicate-argument relations. Moreover, they can be enriched with relations derived from the syntax, which are usually ignored, such as the arguments of infinitives or the anaphora determined by the syntax. One may observe that our syntactic representation of sentences involves plain graphs and not trees. Indeed, these relations can give rise to multiple governors and dependency cycles. On the semantic side, 65 we have also chosen graphs, which are widely used in different formalisms and theories, such as DMRS (Copestake (2009)) or MTT (Mel’ˇcuk (1988)) . The principles being fixed, our problem was then to choose a model of computation well suited to transforming syntactic graphs into semantic graphs. The λ-calculus, which is widely used in formal semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics; it could be due to the difficulty to manage large sets of rules. Among the pioneers in the use of graph rewriting, we mention Hyv¨onen (1984); Bohnet and Wanner (200"
W11-0108,P84-1110,0,0.480987,"Missing"
W11-0108,W07-0208,0,0.0675845,"Missing"
W11-0108,W09-3744,0,\N,Missing
W15-2204,W11-0108,1,0.806411,"egular expressions and are less flexible than a GR rule can be. To our knowledge, our proposal is the first use of the Graph Rewriting framework for symbolic dependency parsing. In Section 2, we describe more precisely the GR framework used in the paper. In Section 3, the GR system considered is detailed. We finally give experimental results in Section 4. 2 det [P] _ [V] mange cat=N e1:SUC une _ cat=N SUC . cat=N cat=PONCT cat=V e2:SUC Jean pomme cat=D SUC suj SUC det mange cat=V une cat=D pomme . cat=N SUC cat=PONCT SUC Figure 1: An example of application for the rule subject_noun stance in (Bonfante et al., 2011) to build a semantic annotation of a French Treebank. The reader can refer to the G REW documentation for a complete description of the GR framework and of the syntax of the rules with G REW. We give here a simple example of rule and of its application; more elaborated rules are shown in the next section. The code below is a simplified version of a rule for the subject relation. 1 rule subject_noun { 2 match { 3 S [cat=N|PRO]; 4 V [cat=V, m=ind|subj]; 5 e1:S -[SUC]-> V; 6 P []; e2:P -[SUC]-> S; 7 } 8 without { V -[suj]-> * } 9 without { S [lemma=&quot;que&quot;|&quot;dont&quot;] } 10 commands { 11 del_edge e1; 12"
W15-2204,F12-2024,0,0.165875,"ural language sentences. We propose to describe the parsing process as a sequence of atomic transformations starting from a list of lexical units (a tokenized sentence) to a dependency tree1 built on the same lexical units. Each atomic transformation is described by a handcrafted rule. Then, instead of defining a grammar that describes the set of well-formed structures, we define rules which describe linguistic contexts in which a dependency relation can appear. The rule system input is made of lemmatized and POS-tagged sentences. For the experiments in this paper, we use the S EQUOIA corpus (Candito and Seddah, 2012) (version 6.02 ) as the gold standard. We experiment our system in two settings: on gold POS-tagged text (taken from S EQUOIA data) and on POS-tagging given by the MElt tagger (Denis and Sagot, 2012). We use the general framework of GR where each transformation is given by two parts: first, the conditions that control when the transformation may apply (the pattern) and second, a description of the way the structure should be modified. In the system we proposed, the input format is a tokenized sentence where each lexical unit is given a lemma and a POS-tag; the output format is a dependency str"
W15-2204,W04-1510,0,0.0606709,"information in the intermediate structures. In the rule system, we use two kinds of relation to express linear order between lexical entities: the relation SUC links the heads of two partial dependency structures; the relation INIT_SUC links two successive lexical unit of the sentence, even if they have also been integrated in partial dependency structures. Structures with these two kinds of relations are graphs and cannot be represented as trees. In a comparison with other works from the literature, we left out data-driven approaches which are far from our proposal. In (Foth et al., 2000), (Debusmann et al., 2004), weighted rules are used to described valid dependency structures and the parsing is expressed as a constraints resolution problem. (Covington, 2000) and (Nivre, 2003) propose rule-based processes to produce dependency structures but they are presented as kind of shift-reduce algorithm where word are treated one by one following the reading order, rules describing how each word can be link to the current state. Each rule only tells that a dependency from a word to another word is acceptable. More close to our work is the proposal of (Oflazer, 2003) which defines a set of rules that are used i"
W15-2204,C96-1058,0,0.120596,"two trees. 4 http://alpage.inria.fr/statgram/frdep/ Publications/FTB-GuideDepSurface.pdf 32 dep obj.p obj aux.tps Après P avoir V mod.rel obj det coupé V le D suj ruban N qui PRO mod en marque CL V det symboliquement ADV l' D entrée N Figure 2: A non-projective result of syntactic annotation with F R D EP -PARSE. To express adjacency between two yields, SUC is used with a larger meaning: if there is a SUC relation from a token w1 to a token w2 , it means that w1 and w2 are roots of intermediate dependency trees, the yield of the first tree immediately preceding the yield of the second tree. (Eisner, 1996) already proposed a CKY-like algorithm for parsing with dependencies, but he differs from our proposal on two points: he uses a statistical approach and to link two dependency trees, he takes only their roots into account and ignores information coming from deeper nodes. The use of G REW for implementing the CKY algorithm in a strict manner has no point, but the Graph Rewriting approach allows to enrich the algorithm in various directions. One of them is to add internal and external constraints on the T1 and T2 trees. Here is an example of constraints introduced by a rule of F R D EP -PARSE. 1"
W15-2204,2000.iwpt-1.11,0,0.0210843,"sible to express more information in the intermediate structures. In the rule system, we use two kinds of relation to express linear order between lexical entities: the relation SUC links the heads of two partial dependency structures; the relation INIT_SUC links two successive lexical unit of the sentence, even if they have also been integrated in partial dependency structures. Structures with these two kinds of relations are graphs and cannot be represented as trees. In a comparison with other works from the literature, we left out data-driven approaches which are far from our proposal. In (Foth et al., 2000), (Debusmann et al., 2004), weighted rules are used to described valid dependency structures and the parsing is expressed as a constraints resolution problem. (Covington, 2000) and (Nivre, 2003) propose rule-based processes to produce dependency structures but they are presented as kind of shift-reduce algorithm where word are treated one by one following the reading order, rules describing how each word can be link to the current state. Each rule only tells that a dependency from a word to another word is acceptable. More close to our work is the proposal of (Oflazer, 2003) which defines a se"
W15-2204,W03-3017,0,0.0380469,"two partial dependency structures; the relation INIT_SUC links two successive lexical unit of the sentence, even if they have also been integrated in partial dependency structures. Structures with these two kinds of relations are graphs and cannot be represented as trees. In a comparison with other works from the literature, we left out data-driven approaches which are far from our proposal. In (Foth et al., 2000), (Debusmann et al., 2004), weighted rules are used to described valid dependency structures and the parsing is expressed as a constraints resolution problem. (Covington, 2000) and (Nivre, 2003) propose rule-based processes to produce dependency structures but they are presented as kind of shift-reduce algorithm where word are treated one by one following the reading order, rules describing how each word can be link to the current state. Each rule only tells that a dependency from a word to another word is acceptable. More close to our work is the proposal of (Oflazer, 2003) which defines a set of rules that are used iteratively until a fixpoint is reached and the rules application do not necessarily follow the reading order of the sentence. However, in (Oflazer, 2003) rules are enco"
W15-2204,J03-4001,0,0.0354701,"proposal. In (Foth et al., 2000), (Debusmann et al., 2004), weighted rules are used to described valid dependency structures and the parsing is expressed as a constraints resolution problem. (Covington, 2000) and (Nivre, 2003) propose rule-based processes to produce dependency structures but they are presented as kind of shift-reduce algorithm where word are treated one by one following the reading order, rules describing how each word can be link to the current state. Each rule only tells that a dependency from a word to another word is acceptable. More close to our work is the proposal of (Oflazer, 2003) which defines a set of rules that are used iteratively until a fixpoint is reached and the rules application do not necessarily follow the reading order of the sentence. However, in (Oflazer, 2003) rules are encoded as regular expressions and are less flexible than a GR rule can be. To our knowledge, our proposal is the first use of the Graph Rewriting framework for symbolic dependency parsing. In Section 2, we describe more precisely the GR framework used in the paper. In Section 3, the GR system considered is detailed. We finally give experimental results in Section 4. 2 det [P] _ [V] mange"
W17-6507,W00-1436,0,0.0169751,", 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatically pre-annotated step by step: the annotation of a given level is rewritten to the next level using the MATE tools (Bohnet et al., 2000). Results and Error Analysis We evaluated the production of enhanced UD graphs in two settings, depending on whether the input UD trees do (PA+) or do not (PA−) contain manual disambiguation of cases (a), (b) and (c) described above. For the PA− case, we applied basic default rules instead, known to use insufficient information. Table 1 reports the F-measures (computed considering all edges or N ∪ A edges only). These results confirm the validity of our approach and highlight the consistency of the resulting graphbanks. Moreover, even if manual preannotations are required in theory, we empiric"
W17-6507,F12-2024,1,0.786077,"Missing"
W17-6507,J16-4009,0,0.0211653,"ations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatical"
W17-6507,candito-etal-2014-deep,1,0.801748,"are implemented through diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightf"
W17-6507,P13-2017,0,0.0733126,"Missing"
W17-6507,C16-1040,1,0.896954,"Missing"
W17-6507,W16-1715,0,0.0193644,"ransitives, then the canonical labels can be made explicit as shown in figure 9. Note that the canonical function of the Passive Passive is by far the most frequent syntactic alternation, and it is fortunately rather easy to identify in a language such as French. Note that because the UD scheme uses several labels for the same argumental slot, depending on the argument’s category, the basic rule of having the passive’s subject being the canonical direct object has to be split. The nsubj:pass dependent is considered the canonical obj. The csubj:pass dependent is 8 This is already identified by Gerdes and Kahane (2016), who advocate for directly adding the semantic argument rank (1,2,3...) on top of the syntactic label. 46 4.3 nsubj:pass argument is iobj if the verb has a direct object (Fig. 9a) or obj otherwise (Fig. 9b). nsubj:pass@iobj aux:pass (a) He was given case orders nsubj:pass@obj aux:pass (b) Orders Impersonal constructions can also be viewed as syntactic alternations: in French the postverbal complement has object-like properties (in particular the pronominalization with the quantitative clitic en (of-it)). obl@nsubj obj were by Impersonal them obl@iobj case given to nsubj@expl aux him Il It nsu"
W17-6507,W13-3724,0,0.0431398,"Missing"
W17-6507,P05-1011,0,0.012789,"Missing"
W17-6507,S14-2008,0,0.203066,"Missing"
W17-6507,F14-2031,1,0.923846,"gh diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightforward (as shown for in"
W17-6507,W12-3602,0,0.0290507,"nnotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-synt"
W17-6507,petrov-etal-2012-universal,0,0.115553,"Missing"
W17-6507,D17-1009,0,0.0785386,"Missing"
W17-6507,D09-1085,0,0.0169912,"e the rise of large annotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting"
W17-6507,L16-1376,0,0.398273,"dah@paris-sorbonne.fr bruno.guillaume@loria.fr marie.candito@linguist.univ-paris-diderot.fr Abstract released annotated versions of their treebanks, following the UD annotation scheme. Although UD has raised criticisms, both on the suitability of the scheme to meet linguistic typology (Croft et al., 2017) and on the current implementation of the UD treebanks (Gerdes and Kahane, 2016), the existence of many treebanks with same syntactic scheme does however ease crosslanguage linguistic analysis and enables parsers to generalize across languages at training time, as demonstrated by Ammar et al. (2016). The UD scheme favors dependencies between content words, in order to maximize parallelism between languages. Although this results in dependencies that are more semantic-oriented, the UD scheme lies at the surface syntax level and thus necessarily lacks abstraction over syntactic variation and does not fit all downstream applications’ needs (Schuster and Manning, 2016). This is partly why de Marneffe and Manning (2008) proposed a decade ago, in the Stanford Dependencies framework, several schemes with various semantic-oriented modifications of syntactic structures. Its graph-based, so-called"
W17-6507,P04-1041,0,\N,Missing
W17-6507,L16-1262,0,\N,Missing
W18-4923,J17-4005,0,0.0607614,"Missing"
W18-4923,C16-1286,1,0.791725,"Savary (2018) involves a gamified interface allowing MWE researchers to guess the meaning of opaque MWEs in other languages. However, we could find no publication concerned with evaluating human ability to identify MWEs in a text, without taking their interpretation into account. On the other hand, voluntary crowdsourcing, especially in the form of Games with a Purpose (GWAPs), has proven effective in terms of both the quantity and quality of the data produced. Successful examples of such platforms include JeuxDeMots (Lafourcade, 2007), Phrase Detectives (Poesio et al., 2013), and ZombiLingo (Guillaume et al., 2016). We created a gamified platform named RigorMortis1 (see Figure 1), the first of its kind, for MWEs annotation in French2 . This platform includes a task enabling evaluation of the participants’ intuition concerning MWEs, the results of which we present here. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 See: rigor-mortis.org 2 We believe it is adaptable to any language. License details: http:// 207 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018"
W18-4923,L16-1262,0,0.0563718,"Missing"
W18-4923,P16-2026,0,0.0276575,"in recent years on the subject, in particular though the PARSEME international network (Savary et al., 2015). However, although some collective expert-based annotation initiatives have been successfully undertaken (Schneider et al., 2016; Savary et al., 2017), language resources are still limited in coverage and the need remains to identify newly-created MWEs. One potential solution is to exploit the so-called &quot;wisdom of the crowd&quot;. There have been several research papers on the interpretation of MWEs by native speakers, in particular by Gibbs (Gibbs, 1992; Gibbs et al., 1997). More recently, Ramisch et al. (2016) involved microworking crowdsourcing on Amazon Mechanical Turk. Finally, the experiment described in Krstev and Savary (2018) involves a gamified interface allowing MWE researchers to guess the meaning of opaque MWEs in other languages. However, we could find no publication concerned with evaluating human ability to identify MWEs in a text, without taking their interpretation into account. On the other hand, voluntary crowdsourcing, especially in the form of Games with a Purpose (GWAPs), has proven effective in terms of both the quantity and quality of the data produced. Successful examples of"
W18-6008,W16-1715,1,0.87219,"rg/u/overview/syntax.html). The goal of “maximizing parallelism between languages” might be of use for parser development of neighboring languages, but reducing language differences makes the resulting treebank by definition less interesting for typological research on syntax. In particular, UD does not account for the hierarchy between functional words and tends to flatten syntactic structures. The content-word-centric annotation is also problematic for the internal cohesion of the treebank (cf. the difficulty of coherently annotating complex prepositions that usually contain a content word, Gerdes & Kahane 2016) and it marks a break with syntactic traditions, where headedness is defined by distributional properties of individual words (Bloomfield 1933), see Section 2.1 One of the central advantages of dependency grammar is the clear distinction of category (the POS, i.e. an intrinsic distributional class) and function (i.e. the specific role a word plays towards another word). Sentences such as She became an architect and proud of it which have given rise to a considerable amount of scholarly discussions (Sag 2003) because an X-bar based phrase structure analysis requires deciding on the category of"
W18-6008,J93-2004,0,0.0604717,"three different UD relation labels (obj/xcomp/ccomp). We propose a new surface-syntactic annotation scheme, similar to UD, that we name SUD for Surface-syntactic Universal Dependencies. We want dependency links as well as the dependency labels to be defined based on purely syntactic criteria (Mel’čuk 1988), giving dependency structures closer to traditional dependency syntax (Meaning-Text Theory, Mel’čuk 1988; Word Grammar, Hudson 1984, 2007; Prague Dependency Treebank, Hajič et al. 2017) and headed constituency trees in phrase structure grammar (X-bar Syntax, Jackendoff 1977; Penn Treebank, Marcus et al. 1993). We also propose a hierarchy of SUD dependency relations that allows for under-specifications of dependency labeling. We conceived the SUD scheme as an alternative to UD and not as a competing annotation scheme, which means that the annotation scheme should have the same information content, the information being only expressed another way. Put differently, we looked for an annotation scheme based on distributionial criteria with an elementary conversion going both ways without loss, i.e. an “isomorphic” annotation. Since the principles underlying SUD are different, the isomorphism with UD ca"
W18-6008,de-marneffe-etal-2006-generating,0,0.0927647,"Missing"
W18-6008,L16-1376,0,0.0266789,"ation scheme that is applicable to all languages and proposing treebanks based on that scheme for more than 70 languages from different language families (Nivre et al. 2016). From the start, considerable efforts have been made to avoid an anglocentric scheme, going as far as analyzing English prepositions as case markers. The project is based on an ongoing and constantly evolving collaborative construction of the annotation scheme itself by means of an open online discussion group. The project welcomes and collaborates with enrichment efforts such as the enhanced UD annotation of deep syntax (Schuster & Manning 2016) or the annotation of multi-word expressions (Savary et al. 2015). Just as any annotation project, UD had to make choices among the different annotation options that commonly reflect opposing goals and downstream applications of the resulting treebanks. UD decided to stick to simple tree structures (compared to graphs with multiple governors) and to favor content words as heads, UD defines headedness indirectly via the category of the word: Content words are heads in UD and content words are usually understood as words belonging to open distributional classes, such as nouns, verbs, adjectives,"
W18-6008,C12-1147,0,0.0494644,"Missing"
W18-6008,W15-2134,0,0.049264,"Such a format is useful for every computation that concerns the form of the sentence such as word order (Chen et al. submitted) and the relation to prosody, etc. Conversely, UD might be a better entry point to the semantic content of the sentence. The lower dependency length gives psycholinguistic support to SUD treebanks. Possibly related is the fact that various experiments on parser performance also 72 7 minimization in 37 languages. Proceedings of the National Academy of Sciences, 112(33), 1033610341. consistently give an advantage to function-wordheaded structures (Schwartz et al. 2012, Silveira and Manning 2015, Kirilin and Versley 2015, Rehbein et al. 2017)15 which provides another raison d’être for parallel SUD treebanks. The whole UD 2.2 database, with its 122 treebanks, has been converted into SUD and is already accessible at https://gitlab.inria.fr/grew/ SUD. We would like to see this alternative to be distributed on the UD website as soon as possible and hope that the new scheme will benefit from discussions with the whole community and evolve in parallel to the UD scheme. Then SUD would become an alternative annotation option for UD treebank developers. As a last point, it appears that the co"
W19-7814,W10-1843,1,0.776051,"deep-syntactic level (@fixed) that there is a fixed expression: each &gt;unk@fixed other, ad &gt;unk@fixed hoc. It is interesting to observe that the fact that some phrase does not behave according to the POS of its head exists also in other contexts not related to MWEs. We also recommend the use of the ExtPOS feature in these cases, together with a Type feature to explicit the construction: • In titles (of books, movies, songs. . . ), the head can have various POS but it is most of the times used as a proper noun: the movie Gone with the wind, ExtPOS=PROPN, Type=Title. • In grafts (Deulofeu, 1999; Deulofeu et al., 2010), which is a phenomenon mainly observed in spoken production, where a clause is used instead of a noun phrase: he bought I think it is called dowels, ExtPOS=NOUN, Type=Graft. We also suggest that UD should adopt the ExpPOS feature or an equivalent mecanism. It will allow for easier generalizations and for more precise validation of the UD treebanks. For instance, in the current validation script of UD, the dependent advmod must be ADV unless it is a MWE, which means in UD, that the dependent has a fixed relation with one of its dependent. If UD adopted a feature-based encoding of MWEs, this co"
W19-7814,W18-6008,1,0.78614,"near isomorphic to UD (Universal Dependencies). Contrary to UD, it is based on syntactic criteria (favoring functional heads) and the relations are defined on distributional and functional bases. In this paper, we will recall and specify the general principles underlying SUD, present the updated set of SUD relations, discuss the central question of MWEs, and introduce an orthogonal layer of deep-syntactic features converted from the deep-syntactic part of the UD scheme. 1 Introduction SUD (Surface-syntactic Universal Dependencies) is an annotation scheme that we proposed in a previous paper (Gerdes et al., 2018) as an alternative of the UD (Universal Dependencies) annotation scheme (Nivre and al., 2019). SUD follows surface syntax criteria (especially distributional criteria) and can be automatically converted into the UD scheme. SUD has now been used in the development of a treebank for Naija (Courtin et al., 2018; Caron et al., 2019) and treebanks for French and Chinese are in development. Some principles underlying SUD have been further clarified and will be exposed here. Section 2 recalls and specifies the general principles of SUD. For a more detailed explanation of these principles, we refer th"
W19-7814,C18-1324,0,0.0679468,"unction) must be linked to their governor by the same relation. The characterization of a relation is based on the whole paradigm of elements that can commute in the dependent position, while UD relations strongly rely on the POS of the dependent. For instance, a unique comp:obj relation for direct object complements is considered in SUD, where UD considers three relations: obj for a nominal object (I imagine a dance), ccomp for a clausal object (I imagine (that) he dances) and xcomp for a clausal object without its own subject (I imagine to dance). This last relation raises another problem. (Przepiórkowski and Patejuk, 2018) extensively argue that UD’s xcomp is particularly unsatisfactory because it is based on a property (not having its own subject), which is orthogonal to the syntactic function and can even be realized with modifiers (He came without running).2 We make a clear distinction between surface-syntactic properties, which determine relation classes, and deep-syntactic properties, such as those expressed by xcomp. In Section 3.2, we will propose to represent deep-syntactic properties with specific relation extensions. Hence, a subset of 17 UD relations (nsubj, csubj, obj, iobj, obl, xcomp, ccomp, amod,"
W19-7814,L16-1376,0,0.0324874,"ed &gt;comp:obl@agent by his attitude; (Fr) il fait pleurer &gt;comp:obj@agent (les) enfants [‘he makes the kids cry’]. UD marks expletive elements with a dedicated relation expl. We consider that this is not a surfacesyntactic relation, but it is possible to keep this information in the dedicated deep-syntactic feature @expl. See an example of an expletive subject in Figure 3. Note that our annotation scheme remains centered around a surface syntactic analysis, but we isolate semantically-oriented features more explicitly. This allows for an easier interface with the Enhanced UD annotation effort (Schuster and Manning, 2016). comp:obj subj@expl It PRON comp:pred is AUX comp:obj@agent unlikely ADJ that SCONJ subj she PRON mod comes VERB now ADV Figure 3: SUD analysis for It is unlikely that she comes now Another example of deep-syntactic features is given by the annotation of light verb constructions: We use the @lvc deep-syntactic feature. It is a feature indicating that the dependent is a predicative noun and that the governor is a light verb without semantic contribution. Nouns in light verb constructions can have a comp:obl@x dependent. • (Fr) Avoir envie de manger [‘having the urge to eat’]: avoir &gt;comp:obj@l"
