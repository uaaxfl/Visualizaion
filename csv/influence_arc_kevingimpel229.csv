2020.acl-main.251,D18-1035,0,0.0180937,"l., 2020) and rescoring with autoregressive models (Kaiser et al., 2018; Wei et al., 2019; Ma et al., 2019; Sun et al., 2019). Ghazvininejad et al. (2020) and Saharia et al. (2020) proposed aligned cross entropy or latent alignment models and achieved the best results of all non-autoregressive models without refinement or rescoring. We propose training inference networks with autoregressive energies and outperform the best purely non-autoregressive methods. Another related approach trains an “actor” network to manipulate the hidden state of an autoregressive neural MT system (Gu et al., 2017; Chen et al., 2018; Zhou et al., 2020) in order to bias it toward outputs with better BLEU scores. This work modifies the original pretrained network rather than using it to define an energy for training an inference network. Energy-based models have had limited application in text generation due to the computational challenges involved in learning and inference in extremely large search spaces (Bakhtin et al., 2020). The use of inference networks to output approximate minimizers of a loss function is popular in variational inference (Kingma and Welling, 2013; Rezende et al., 2014), and, more recently, in struc"
2020.acl-main.251,D19-1633,0,0.189976,"egressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.1 1 Introduction The performance of non-autoregressive neural machine translation (NAT) systems, which predict tokens in the target language independently of each other conditioned on the source sentence, has been improving steadily in recent years (Lee et al., 2018; Ghazvininejad et al., 2019; Ma et al., 2019). One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations (Kim and Rush, 2016). This distilled corpus consists of source sentences paired with the translations produced by a pretrained autoregressive “teacher” system. As an alternative to training non-autoregressive translation systems on distilled corpora, we instead propose to train them to minimize the energy defined by a pretrained autoregressive teacher model. That is, we view non-autoregressive machine trans∗ Work partly done at Toyota Technologi"
2020.acl-main.251,D16-1139,0,0.312219,"ves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.1 1 Introduction The performance of non-autoregressive neural machine translation (NAT) systems, which predict tokens in the target language independently of each other conditioned on the source sentence, has been improving steadily in recent years (Lee et al., 2018; Ghazvininejad et al., 2019; Ma et al., 2019). One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations (Kim and Rush, 2016). This distilled corpus consists of source sentences paired with the translations produced by a pretrained autoregressive “teacher” system. As an alternative to training non-autoregressive translation systems on distilled corpora, we instead propose to train them to minimize the energy defined by a pretrained autoregressive teacher model. That is, we view non-autoregressive machine trans∗ Work partly done at Toyota Technological Institute at Chicago and the University of Chicago. 1 Code is available at https://github.com/ lifu-tu/ENGINE lation systems as inference networks (Tu and Gimpel, 2018"
2020.acl-main.251,D18-1149,0,0.367635,"aining a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.1 1 Introduction The performance of non-autoregressive neural machine translation (NAT) systems, which predict tokens in the target language independently of each other conditioned on the source sentence, has been improving steadily in recent years (Lee et al., 2018; Ghazvininejad et al., 2019; Ma et al., 2019). One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations (Kim and Rush, 2016). This distilled corpus consists of source sentences paired with the translations produced by a pretrained autoregressive “teacher” system. As an alternative to training non-autoregressive translation systems on distilled corpora, we instead propose to train them to minimize the energy defined by a pretrained autoregressive teacher model. That is, we view non-autoregressive machine trans∗ Work part"
2020.acl-main.251,D17-1210,0,0.0441455,"Missing"
2020.acl-main.251,D18-1336,0,0.10263,"Missing"
2020.acl-main.251,N18-2081,0,0.14306,"ts on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, we show that training to minimize the teacher’s energy significantly outperforms training with distilled outputs. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art results for non-autoregressive translation on these datasets, approaching the results of the autoregressive teachers. Our hope is that ENGINE will enable energy-based models to be applied more broadly for non-autoregressive generation in the future. 2 Related Work Non-autoregressive neural machine translation began with the work of Gu et al. (2018a), who found benefit from using knowledge distillation (Hinton et al., 2015), and in particular sequence-level distilled outputs (Kim and Rush, 2016). Subsequent work has narrowed the gap between nonautoregressive and autoregressive translation, including multi-iteration refinements (Lee et al., 2819 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2819–2826 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2018; Ghazvininejad et al., 2019; Saharia et al., 2020; Kasai et al., 2020) and rescoring with autoregressive models (Kaiser"
2020.acl-main.251,D15-1166,0,0.0199224,"Each row corresponds to the same O1 , and each column corresponds to the same O2 . 4 4.1 Experimental Setup Datasets We evaluate our methods on two datasets: IWSLT14 German (DE) → English (EN) and WMT16 Romanian (RO) → English (EN). All data are tokenized and then segmented into subword units using byte-pair encoding (Sennrich et al., 2016). We use the data provided by Lee et al. (2018) for RO-EN. 4.2 Autoregressive Energies We consider two architectures for the pretrained autoregressive (AR) energy function. The first is an autoregressive sequence-to-sequence (seq2seq) model with attention (Luong et al., 2015). The encoder is a two-layer BiLSTM with 512 units in each direction, the decoder is a two-layer LSTM with 768 units, and the word embedding size is 512. The second is an autoregressive transformer model (Vaswani et al., 2017), where both the encoder and decoder have 6 layers, 8 attention heads per layer, model dimension 512, and hidden dimension 2048. 4.3 Inference Network Architectures We choose two different architectures: a BiLSTM “tagger” (a 2-layer BiLSTM followed by a fullyconnected layer) and a conditional masked language model (CMLM; Ghazvininejad et al., 2019), a transformer with 6 l"
2020.acl-main.251,D19-1437,0,0.47424,"ed corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.1 1 Introduction The performance of non-autoregressive neural machine translation (NAT) systems, which predict tokens in the target language independently of each other conditioned on the source sentence, has been improving steadily in recent years (Lee et al., 2018; Ghazvininejad et al., 2019; Ma et al., 2019). One common ingredient in getting non-autoregressive systems to perform well is to train them on a corpus of distilled translations (Kim and Rush, 2016). This distilled corpus consists of source sentences paired with the translations produced by a pretrained autoregressive “teacher” system. As an alternative to training non-autoregressive translation systems on distilled corpora, we instead propose to train them to minimize the energy defined by a pretrained autoregressive teacher model. That is, we view non-autoregressive machine trans∗ Work partly done at Toyota Technological Institute at C"
2020.acl-main.251,P19-1125,0,0.160671,"enefit from using knowledge distillation (Hinton et al., 2015), and in particular sequence-level distilled outputs (Kim and Rush, 2016). Subsequent work has narrowed the gap between nonautoregressive and autoregressive translation, including multi-iteration refinements (Lee et al., 2819 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2819–2826 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2018; Ghazvininejad et al., 2019; Saharia et al., 2020; Kasai et al., 2020) and rescoring with autoregressive models (Kaiser et al., 2018; Wei et al., 2019; Ma et al., 2019; Sun et al., 2019). Ghazvininejad et al. (2020) and Saharia et al. (2020) proposed aligned cross entropy or latent alignment models and achieved the best results of all non-autoregressive models without refinement or rescoring. We propose training inference networks with autoregressive energies and outperform the best purely non-autoregressive methods. Another related approach trains an “actor” network to manipulate the hidden state of an autoregressive neural MT system (Gu et al., 2017; Chen et al., 2018; Zhou et al., 2020) in order to bias it toward outputs with better BLEU"
2020.acl-main.251,2020.emnlp-main.83,0,0.239224,"Work Non-autoregressive neural machine translation began with the work of Gu et al. (2018a), who found benefit from using knowledge distillation (Hinton et al., 2015), and in particular sequence-level distilled outputs (Kim and Rush, 2016). Subsequent work has narrowed the gap between nonautoregressive and autoregressive translation, including multi-iteration refinements (Lee et al., 2819 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2819–2826 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2018; Ghazvininejad et al., 2019; Saharia et al., 2020; Kasai et al., 2020) and rescoring with autoregressive models (Kaiser et al., 2018; Wei et al., 2019; Ma et al., 2019; Sun et al., 2019). Ghazvininejad et al. (2020) and Saharia et al. (2020) proposed aligned cross entropy or latent alignment models and achieved the best results of all non-autoregressive models without refinement or rescoring. We propose training inference networks with autoregressive energies and outperform the best purely non-autoregressive methods. Another related approach trains an “actor” network to manipulate the hidden state of an autoregressive neural MT system (Gu et"
2020.acl-main.251,P16-1162,0,0.00946776,"s) on the IWSLT14 DE-EN dev set with two energy/inference network combinations. Oracle lengths are used for decoding. O1 is the operation for feeding inference network outputs into the decoder input slots in the energy. O2 is the operation for computing the energy on the output. Each row corresponds to the same O1 , and each column corresponds to the same O2 . 4 4.1 Experimental Setup Datasets We evaluate our methods on two datasets: IWSLT14 German (DE) → English (EN) and WMT16 Romanian (RO) → English (EN). All data are tokenized and then segmented into subword units using byte-pair encoding (Sennrich et al., 2016). We use the data provided by Lee et al. (2018) for RO-EN. 4.2 Autoregressive Energies We consider two architectures for the pretrained autoregressive (AR) energy function. The first is an autoregressive sequence-to-sequence (seq2seq) model with attention (Luong et al., 2015). The encoder is a two-layer BiLSTM with 512 units in each direction, the decoder is a two-layer LSTM with 768 units, and the word embedding size is 512. The second is an autoregressive transformer model (Vaswani et al., 2017), where both the encoder and decoder have 6 layers, 8 attention heads per layer, model dimension 5"
2020.acl-main.251,N19-1335,1,0.69935,"Missing"
2020.acl-main.481,D18-1130,0,0.0195265,"on et al., 2015; Sukhbaatar et al., 2015), neural Turing machines (Graves et al., 2014), and differentiable neural computers (Graves et al., 2016). This paper focuses on models with inductive biases that produce particular structures in the memory, specifically those related to entities. Models for tracking and relating entities. A number of existing models have targeted entity tracking and coreference links for a variety of tasks. EntNet (Henaff et al., 2017) aims to track entities via a memory model. EntityNLM (Ji et al., 2017) represents entities dynamically within a neural language model. Hoang et al. (2018) augment a reading comprehension model to track entities, incorporating a set of auxiliary losses to encourage capturing of reference relations in the text. Dhingra et al. (2018) introduce a modified GRU layer designed to aggregate information across coreferent mentions. Neural models for coreference resolution. Several neural models have been developed for coreference resolution, most of them focused on modeling pairwise interactions among mentions or spans in a document (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017, 2018). These models use heuristics to avoid computing sc"
2020.acl-main.481,D17-1195,0,0.0200727,". Neural network architectures with external memory include memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), neural Turing machines (Graves et al., 2014), and differentiable neural computers (Graves et al., 2016). This paper focuses on models with inductive biases that produce particular structures in the memory, specifically those related to entities. Models for tracking and relating entities. A number of existing models have targeted entity tracking and coreference links for a variety of tasks. EntNet (Henaff et al., 2017) aims to track entities via a memory model. EntityNLM (Ji et al., 2017) represents entities dynamically within a neural language model. Hoang et al. (2018) augment a reading comprehension model to track entities, incorporating a set of auxiliary losses to encourage capturing of reference relations in the text. Dhingra et al. (2018) introduce a modified GRU layer designed to aggregate information across coreferent mentions. Neural models for coreference resolution. Several neural models have been developed for coreference resolution, most of them focused on modeling pairwise interactions among mentions or spans in a document (Wiseman et al., 2015; Clark and Mannin"
2020.acl-main.481,D19-1588,0,0.210103,"ed over the 100 annotated examples. We select the best threshold from the set {0.01, 0.02, · · · , 1.00}. The metric is then the number of errors corresponding to the best threshold.7 3.7 The Referential Reader (Liu et al., 2019a) is the most relevant baseline in the literature, and the most similar to PeTra. The numbers reported by Liu et al. (2019a) are obtained by a version of the model using BERTBASE , with only two memory cells. To compare against PeTra for other configurations, we retrain the Referential Reader using the code made available by the authors.8 We also report the results of Joshi et al. (2019) and Wu et al. (2019), although these numbers are not comparable since both of them train on the much larger OntoNotes corpus and just test on GAP. 4 4.1 1[oit ≥ α] Evaluation Metrics For GAP we evaluate models using F-score.6 First, we pick a threshold from the set {0.01, 0.02, · · · , 5 The computation of this probability includes the mention detection steps required byWebster et al. (2018). 6 GAP also includes evaluation related to gender bias, but this is not a focus of this paper so we do not report it. Baselines Results GAP results We train all the memory models, including the Referentia"
2020.acl-main.481,P10-2012,0,0.162766,"both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation. 1 Introduction Understanding text narratives requires maintaining and resolving entity references over arbitrary-length spans. Current approaches for coreference resolution (Clark and Manning, 2016b; Lee et al., 2017, 2018; Wu et al., 2019) scale quadratically (without heuristics) with length of text, and hence are impractical for long narratives. These models are also cognitively implausible, lacking the incrementality of human language processing (Tanenhaus et al., 1995; Keller, 2010). Memory models with finite memory and online/quasi-online entity resolution have linear runtime complexity, offering more scalability, cognitive plausibility, and interpretability. Memory models can be viewed as general problem solvers with external memory mimicking a Turing tape (Graves et al., 2014, 2016). Some of the earliest applications of memory networks in language understanding were for question answering, where the external memory simply stored all of the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretabilit"
2020.acl-main.481,D17-1018,0,0.41772,"s, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation. 1 Introduction Understanding text narratives requires maintaining and resolving entity references over arbitrary-length spans. Current approaches for coreference resolution (Clark and Manning, 2016b; Lee et al., 2017, 2018; Wu et al., 2019) scale quadratically (without heuristics) with length of text, and hence are impractical for long narratives. These models are also cognitively implausible, lacking the incrementality of human language processing (Tanenhaus et al., 1995; Keller, 2010). Memory models with finite memory and online/quasi-online entity resolution have linear runtime complexity, offering more scalability, cognitive plausibility, and interpretability. Memory models can be viewed as general problem solvers with external memory mimicking a Turing tape (Graves et al., 2014, 2016). Some of the ea"
2020.acl-main.481,N18-2108,0,0.112144,"Missing"
2020.acl-main.481,P18-2045,0,0.156791,", 2016). Some of the earliest applications of memory networks in language understanding were for question answering, where the external memory simply stored all of the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to as"
2020.acl-main.481,N18-2045,0,0.164128,", 2016). Some of the earliest applications of memory networks in language understanding were for question answering, where the external memory simply stored all of the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to as"
2020.acl-main.481,P19-1593,0,0.0678902,"the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to assess various memory architecture decisions, such as learning of memory initialization and separation of memory slots into key/value pairs. To test interpretability"
2020.acl-main.481,N19-1112,0,0.0937281,"the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to assess various memory architecture decisions, such as learning of memory initialization and separation of memory slots into key/value pairs. To test interpretability"
2020.acl-main.481,P18-1118,0,0.0305724,"pproach with far fewer parameters and a simpler architecture. We propose a new diagnostic evaluation and conduct a human evaluation to test the interpretability of the model, and find that our model again does better on this evaluation. In future work, we plan to extend this work to longer documents such as the recently released dataset of Bamman et al. (2019). Acknowledgments Memory models for NLP tasks. Memory models have been applied to several other NLP tasks in addition to coreference resolution, including targeted aspect-based sentiment analysis (Liu et al., 2018b), machine translation (Maruf and Haffari, 2018), narrative modeling (Liu et al., 2018a), and dialog state tracking (Perez and Liu, 2017). Our study of architectural choices for memory may also be relevant to models for these tasks. This material is based upon work supported by the National Science Foundation under Award Nos. 1941178 and 1941160. We thank the ACL reviewers, Sam Wiseman, and Mrinmaya Sachan for their valuable feedback. We thank Fei Liu and Jacob Eisenstein for answering questions regarding the Referential Reader. Finally, we want to thank all the annotators at TTIC who participated in the human evaluation study. 5423 Referen"
2020.acl-main.481,D16-1147,0,0.0334331,"entity resolution have linear runtime complexity, offering more scalability, cognitive plausibility, and interpretability. Memory models can be viewed as general problem solvers with external memory mimicking a Turing tape (Graves et al., 2014, 2016). Some of the earliest applications of memory networks in language understanding were for question answering, where the external memory simply stored all of the word/sentence embeddings for a document (Sukhbaatar et al., 2015; Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architectur"
2020.acl-main.481,Q18-1042,0,0.240106,"Kumar et al., 2016). To endow more structure and interpretability to memory, key-value memory networks were introduced by Miller et al. (2016). The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision (Henaff et al., 2017; Liu et al., 2018a,b, 2019a). We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model (Liu et al., 2019a) but substantially simpler. Experiments on the GAP (Webster et al., 2018) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to assess various memory architecture decisions, such as learning of memory initialization and separation of memory slots into key/value pairs. To test interpretability of memory models’ entity tracking, we propose a new diagnostic evaluation b"
2020.acl-main.481,P15-1137,0,0.02141,"ory model. EntityNLM (Ji et al., 2017) represents entities dynamically within a neural language model. Hoang et al. (2018) augment a reading comprehension model to track entities, incorporating a set of auxiliary losses to encourage capturing of reference relations in the text. Dhingra et al. (2018) introduce a modified GRU layer designed to aggregate information across coreferent mentions. Neural models for coreference resolution. Several neural models have been developed for coreference resolution, most of them focused on modeling pairwise interactions among mentions or spans in a document (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017, 2018). These models use heuristics to avoid computing scores for all possible span pairs in a document, an operation which is quadratic in the document length T assuming a maximum span length. Memory models for coreference resolution, including our model, differ by seeking to store information about entities in memory cells and then modeling the relationship between a token and a memory cell. This reduces computation from O(T 2 ) to O(T N ), where N is the number of memory cells, allowing memory models to be applied to longer texts by using the glo"
2020.acl-main.481,N16-1114,0,0.0979794,"Missing"
2020.acl-main.481,W12-4501,0,\N,Missing
2020.acl-main.481,P16-1061,0,\N,Missing
2020.acl-main.481,E17-1029,0,\N,Missing
2020.acl-main.481,N19-1423,0,\N,Missing
2020.bea-1.10,W18-0533,0,0.07803,"the correct answers, both with and without the surrounding passage context in the cloze questions. Simple features of the distractor and correct answer correlate with the annotations, though we find substantial benefit to additionally using large-scale pretrained models to measure the fit of the distractor in the context. Based on these analyses, we propose and train models to automatically select distractors, and measure the importance of model components quantitatively. 1 Introduction Multiple-choice cloze questions (MCQs) are widely used in examinations and exercises for language learners (Liang et al., 2018). The quality of MCQs depends not only on the question and choice of blank, but also on the choice of distractors, i.e., incorrect answers. Distractors, which could be phrases or single words, are incorrect answers that distract students from the correct ones. According to Pho et al. (2014), distractors tend to be syntactically and semantically homogeneous with respect to the correct answers. Distractor selection may be done manually through expert curation or automatically using simple methods based on similarity and dissimilarity to the correct answer (Pino et al., 2008; Alsubait et al., 201"
2020.bea-1.10,W03-0203,0,0.0968913,"with replacement 1000 times from the original test set with sample size equal to the corresponding test set size, and compare the F1 scores of two models. We use the thresholds tuned by the development set for F1 score computations, and assume significance at a p value of 0.05. 109 Figure 4: Ranks of distractors for question “The bank will notify its customers of the new policy.” The colors represent the normalized scores of the models and the numbers in the cells are the ranks of the candidates. 6 Related Work Existing approaches to distractor selection use WordNet (Fellbaum, 1998) metrics (Mitkov and Ha, 2003; Chen et al., 2015), word embedding similarities (Jiang and Lee, 2017), thesauruses (Sumita et al., 2005; Smith et al., 2010), and phonetic and morphological similarities (Pino and Eskenazi, 2009). Other approaches consider grammatical correctness, and introduce structural similarities in an ontology (Stasaski and Hearst, 2017), and syntactic similarities (Chen et al., 2006). When using broader context, bigram or n-gram co-occurrence (Susanti et al., 2018; Hill and Simha, 2016), context similarity (Pino et al., 2008), and context sensitive inference (Zesch and Melamud, 2014) have also been ap"
2020.bea-1.10,D14-1162,0,0.0834232,"Missing"
2020.bea-1.10,N18-1202,0,0.21178,"eclined dramatically. thousands T How many people are planning to attend the party? The large automobile manufacturer has a factory near here. The large automobile manufacturer has a factory near here. The large automobile manufacturer has a factory near here. MCDPARA Table 1: Example instances from MCDS ENT and MCDPARA. Contexts are shown and correct answers are bold and underlined. Part of the paragraph contexts are replaced by ellipses. We develop and train models for automatic distractor selection that combine simple features with representations from pretrained models like BERT and ELMo (Peters et al., 2018). Our results show that the pretrained models improve performance drastically over the feature-based models, leading to performance rivaling that of humans asked to perform the same task. By analyzing the models, we find that the pretrained models tend to give higher score to grammatically-correct distractors that are similar in terms of morphology and length to the correct answer, while differing sufficiently in semantics so as to avoid unaswerability. 2 Our target audience is Japanese business people with TOEIC level 300-800, which translates to preintermediate to upper-intermediate level. T"
2020.emnlp-main.449,W05-0620,0,0.204933,"Missing"
2020.emnlp-main.449,J07-2003,0,0.0619199,"d sequence labeling, there has been a great deal of work in the NLP community in designing non-local features, often combined with the development of approximate algorithms to incorporate them during inference. These include n-best reranking (Och et al., 2004), beam search (Lowerre, 1976), loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), stacked learning (Cohen and de Carvalho, 2005; Krishnan and Manning, 2006), sequential Monte Carlo algorithms (Yang and Eisenstein, 2013), dynamic programming approximations like cube pruning (Chiang, 2007; Huang and Chiang, 2007), dual decomposition (Rush et al., 2010; Martins et al., 2011), and methods based on black-box optimization like integer linear programming (Roth and Yih, 2004). These methods are often developed or applied with particular types of non-local energy terms in mind. By contrast, here we find that the framework of SPEN learning with inference networks can support a wide range of high-order energies for sequence labeling. 5 Experimental Setup We perform experiments on four tasks: Twitter partof-speech tagging (POS), named entity recognition (NER), CCG supertagging (CCG), an"
2020.emnlp-main.449,P11-2008,1,0.646295,"Missing"
2020.emnlp-main.449,hockenmaier-steedman-2002-acquiring,0,0.09414,"Missing"
2020.emnlp-main.449,P07-1019,0,0.0888004,"eling, there has been a great deal of work in the NLP community in designing non-local features, often combined with the development of approximate algorithms to incorporate them during inference. These include n-best reranking (Och et al., 2004), beam search (Lowerre, 1976), loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), stacked learning (Cohen and de Carvalho, 2005; Krishnan and Manning, 2006), sequential Monte Carlo algorithms (Yang and Eisenstein, 2013), dynamic programming approximations like cube pruning (Chiang, 2007; Huang and Chiang, 2007), dual decomposition (Rush et al., 2010; Martins et al., 2011), and methods based on black-box optimization like integer linear programming (Roth and Yih, 2004). These methods are often developed or applied with particular types of non-local energy terms in mind. By contrast, here we find that the framework of SPEN learning with inference networks can support a wide range of high-order energies for sequence labeling. 5 Experimental Setup We perform experiments on four tasks: Twitter partof-speech tagging (POS), named entity recognition (NER), CCG supertagging (CCG), and semantic role labeling"
2020.emnlp-main.449,P14-1130,0,0.0212174,"high-order energy as a CNN, we can think of the summation in Eq. 4 as corresponding to sum pooling over time of the feature map outputs. Tag Language Model (TLM): Tu and Gimpel (2018) defined an energy term based on a pretrained “tag language model”, which computes the probability of an entire sequence of labels. We also use a TLM, scoring a sequence of M + 1 consecutive labels in a way similar to Tu and Gimpel (2018); however, the parameters of the TLM are trained in our setting: F (yt−M , . . . , yt ) = − k=1 k=2 k>2 X t X yt>0 log(TLM(hyt−M , ..., yt0 −1 i)) t0 =t−M +1 There are some work (Lei et al., 2014; Srikumar and Manning, 2014; Yu et al., 2016) that use Kronecker product for higher order feature combinations with low-rank tensors. Here we use this form to express the computation when scoring the consecutive labels. where TLM(hyt−M , ..., yt0 −1 i) returns the softmax distribution over tags at position t0 (under the tag language model) given the preceding tag vectors. When each yt0 is a one-hot vector, this energy reduces to the negative log-likelihood of the tag sequence specified by yt−M , . . . , yt . 5572 Self-Attention (S-Att): We adopt the multi-head self-attention formulation from"
2020.emnlp-main.449,D13-1032,0,0.0573299,"Missing"
2020.emnlp-main.449,D18-1548,0,0.0341282,"Missing"
2020.emnlp-main.449,N19-1335,1,0.752435,"sifiers for sequence labeling, so test time inference speeds are unchanged between local models and our method. Enlarging the inference network architecture by adding one layer leads consistently to better results, rivaling or improving over a BiLSTM-CRF baseline, suggesting that training efficient inference networks with high-order energy terms can make up for errors arising from approximate inference. While we focus on sequence labeling in this paper, our results show the potential of developing high-order structured models for other NLP tasks in the future. 2 2.1 AΨ (x) ≈ arg min EΘ (x, y) Tu and Gimpel (2019) show that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent given pretrained energy functions. Joint training of energy functions and inference networks. Belanger and McCallum (2016) proposed a structured hinge loss for learning the energy function parameters Θ, using gradient descent for the “cost-augmented” inference step required during learning. Tu and Gimpel (2018) replaced the cost-augmented inference step in the structured hinge loss with training of a “cost-augmented inference network” FΦ (x) trained with the following goal: FΦ (x) ≈ arg m"
2020.emnlp-main.449,D13-1007,0,0.0237676,"and previously for sequence labeling (Tu and Gimpel, 2019). Moving beyond CRFs and sequence labeling, there has been a great deal of work in the NLP community in designing non-local features, often combined with the development of approximate algorithms to incorporate them during inference. These include n-best reranking (Och et al., 2004), beam search (Lowerre, 1976), loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), stacked learning (Cohen and de Carvalho, 2005; Krishnan and Manning, 2006), sequential Monte Carlo algorithms (Yang and Eisenstein, 2013), dynamic programming approximations like cube pruning (Chiang, 2007; Huang and Chiang, 2007), dual decomposition (Rush et al., 2010; Martins et al., 2011), and methods based on black-box optimization like integer linear programming (Roth and Yih, 2004). These methods are often developed or applied with particular types of non-local energy terms in mind. By contrast, here we find that the framework of SPEN learning with inference networks can support a wide range of high-order energies for sequence labeling. 5 Experimental Setup We perform experiments on four tasks: Twitter partof-speech taggi"
2020.emnlp-main.449,N16-1117,0,0.0179013,"e summation in Eq. 4 as corresponding to sum pooling over time of the feature map outputs. Tag Language Model (TLM): Tu and Gimpel (2018) defined an energy term based on a pretrained “tag language model”, which computes the probability of an entire sequence of labels. We also use a TLM, scoring a sequence of M + 1 consecutive labels in a way similar to Tu and Gimpel (2018); however, the parameters of the TLM are trained in our setting: F (yt−M , . . . , yt ) = − k=1 k=2 k>2 X t X yt>0 log(TLM(hyt−M , ..., yt0 −1 i)) t0 =t−M +1 There are some work (Lei et al., 2014; Srikumar and Manning, 2014; Yu et al., 2016) that use Kronecker product for higher order feature combinations with low-rank tensors. Here we use this form to express the computation when scoring the consecutive labels. where TLM(hyt−M , ..., yt0 −1 i) returns the softmax distribution over tags at position t0 (under the tag language model) given the preceding tag vectors. When each yt0 is a one-hot vector, this energy reduces to the negative log-likelihood of the tag sequence specified by yt−M , . . . , yt . 5572 Self-Attention (S-Att): We adopt the multi-head self-attention formulation from Vaswani et al. (2017). Given a matrix of the M"
2020.emnlp-main.449,P15-1109,0,0.0231639,"e networks using Adam with learning rate 0.001. 5.2 6 Training Local Classifiers. We consider local baselines that use a BiLSTM trained with the local loss `token . For POS, NER and CCG, we use a 1-layer BiLSTM with hidden size 100, and the word embeddings are fixed during training. For SRL, we use a 4layer BiLSTM with hidden size 300 and the word embeddings are fine-tuned. BiLSTM-CRF. We also train BiLSTM-CRF models with the standard conditional log-likelihood objective. A 1-layer BiLSTM with hidden size 100 is used for extracting input features. The CRF 3 Our SRL baseline is most similar to Zhou and Xu (2015), though there are some differences. We use GloVe embeddings while they train word embeddings on Wikipedia. We both use the same predicate context features. Results Parameterizations for High-Order Energies. We first compare several choices for energy functions within our inference network learning framework. In Section 3.3, we considered several ways to define the high-order energy function F . We compare performance of the parameterizations on three tasks: POS, NER, and CCG. The results are shown in Table 2. For VKP high-order energies, there are small differences between 2nd and 3rd order m"
2020.emnlp-main.449,W17-2632,1,0.668165,"Missing"
2020.emnlp-main.449,2020.spnlp-1.8,1,0.896266,"rk” where 4 is a structured cost function that computes the distance between its two arguments. The new optimization objective becomes: X min max [4(FΦ (x), y) Θ Φ hx,yi∈D − EΘ (x, FΦ (x)) + EΘ (x, y)]+ where D is the set of training pairs and [h]+ = max(0, h). Tu and Gimpel (2018) alternatively optimized Θ and Φ, which is similar to training in generative adversarial networks (Goodfellow et al., 2014). 2.3 An Objective for Joint Learning of Inference Networks One challenge with the optimization problem above is that it still requires training an inference network AΨ for test-time prediction. Tu et al. (2020a) proposed a “compound” objective that avoids this by training two inference networks jointly (with shared parameters), FΦ for cost-augmented inference and AΨ for test-time inference: X min max 5570 Θ Φ,Ψ hx,yi∈D [4(FΦ (x), y)−EΘ (x, FΦ (x))+EΘ (x, y)]+ | {z } margin-rescaled hinge loss + λ [−EΘ (x, AΨ (x))+EΘ (x, y)]+ {z } | perceptron loss As indicated, this loss can be viewed as the sum of the margin-rescaled and perceptron losses. Θ, Φ, and Ψ are alternatively optimized. The objective for the energy function parameters Θ is: ˆ ← arg min Θ Θ   4(FΦ (x), y) − EΘ (x, FΦ (x)) + EΘ (x, y) +"
2020.emnlp-main.449,2020.acl-main.251,1,0.541372,"rk” where 4 is a structured cost function that computes the distance between its two arguments. The new optimization objective becomes: X min max [4(FΦ (x), y) Θ Φ hx,yi∈D − EΘ (x, FΦ (x)) + EΘ (x, y)]+ where D is the set of training pairs and [h]+ = max(0, h). Tu and Gimpel (2018) alternatively optimized Θ and Φ, which is similar to training in generative adversarial networks (Goodfellow et al., 2014). 2.3 An Objective for Joint Learning of Inference Networks One challenge with the optimization problem above is that it still requires training an inference network AΨ for test-time prediction. Tu et al. (2020a) proposed a “compound” objective that avoids this by training two inference networks jointly (with shared parameters), FΦ for cost-augmented inference and AΨ for test-time inference: X min max 5570 Θ Φ,Ψ hx,yi∈D [4(FΦ (x), y)−EΘ (x, FΦ (x))+EΘ (x, y)]+ | {z } margin-rescaled hinge loss + λ [−EΘ (x, AΨ (x))+EΘ (x, y)]+ {z } | perceptron loss As indicated, this loss can be viewed as the sum of the margin-rescaled and perceptron losses. Θ, Φ, and Ψ are alternatively optimized. The objective for the energy function parameters Θ is: ˆ ← arg min Θ Θ   4(FΦ (x), y) − EΘ (x, FΦ (x)) + EΘ (x, y) +"
2020.emnlp-main.614,C18-1270,0,0.0267473,"ch methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach (§3) can be viewed as few-shot constituency parsing. 3 Few-Shot Constituency Parsing We apply Benepar (§3.1; Kitaev and Klein, 2018) as the base model for few-shot parsing. We present a simple data augmentation method (§3.2) and an iterative self-training strategy (§3.3) to further improve the performance. We suggest that such an approach should serve as a strong baseline for unsupervised parsing with supervised tuning. 3.1 Parsing Model The Benepar parsing model consists of (i) word embeddings, (ii) transformer–ba"
2020.emnlp-main.614,P06-1109,0,0.0797038,"Processing, pages 7611–7621, c November 16–20, 2020. 2020 Association for Computational Linguistics prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; D"
2020.emnlp-main.614,W06-2912,0,0.0596558,"Processing, pages 7611–7621, c November 16–20, 2020. 2020 Association for Computational Linguistics prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; D"
2020.emnlp-main.614,D11-1018,0,0.0501123,"Missing"
2020.emnlp-main.614,N19-1116,0,0.388037,"improved by a simple data augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.1 1 Introduction Recent work has considered neural unsupervised constituency parsing (Shen et al., 2018a; Drozdov et al., 2019; Kim et al., 2019b, inter alia), showing that it can achieve much better performance than trivial baselines. However, many of these approaches use the gold parse trees of all sentences in a development set for either early stopping (Shen et al., 2018a, 2019; Drozdov et al., 2019, inter alia) or hyperparameter tuning (Kim et al., 2019a). In contrast, models trained and tuned without any labeled data (Kim et al., 2019b; Peng et al., 2019) are much less competitive. 1 Project page: https://ttic.uchicago.edu/ freda/project/rsucp/ ˜ Are the labeled examples important in order to obtain decent unsu"
2020.emnlp-main.614,N16-1024,0,0.0245205,"19; Li et al., 2020) and model stability across different random seeds (Shi et al., 2019), for model selection, as discussed in unsupervised learning work (Smith and Eisner, 2005, 2006; Spitkovsky et al., 2010a,b, inter alia). An alternative is to use as few labeled examples in the development set as possible, and compare to few-shot parsing trained on the used examples as a strong baseline. In addition, we find that self-training is a useful post-processing step for unsupervised parsing. 7 A similar idea and similar results have been presented by Kim et al. (2019a), where they train an RNNG (Dyer et al., 2016) to fit the prediction of unsupervised parsing models. Our work does not necessarily imply that unsupervised parsers produce poor parses; they may be producing good parses that clash with the conventions of treebanks (Klein, 2005). If this is the case, then extrinsic evaluation of parsers in downstream tasks (Shi et al., 2018), e.g., machine translation (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Gimpel and Smith, 2014), may better show the potential of unsupervised methods. Acknowledgments We thank Allyson Ettinger, Yoon Kim, Jiayuan Mao, Shane Settle, and Shubham Toshniwal for helpful"
2020.emnlp-main.614,J14-2005,1,0.858884,"Missing"
2020.emnlp-main.614,W18-5452,0,0.0220471,"002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specif"
2020.emnlp-main.614,Q18-1016,0,0.012429,"ng (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a"
2020.emnlp-main.614,P19-1234,0,0.288106,"Missing"
2020.emnlp-main.614,P19-1228,0,0.521186,"Missing"
2020.emnlp-main.614,N19-1114,0,0.302588,"ata augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.1 1 Introduction Recent work has considered neural unsupervised constituency parsing (Shen et al., 2018a; Drozdov et al., 2019; Kim et al., 2019b, inter alia), showing that it can achieve much better performance than trivial baselines. However, many of these approaches use the gold parse trees of all sentences in a development set for either early stopping (Shen et al., 2018a, 2019; Drozdov et al., 2019, inter alia) or hyperparameter tuning (Kim et al., 2019a). In contrast, models trained and tuned without any labeled data (Kim et al., 2019b; Peng et al., 2019) are much less competitive. 1 Project page: https://ttic.uchicago.edu/ freda/project/rsucp/ ˜ Are the labeled examples important in order to obtain decent unsupervised parsing p"
2020.emnlp-main.614,P18-1249,0,0.43109,"t for either early stopping (Shen et al., 2018a, 2019; Drozdov et al., 2019, inter alia) or hyperparameter tuning (Kim et al., 2019a). In contrast, models trained and tuned without any labeled data (Kim et al., 2019b; Peng et al., 2019) are much less competitive. 1 Project page: https://ttic.uchicago.edu/ freda/project/rsucp/ ˜ Are the labeled examples important in order to obtain decent unsupervised parsing performance? How well can we do if we train on these labeled examples rather than merely using them for tuning? In this work, we consider training a supervised constituency parsing model (Kitaev and Klein, 2018) with very few examples as a strong baseline for unsupervised parsing tuned on labeled examples. We empirically characterize unsupervised and few-shot parsing across the spectrum of labeled data availability, finding that (i) tuning based on a few (as few as 15) labeled examples is sufficient to improve unsupervised parsers over fully unsupervised criteria by a significant margin; (ii) unsupervised parsing with supervised tuning does outperform few-shot parsing with fewer than 15 labeled examples, but few-shot parsing quickly dominates once there are more than 55 examples; and (iii) when few-s"
2020.emnlp-main.614,P04-1061,0,0.406969,"s prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Da"
2020.emnlp-main.614,P02-1017,0,0.268869,"cal Methods in Natural Language Processing, pages 7611–7621, c November 16–20, 2020. 2020 Association for Computational Linguistics prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htu"
2020.emnlp-main.614,N18-2072,0,0.0232047,"im et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach"
2020.emnlp-main.614,P19-1338,0,0.0204553,"ised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and p"
2020.emnlp-main.614,2020.acl-main.300,0,0.0253535,"step for unsupervised parsing. 5 Discussion While many state-of-the-art unsupervised parsing models are tuned on all labeled examples in a development set (Drozdov et al., 2019; Kim et al., 2019b; Wang et al., 2019, inter alia), we have demonstrated that, given the same data, few-shot parsing with simple data augmentation and self-training can consistently outperform all of these models by a large margin. We suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (Shen et al., 2018a, 2019; Kim et al., 2019b; Peng et al., 2019; Li et al., 2020) and model stability across different random seeds (Shi et al., 2019), for model selection, as discussed in unsupervised learning work (Smith and Eisner, 2005, 2006; Spitkovsky et al., 2010a,b, inter alia). An alternative is to use as few labeled examples in the development set as possible, and compare to few-shot parsing trained on the used examples as a strong baseline. In addition, we find that self-training is a useful post-processing step for unsupervised parsing. 7 A similar idea and similar results have been presented by Kim et al. (2019a), where they train an RNNG (Dyer et al., 2016) t"
2020.emnlp-main.614,J93-2004,0,0.0692211,"an iterative self-training strategy after obtaining each supervised or unsupervised parsing model. Concretely, we start with an arbitrary parsing model M0 . At the ith step of self-training, we (i) use the trained model from the previous step (i.e., Mi−1 ) to predict parse trees for sentences in the WSJ training set and those in the WSJ development set, and (ii) train a supervised parsing model Mi (Kitaev and Klein, 2018) to fit the prediction of Mi−1 . No gold labels are used in self-training. 4 4.1 Experiments Dataset and Training Details We use the WSJ portion of the Penn Treebank corpus (Marcus et al., 1993) to train and evaluate the models, replace all number tokens with a special token, and split standard train/dev/test sets following Kim et al. (2019b).3 For each criterion, we tune the hyperparameters of each model with respect to its 3 For analysis purposes (§4.5 and Figure 2), we use WSJ Section 24, instead of the standard development set (Section 22) as we train few-shot parsing on part of it. We do not use the standard test split (Section 23) to avoid tuning on the test set, hence our analysis numbers are not directly comparable with those reported in original papers. 4.2 Models and Tuning"
2020.emnlp-main.614,N06-1020,0,0.492384,"ed and few-shot parsing across the spectrum of labeled data availability, finding that (i) tuning based on a few (as few as 15) labeled examples is sufficient to improve unsupervised parsers over fully unsupervised criteria by a significant margin; (ii) unsupervised parsing with supervised tuning does outperform few-shot parsing with fewer than 15 labeled examples, but few-shot parsing quickly dominates once there are more than 55 examples; and (iii) when few-shot parsing is combined with a simple data augmentation method and self-training (Steedman et al., 2003; Reichart and Rappoport, 2007; McClosky et al., 2006, inter alia), only 15 examples are needed for few-shot parsing to begin to dominate. Based on these results, we propose the following two protocols for future work on unsupervised parsing: 1. Derive and use fully unsupervised criteria for hyperparameter tuning and model selection. 2. Use as few labeled examples as possible for model development and tuning, and compare to few-shot parsing models trained on the used examples as a strong baseline. We suggest future work to tune and compare models under each protocol separately. In addition, we present two side findings on unsupervised parsing: ("
2020.emnlp-main.614,W19-7713,0,0.027,"ound helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach (§3) can be viewed as few-shot constituency parsing. 3 Few-Shot Constituency Parsing We apply Benepar (§3.1; Kitaev and Klein, 2018) as the base model for few-shot parsing. We present a simple data augmentation method (§3.2) and an iterative self-training strategy (§3.3) to further improve the performance. We suggest that such an approach should serve as a strong baseline for unsupervised parsing with supervised tuning. 3.1 Parsing Model The Benepar parsing model consists of (i) word embeddings, (ii) transformer–based (Vaswani et al., 2017) word-"
2020.emnlp-main.614,D12-1077,0,0.0728124,"Missing"
2020.emnlp-main.614,N18-1202,0,0.0776369,"Missing"
2020.emnlp-main.614,P07-1078,0,0.197921,"ically characterize unsupervised and few-shot parsing across the spectrum of labeled data availability, finding that (i) tuning based on a few (as few as 15) labeled examples is sufficient to improve unsupervised parsers over fully unsupervised criteria by a significant margin; (ii) unsupervised parsing with supervised tuning does outperform few-shot parsing with fewer than 15 labeled examples, but few-shot parsing quickly dominates once there are more than 55 examples; and (iii) when few-shot parsing is combined with a simple data augmentation method and self-training (Steedman et al., 2003; Reichart and Rappoport, 2007; McClosky et al., 2006, inter alia), only 15 examples are needed for few-shot parsing to begin to dominate. Based on these results, we propose the following two protocols for future work on unsupervised parsing: 1. Derive and use fully unsupervised criteria for hyperparameter tuning and model selection. 2. Use as few labeled examples as possible for model development and tuning, and compare to few-shot parsing models trained on the used examples as a strong baseline. We suggest future work to tune and compare models under each protocol separately. In addition, we present two side findings on"
2020.emnlp-main.614,W08-0504,0,0.0505567,"beled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach (§3) can be viewed as few-shot constituency parsing. 3 Few-Shot Constituency Parsing We apply Benepar (§3.1; Kitaev and Klein, 2018) as the base model for few-shot parsing. We present a simple data augmentation method (§3.2) and an iterative self-training strategy (§3.3) to further improve the perform"
2020.emnlp-main.614,D18-1545,0,0.0361292,"Missing"
2020.emnlp-main.614,P19-1343,0,0.0226366,"a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach (§3) can be viewed as f"
2020.emnlp-main.614,P18-1108,0,0.210358,"sing can be further improved by a simple data augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.1 1 Introduction Recent work has considered neural unsupervised constituency parsing (Shen et al., 2018a; Drozdov et al., 2019; Kim et al., 2019b, inter alia), showing that it can achieve much better performance than trivial baselines. However, many of these approaches use the gold parse trees of all sentences in a development set for either early stopping (Shen et al., 2018a, 2019; Drozdov et al., 2019, inter alia) or hyperparameter tuning (Kim et al., 2019a). In contrast, models trained and tuned without any labeled data (Kim et al., 2019b; Peng et al., 2019) are much less competitive. 1 Project page: https://ttic.uchicago.edu/ freda/project/rsucp/ ˜ Are the labeled examples important in orde"
2020.emnlp-main.614,P19-1180,1,0.898652,"ing the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech taggi"
2020.emnlp-main.614,D18-1492,1,0.862261,"Missing"
2020.emnlp-main.614,P05-1044,0,0.120936,"et (Drozdov et al., 2019; Kim et al., 2019b; Wang et al., 2019, inter alia), we have demonstrated that, given the same data, few-shot parsing with simple data augmentation and self-training can consistently outperform all of these models by a large margin. We suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (Shen et al., 2018a, 2019; Kim et al., 2019b; Peng et al., 2019; Li et al., 2020) and model stability across different random seeds (Shi et al., 2019), for model selection, as discussed in unsupervised learning work (Smith and Eisner, 2005, 2006; Spitkovsky et al., 2010a,b, inter alia). An alternative is to use as few labeled examples in the development set as possible, and compare to few-shot parsing trained on the used examples as a strong baseline. In addition, we find that self-training is a useful post-processing step for unsupervised parsing. 7 A similar idea and similar results have been presented by Kim et al. (2019a), where they train an RNNG (Dyer et al., 2016) to fit the prediction of unsupervised parsing models. Our work does not necessarily imply that unsupervised parsers produce poor parses; they may be producing"
2020.emnlp-main.614,P06-1072,0,0.0223082,"ects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data au"
2020.emnlp-main.614,P09-1009,0,0.0437646,"c November 16–20, 2020. 2020 Association for Computational Linguistics prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a;"
2020.emnlp-main.614,P07-1049,0,0.053712,"ages 7611–7621, c November 16–20, 2020. 2020 Association for Computational Linguistics prior work, greatly affects the performance of all unsupervised parsing models tested; and (ii) selftraining can help improve all investigated unsupervised parsing (Shen et al., 2018a, 2019; Drozdov et al., 2019; Kim et al., 2019a) and few-shot parsing models, and thus can be considered as a post-processing step in future work. 2 Related Work Unsupervised parsing. During the past two decades, there has been a lot of work on both unsupervised constituency parsing (Klein and Manning, 2002, 2004; Bod, 2006a,b; Seginer, 2007; Snyder et al., 2009, inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 201"
2020.emnlp-main.614,D11-1118,0,0.0371365,"Missing"
2020.emnlp-main.614,W10-2902,0,0.0358611,"et al., 2019b; Wang et al., 2019, inter alia), we have demonstrated that, given the same data, few-shot parsing with simple data augmentation and self-training can consistently outperform all of these models by a large margin. We suggest that one possibility for future work is to focus on fully unsupervised criteria, such as language model perplexity (Shen et al., 2018a, 2019; Kim et al., 2019b; Peng et al., 2019; Li et al., 2020) and model stability across different random seeds (Shi et al., 2019), for model selection, as discussed in unsupervised learning work (Smith and Eisner, 2005, 2006; Spitkovsky et al., 2010a,b, inter alia). An alternative is to use as few labeled examples in the development set as possible, and compare to few-shot parsing trained on the used examples as a strong baseline. In addition, we find that self-training is a useful post-processing step for unsupervised parsing. 7 A similar idea and similar results have been presented by Kim et al. (2019a), where they train an RNNG (Dyer et al., 2016) to fit the prediction of unsupervised parsing models. Our work does not necessarily imply that unsupervised parsers produce poor parses; they may be producing good parses that clash with the"
2020.emnlp-main.614,E03-1008,0,0.485851,"eled examples. We empirically characterize unsupervised and few-shot parsing across the spectrum of labeled data availability, finding that (i) tuning based on a few (as few as 15) labeled examples is sufficient to improve unsupervised parsers over fully unsupervised criteria by a significant margin; (ii) unsupervised parsing with supervised tuning does outperform few-shot parsing with fewer than 15 labeled examples, but few-shot parsing quickly dominates once there are more than 55 examples; and (iii) when few-shot parsing is combined with a simple data augmentation method and self-training (Steedman et al., 2003; Reichart and Rappoport, 2007; McClosky et al., 2006, inter alia), only 15 examples are needed for few-shot parsing to begin to dominate. Based on these results, we propose the following two protocols for future work on unsupervised parsing: 1. Derive and use fully unsupervised criteria for hyperparameter tuning and model selection. 2. Use as few labeled examples as possible for model development and tuning, and compare to few-shot parsing models trained on the used examples as a strong baseline. We suggest future work to tune and compare models under each protocol separately. In addition, we"
2020.emnlp-main.614,D19-1098,0,0.0430062,"inter alia) and unsupervised dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2011, 2013, inter alia). Recent work has proposed several effective models for unsupervised or distantly supervised constituency parsing, optimizing either a language modeling objective (Shen et al., 2018a, 2019; Kim et al., 2019b,a, inter alia) or other downstream semantic objectives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-sh"
2020.emnlp-main.614,C16-1138,0,0.0254558,"ctives (Li et al., 2019; Shi et al., 2019). Some of them are tuned with labeled examples in the WSJ development set (Shen et al., 2018a, 2019; Htut et al., 2018; Drozdov et al., 2019; Kim et al., 2019a; Wang et al., 2019) or other labeled examples (Jin et al., 2018, 2019). Data augmentation. Data augmentation is a strategy for automatically increasing the amount and variety of data for training models, without actually collecting any new data. Such methods have been found helpful on many NLP tasks, including text classification (Kobayashi, 2018; Samanta et al., 2019), relation classification (Xu et al., 2016), and part-of-speech tagging (S¸ahin and Steedman, 2018). Part of our approach also falls into the category of data augmentation, applied specifically to constituency parsing from very few examples. Few-shot parsing. Sagae et al. (2008) show that a supervised dependency parsing model trained on 100 examples can work surprisingly well. Recent work has demonstrated the potential of few-shot dependency parsing on multiple languages (Aufrant et al., 2018; Meechan-Maddon and Nivre, 2019; Vania et al., 2019, inter alia). Our approach (§3) can be viewed as few-shot constituency parsing. 3 Few-Shot Co"
2020.emnlp-main.657,N10-1112,1,0.874309,"g step after estimating the generative classifier distributions. They used log loss as their discriminative objective. We also consider using a discriminative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overa"
2020.emnlp-main.657,P18-2103,0,0.0686436,"ich first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-"
2020.emnlp-main.657,P16-1154,0,0.177104,"simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imbalanced label distributions, and label n"
2020.emnlp-main.657,N18-2017,0,0.0373154,"uch methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most exi"
2020.emnlp-main.657,2021.ccl-1.108,0,0.178082,"Missing"
2020.emnlp-main.657,D15-1166,0,0.217154,"ing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imba"
2020.emnlp-main.657,marelli-etal-2014-sick,0,0.421086,"n its discriminative analogue, logistic regression. Yogatama et al. (2017) compared the performance of generative and discriminative classifiers and showed the advantages of neural generative classifiers in terms of sample complexity, data shift, and zero-shot and continual learning settings. Ding and Gimpel (2019) further improved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased t"
2020.emnlp-main.657,P19-1334,0,0.0854708,"Missing"
2020.emnlp-main.657,K18-1007,0,0.0945586,"o sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-likelihood of the label give"
2020.emnlp-main.657,W17-5308,0,0.0232681,"ved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a na"
2020.emnlp-main.657,D16-1244,0,0.214468,"Missing"
2020.emnlp-main.657,D14-1162,0,0.0901545,"strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models are initialized with GloVe pretrained word embeddings (Pennington et al., 2014).6 The word embedding dimension and the LSTM hidden state dimension are set to 300. All parameters, including the word embeddings, are updated during training. The label embedding dimensionality for GenNLI is set to 100. All the experiments are conducted 5 times with different random seeds and we report the median scores. GenNLI. The training includes two steps: the model is first trained with the generative objective only (Equation 1) for 20 epochs, followed by the discriminative fine-tuning objective only (one of the objectives in Table 1) for 15 epochs. Unless otherwise specified, we use in"
2020.emnlp-main.657,N18-1202,0,0.0641951,"ent uses a BiLSTM network with max pooling (Collobert and Weston, 2008) to learn generic sentence embeddings that perform well on several NLI tasks. ESIM has a relatively complicated network structure, including a recursive architecture of local inference modeling (MacCartney, 2009; Parikh et al., 2016) and inference composition. The pretrained models we compare to are BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). We select these models as our baselines because (1) they are open-source and are frequently used as baselines for NLI tasks in related work (Peters et al., 2018; Williams et al., 2018), and (2) their performance is strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models a"
2020.emnlp-main.657,S18-2023,0,0.0768613,"Missing"
2020.emnlp-main.657,D16-1264,0,0.0308844,"he training data shows severe label imbalance and when training labels are randomly corrupted. We additionally use GenNLI to generate hypotheses for given premises and labels. While the generations tend to have low diversity due to high lexical overlap with the premise, they are generally fluent and comport with the given labels, even in the small data setting. 2 2.1 Background and Related Work Generative Classifiers into the generative story. Lewis and Fan (2019) developed generative classifiers for question answering and achieved comparable performance to discriminative models on the SQuAD (Rajpurkar et al., 2016) dataset, and much better performance in challenging experimental settings. In this paper, we develop generative models for natural language inference inspired by models for sequence-to-sequence tasks. We additionally contribute an exploration of several discriminative objectives for fine-tuning our generative classifiers, finding multiple choices to outperform log loss used in prior work. We also compare our generative classifiers with fine-tuning of large-scale pretrained models, and characterize performance under other realistic settings such as imbalanced and noisy datasets. 2.2 While disc"
2020.emnlp-main.657,P06-2101,0,0.0910095,"minative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overall in our experiments. We call it the infinilog loss and define it as 2 Again, the label prior p(y) ends up canceling out because it is uniform over l"
2020.emnlp-main.657,W18-5446,0,0.0415137,"Missing"
2020.emnlp-main.657,N18-1101,0,0.554735,"erimental settings, including small training sets, imbalanced label distributions, and label noise. 1 Introduction Natural language inference (NLI) is the task of identifying the relationship between two fragments of text, called the premise and the hypothesis (Dagan et al., 2005; Dagan et al., 2013). The task was originally defined as binary classification, in which the labels are entailment (the premise implies the hypothesis) or not entailment. Subsequent variations added a third contradiction label. Most models for NLI are trained and evaluated on standard benchmarks (Bowman et al., 2015; Williams et al., 2018; Wang et al., 2018) in a discriminative manner (Conneau et al., 2017; Chen et al., 2017a). These benchmarks typically have relatively clean, balanced, and abundant annotated data, and there ∗ † Equal contribution. Contribution during visiting TTIC. is no distribution shift between the training and test sets. However, when data quality and conditions are not ideal, there is a substantial performance decrease for existing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative"
2020.emnlp-main.657,D18-1408,1,0.849005,"ssification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural"
2020.emnlp-main.685,P15-1136,0,0.0173983,"i et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2018) , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can still become impractically large when processing long documents, making the storing of all entity representations problematic. Is it necessary to maintain an unbounded number of mentions or entities? Psycholinguistic evidence suggests it is not, as human language processing i"
2020.emnlp-main.685,2020.tacl-1.5,0,0.227449,"next describe models that require tracking only a small, bounded number of entities at any time. To make coreference predictions for a document, we first encode the document and propose candilong-doc-coref date mentions. The proposed mentions are then processed sequentially and are either: (a) added to an existing entity cluster, (b) added to a new cluster, (c) ignored due to limited memory capacity (for bounded memory models), or (d) ignored as an invalid mention. Document Encoding is done using the SpanBERTLARGE model finetuned for OntoNotes and released as part of the coreference model of Joshi et al. (2020). We don’t further finetune the SpanBERT model. To encode long documents, we segment the document using the independent and overlap strategies described in Joshi et al. (2019).3 In overlap segmentation, for a token present in overlapping BERT windows, the token’s representation is taken from the BERT window with the most neighboring tokens of the concerned token. For both datasets we find that overlap slightly outperforms independent. Mention Proposal Given the encoded document, we next predict the top-scoring mentions which are to be clustered. The goal of this step is to have high recall, an"
2020.emnlp-main.685,D19-1588,0,0.647282,"ugmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy. 1 Introduction Long document coreference resolution poses runtime and memory challenges. Current best models for coreference resolution have large memory requirements and quadratic runtime in the document length (Joshi et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Mann"
2020.emnlp-main.685,P10-2012,0,0.0253797,"8) , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can still become impractically large when processing long documents, making the storing of all entity representations problematic. Is it necessary to maintain an unbounded number of mentions or entities? Psycholinguistic evidence suggests it is not, as human language processing is incremental (Tanenhaus et al., 1995; Keller, 2010) and has limited working memory (Baddeley, 1986). In practice, we find that most entities have a small spread (number of tokens from first to last mention of an entity), and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational issues, albeit at a potential accuracy tradeoff. Previous work on finite memory models for coreference resolution has shown potential, but has been tested only on short documents (Liu et al., 2019; Toshniwal et al., 2020). Moreover, this previous work make"
2020.emnlp-main.685,C12-1154,0,0.03426,"n the document length (Joshi et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2018) , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can still become impractically large when processing long documents, making the storing of all entity representations problematic. Is it necessary to maintain an unbounded number of mentions or entities? Psycholinguistic evidence suggests it is not, as hu"
2020.emnlp-main.685,2020.acl-main.481,1,0.843582,"sing is incremental (Tanenhaus et al., 1995; Keller, 2010) and has limited working memory (Baddeley, 1986). In practice, we find that most entities have a small spread (number of tokens from first to last mention of an entity), and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational issues, albeit at a potential accuracy tradeoff. Previous work on finite memory models for coreference resolution has shown potential, but has been tested only on short documents (Liu et al., 2019; Toshniwal et al., 2020). Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations. We propose a finite memory model that performs quasi-online coreference resolution,1 and test it on LitBank (Bamman et al., 2020) and OntoNotes (Pradhan et al., 2012). The model is trained to manage its limited memory by predicting whether to “forget"" an entity already being tracked in exchange for a new (currently untracked) entity. Our empirical results show that: (a) the model is competitive with an unbounded memory version, and (b) the model’s learned memory managem"
2020.emnlp-main.685,C14-1201,0,0.019401,"that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy. 1 Introduction Long document coreference resolution poses runtime and memory challenges. Current best models for coreference resolution have large memory requirements and quadratic runtime in the document length (Joshi et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2018) , the entity-mention paradigm stores representations only of the entity clusters, which are updated incremen"
2020.emnlp-main.685,N16-1114,1,0.919385,"Missing"
2020.emnlp-main.685,2020.acl-main.622,0,0.803796,"ork that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy. 1 Introduction Long document coreference resolution poses runtime and memory challenges. Current best models for coreference resolution have large memory requirements and quadratic runtime in the document length (Joshi et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman"
2020.emnlp-main.685,D13-1027,0,0.0115302,"LB-MEM vs. RB-MEM Table 6 compares the number of mentions ignored by LB-MEM and RBMEM. The LB-MEM model ignores far fewer mentions than RB-MEM. This is because while the RB-MEM model can only evict the LRU entity, which might not be optimal, the LB-MEM model can choose any entity for eviction. These statistics combined with the fact that the LB-MEM model typically outperforms RB-MEM mean that the LBMEM model is able to anticipate which entities are important and which are not. Error Analysis Table 7 presents the results of automated error analysis done using the Berkeley Coreference Analyzer (Kummerfeld and Klein, 2013) for the OntoNotes dev set. As the memory capacity of models increases, the errors shift from missing mention, missing entity, and divided entity categories, to conflated entities, extra mention, and extra entity categories. For the 5-cell configuration, the LB-MEM model outperforms RB-MEM in terms of tracking more entities. 7 Conclusion and Future Work We propose a memory model which tracks a small, bounded number of entities. The proposed model guarantees a linear runtime in document length, and in practice significantly reduces peak memory usage during training. Empirical results on LitBank"
2020.emnlp-main.685,D17-1018,0,0.357331,"ments, we segment the document using the independent and overlap strategies described in Joshi et al. (2019).3 In overlap segmentation, for a token present in overlapping BERT windows, the token’s representation is taken from the BERT window with the most neighboring tokens of the concerned token. For both datasets we find that overlap slightly outperforms independent. Mention Proposal Given the encoded document, we next predict the top-scoring mentions which are to be clustered. The goal of this step is to have high recall, and we follow previous work to threshold the number of spans chosen (Lee et al., 2017). Given a document D, we choose 0.3×|D |top spans for LitBank, and 0.4 × |D |for OntoNotes. Note that we pretrain the mention proposal model before training the mention proposal and mention clustering pipeline end-to-end, as done by Wu et al. (2020). The reason is that without pretraining, most of the mentions proposed by the mention proposal model would be invalid mentions, i.e., spans that are not mentions, which would not provide any training signal to the mention clustering stage. For both datasets, we sample invalid spans with 0.2 probability during training, so as to roughly equalize the"
2020.emnlp-main.685,N18-2108,0,0.131588,"mpractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2018) , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can still become impractically large when processing long documents, making the storing of all entity representations problematic. Is it necessary to maintain an unbounded number of mentions or entities? Psycholinguistic evidence suggests it is not, as human language processing is incremental (Tanenhaus et al., 1995; Ke"
2020.emnlp-main.685,P19-1593,0,0.0185081,"an language processing is incremental (Tanenhaus et al., 1995; Keller, 2010) and has limited working memory (Baddeley, 1986). In practice, we find that most entities have a small spread (number of tokens from first to last mention of an entity), and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational issues, albeit at a potential accuracy tradeoff. Previous work on finite memory models for coreference resolution has shown potential, but has been tested only on short documents (Liu et al., 2019; Toshniwal et al., 2020). Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations. We propose a finite memory model that performs quasi-online coreference resolution,1 and test it on LitBank (Bamman et al., 2020) and OntoNotes (Pradhan et al., 2012). The model is trained to manage its limited memory by predicting whether to “forget"" an entity already being tracked in exchange for a new (currently untracked) entity. Our empirical results show that: (a) the model is competitive with an unbounded memory version, and (b) the model"
2020.emnlp-main.685,P04-1018,0,0.0571478,"document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy. 1 Introduction Long document coreference resolution poses runtime and memory challenges. Current best models for coreference resolution have large memory requirements and quadratic runtime in the document length (Joshi et al., 2019; Wu et al., 2020), making them impractical for long documents. Recent work revisiting the entity-mention paradigm (Luo et al., 2004; Webster and Curran, 2014), which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-theart models (Xia et al., 2020). In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2018) , the entity-mention paradigm stores representations only of the entity clusters,"
2020.emnlp-main.685,W12-4501,0,0.394002,"vation suggests that tracking a limited, small number of entities at any time can resolve the computational issues, albeit at a potential accuracy tradeoff. Previous work on finite memory models for coreference resolution has shown potential, but has been tested only on short documents (Liu et al., 2019; Toshniwal et al., 2020). Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations. We propose a finite memory model that performs quasi-online coreference resolution,1 and test it on LitBank (Bamman et al., 2020) and OntoNotes (Pradhan et al., 2012). The model is trained to manage its limited memory by predicting whether to “forget"" an entity already being tracked in exchange for a new (currently untracked) entity. Our empirical results show that: (a) the model is competitive with an unbounded memory version, and (b) the model’s learned memory management outperforms a strong rule-based baseline.2 1 “Quasi-online” because document encoding uses bidirectional transformers with access to future tokens. 2 Code at https://github.com/shtoshni92/ 8519 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages"
2020.findings-emnlp.313,W14-3000,0,0.0546905,"Missing"
2020.findings-emnlp.313,P14-1098,0,0.0492436,"Missing"
2020.findings-emnlp.313,E12-1004,0,0.0252224,"I dataset called “Break” using external knowledge bases such as WordNet. Since sentence pairs in the dataset only differ by one or two words, similar to a pair of adversarial examples, it has broken many NLI systems. Due to the fact that Break does not have a training split, we use the aforementioned subsampled MNLI training set as a training set for this dataset. We select the best performing model on the development set of MNLI and evaluate it on Break. 4.2.2 Lexical Entailment We use the lexical splits for 3 datasets from Levy et al. (2015), including K2010 (Kotlerman et al., 2009), B2012 (Baroni et al., 2012), and T2014 (Turney and Mohammad, 2015). These datasets all similarly formulate lexical entailment as a binary task, and they were constructed from diverse sources, including human annotations, WordNet, and Wikidata. 3503 BERT +WordNet +Wikidata +W IKI NLI RoBERTa +WordNet +Wikidata +W IKI NLI MNLI 75.0 75.8 75.7 76.4 82.5 83.8 84.0 84.4 Natural Language Inference RTE PPDB Break SciTail 69.9 66.7 80.2 92.3 71.3 71.1 83.5 90.8 71.3 75.0 81.3 91.5 70.9 70.7 85.7 91.8 78.8 65.9 81.3 93.6 82.2 72.0 82.3 93.9 82.3 72.5 83.2 92.9 83.1 71.7 83.8 93.0 Lexical Entailment K2010 B2012 T2014 85.2 79.4 63."
2020.findings-emnlp.313,J15-2003,0,0.0208077,"et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s"
2020.findings-emnlp.313,D07-1017,0,0.13943,"Missing"
2020.findings-emnlp.313,D15-1075,0,0.0395989,"uage inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the natu"
2020.findings-emnlp.313,D19-1040,1,0.833327,"hmark several resources on XNLI (Conneau et al., 2018), showing that W IKI NLI benefits performance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the d"
2020.findings-emnlp.313,N19-1423,0,0.459826,"iversity, NJ, USA 2 University of Chicago, IL, USA 3 Toyota Technological Institute at Chicago, IL, USA {mchen,kgimpel}@ttic.edu,zeweichu@gmail.com,stratos@cs.rutgers.edu Abstract Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce W IKI NLI: a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in Wikipedia. We show that we can improve strong baselines such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) by pretraining them on W IKI NLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It ha"
2020.findings-emnlp.313,W07-1401,0,0.191366,"Missing"
2020.findings-emnlp.313,P18-2103,0,0.0435565,"Missing"
2020.findings-emnlp.313,C92-2082,0,0.174568,"without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text"
2020.findings-emnlp.313,D19-1060,1,0.892523,"Missing"
2020.findings-emnlp.313,P18-1152,0,0.0119571,"conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this"
2020.findings-emnlp.313,P18-1224,0,0.0626421,"ew Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet. W IKI NLI, on the other hand, seeks a middle road, striving to find largescale, naturally-annotated data that can improve performance on NLI tasks. The second approach aims to enable the model to leverage knowledge resources during prediction, for instance by computing attention weights over lexical relations in WordNet (Chen et al., 2018) or linking to reference entities in knowledge bases within the transformer block (Peters et al., 2019). While effective, this approach requires nontrivial and domain-specific modifications of the model itself. In contrast, we develop a simple pretraining method to leverage knowledge bases that can likewise improve the performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of ent"
2020.findings-emnlp.313,Q18-1048,0,0.0130016,"ey are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wiki"
2020.findings-emnlp.313,P09-2018,0,0.0511949,"Missing"
2020.findings-emnlp.313,P19-1313,0,0.0115826,"ent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet. W IKI NLI, on the"
2020.findings-emnlp.313,N15-1098,0,0.0242788,"test sets. Break. Glockner et al. (2018) constructed a challenging NLI dataset called “Break” using external knowledge bases such as WordNet. Since sentence pairs in the dataset only differ by one or two words, similar to a pair of adversarial examples, it has broken many NLI systems. Due to the fact that Break does not have a training split, we use the aforementioned subsampled MNLI training set as a training set for this dataset. We select the best performing model on the development set of MNLI and evaluate it on Break. 4.2.2 Lexical Entailment We use the lexical splits for 3 datasets from Levy et al. (2015), including K2010 (Kotlerman et al., 2009), B2012 (Baroni et al., 2012), and T2014 (Turney and Mohammad, 2015). These datasets all similarly formulate lexical entailment as a binary task, and they were constructed from diverse sources, including human annotations, WordNet, and Wikidata. 3503 BERT +WordNet +Wikidata +W IKI NLI RoBERTa +WordNet +Wikidata +W IKI NLI MNLI 75.0 75.8 75.7 76.4 82.5 83.8 84.0 84.4 Natural Language Inference RTE PPDB Break SciTail 69.9 66.7 80.2 92.3 71.3 71.1 83.5 90.8 71.3 75.0 81.3 91.5 70.9 70.7 85.7 91.8 78.8 65.9 81.3 93.6 82.2 72.0 82.3 93.9 82.3 72.5 83.2 92.9"
2020.findings-emnlp.313,2021.ccl-1.108,0,0.0511015,"Missing"
2020.findings-emnlp.313,P19-1598,0,0.028384,"mance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first app"
2020.findings-emnlp.313,P19-1335,0,0.0145051,"other languages and benchmark several resources on XNLI (Conneau et al., 2018), showing that W IKI NLI benefits performance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durre"
2020.findings-emnlp.313,I08-2112,0,0.0473055,"20) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet."
2020.findings-emnlp.313,W13-2117,0,0.0242457,"and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources"
2020.findings-emnlp.313,nastase-etal-2010-wikinet,0,0.0279548,"ransformer block (Peters et al., 2019). While effective, this approach requires nontrivial and domain-specific modifications of the model itself. In contrast, we develop a simple pretraining method to leverage knowledge bases that can likewise improve the performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of entities from the Wikipedia category graphs using predefined rules. Nastase et al. (2010) build a dataset based on Wikipedia article or category titles as well as the relations between categories and pages (“WikiNet”), but they do not empirically validate the usefulness of the dataset. In a similarly non-empirical vein, Zesch and Gurevych (2007) analyze the differences between the graphs from WordNet and the ones from Wikipedia categories. Instead, we address the empirical benefits of leveraging the category information in the modern setting of pretrained text representations. 3 W IKI NLI We now describe how the W IKI NLI dataset is constructed from Wikipedia and its principal cha"
2020.findings-emnlp.313,P19-1442,0,0.0155073,"uages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed"
2020.findings-emnlp.313,2020.acl-main.441,0,0.141866,"ing the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the naturally-annotated Wikipedia category graph,"
2020.findings-emnlp.313,P15-1146,0,0.037768,"Missing"
2020.findings-emnlp.313,N18-1202,0,0.00983287,"constructing W IKI NLI are as follows. We use the tables “categorylinks” and “page”: these two pages provide category pairs in which one category is the parent of the other. We use all direct category relations. To eliminate trivial pairs, we remove pairs where either is a substring of the other. To construct neutral pairs, we randomly sample two categories where neither category is the ancestor of the other in the category graph. To make neutral pairs more “related” (so that they are harder to discriminate from direct relations), we encode both categories into continuous vectors using ELMo (Peters et al., 2018) (averaging its three layers over all positions) and compute the cosine similarities between pairs.2 We pick the topranked pairs as neutral pairs in W IKI NLI. After the above processing, we remove categories longer than 50 characters and those containing certain keywords3 (see supplementary material for more results and examples on filtering criteria). We ensure the dataset is balanced, and the final dataset has 428,899 unique pairs. For the following experiments, unless otherwise specified, we only use 100,000 samples from W IKI NLI as training data and 5,000 as the development set since we"
2020.findings-emnlp.313,D19-1005,0,0.0742555,"Missing"
2020.findings-emnlp.313,N13-1008,0,0.0192546,"on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al."
2020.findings-emnlp.313,P16-1226,0,0.0232149,"Missing"
2020.findings-emnlp.313,P06-1101,0,0.109827,"Missing"
2020.findings-emnlp.313,N18-1101,0,0.224903,"is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the naturally-annotated Wikiped"
2020.findings-emnlp.313,W07-0201,0,0.0119581,"e performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of entities from the Wikipedia category graphs using predefined rules. Nastase et al. (2010) build a dataset based on Wikipedia article or category titles as well as the relations between categories and pages (“WikiNet”), but they do not empirically validate the usefulness of the dataset. In a similarly non-empirical vein, Zesch and Gurevych (2007) analyze the differences between the graphs from WordNet and the ones from Wikipedia categories. Instead, we address the empirical benefits of leveraging the category information in the modern setting of pretrained text representations. 3 W IKI NLI We now describe how the W IKI NLI dataset is constructed from Wikipedia and its principal characteristics. Each Wikipedia article is associated with crowd-sourced categories that correspond to topics or concepts covered by that article. Wikipedia or3501 ganizes these categories into a directed graph that models their hierarchical relations. For inst"
2020.findings-emnlp.313,C08-1107,0,0.0527167,"ry relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Fi"
2020.findings-emnlp.313,W04-3206,0,0.106866,"are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … …"
2020.repl4nlp-1.20,D17-1070,0,0.189919,"ver the contextualized token embeddings in the span: exp(αk ) αk = v · ek ; ak = Pj `=i exp(α` ) sij = j X Experimental Setup ak · e k k=i where v is a learned parameter vector. This pooling method is a popular choice for many NLP tasks (Lee et al., 2017; Lin et al., 2017), and is the one used by Tenney et al. (2019b). Max pooling takes the maximum value over time for each dimension of the contextualized embeddings within the span. Max pooling has been frequently used to obtain fixed-dimensional sentence representations for classification tasks (Collobert et al., 2011; Hashimoto et al., 2017; Conneau et al., 2017). Endpoint is a simple concatenation of the endpoints of the span: sij = [ei ; ej ]. This is a popular choice for representing answer spans (Lee et al., 2016) in extractive question-answering tasks such as SQuAD (Rajpurkar et al., 2016). Note that in this case sij ∈ R2d . Implementation details All input strings are passed through contextual encoder models to obtain an embedding for each token. With frozen encoders the weighted average of outputs from all layers is used as the token representation ek (Tenney et al., 2019b), while for fine-tuned encoders the last layer output is used unless oth"
2020.repl4nlp-1.20,D16-1001,0,0.026406,"previously for specific tasks, such as the attention-weighted pooling of Lee et al. (2017) for coreference resolution; the endpointbased representation of Lee et al. (2016) and the “coherent” endpoint-based representation of Seo et al. (2019) for question answering; and combinations of differences and sums of endpoint representations for parsing and semantic role labeling (Stern et al., 2017; Ouchi et al., 2018). These are described in more detail in Section 3.2. Other recent work has considered pooling approaches such as the difference between endpoint representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant e"
2020.repl4nlp-1.20,N19-1423,0,0.445014,"et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive evaluation comparing various span representation methods across tasks is still lacking. In this work, we systematically compare and analyze a wide range of span"
2020.repl4nlp-1.20,N19-1116,0,0.0181008,"fference between endpoint representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work focuses on pooling over word representations, often finding that simple pooling operations like averaging perform surprisingly well (Wieting et al., 2016; Shen et al., 2018). Shen et al. (2018) did a similar empirical study to ours in spirit, comparing a variety of pooli"
2020.repl4nlp-1.20,D17-1206,0,0.0252897,"arned weighted average over the contextualized token embeddings in the span: exp(αk ) αk = v · ek ; ak = Pj `=i exp(α` ) sij = j X Experimental Setup ak · e k k=i where v is a learned parameter vector. This pooling method is a popular choice for many NLP tasks (Lee et al., 2017; Lin et al., 2017), and is the one used by Tenney et al. (2019b). Max pooling takes the maximum value over time for each dimension of the contextualized embeddings within the span. Max pooling has been frequently used to obtain fixed-dimensional sentence representations for classification tasks (Collobert et al., 2011; Hashimoto et al., 2017; Conneau et al., 2017). Endpoint is a simple concatenation of the endpoints of the span: sij = [ei ; ej ]. This is a popular choice for representing answer spans (Lee et al., 2016) in extractive question-answering tasks such as SQuAD (Rajpurkar et al., 2016). Note that in this case sij ∈ R2d . Implementation details All input strings are passed through contextual encoder models to obtain an embedding for each token. With frozen encoders the weighted average of outputs from all layers is used as the token representation ek (Tenney et al., 2019b), while for fine-tuned encoders the last layer ou"
2020.repl4nlp-1.20,N19-1112,0,0.233116,"constituent detection, constituent labeling, named entity labeling, semantic role labeling, mention detection, and coreference arc prediction (Section 2).1 All of the tasks we consider naturally involve span representations. Similar comparisons are done by Tenney et al. (2019b), where they use this probing approach to compare several pretrained contextual embedding models, while keeping the span representation method fixed to self-attentive pooling (Lin et al., 2017; Lee et al., 2017). Here we vary both the choice of contextualized embedding models (among BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XL1 Code available at shtoshni92/span-rep https://github.com/ 166 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 166–176 c July 9, 2020. 2020 Association for Computational Linguistics Net (Yang et al., 2019), and SpanBERT (Joshi et al., 2019a)) and the span representation methods. By analyzing the performance of each span representation method for multiple tasks, we aim to uncover the importance of choice of span representation. We follow the “edge probing” setup of Tenney et al. (2019b) and introduce two new tasks to this setup, namely constituen"
2020.repl4nlp-1.20,2021.ccl-1.108,0,0.119387,"Missing"
2020.repl4nlp-1.20,P15-1162,0,0.0567571,"Missing"
2020.repl4nlp-1.20,D19-1588,0,0.26273,"by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive evaluation comparing various span representation methods across tasks is still lacking. In this work, we systematically compare and analyze a wide range of span representations (Section 3.2) by probing the representations via various NLP tasks, including consti"
2020.repl4nlp-1.20,P14-1062,0,0.0498839,"es concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work focuses on pooling over word representations, often finding that simple pooling operations like averaging perform surprisingly well (Wieting et al., 2016; Shen et al., 2018). Shen et al. (2018) did a similar empirical study to ours in spirit, comparing a variety of pooling models for sentence representations across tasks. In this work we are mainly focusing on the models for computing span representations given pretrained token embeddings, but we also include"
2020.repl4nlp-1.20,P19-1340,0,0.0181763,"not. Figure 1: Probing architectures for span representation methods. The models are very similar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive evaluation comparing various span representa"
2020.repl4nlp-1.20,P18-1249,0,0.0184012,"ers to a constituent or not. Figure 1: Probing architectures for span representation methods. The models are very similar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive evaluation comparing va"
2020.repl4nlp-1.20,P19-1464,0,0.021021,"ring; and combinations of differences and sums of endpoint representations for parsing and semantic role labeling (Stern et al., 2017; Ouchi et al., 2018). These are described in more detail in Section 3.2. Other recent work has considered pooling approaches such as the difference between endpoint representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work foc"
2020.repl4nlp-1.20,D17-1018,0,0.284628,"This model can be used to decide whether a span (here [1, 3]) refers to a constituent or not. Figure 1: Probing architectures for span representation methods. The models are very similar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the"
2020.repl4nlp-1.20,D18-1191,0,0.0880358,"-dimensional contextual span embeddings. Finally, the span embeddings are fed into a two-layer MLP followed by a sigmoid layer to predict the labels. For multiclass probing tasks with |L |labels, the predictions are made independently with sep167 arate MLPs per label resulting in a [0, 1]|L |vector. Finally, some tasks involve a single span, whereas others (coreference, semantic role labeling) involve two spans; in the latter case, the MLP takes as input the concatenation of the representations corresponding to the two spans. 3.2 variants have been used in parsing and SRL (Stern et al., 2017; Ouchi et al., 2018). As in endpoint, sij ∈ R2d . Coherent is a span representation proposed by Seo et al. (2019) for indexing phrases in a queryagnostic manner for question answering. First, the endpoints of the span are split into four parts: Span Representation Methods Given a span s = [i, j] and its corresponding contextualized embeddings [ei , · · · , ej ], where ek ∈ Rd , a span representation module outputs a fixeddimensional span representation sij . Below we describe the various span representation methods compared in this work. Average pooling is a simple average of the contextualized embeddings in the"
2020.repl4nlp-1.20,N18-1202,0,0.0616991,"ar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive evaluation comparing various span representation methods across tasks is still lacking. In this work, we systematically compare and analyze"
2020.repl4nlp-1.20,W12-4501,0,0.0442545,"the predicate and its argument are given, and the goal is to classify the argument into its specific semantic roles (ARG0, ARG1, etc.). Mention detection is the task of predicting whether a span represents a mention of an entity or not. For example, in the sentence “Mary goes to the market”, the spans “Mary” and “the market” refer to mentions while all other spans are not mentions. The task is similar to named entity recognition (Tjong Kim Sang and De Meulder, 2003), but the mentions are not limited to named entities. We introduce this task as it is the first step for coreference resolution (Pradhan et al., 2012), if the candidate mentions are not explicitly given. Coreference arc prediction is the task of predicting whether a pair of spans refer to the same entity. For example, in the sentence “John is his own enemy”, “John” and “his” refer to the same entity. 3 Models In this section, we first briefly describe the probing model, which is borrowed from Tenney et al. (2019b) with the extension to different span representations (Figure 1), followed by details of the various span representation methods we compare in this work. 3.1 Probing Model The input to the model is a sentence d = {d1 , · · · , dT }"
2020.repl4nlp-1.20,D16-1264,0,0.0493989,"ks (Lee et al., 2017; Lin et al., 2017), and is the one used by Tenney et al. (2019b). Max pooling takes the maximum value over time for each dimension of the contextualized embeddings within the span. Max pooling has been frequently used to obtain fixed-dimensional sentence representations for classification tasks (Collobert et al., 2011; Hashimoto et al., 2017; Conneau et al., 2017). Endpoint is a simple concatenation of the endpoints of the span: sij = [ei ; ej ]. This is a popular choice for representing answer spans (Lee et al., 2016) in extractive question-answering tasks such as SQuAD (Rajpurkar et al., 2016). Note that in this case sij ∈ R2d . Implementation details All input strings are passed through contextual encoder models to obtain an embedding for each token. With frozen encoders the weighted average of outputs from all layers is used as the token representation ek (Tenney et al., 2019b), while for fine-tuned encoders the last layer output is used unless otherwise stated. We investigate four pretrained models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), SpanBERT (Joshi et al., 2019a), and XLNet (Yang et al., 2019). Each has both “base” and “large” variants, and we experiment w"
2020.repl4nlp-1.20,P19-1436,0,0.214177,"5 (b) Probing model for single-span tasks. This model can be used to decide whether a span (here [1, 3]) refers to a constituent or not. Figure 1: Probing architectures for span representation methods. The models are very similar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed"
2020.repl4nlp-1.20,P18-1041,0,0.018881,"al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work focuses on pooling over word representations, often finding that simple pooling operations like averaging perform surprisingly well (Wieting et al., 2016; Shen et al., 2018). Shen et al. (2018) did a similar empirical study to ours in spirit, comparing a variety of pooling models for sentence representations across tasks. In this work we are mainly focusing on the models for computing span representations given pretrained token embeddings, but we also include a variety of pretrained contextual embeddings. One 8 Conclusion We systematically compared multiple spa"
2020.repl4nlp-1.20,D14-1220,0,0.0285323,"are described in more detail in Section 3.2. Other recent work has considered pooling approaches such as the difference between endpoint representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work focuses on pooling over word representations, often finding that simple pooling operations like averaging perform surprisingly well (Wieting et al., 2016;"
2020.repl4nlp-1.20,P19-1180,1,0.831977,"int representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or even longer texts (Kalchbrenner et al., 2014; Iyyer et al., 2015; Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Shen et al., 2018, inter alia). Much of this work focuses on pooling over word representations, often finding that simple pooling operations like averaging perform surprisingly well (Wieting et al., 2016; Shen et al., 2018). Shen et al. (2018) did a similar empirical study to ours in spirit, comparing a variety of pooling models for sente"
2020.repl4nlp-1.20,D16-1035,0,0.0210301,"for coreference resolution; the endpointbased representation of Lee et al. (2016) and the “coherent” endpoint-based representation of Seo et al. (2019) for question answering; and combinations of differences and sums of endpoint representations for parsing and semantic role labeling (Stern et al., 2017; Ouchi et al., 2018). These are described in more detail in Section 3.2. Other recent work has considered pooling approaches such as the difference between endpoint representations (Wang and Chang, 2016; Cross and Huang, 2016) or a concatenation of endpoint and attention-based representations (Li et al., 2016). Other approaches concatenate additional specialized feature vectors, such as the span length or position information (Lee et al., 2017; He et al., 2018; Kuribayashi et al., 2019). Some work has also considered explicitly composing span representations via syntactic parse trees, such as recursive neural networks (Li et al., 2014), and some unsupervised parsing models produce span representations as a byproduct of training (Drozdov et al., 2019; Shi et al., 2019). At the same time, there has been significant effort devoted to the related problem of learning representations for sentences or eve"
2020.repl4nlp-1.20,P17-1076,0,0.332072,"an (here [1, 3]) refers to a constituent or not. Figure 1: Probing architectures for span representation methods. The models are very similar to that of Tenney et al. (2019b) but we explicitly separate the span representation part into a projection step followed by a choice among span representation methods. Introduction Fixed-dimensional span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering (Lee et al., 2016; Seo et al., 2019), coreference resolution (Lee et al., 2017), and constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019, inter alia). Such models initialized with contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) have achieved new state-of-the-art results for these tasks (Kitaev et al., 2019; Joshi et al., 2019b). Since spans can have arbitrary length (i.e., number of tokens), fixed-dimensional span representations involve some form of (parameterized) pooling of the token representations. Existing models typically pick a span representation method (dashed boxes in Figure 1) that works well for the task(s) of interest. However, a comprehensive"
2020.repl4nlp-1.3,P17-1151,0,0.022369,"ord in the sentence transforms the random variable sequentially, leading to a random variable that encodes its semantic information. Our word linear operator model (WLO) has two types of parameters for each word wi : a scaling factor Ai ∈ Rk and a translation factor Bi ∈ Rk . The word operators produce a sequence of random variables z0 , z1 , · · · , zn with z0 ∼ N (0, Ik ), where Ik is a k × k identity matrix, and the operations are defined as Introduction Probabilistic word embeddings have been shown to be useful for capturing notions of generality and entailment (Vilnis and McCallum, 2014; Athiwaratkun and Wilson, 2017; Athiwaratkun et al., 2018). In particular, researchers have found that the entropy of a word roughly encodes its generality, even though there is no training signal explicitly targeting this effect. For example, hypernyms tend to have larger variance than their corresponding hyponyms (Vilnis and McCallum, 2014). However, there is very little work on doing the analogous type of investigation for sentences. In this paper, we define probabilistic models that produce distributions for sentences. In particular, we choose a simple and interpretable probabilistic model that treats each word as an o"
2020.repl4nlp-1.3,P18-1001,0,0.0191889,"the random variable sequentially, leading to a random variable that encodes its semantic information. Our word linear operator model (WLO) has two types of parameters for each word wi : a scaling factor Ai ∈ Rk and a translation factor Bi ∈ Rk . The word operators produce a sequence of random variables z0 , z1 , · · · , zn with z0 ∼ N (0, Ik ), where Ik is a k × k identity matrix, and the operations are defined as Introduction Probabilistic word embeddings have been shown to be useful for capturing notions of generality and entailment (Vilnis and McCallum, 2014; Athiwaratkun and Wilson, 2017; Athiwaratkun et al., 2018). In particular, researchers have found that the entropy of a word roughly encodes its generality, even though there is no training signal explicitly targeting this effect. For example, hypernyms tend to have larger variance than their corresponding hyponyms (Vilnis and McCallum, 2014). However, there is very little work on doing the analogous type of investigation for sentences. In this paper, we define probabilistic models that produce distributions for sentences. In particular, we choose a simple and interpretable probabilistic model that treats each word as an operator that translates and"
2020.repl4nlp-1.3,D15-1075,0,0.0877939,"Missing"
2020.repl4nlp-1.3,K16-1002,0,0.042952,"8) and fix the megabatch size at 20. For deterministic models, d is cosine similarity, while for probabilistic models, we use the expected inner product of Gaussians. 4 4.1 Experiments Baseline Methods We consider two baselines that have shown strong results on sentence similarity tasks (Wieting and Gimpel, 2018). The first, word averaging (W ORDAVG), simply averages the word embeddings in the sentence. The second, long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) averaging (LSTM AVG), uses an LSTM to encode the sentence and averages the hidden vectors. Inspired by sentence VAEs (Bowman et al., 2016), we consider an LSTM based probabilistic baseline (LSTMG AUSSIAN) which builds upon LSTM AVG and uses separate linear transformations on the averaged hidden states to produce the mean and variance of a Gaussian distribution. We also benchmark several pretrained models, including GloVe (Pennington et al., 2014), Skipthought (Kiros et al., 2015), InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019), and ELMo (Peters et al., 2018). When using GloVe, we either sum embeddings (GloVe SUM) or average them (GloVe AVG) to produce a sentence vector. Similarly, for ELMo, we either sum the output"
2020.repl4nlp-1.3,S17-2001,0,0.0283055,"For evaluating sentence specificity, we use human-annotated test sets from four domains, including news, Twitter, Yelp reviews, and movie reviews, from Li and Nenkova (2015) and Ko et al. (2019). For the news dataset, labels are either “general” or “specific” and there is additionally a training set. For the other datasets, labels are real values indicating specificity. Statistics for these datasets are shown in Table 1. For analysis we also use the semantic textual s∈{s1 ,s2 ,n1 ,n2 } w∈s where λ is a hyperparameter tuned based on the performance on the 2017 semantic textual similarity (STS; Cer et al., 2017) data. We found prior 18 News Twitter Yelp Movie Majority baseline 54.6 73.4 44.5 67.6 58.1 Length Word Freq. SUM 55.5 10.1 54.6 22.1 Word Freq. AVG 61.5 0.0 28.5 0.0 Prior work trained on labeled sentence specificity data Li and Nenkova (2015) 81.6 55.3 63.3 57.5 67.9 75.0 70.6 Ko et al. (2019) Sentence embeddings from pretrained models GloVe SUM 70.4 32.2 62.8 49.0 54.6 -49.6 -59.0 -38.2 GloVe AVG InferSent 75.0 60.5 76.6 61.2 Skip-thought 57.7 2.9 14.1 27.2 BERT 64.5 20.8 29.5 18.1 ELMo SUM 65.4 46.2 72.7 59.3 ELMo AVG 56.2 -9.4 -0.9 -22.5 Our work W ORDAVG 54.6 -10.6 -32.3 -27.2 W ORDSUM 7"
2020.repl4nlp-1.3,I11-1068,0,0.0624401,"Missing"
2020.repl4nlp-1.3,D17-1070,0,0.0266413,"embeddings in the sentence. The second, long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) averaging (LSTM AVG), uses an LSTM to encode the sentence and averages the hidden vectors. Inspired by sentence VAEs (Bowman et al., 2016), we consider an LSTM based probabilistic baseline (LSTMG AUSSIAN) which builds upon LSTM AVG and uses separate linear transformations on the averaged hidden states to produce the mean and variance of a Gaussian distribution. We also benchmark several pretrained models, including GloVe (Pennington et al., 2014), Skipthought (Kiros et al., 2015), InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019), and ELMo (Peters et al., 2018). When using GloVe, we either sum embeddings (GloVe SUM) or average them (GloVe AVG) to produce a sentence vector. Similarly, for ELMo, we either sum the outputs from the last layer (ELMo SUM) or average them (ELMo AVG). For BERT, we take the representation for the “[CLS]” token. Expected Inner Product of Gaussians Let µ1 , µ2 be mean vectors and Σ1 , Σ2 be the variances predicted by models for a pair of input sentences. For the choice of d, following Vilnis and McCallum (2014), we use the expected inner product of Gaussian distributi"
2020.repl4nlp-1.3,N19-1423,0,0.0226721,"he second, long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) averaging (LSTM AVG), uses an LSTM to encode the sentence and averages the hidden vectors. Inspired by sentence VAEs (Bowman et al., 2016), we consider an LSTM based probabilistic baseline (LSTMG AUSSIAN) which builds upon LSTM AVG and uses separate linear transformations on the averaged hidden states to produce the mean and variance of a Gaussian distribution. We also benchmark several pretrained models, including GloVe (Pennington et al., 2014), Skipthought (Kiros et al., 2015), InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019), and ELMo (Peters et al., 2018). When using GloVe, we either sum embeddings (GloVe SUM) or average them (GloVe AVG) to produce a sentence vector. Similarly, for ELMo, we either sum the outputs from the last layer (ELMo SUM) or average them (ELMo AVG). For BERT, we take the representation for the “[CLS]” token. Expected Inner Product of Gaussians Let µ1 , µ2 be mean vectors and Σ1 , Σ2 be the variances predicted by models for a pair of input sentences. For the choice of d, following Vilnis and McCallum (2014), we use the expected inner product of Gaussian distributions: Z N (x; µ1 , Σ1 )N (x;"
2020.repl4nlp-1.3,D14-1162,0,0.0874647,"2018). The first, word averaging (W ORDAVG), simply averages the word embeddings in the sentence. The second, long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) averaging (LSTM AVG), uses an LSTM to encode the sentence and averages the hidden vectors. Inspired by sentence VAEs (Bowman et al., 2016), we consider an LSTM based probabilistic baseline (LSTMG AUSSIAN) which builds upon LSTM AVG and uses separate linear transformations on the averaged hidden states to produce the mean and variance of a Gaussian distribution. We also benchmark several pretrained models, including GloVe (Pennington et al., 2014), Skipthought (Kiros et al., 2015), InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019), and ELMo (Peters et al., 2018). When using GloVe, we either sum embeddings (GloVe SUM) or average them (GloVe AVG) to produce a sentence vector. Similarly, for ELMo, we either sum the outputs from the last layer (ELMo SUM) or average them (ELMo AVG). For BERT, we take the representation for the “[CLS]” token. Expected Inner Product of Gaussians Let µ1 , µ2 be mean vectors and Σ1 , Σ2 be the variances predicted by models for a pair of input sentences. For the choice of d, following Vilnis and McCal"
2020.repl4nlp-1.3,N18-1202,0,0.0483705,"(LSTM; Hochreiter and Schmidhuber, 1997) averaging (LSTM AVG), uses an LSTM to encode the sentence and averages the hidden vectors. Inspired by sentence VAEs (Bowman et al., 2016), we consider an LSTM based probabilistic baseline (LSTMG AUSSIAN) which builds upon LSTM AVG and uses separate linear transformations on the averaged hidden states to produce the mean and variance of a Gaussian distribution. We also benchmark several pretrained models, including GloVe (Pennington et al., 2014), Skipthought (Kiros et al., 2015), InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019), and ELMo (Peters et al., 2018). When using GloVe, we either sum embeddings (GloVe SUM) or average them (GloVe AVG) to produce a sentence vector. Similarly, for ELMo, we either sum the outputs from the last layer (ELMo SUM) or average them (ELMo AVG). For BERT, we take the representation for the “[CLS]” token. Expected Inner Product of Gaussians Let µ1 , µ2 be mean vectors and Σ1 , Σ2 be the variances predicted by models for a pair of input sentences. For the choice of d, following Vilnis and McCallum (2014), we use the expected inner product of Gaussian distributions: Z N (x; µ1 , Σ1 )N (x; µ2 , Σ2 )dx x∈Rk = log N (0; µ1"
2020.repl4nlp-1.3,P18-1042,1,0.853996,"test sets for sentence specificity. max(0, δ − d(s1 , s2 ) + d(s1 , n1 ))+ 3.1 News 900 Regularization To avoid the mean or variance of the Gaussian distributions from becoming unbounded during training, resulting in degenerate solutions, we impose prior constraints on the operators introduced above. We force the transformed distribution after each operator to be relatively close to N (0, Ik ), which can be thought of as our “prior” knowledge of the operator. Then our training additionally minimizes X X λ KL(N (µ(w), Σ(w))kN (0, I)) 4.2 Datasets We use the preprocessed version of ParaNMT50M (Wieting and Gimpel, 2018) as our training set, which consists of 5 million paraphrase pairs. For evaluating sentence specificity, we use human-annotated test sets from four domains, including news, Twitter, Yelp reviews, and movie reviews, from Li and Nenkova (2015) and Ko et al. (2019). For the news dataset, labels are either “general” or “specific” and there is additionally a training set. For the other datasets, labels are real values indicating specificity. Statistics for these datasets are shown in Table 1. For analysis we also use the semantic textual s∈{s1 ,s2 ,n1 ,n2 } w∈s where λ is a hyperparameter tuned bas"
2020.repl4nlp-1.3,D18-1480,0,0.0491643,"Missing"
2020.spnlp-1.8,C18-1139,0,0.0120582,"convergence and improves training stability. For example, on NER, using the CE term reduces the number of epochs chosen by early stopping from ∼100 to ∼25. For POS, using the CE term reduces the number of epochs from ∼150 to ∼60. Global Energies. The results are shown in Table 4. Adding the backward (b) and word-augmented TLMs (c) improves over using only the forward TLM from Tu and Gimpel (2018). With the global energies, our performance is comparable to several strong results (90.94 of Lample et al., 2016 and 91.37 of Ma and Hovy, 2016). However, it is still lower than the state of the art (Akbik et al., 2018; Devlin et al., 2019), likely due to the lack of contextualized embeddings. In other work, we proposed and evaluated several other high-order energy terms for sequence labeling using our framework (Tu et al., 2020a). 8 Effect of Compound Objective and Joint Parameterizations. The compound objective is the sum of the margin-rescaled and perceptron losses, and outperforms them both (see Table 2). Across all tasks, the shared and stacked parameterizations are more accurate than the previous objectives. For the separated parameterization, the performance Related Work There are several efforts aim"
2020.spnlp-1.8,N16-1030,0,0.0110389,"more local task. Regardless, for both tasks, as shown in Section 4.2, the inclusion of the CE term speeds convergence and improves training stability. For example, on NER, using the CE term reduces the number of epochs chosen by early stopping from ∼100 to ∼25. For POS, using the CE term reduces the number of epochs from ∼150 to ∼60. Global Energies. The results are shown in Table 4. Adding the backward (b) and word-augmented TLMs (c) improves over using only the forward TLM from Tu and Gimpel (2018). With the global energies, our performance is comparable to several strong results (90.94 of Lample et al., 2016 and 91.37 of Ma and Hovy, 2016). However, it is still lower than the state of the art (Akbik et al., 2018; Devlin et al., 2019), likely due to the lack of contextualized embeddings. In other work, we proposed and evaluated several other high-order energy terms for sequence labeling using our framework (Tu et al., 2020a). 8 Effect of Compound Objective and Joint Parameterizations. The compound objective is the sum of the margin-rescaled and perceptron losses, and outperforms them both (see Table 2). Across all tasks, the shared and stacked parameterizations are more accurate than the previous"
2020.spnlp-1.8,P16-1101,0,0.0285363,"both tasks, as shown in Section 4.2, the inclusion of the CE term speeds convergence and improves training stability. For example, on NER, using the CE term reduces the number of epochs chosen by early stopping from ∼100 to ∼25. For POS, using the CE term reduces the number of epochs from ∼150 to ∼60. Global Energies. The results are shown in Table 4. Adding the backward (b) and word-augmented TLMs (c) improves over using only the forward TLM from Tu and Gimpel (2018). With the global energies, our performance is comparable to several strong results (90.94 of Lample et al., 2016 and 91.37 of Ma and Hovy, 2016). However, it is still lower than the state of the art (Akbik et al., 2018; Devlin et al., 2019), likely due to the lack of contextualized embeddings. In other work, we proposed and evaluated several other high-order energy terms for sequence labeling using our framework (Tu et al., 2020a). 8 Effect of Compound Objective and Joint Parameterizations. The compound objective is the sum of the margin-rescaled and perceptron losses, and outperforms them both (see Table 2). Across all tasks, the shared and stacked parameterizations are more accurate than the previous objectives. For the separated pa"
2020.spnlp-1.8,N19-1423,0,0.0157207,"oves training stability. For example, on NER, using the CE term reduces the number of epochs chosen by early stopping from ∼100 to ∼25. For POS, using the CE term reduces the number of epochs from ∼150 to ∼60. Global Energies. The results are shown in Table 4. Adding the backward (b) and word-augmented TLMs (c) improves over using only the forward TLM from Tu and Gimpel (2018). With the global energies, our performance is comparable to several strong results (90.94 of Lample et al., 2016 and 91.37 of Ma and Hovy, 2016). However, it is still lower than the state of the art (Akbik et al., 2018; Devlin et al., 2019), likely due to the lack of contextualized embeddings. In other work, we proposed and evaluated several other high-order energy terms for sequence labeling using our framework (Tu et al., 2020a). 8 Effect of Compound Objective and Joint Parameterizations. The compound objective is the sum of the margin-rescaled and perceptron losses, and outperforms them both (see Table 2). Across all tasks, the shared and stacked parameterizations are more accurate than the previous objectives. For the separated parameterization, the performance Related Work There are several efforts aimed at stabilizing and"
2020.spnlp-1.8,P11-2008,1,0.625344,"Missing"
2020.spnlp-1.8,2021.acl-long.349,0,0.113405,"Missing"
2020.spnlp-1.8,N13-1039,1,0.710476,"Missing"
2020.spnlp-1.8,D14-1162,0,0.0833462,"rd direction (denoted y 0t analogously to the forward TLM). Second, we include words as additional inputs to forward and backward TLMs. We define yet = g(x0 , ..., xt−1 , y0 , ..., yt−1 ) where g is a forward LSTM TLM. We define the backward version similarly (denoted yet0 ). The global energy is therefore E GE (y) = − T +1 X Named Entity Recognition (NER). We use the CoNLL 2003 English dataset (Tjong Kim Sang and De Meulder, 2003). We use the BIOES tagging scheme, following previous work (Ratinov and Roth, 2009), resulting in 17 NER labels. We use 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014). The task is evaluated using F1 score computed with the conlleval script. The architectures for the feature networks in the energy function and inference networks are all BiLSTMs. The architectures for tag language models are LSTMs. We use a dropout keep-prob of 0.7 for all LSTM cells. The hidden size for all LSTMs is 128. We use Adam (Kingma and Ba, 2014) and do early stopping on the development set. We use a learning rate of 5 · 10−4 . Similar to above, the weight for the CE term is set to 1. We consider three NER modeling configurations. NER uses only words as input and pretrained, fixed l"
2020.spnlp-1.8,W09-1119,0,0.112827,"equence of labels as input and returns a distribution over next labels. First, we add a TLM in the backward direction (denoted y 0t analogously to the forward TLM). Second, we include words as additional inputs to forward and backward TLMs. We define yet = g(x0 , ..., xt−1 , y0 , ..., yt−1 ) where g is a forward LSTM TLM. We define the backward version similarly (denoted yet0 ). The global energy is therefore E GE (y) = − T +1 X Named Entity Recognition (NER). We use the CoNLL 2003 English dataset (Tjong Kim Sang and De Meulder, 2003). We use the BIOES tagging scheme, following previous work (Ratinov and Roth, 2009), resulting in 17 NER labels. We use 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014). The task is evaluated using F1 score computed with the conlleval script. The architectures for the feature networks in the energy function and inference networks are all BiLSTMs. The architectures for tag language models are LSTMs. We use a dropout keep-prob of 0.7 for all LSTM cells. The hidden size for all LSTMs is 128. We use Adam (Kingma and Ba, 2014) and do early stopping on the development set. We use a learning rate of 5 · 10−4 . Similar to above, the weight for the CE term is set"
2020.spnlp-1.8,N18-2021,0,0.0172812,"acked test-time (AΨ ) common noun proper noun common noun proper noun adverb preposition adverb verb adjective POS AΨ − FΦ 0.2 2.2 1.9 2.6 NER AΨ − FΦ 0 0.4 0.5 1.7 clipping was subsequently replaced with a gradient norm constraint (Gulrajani et al., 2017). Miyato et al. (2018) proposed a novel weight normalization technique called spectral normalization. These methods may be applicable to the similar optimization problems solved in learning SPENs. Another direction may be to explore alternative training objectives for SPENs, such as those that use weaker supervision than complete structures (Rooshenas et al., 2018, 2019; Naskar et al., 2020). cost-augmented (FΦ ) proper noun common noun adjective proper noun + possessive adjective adverb preposition common noun verb 9 We contributed several strategies to stabilize and improve joint training of SPENs and inference networks. Our use of joint parameterizations mitigates the need for inference network fine-tuning, leads to complementarity in the learned inference networks, and yields improved performance overall. These developments offer promise for SPENs to be more easily applied to a broad range of NLP tasks. Future work will explore other structured pre"
2020.spnlp-1.8,W17-2632,1,0.717632,"we include global energy (GE) terms in the form of Eq. (8). We use h to denote an LSTM tag language model (TLM) that takes a sequence of labels as input and returns a distribution over next labels. We define y t = h(y0 , . . . , yt−1 ) to be the distribution given the preceding label vectors (under a LSTM language model). Then, the energy term is: E TLM (y) = − T +1 X   log yt> y t Twitter Part-of-Speech (POS) Tagging. We use the Twitter POS data from Gimpel et al. (2011) and Owoputi et al. (2013) which contain 25 tags. We use 100-dimensional skip-gram (Mikolov et al., 2013) embeddings from Tu et al. (2017). Like Tu and Gimpel (2018), we use a BiLSTM to compute the input feature vector for each position, using hidden size 100. We also use BiLSTMs for the inference networks. The output of the inference network is a softmax function, so the inference network will produce a distribution over labels at each position. The ∆ is L1 distance. We train the inference network using stochastic gradient descent (SGD) with momentum and train the energy parameters using Adam (Kingma and Ba, 2014). We also explore training the inference network using Adam when not using the local CE loss.1 In experiments with t"
2020.spnlp-1.8,2020.emnlp-main.449,1,0.896266,"are not widely used in NLP. Structured prediction is extremely common in NLP, but is typically approached using methods that are more limited than SPENs (such as conditional random fields) or models that suffer from a train/test mismatch (such as most auto-regressive models). SPENs offer a maximally expressive framework for structured prediction while avoiding the train/test mismatch and therefore offer great potential for NLP. However, the training and inference have deterred NLP researchers. While we have found benefit from training inference networks for machine translation in recent work (Tu et al., 2020b), that work assumed a fixed, pretrained energy function. Our hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications, including generation tasks in the future. 2 (1) AΨ (x) ≈ arg min EΘ (x, y) (2) y∈YR (x) When training the energy function parameters Θ, Tu and Gimpel (2018) replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network FΦ : FΦ (x) ≈ arg min (EΘ (x, y) − 4(y, y ∗ )) (3) y∈YR (x) where 4 is a structured cost function that computes the distance betwee"
2020.spnlp-1.8,2020.acl-main.251,1,0.50348,"are not widely used in NLP. Structured prediction is extremely common in NLP, but is typically approached using methods that are more limited than SPENs (such as conditional random fields) or models that suffer from a train/test mismatch (such as most auto-regressive models). SPENs offer a maximally expressive framework for structured prediction while avoiding the train/test mismatch and therefore offer great potential for NLP. However, the training and inference have deterred NLP researchers. While we have found benefit from training inference networks for machine translation in recent work (Tu et al., 2020b), that work assumed a fixed, pretrained energy function. Our hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications, including generation tasks in the future. 2 (1) AΨ (x) ≈ arg min EΘ (x, y) (2) y∈YR (x) When training the energy function parameters Θ, Tu and Gimpel (2018) replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network FΦ : FΦ (x) ≈ arg min (EΘ (x, y) − 4(y, y ∗ )) (3) y∈YR (x) where 4 is a structured cost function that computes the distance betwee"
2020.spnlp-1.8,N18-1007,1,0.836578,"Missing"
2020.spnlp-1.8,N19-1335,1,0.853044,"Missing"
2021.crac-1.12,N18-2108,0,0.0256608,"47.5 50.7 36.8 36.4 37.0 40.1 50.5 46.5 88.9 89.8 89.0 89.3 85.3 87.3 59.8 59.4 58.7 61.3 32.8 62.7 59.1 60.3 60.3 61.8 56.3 60.5 longdoc S longdoc S Joint Joint + PS 30K 79.2 79.6 78.2 78.2 87.6 87.5 59.4 58.4 60.3 62.5 42.9 45.5 88.6 88.7 60.1 59.4 69.5 70.0 Table 2: Performance of each model on 8 datasets measured by CoNLL F1 (Pradhan et al., 2012), except for GAP (F1 ) and WSC (accuracy). Some models use speaker (S ) features, genre (G ) features, or pseudo-singletons (PS). 3 3.1 Models Baselines We first evaluate a recent system (Xu and Choi, 2020) which extends a mention-ranking model (Lee et al., 2018) by making modifications in the decoding step. We find disappointing out-of-domain performance and difficulties with longer documents present in LB0 and WC (Appendix B.1). To overcome this issue, we study the longdoc model by Toshniwal et al. (2020), which is an entity-ranking model designed for long documents that reported strong results on both OntoNotes and LitBank. The original longdoc model uses a pretrained SpanBERT (Joshi et al., 2020) encoder which we replace with Longformer-large (Beltagy et al., 2020) as it can incorporate longer context. We retrain the longdoc model and finetune the"
2021.crac-1.12,2020.findings-emnlp.222,0,0.0514841,"Missing"
2021.crac-1.12,D19-1118,0,0.0144935,"asets. We hope that future work will continue to use this benchmark to measure progress towards truly general-purpose coreference resolution. Joint training is commonly used in NLP for training robust models, usually aided by learning dataset, language, or domain embeddings (e.g., (Stymne et al., 2018) for parsing; (Kobus et al., 2017; Tan et al., 2019) for machine translation). This is essentially what models for OntoNotes already do with genre embeddings (Lee et al., 2017). Unlike prior work, our test domains are unseen, so we cannot learn test-domain embeddings. For coreference resolution, Aralikatte et al. (2019) augment annotations using relation extraction systems to better incorporate world knowledge, a step towards generalization. Subramanian Acknowledgements and Roth (2019) use adversarial training to target This material is based upon work supported by names, with improvements on GAP. Moosavi and Strube (2018) incorporate linguistic features to im- the National Science Foundation under Award prove generalization to WC. Recently, Zhu et al. No. 1941178. (2021) proposed the OntoGUM dataset which consists of multiple genres. However, compared to the datasets used in our work, OntoGUM is much smalle"
2021.crac-1.12,2020.lrec-1.6,0,0.0324492,"statistics. 2.1 Training Datasets OntoNotes 5.0 (ON) (Weischedel et al., 2013) is a collection of news-like, web, and religious texts spanning seven distinct genres. Some genres are transcripts (phone conversations and news). As the primary training and evaluation set for developing coreference resolution models, many features specific to this corpus are tightly integrated into publicly released models. For example, the metadata includes information on the document genre and the speaker of every token (for spoken transcripts). Notably, it does not contain singleton annotations. LitBank (LB) (Bamman et al., 2020) is a set of public domain works of literature drawn from Project Gutenberg. On average, coreference in the first 2,000 tokens of each work is fully annotated for six entity types.2 We only use the first crossvalidation fold of LitBank, which we call LB0 . PreCo (PC) (Chen et al., 2018) contains documents from reading comprehension examinations, each fully annotated for coreference resolution. Notably, the corpus is the largest such dataset released. 2 They are people, facilities, locations, geopolitical entities, organizations, and vehicles. Evaluation Datasets Character Identification (CI) ("
2021.crac-1.12,W16-0708,1,0.821354,"i, klivescu, kgimpel}@ttic.edu, paxia@cs.jhu.edu, swiseman@cs.duke.edu Abstract capture idiosyncrasies inherent in that dataset. As an example, OntoNotes (Weischedel et al., 2013), While coreference resolution is defined indea widely-used general-purpose dataset, provides pendently of dataset domain, most models metadata, like the document genre and speaker infor performing coreference resolution do not formation. However, this assumption cannot be transfer well to unseen domains. We consolimade more broadly, especially if the input is raw date a set of 8 coreference resolution datasets text (Wiseman et al., 2016). targeting different domains to evaluate the offthe-shelf performance of models. We then mix Furthermore, while there are datasets aimed at three datasets for training; even though their capturing a broad set of genres (Weischedel et al., domain, annotation guidelines, and metadata 2013; Poesio et al., 2018; Zhu et al., 2021), they are differ, we propose a method for jointly trainnot mutually compatible due to differences in aning a single model on this heterogeneous data notation guidelines. For example, some datasets do mixture by using data augmentation to account not annotate singleton cl"
2021.crac-1.12,2020.acl-main.622,0,0.0856119,"et al. (2020), which is an entity-ranking model designed for long documents that reported strong results on both OntoNotes and LitBank. The original longdoc model uses a pretrained SpanBERT (Joshi et al., 2020) encoder which we replace with Longformer-large (Beltagy et al., 2020) as it can incorporate longer context. We retrain the longdoc model and finetune the Longformer encoder for each dataset, which proves to be competitive for coreference.4 For OntoNotes we train with and without the metadata of: (a) genre embedding, and (b) speaker identity which is introduced as part of the text as in Wu et al. (2020). 3.2 Joint Training With copious amounts of text in OntoNotes, PreCo, and LitBank, we can train a joint model on the combined dataset. However, this is impractical as the annotation guidelines between the datasets are misaligned (OntoNotes does not annotate singletons and uses metadata) and because there are substantially more documents in PreCo. first training a mention detector on OntoNotes and selecting the top-scoring mentions outside the gold mentions.5 We experiment with adding 30K, 60K, and 90K pseudo-singletons (in total, there are 156K gold mentions). We find adding 60K to be the bes"
2021.crac-1.12,2020.emnlp-main.695,1,0.812916,"Missing"
2021.crac-1.12,2020.emnlp-main.686,0,0.0185291,"6.8 87.8 49.8 58.7 59.5 55.6 53.3 39.5 59.6 60.1 59.9 62.1 47.5 50.7 36.8 36.4 37.0 40.1 50.5 46.5 88.9 89.8 89.0 89.3 85.3 87.3 59.8 59.4 58.7 61.3 32.8 62.7 59.1 60.3 60.3 61.8 56.3 60.5 longdoc S longdoc S Joint Joint + PS 30K 79.2 79.6 78.2 78.2 87.6 87.5 59.4 58.4 60.3 62.5 42.9 45.5 88.6 88.7 60.1 59.4 69.5 70.0 Table 2: Performance of each model on 8 datasets measured by CoNLL F1 (Pradhan et al., 2012), except for GAP (F1 ) and WSC (accuracy). Some models use speaker (S ) features, genre (G ) features, or pseudo-singletons (PS). 3 3.1 Models Baselines We first evaluate a recent system (Xu and Choi, 2020) which extends a mention-ranking model (Lee et al., 2018) by making modifications in the decoding step. We find disappointing out-of-domain performance and difficulties with longer documents present in LB0 and WC (Appendix B.1). To overcome this issue, we study the longdoc model by Toshniwal et al. (2020), which is an entity-ranking model designed for long documents that reported strong results on both OntoNotes and LitBank. The original longdoc model uses a pretrained SpanBERT (Joshi et al., 2020) encoder which we replace with Longformer-large (Beltagy et al., 2020) as it can incorporate long"
2021.findings-acl.17,2021.naacl-main.278,0,0.0870314,"Missing"
2021.findings-acl.17,2020.emnlp-main.525,0,0.0327541,"are distributed at score 3, which means that there is no fact supported by or contradictory to the shown tables. While the open-endedness of an instance usually depends on its topics (e.g., movie plots are open-ended), there are many cases where the models can benefit from better entity modeling, such as understanding what a particular entity type is capable of (e.g., see the last example in Sec. 6.3). Recent work has also found conducting human evaluation for long-form generation to be challenging, for example in the context of question answering (Krishna et al., 2021) and story generation (Akoury et al., 2020). Our observations for data-to-text generation complement theirs and we hope that our dataset can inspire future research on human evaluation for long-form text generation. 6.2 Distribution of Perplexity To determine the fraction of W IKI TABLE T that can be seen as constrained, we report the percentiles of perplexities for training and development splits in Table 9. From Table 8, it can be observed that instances with perplexities around 9.0 generally lead to model generations that are closely relevant to the reference texts and mostly supported by the input tables, and therefore are likely t"
2021.findings-acl.17,W05-0909,0,0.0223313,"ower, we parameterize our backward model as a transformer model with a 2-layer encoder and a 2-layer decoder.7 We use BPE with 30k merging operations. We randomly sample 500k instances from the training set and train base models on them when exploring different training strategies. We train a large model with the best setting (using the copy mechanism and cyclic loss) on the full training set. We train both models for 5 epochs. During training we perform early stopping on the development set using greedy decoding. We report BLEU (Papineni et al., 2002), ROUGE-L (RL) (Lin, 2004), METEOR (MET) (Banerjee and Lavie, 2005), and PARENT (Dhingra et al., 2019), including precision (PAR-P), recall (PAR-R), and F1 (PAR-F1) scores. The first three metrics consider the similarities between generated texts and references, whereas PARENT also considers the similarity between the generation and the table. When using PARENT, we use all three tables, i.e., the section, article, and title tables. As we are also interested in the repetitiveness of generated texts, we define a metric based on ngram repetitions which we call “REP”. REP computes the ratio of the number of repeated n-grams to the total number of n-grams within a"
2021.findings-acl.17,2020.acl-main.708,0,0.0191883,"sks with different levels of flexibility. We benchmark several training and decoding strategies on W IKI TABLE T. Our qualitative analysis shows that the best approaches can generate fluent and high quality texts but they struggle with coherence and factuality, showing the potential for our dataset to inspire future work on long-form generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating text based on structured data. Most existing data-to-text datasets focus on single-sentence generation, such as W IKI B IO (Lebret et al., 2016), LogicNLG (Chen et al., 2020), and ToTTo (Parikh et al., 2020). Other datasets are relatively small-scale and focus on long-form text generation, such as ROTOW IRE (Wiseman et al., 2017) and MLB (Puduppully et al., 2019). In this work, we cast generating Wikipedia sections as a data-to-text generation task and build a large-scale dataset targeting multi-sentence data-to-text generation with a variety of domains and data sources. To this end, we create a dataset that we call W IKI TABLE T (“Wikipedia Tables to Text”) that pairs Wikipedia sections with their corresponding 1 Code, data, and pretrained models are available at"
2021.findings-acl.17,N19-1235,0,0.0170159,"ere have been efforts in creating data-to-text datasets from various resources, including sports summaries (Wiseman et al., 2017; Puduppully et al., 2019), weather forecasts (Liang et al., 2009), and commentaries (Chen and Mooney, 2008). Most of the recent datasets focus on generating single sentences given tables, such as W IKI B IO, ToTTo, LogicNLG, and WikiTableText (Bao et al., 2018), or other types of data formats, such as data triples (Vougiouklis et al., 2017; Gardent et al., 2017; Nan et al., 2021), abstract meaning representations (Flanigan et al., 2016), minimal recursion semantics (Hajdik et al., 2019), or a set of concepts (Lin et al., 2020). Other than single sentences, there have been efforts in generating groups of sentences describing humans and animals (Wang et al., 2018), and generating a post-modifier phrase for a target sentence given a sentence context (Kang et al., 2019). In this work, our focus is long-form text generation and we are interested in automatically creating a large-scale dataset containing multiple types of data-to-text instances. As shown in Table 1, W IKI TABLE T differs from these datasets in that it is larger in scale and contains multi-sentence texts. More deta"
2021.findings-acl.17,2020.acl-main.398,0,0.0250307,"Missing"
2021.findings-acl.17,N19-1089,1,0.852212,"ingle sentences given tables, such as W IKI B IO, ToTTo, LogicNLG, and WikiTableText (Bao et al., 2018), or other types of data formats, such as data triples (Vougiouklis et al., 2017; Gardent et al., 2017; Nan et al., 2021), abstract meaning representations (Flanigan et al., 2016), minimal recursion semantics (Hajdik et al., 2019), or a set of concepts (Lin et al., 2020). Other than single sentences, there have been efforts in generating groups of sentences describing humans and animals (Wang et al., 2018), and generating a post-modifier phrase for a target sentence given a sentence context (Kang et al., 2019). In this work, our focus is long-form text generation and we are interested in automatically creating a large-scale dataset containing multiple types of data-to-text instances. As shown in Table 1, W IKI TABLE T differs from these datasets in that it is larger in scale and contains multi-sentence texts. More details are in the next section. Wikipedia has also been used to construct datasets for other text generation tasks, such as generating Wikipedia movie plots (Orbach and Goldberg, 2020; Rashkin et al., 2020) and short Wikipedia event summaries (Gholipour Ghalandari et al., 2020), and summ"
2021.findings-acl.17,P17-4012,0,0.0208195,"Missing"
2021.findings-acl.17,2021.naacl-main.393,0,0.0435041,"Missing"
2021.findings-acl.17,P83-1022,0,0.308666,"irs Wikipedia sections with their corresponding tabular data and various metadata. W IKI TABLE T contains millions of instances, covering a broad range of topics, as well as a variety of flavors of generation tasks with different levels of flexibility. We benchmark several training and decoding strategies on W IKI TABLE T. Our qualitative analysis shows that the best approaches can generate fluent and high quality texts but they struggle with coherence and factuality, showing the potential for our dataset to inspire future work on long-form generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating text based on structured data. Most existing data-to-text datasets focus on single-sentence generation, such as W IKI B IO (Lebret et al., 2016), LogicNLG (Chen et al., 2020), and ToTTo (Parikh et al., 2020). Other datasets are relatively small-scale and focus on long-form text generation, such as ROTOW IRE (Wiseman et al., 2017) and MLB (Puduppully et al., 2019). In this work, we cast generating Wikipedia sections as a data-to-text generation task and build a large-scale dataset targeting multi-sentence data-to-text generation with a variety of domai"
2021.findings-acl.17,P19-1598,0,0.0287671,"More details are in the next section. Wikipedia has also been used to construct datasets for other text generation tasks, such as generating Wikipedia movie plots (Orbach and Goldberg, 2020; Rashkin et al., 2020) and short Wikipedia event summaries (Gholipour Ghalandari et al., 2020), and summarizing Wikipedia documents (Zopf, 2018; Liu* et al., 2018) or summaries of aspects of interests (Hayashi et al., 2020) from relevant documents. As part of this work involves finding aligned tables and text, it is related to prior work on aligning Wikipedia texts to knowledge bases (Elsahar et al., 2018; Logan et al., 2019). 3 The W IKI TABLE T Dataset The W IKI TABLE T dataset pairs Wikipedia sections2 with their corresponding tabular data and various metadata; some of this data is relevant to entire Wikipedia articles (“article data”) or article structure (“title data”), while some is section-specific (“section data”). Each data table consists of a set of records, each of which is a tuple containing an attribute and a value. The instances in W IKI TABLE T cover a range of flavors of language generation. Some have more flexibility, requiring models to generate coherent stories based on the entities and knowledg"
2021.findings-acl.17,2020.emnlp-main.348,0,0.0297307,"Missing"
2021.findings-acl.17,D14-1162,0,0.0838645,"Missing"
2021.findings-acl.17,P19-1146,0,0.0127046,"ld be multiple qualifiers attached to a record, in which case we replicate the record for each qualifier separately. Similar linearization approaches have been used in prior work (Dhingra et al., 2019; Hwang et al., 2019; Herzig et al., 2020; Yin et al., 2020). With linearized tables, training and inference become similar to other sequence-to-sequence settings. We train our models with teacher-forcing and standard cross entropy loss unless otherwise specified. 4.1 Training Strategies We experiment with three types of modifications to standard sequence-to-sequence training: α-entmax. α-entmax (Peters et al., 2019) is a mapping from scores to a distribution that permits varying the level of sparsity in the distribution. This mapping function has been used in machine translation (Peters et al., 2019) and text generation (Martins et al., 2020). When using α-entmax in the decoder, we also replace the cross entropy loss with the α-entmax loss (Peters et al., 2019). Both 196 WikiTableText W IKI B IO ROTOW IRE MLB LogicNLG ToTTo DART W IKI TABLE T Vocab. 400.0k 11.3k 38.9k 122.0k 136.0k 33.2k 1.9M Tokens 185.0k 19.0M 1.6M 14.3M 52.7k 1.3M 717.1k 169.0M Examples 13.3k 728.0k 4.9k 26.3k 37.0k 136.0k 82.2k 1.5M∗"
2021.findings-acl.17,P19-1195,0,0.0518064,"Missing"
2021.findings-acl.17,W18-6502,0,0.0171309,"et al., 2009), and commentaries (Chen and Mooney, 2008). Most of the recent datasets focus on generating single sentences given tables, such as W IKI B IO, ToTTo, LogicNLG, and WikiTableText (Bao et al., 2018), or other types of data formats, such as data triples (Vougiouklis et al., 2017; Gardent et al., 2017; Nan et al., 2021), abstract meaning representations (Flanigan et al., 2016), minimal recursion semantics (Hajdik et al., 2019), or a set of concepts (Lin et al., 2020). Other than single sentences, there have been efforts in generating groups of sentences describing humans and animals (Wang et al., 2018), and generating a post-modifier phrase for a target sentence given a sentence context (Kang et al., 2019). In this work, our focus is long-form text generation and we are interested in automatically creating a large-scale dataset containing multiple types of data-to-text instances. As shown in Table 1, W IKI TABLE T differs from these datasets in that it is larger in scale and contains multi-sentence texts. More details are in the next section. Wikipedia has also been used to construct datasets for other text generation tasks, such as generating Wikipedia movie plots (Orbach and Goldberg, 202"
2021.findings-acl.17,2020.acl-main.745,0,0.0280267,"o the embeddings for each record, which helps models align attributes with values; and (3) we restart the positional embeddings at each hboci, such that models will not use the ordering of the input records. In addition, we add a special embedding to each record to indicate if it is from the section table or the article/title table. In Wikidata, there could be multiple qualifiers attached to a record, in which case we replicate the record for each qualifier separately. Similar linearization approaches have been used in prior work (Dhingra et al., 2019; Hwang et al., 2019; Herzig et al., 2020; Yin et al., 2020). With linearized tables, training and inference become similar to other sequence-to-sequence settings. We train our models with teacher-forcing and standard cross entropy loss unless otherwise specified. 4.1 Training Strategies We experiment with three types of modifications to standard sequence-to-sequence training: α-entmax. α-entmax (Peters et al., 2019) is a mapping from scores to a distribution that permits varying the level of sparsity in the distribution. This mapping function has been used in machine translation (Peters et al., 2019) and text generation (Martins et al., 2020). When us"
2021.findings-acl.17,L18-1510,0,0.0171106,"long-form text generation and we are interested in automatically creating a large-scale dataset containing multiple types of data-to-text instances. As shown in Table 1, W IKI TABLE T differs from these datasets in that it is larger in scale and contains multi-sentence texts. More details are in the next section. Wikipedia has also been used to construct datasets for other text generation tasks, such as generating Wikipedia movie plots (Orbach and Goldberg, 2020; Rashkin et al., 2020) and short Wikipedia event summaries (Gholipour Ghalandari et al., 2020), and summarizing Wikipedia documents (Zopf, 2018; Liu* et al., 2018) or summaries of aspects of interests (Hayashi et al., 2020) from relevant documents. As part of this work involves finding aligned tables and text, it is related to prior work on aligning Wikipedia texts to knowledge bases (Elsahar et al., 2018; Logan et al., 2019). 3 The W IKI TABLE T Dataset The W IKI TABLE T dataset pairs Wikipedia sections2 with their corresponding tabular data and various metadata; some of this data is relevant to entire Wikipedia articles (“article data”) or article structure (“title data”), while some is section-specific (“section data”). Each data"
2021.findings-acl.307,D18-1316,0,0.0189268,"en-level manipulation. Token-level manipulation methods have been widely studied in recent years. They typically create new examples by substituting (word) tokens with ones having the same desired features, such as synonym substitution (Zhang et al., 2015; Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018) or substitution with words having the same morphological features (Silfverberg et al., 2017). Such methods have been applied to generate adversarial or negative examples which help improve the robustness of neural network–based NLP models (Belinkov and Bisk, 2018; Shi et al., 2018a; Alzantot et al., 2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I sleeping Generated Sentences S VP cat love books S NP The VP love NP I books VP is sleeping (b) Constituency parsing. Original Sentences root poss My cat root dobj nsubj likes dobj nsubj milk I read books Other token-level manipulation methods introduce noise, such as random token shuffling and deletion (Wang et al., 2018; Wei and Zou, 2019; Dai a"
2021.findings-acl.307,2020.acl-main.676,0,0.151686,"rce languages, while Vania et al. (2019) demonstrate that such methods also help dependency parsing when very limited training data is available. S UB2 also falls into this category. The idea of same-label substructure substitution has been used to improve performance on structured prediction tasks such as semantic parsing (Jia and Liang, 2016), constituency parsing (Shi et al., 2020), dependency parsing (Dehouck and G´omezRodr´ıguez, 2020), named entity recognition (Dai and Adel, 2020), meaning representation–based text generation (Kedzie and McKeown, 2020), and compositional generalization (Andreas, 2020). To the best of our knowledge, however, S UB2 has not been systematically studied as a general data augmentation method for NLP tasks. In this work, we not only extend S UB2 to part-of-speech tagging and structured sentiment classification, but also present a variation that allows a broader range of NLP tasks (e.g., text classification) to benefit from syntactic parse trees. We evaluate S UB2 and several representative general data augmentation methods, which can be widely applied to various NLP tasks. Method Algorithm 1: S UB2 . Input: Original dataset D, desired dataset size N &gt; |D| Output:"
2021.findings-acl.307,2020.acl-main.499,0,0.106969,"e method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments the original dataset by linearly interpolating the vector representations of text and labels, to text classification (Guo et al., 2019; Sun et al., 2020), named entity recog"
2021.findings-acl.307,K17-2002,0,0.0348473,"Missing"
2021.findings-acl.307,2020.emnlp-main.95,0,0.0347059,"the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments the original dataset by linearly interpolating the vector representations of text and labels, to text classification (Guo et al., 2019; Sun et al., 2020), named entity recognition (Chen et al., 2020), and compositional generalization (Guo et al., 2020). a connection between S UB2 and tree substitution grammars (TSGs; Schabes, 1990), where the approach can be viewed as (1) estimating a TSG using the given corpus and (2) drawing new sentences from the estimated TSG. 3 We introduce the general framework we investigate in Section 3.1, and describe the variations of S UB2 which can be applied to text classification and other NLP applications in Section 3.2. 3.1 When constituency parse trees are used, there is Substructure Substitution (S UB2 ) As shown in Figure 1, given the original training"
2021.findings-acl.307,P19-4007,0,0.049805,"Missing"
2021.findings-acl.307,2020.coling-main.343,0,0.756245,"2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I sleeping Generated Sentences S VP cat love books S NP The VP love NP I books VP is sleeping (b) Constituency parsing. Original Sentences root poss My cat root dobj nsubj likes dobj nsubj milk I read books Other token-level manipulation methods introduce noise, such as random token shuffling and deletion (Wang et al., 2018; Wei and Zou, 2019; Dai and Adel, 2020). Models trained on the augmented datasets are expected to be more robust against the considered noise. Generated Sentences root poss My nsubj cat root dobj likes nsubj books dobj I read milk (c) Dependency parsing. Original Sentences I like the book Label: positive Generated Sentences I the movie book Label: positive I like the movie Label: positive I like like the Label: positive (d) Text classification. To apply S UB2 , we use text spans as substructures, with both the number of words in the span and the text classification label as constraints (see Sec. 3.2). Figure 1: Illustration of S UB"
2021.findings-acl.307,2020.coling-main.339,0,0.0808806,"Missing"
2021.findings-acl.307,N19-1423,0,0.0333966,"multi-layer perceptron for dependency parsing), we vary it between 128 and 512. We apply a 0.2 dropout ratio to the contextualized embeddings in the training stage. All other hyperparameters are the same as the default settings in the released codebases. 4.2 Lang. Aug. We compare S UB2 to the following baselines: • No augmentation (N OAUG), where the original training and development set are used. • Contextualized substitution (C TX S UB), where we apply contextualized augmentation (Kobayashi, 2018), masking out a random word token from the existing dataset and using multilingual BERT (mBERT; Devlin et al., 2019) to generate a different word. • Random shuffle (S HUF), where we randomly shuffle all the words in the original sentence, while keeping the original structured or nonstructured labels. It is worth noting that for dependency parsing, we shuffle the words, while maintaining the dependency arcs between individual word tokens; for constituency parsing, we shuffle the terminal nodes, and insert them back into the tree structure. Our S HUF method for constituency parsing is arguably more noisy than that for dependency parsing. All of the data augmentation baselines are explicit augmentations where"
2021.findings-acl.307,2020.emnlp-main.488,0,0.266406,"abel as constraints (see Sec. 3.2). Figure 1: Illustration of S UB2 for investigated tasks. We generate new examples by same-label substructure substitution, whether or not the generated examples are semantically or syntactically acceptable. Best viewed in color. 2 Related Work Data augmentation aims to generate new examples based on available ones, without actually collecting new data. Such methods reduce the cost of dataset Constrained text generation. Recent work has explored generating new examples by training a conditional text generation model (Bergmanis et al., 2017; Liu et al., 2020a; Ding et al., 2020; Liu et al., 2020b, inter alia), or applying post-processing on the examples generated by pretrained models (Yang et al., 2020; Wan et al., 2020; Yoo et al., 2020). In the data augmentation stage, given task-specific constraints, such models generate associated text accordingly. The generated examples, together with the original datasets, are used to further train models for the primary tasks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditio"
2021.findings-acl.307,P17-2090,0,0.374014,"and settings. S UB2 naturally fits structured prediction tasks such as part-of-speech tagging and parsing, where substructures exist in the annotations of the tasks. For more general NLP tasks such as text classification, we present variations of S UB2 which (1) define substructures based on text spans or parse trees for existing examples, and (2) generate new examples by substructure substitution based on the substructures and various kinds of constraints. Introduction Data augmentation has been found effective for various natural language processing (NLP) tasks, such as machine translation (Fadaee et al., 2017; Gao et al., 2019; Xia et al., 2019, inter alia), text classification (Wei and Zou, 2019; Quteineh et al., 2020), syntactic and semantic parsing (Jia and Liang, 2016; Shi et al., 2020; Dehouck and G´omez-Rodr´ıguez, 2020), semantic role labeling (F¨urstenau and Lapata, 2009), and dialogue understanding (Hou et al., 2018; Niu and Bansal, 2019). Such methods enhance the diversity of the training set by generating examples based on existing ones, and can make the learned models more robust against noise (Xie et al., 2020). Most existing work focuses on word-level manipulation (Kobayashi, 1 Proje"
2021.findings-acl.307,E09-1026,0,0.0598112,"Missing"
2021.findings-acl.307,ganitkevitch-callison-burch-2014-multilingual,0,0.0324961,"trube (2019), where all numbers in the S OTA column are not necessarily produced by the same model. † denotes new state-of-the-art results. 4.3 Part-of-Speech Tagging We conduct our experiments using the Universal Dependencies (UD; Nivre et al., 2016, 2020)7 dataset. First, we compare both N OAUG and S UB2 to the previous state-of-the-art performance (Heinzerling and Strube, 2019) to ensure that our baselines are strong enough (Table 1).8 Heinzerling and Strube (2019) take the token-wise concatenation of mBERT last-layer representations, byte-pair en6 Specifically, we use the lexical PPDB-XL (Ganitkevitch and Callison-Burch, 2014; Pavlick et al., 2015) of the appropriate language when applicable. 3498 7 8 http://universaldependencies.org/ We use UD v1.2 for direct comparison with existing work. Language (Treebank) 10 50 |D| 100 500 1,000 2× (C TX S UB) 5× (C TX S UB) 10× (C TX S UB) 50× (C TX S UB) 100× (C TX S UB) 38.3 35.5 39.8 31.2 32.0 55.1 55.9 55.1 52.3 53.1 62.9 62.1 61.7 60.9 58.2 78.1 81.4 81.7 79.3 77.1 80.1 81.0 80.8 78.0 75.9 2× (S HUF) 5× (S HUF) 10× (S HUF) 50× (S HUF) 100× (S HUF) 32.8 34.4 39.8 34.0 39.1 55.9 52.7 53.1 52.7 55.9 62.5 60.5 63.7 60.9 61.3 76.7 77.5 77.9 79.1 80.4 78.4 81.6 81.9 79.6 77.4"
2021.findings-acl.307,W17-3529,0,0.029081,"sks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments the original dataset by linearly interpolating the vector representations of text and labels, to text classification (Guo et al., 2019; Sun et al"
2021.findings-acl.307,D15-1157,0,0.0622221,"Missing"
2021.findings-acl.307,2020.emnlp-main.419,0,0.025219,"ncy parse trees can help part-of-speech tagging for low-resource languages, while Vania et al. (2019) demonstrate that such methods also help dependency parsing when very limited training data is available. S UB2 also falls into this category. The idea of same-label substructure substitution has been used to improve performance on structured prediction tasks such as semantic parsing (Jia and Liang, 2016), constituency parsing (Shi et al., 2020), dependency parsing (Dehouck and G´omezRodr´ıguez, 2020), named entity recognition (Dai and Adel, 2020), meaning representation–based text generation (Kedzie and McKeown, 2020), and compositional generalization (Andreas, 2020). To the best of our knowledge, however, S UB2 has not been systematically studied as a general data augmentation method for NLP tasks. In this work, we not only extend S UB2 to part-of-speech tagging and structured sentiment classification, but also present a variation that allows a broader range of NLP tasks (e.g., text classification) to benefit from syntactic parse trees. We evaluate S UB2 and several representative general data augmentation methods, which can be widely applied to various NLP tasks. Method Algorithm 1: S UB2 . Input: Origin"
2021.findings-acl.307,D14-1181,0,0.00325738,"2 typically achieves competitive performance among all investigated combinations of constraints, the gain over S UB2 with balanced trees is not consistent. Our results are in line with Shi et al. (2018b). We further use S UB2 with constraints of (1) text label, (2) phrase, and (3) phrase sentiment label, to augment the full SST training set, since it is the best augmentation method for few-shot sentiment classification, in terms of devtest accuracy (Table 7). In addition to sentences, we also add phrases (i.e., subtrees) as training examples, following most existing work (Socher et al., 2013; Kim, 2014; Brahma, 2018, inter alia),15 to boost performance. In this setting, we find that S UB2 helps set a new state of the art on the SST dataset (Table 8). 15 That is, unlike in Table 7, we apply the same settings as most existing work to produce numbers in Table 8. Conclusion and Future Work We investigate substructure substitution (S UB2 ), a family of data augmentation methods that generates new examples by same-label substructure substitution. Such methods help achieve competitive or better performance on the tasks of part-of-speech tagging, dependency parsing, constituency parsing, and text c"
2021.findings-acl.307,P18-1249,0,0.184947,"ication (Section 4.6). 4.1 Setup For part-of-speech tagging and text classification, we add a two-layer perceptron on top of XLMR (Conneau et al., 2019) embeddings, where we calculate contextualized token embeddings by a learnable weighted average across layers. We use endpoint concatenation (i.e., the concatenation of the first and last token representation) to obtain fixed-dimensional span or sentence features, and keep the pretrained model frozen during training.3 For dependency parsing, we use the SuPar implementation of Dozat and Manning (2017).4 For constituency parsing, we use Benepar (Kitaev and Klein, 2018).5 For all data augmentation methods, including the baselines (Section 4.2), we only augment the training set, and use the original development set. If not specified, we introduce 20 times more examples than the original training set when applying an augmentation method. When introducing k× new examples, we also replicate the original training set k times to ensure that the model can access sufficient examples from the original distribution. All models are initialized with the XLM-R base model (Conneau et al., 2019) if not specified. We train models for 20 epochs in high-resource settings (i.e"
2021.findings-acl.307,2020.acl-main.212,0,0.0246263,"tion methods have been widely studied in recent years. They typically create new examples by substituting (word) tokens with ones having the same desired features, such as synonym substitution (Zhang et al., 2015; Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018) or substitution with words having the same morphological features (Silfverberg et al., 2017). Such methods have been applied to generate adversarial or negative examples which help improve the robustness of neural network–based NLP models (Belinkov and Bisk, 2018; Shi et al., 2018a; Alzantot et al., 2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I sleeping Generated Sentences S VP cat love books S NP The VP love NP I books VP is sleeping (b) Constituency parsing. Original Sentences root poss My cat root dobj nsubj likes dobj nsubj milk I read books Other token-level manipulation methods introduce noise, such as random token shuffling and deletion (Wang et al., 2018; Wei and Zou, 2019; Dai and Adel, 2020). Models trained on the"
2021.findings-acl.307,2020.emnlp-main.600,0,0.262015,"re substructures exist in the annotations of the tasks. For more general NLP tasks such as text classification, we present variations of S UB2 which (1) define substructures based on text spans or parse trees for existing examples, and (2) generate new examples by substructure substitution based on the substructures and various kinds of constraints. Introduction Data augmentation has been found effective for various natural language processing (NLP) tasks, such as machine translation (Fadaee et al., 2017; Gao et al., 2019; Xia et al., 2019, inter alia), text classification (Wei and Zou, 2019; Quteineh et al., 2020), syntactic and semantic parsing (Jia and Liang, 2016; Shi et al., 2020; Dehouck and G´omez-Rodr´ıguez, 2020), semantic role labeling (F¨urstenau and Lapata, 2009), and dialogue understanding (Hou et al., 2018; Niu and Bansal, 2019). Such methods enhance the diversity of the training set by generating examples based on existing ones, and can make the learned models more robust against noise (Xie et al., 2020). Most existing work focuses on word-level manipulation (Kobayashi, 1 Project page: https://home.ttic.edu/ freda/project/sub2 ˜ While data augmentation methods can often be task-specific o"
2021.findings-acl.307,2020.findings-emnlp.65,0,0.0243032,"Missing"
2021.findings-acl.307,D18-1545,0,0.195628,"Missing"
2021.findings-acl.307,D19-1132,0,0.0162852,"nd (2) generate new examples by substructure substitution based on the substructures and various kinds of constraints. Introduction Data augmentation has been found effective for various natural language processing (NLP) tasks, such as machine translation (Fadaee et al., 2017; Gao et al., 2019; Xia et al., 2019, inter alia), text classification (Wei and Zou, 2019; Quteineh et al., 2020), syntactic and semantic parsing (Jia and Liang, 2016; Shi et al., 2020; Dehouck and G´omez-Rodr´ıguez, 2020), semantic role labeling (F¨urstenau and Lapata, 2009), and dialogue understanding (Hou et al., 2018; Niu and Bansal, 2019). Such methods enhance the diversity of the training set by generating examples based on existing ones, and can make the learned models more robust against noise (Xie et al., 2020). Most existing work focuses on word-level manipulation (Kobayashi, 1 Project page: https://home.ttic.edu/ freda/project/sub2 ˜ While data augmentation methods can often be task-specific or have inconsistent performance, extensive experiments show that S UB2 consistently helps models achieve competitive or better performance than training on the original dataset across structured prediction tasks and original dataset"
2021.findings-acl.307,L16-1262,0,0.0212119,"Missing"
2021.findings-acl.307,D13-1170,0,0.171958,"d trees, analogously to constituency parse trees, as the backbone for S UB2 : we (1) generate balanced trees by recursively splitting a span of n words into two n consecutive groups, which consist of 2 and n words respectively, and (2) treat each non2 terminal in the balanced tree as a substructure to perform S UB2 . We also investigate combinations of the above constraints, where we require all the chosen constraints to be the same to perform S UB2 . For example, combining text classification label and number 2 There can be other text span labels such as sentiment labels of constituents (Socher et al., 2013). of words (Figure 1d) requires the original and the alternative span to have the same text label and the same number of words. 4 Experiments We introduce our experimental setups (Section 4.1), and evaluate S UB2 and several data augmentation baselines (Section 4.2) on four tasks: part-ofspeech tagging (Section 4.3), dependency parsing (Section 4.4), constituency parsing (Section 4.5), and text classification (Section 4.6). 4.1 Setup For part-of-speech tagging and text classification, we add a two-layer perceptron on top of XLMR (Conneau et al., 2019) embeddings, where we calculate contextuali"
2021.findings-acl.307,D18-1100,0,0.0822613,"8; Shi et al., 2018a; Alzantot et al., 2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I sleeping Generated Sentences S VP cat love books S NP The VP love NP I books VP is sleeping (b) Constituency parsing. Original Sentences root poss My cat root dobj nsubj likes dobj nsubj milk I read books Other token-level manipulation methods introduce noise, such as random token shuffling and deletion (Wang et al., 2018; Wei and Zou, 2019; Dai and Adel, 2020). Models trained on the augmented datasets are expected to be more robust against the considered noise. Generated Sentences root poss My nsubj cat root dobj likes nsubj books dobj I read milk (c) Dependency parsing. Original Sentences I like the book Label: positive Generated Sentences I the movie book Label: positive I like the movie Label: positive I like like the Label: positive (d) Text classification. To apply S UB2 , we use text spans as substructures, with both the number of words in the span and the text classification label as constraints (see S"
2021.findings-acl.307,N18-2090,0,0.0185114,"cordingly. The generated examples, together with the original datasets, are used to further train models for the primary tasks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments the original dataset b"
2021.findings-acl.307,2020.coling-main.305,0,0.033322,"al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments the original dataset by linearly interpolating the vector representations of text and labels, to text classification (Guo et al., 2019; Sun et al., 2020), named entity recognition (Chen et al., 2020), and compositional generalization (Guo et al., 2020). a connection between S UB2 and tree substitution grammars (TSGs; Schabes, 1990), where the approach can be viewed as (1) estimating a TSG using the given corpus and (2) drawing new sentences from the estimated TSG. 3 We introduce the general framework we investigate in Section 3.1, and describe the variations of S UB2 which can be applied to text classification and other NLP applications in Section 3.2. 3.1 When constituency parse trees are used, there is Substructure Substitution (S UB2 ) As s"
2021.findings-acl.307,D15-1306,0,0.0300221,"nguistics Original Sentences I have a book PRP VBP DT NN collection, and usually boost model performance on desired tasks. Most existing data augmentation methods for NLP tasks can be classified into the following categories: They ate an orange PRP VBD DT NN Generated Sentences I have an orange PRP VBP DT NN They ate a book PRP VBD DT NN Token-level manipulation. Token-level manipulation methods have been widely studied in recent years. They typically create new examples by substituting (word) tokens with ones having the same desired features, such as synonym substitution (Zhang et al., 2015; Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018) or substitution with words having the same morphological features (Silfverberg et al., 2017). Such methods have been applied to generate adversarial or negative examples which help improve the robustness of neural network–based NLP models (Belinkov and Bisk, 2018; Shi et al., 2018a; Alzantot et al., 2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I slee"
2021.findings-acl.307,P19-1579,0,0.116327,"ctured prediction tasks such as part-of-speech tagging and parsing, where substructures exist in the annotations of the tasks. For more general NLP tasks such as text classification, we present variations of S UB2 which (1) define substructures based on text spans or parse trees for existing examples, and (2) generate new examples by substructure substitution based on the substructures and various kinds of constraints. Introduction Data augmentation has been found effective for various natural language processing (NLP) tasks, such as machine translation (Fadaee et al., 2017; Gao et al., 2019; Xia et al., 2019, inter alia), text classification (Wei and Zou, 2019; Quteineh et al., 2020), syntactic and semantic parsing (Jia and Liang, 2016; Shi et al., 2020; Dehouck and G´omez-Rodr´ıguez, 2020), semantic role labeling (F¨urstenau and Lapata, 2009), and dialogue understanding (Hou et al., 2018; Niu and Bansal, 2019). Such methods enhance the diversity of the training set by generating examples based on existing ones, and can make the learned models more robust against noise (Xie et al., 2020). Most existing work focuses on word-level manipulation (Kobayashi, 1 Project page: https://home.ttic.edu/ fred"
2021.findings-acl.307,N18-1057,0,0.0915569,"et al., 2020; Yoo et al., 2020). In the data augmentation stage, given task-specific constraints, such models generate associated text accordingly. The generated examples, together with the original datasets, are used to further train models for the primary tasks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word"
2021.findings-acl.307,C16-1138,0,0.0265448,"nd describe the variations of S UB2 which can be applied to text classification and other NLP applications in Section 3.2. 3.1 When constituency parse trees are used, there is Substructure Substitution (S UB2 ) As shown in Figure 1, given the original training set D, S UB2 generates new examples using samelabel substructure substitution, and repeats the process until the training set reaches the desired size. The general S UB2 procedure is presented in Algorithm 1. Structure-aware data augmentation. Existing work has also sought potential gain from structures associated with natural language: Xu et al. (2016) improve word relation classification by dependency path–based augmentation. S¸ahin and Steedman (2018) show that subtree cropping and rotation based on dependency parse trees can help part-of-speech tagging for low-resource languages, while Vania et al. (2019) demonstrate that such methods also help dependency parsing when very limited training data is available. S UB2 also falls into this category. The idea of same-label substructure substitution has been used to improve performance on structured prediction tasks such as semantic parsing (Jia and Liang, 2016), constituency parsing (Shi et al"
2021.findings-acl.307,2020.findings-emnlp.90,0,0.315277,"Missing"
2021.findings-acl.307,P17-1096,0,0.0274389,"associated text accordingly. The generated examples, together with the original datasets, are used to further train models for the primary tasks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit generation of concrete examples, soft augmentation directly represents generated examples in a continuous vector space: Gao et al. (2019) propose to perform soft word substitution for machine translation; recent work has adapted the mixup method (Zhang et al., 2018), which augments th"
2021.findings-acl.307,2020.acl-main.302,0,0.0130264,"al., 2020a; Ding et al., 2020; Liu et al., 2020b, inter alia), or applying post-processing on the examples generated by pretrained models (Yang et al., 2020; Wan et al., 2020; Yoo et al., 2020). In the data augmentation stage, given task-specific constraints, such models generate associated text accordingly. The generated examples, together with the original datasets, are used to further train models for the primary tasks. A representative method is back-translation (Sennrich et al., 2016), which is effective for not only machine translation, but also style transfer (Prabhumoye et al., 2018; Zhang et al., 2020a), conditional text generation (Sobrevilla Cabezudo et al., 2019), text classification (Iyyer et al., 2018), and grammatical error correction (Xie et al., 2018). Relatedly, automatic question generation has been used in data augmentation for question answering (Yang et al., 2017; Song et al., 2018). 3495 Another approach to example generation is to generate new examples based on predefined templates (Kafle et al., 2017; Asai and Hajishirzi, 2020), where the templates are designed following heuristic, and usually task-specific, rules. Soft data augmentation. As an alternative to explicit gener"
2021.findings-acl.307,P19-1161,0,0.101392,"ens with ones having the same desired features, such as synonym substitution (Zhang et al., 2015; Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018) or substitution with words having the same morphological features (Silfverberg et al., 2017). Such methods have been applied to generate adversarial or negative examples which help improve the robustness of neural network–based NLP models (Belinkov and Bisk, 2018; Shi et al., 2018a; Alzantot et al., 2018; Zhang et al., 2019; Min et al., 2020, inter alia), or to generate counterfactual examples which help mitigate bias in natural language (Zmigrod et al., 2019; Lu et al., 2020). (a) Part-of-speech tagging. Original Sentences S NP The S VP cat is NP I sleeping Generated Sentences S VP cat love books S NP The VP love NP I books VP is sleeping (b) Constituency parsing. Original Sentences root poss My cat root dobj nsubj likes dobj nsubj milk I read books Other token-level manipulation methods introduce noise, such as random token shuffling and deletion (Wang et al., 2018; Wei and Zou, 2019; Dai and Adel, 2020). Models trained on the augmented datasets are expected to be more robust against the considered noise. Generated Sentences root poss My nsubj c"
2021.findings-acl.365,P18-1031,0,0.0642031,"Missing"
2021.findings-acl.365,C16-1252,0,0.0235649,"d makes the classifier more robust to the choice of label descriptions.1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular labe"
2021.findings-acl.365,N09-1069,0,0.0660264,"labels with a modified k-means clustering algorithm. While dataless text classifiers are designed to handle an unbounded set of categories, they are used and evaluated on a particular set of documents with a set of labels. The idea of our approach is to leverage the assumption that the documents in a text classification dataset are separable according to the accompanying set of labels. That is, given a strong document encoder, the documents should be separable by label in the encoded space. This assumption is similarly made when performing clustering for unsupervised document classification (Liang and Klein, 2009). We use the set of unlabeled input texts to refine the predictions of our dataless classifiers via clustering. To better inform the algorithm, we initialize the clusters by using our dataless classifiers run on the provided label set for each task. The algorithm takes on different forms for the dual and single encoder models. Details are provided below. 3.1 ULR for Dual Encoder Architectures In the setting of a dual encoder model, we propose to perform k-means clustering among text representations, i.e., of vectors produced by the text encoder. The assumption is that texts falling under the s"
2021.findings-acl.365,2021.ccl-1.108,0,0.050426,"Missing"
2021.findings-acl.365,C16-1017,0,0.0299601,"ifier more robust to the choice of label descriptions.1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular label set in a downst"
2021.findings-acl.365,D19-6116,0,0.0553819,"Missing"
2021.findings-acl.365,N19-1108,0,0.0225142,"1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular label set in a downstream task dataset by scoring each possible label for a do"
2021.findings-emnlp.346,P19-1285,0,0.0190974,"tokens processed at a time. A very small k will require many more forward passes and will not take advantage of GPU parallelism. Table 2: Language modeling validation/test perplexity with GPT-2 (345M parameters). 2019) models implemented using FLAX (Heek et al., 2020) and Haiku (Hennigan et al., 2020), on top of JAX (Bradbury et al., 2018). The TXL model is initialized from the HuggingFace Transformers (Wolf et al., 2020) model trained on WikiText103 (WT-103). The GPT-2 models are initialized from the OpenAI checkpoints. 4.1 Language modeling We demonstrate HSO with the Transformer-XL (TXL) (Dai et al., 2019) and GPT-22 (Radford et al., We test HSO with the TXL and 345M parameter GPT-2 models on the pre-tokenized WikiText103 (Merity et al., 2017) and PG-19 (Rae et al., 2020) datasets. As the TXL was trained on WT103, this covers both an in-distribution and out-ofdistribution (OOD) evaluation for it. We found that TXL was not stable in the OOD setting, but that resetting its hidden states to zeros upon reaching its maximum context size reduced the baseline perplexity significantly. We do not do this for HSO as it does not appear to need this stabilization. We evaluate GPT-2 with non-overlapping con"
2021.findings-emnlp.346,N19-1423,0,0.0343405,". This process is repeated for each window of 1025 tokens, updating the cached hidden states each time. Attending to these modified hidden states creates higher quality predictions for future tokens. As an example of how future information can help embed past tokens, consider the garden path sentence: “The old man the boat.” The embedding for “man” will only depend on “The”, “old”, and “man”, so it will not reflect that “man” is being used as a verb. HSO can be seen as a method of incorporating future information into the representation of a context while still using a left-to-right LM. BERT (Devlin et al., 2019) showed that bidirectional information passing improves embedding quality, which suggests that doing so should improve performance on downstream tasks. We demonstrate HSO in the setting of language model evaluation on the WikiText-103 (Merity et al., 2017) and PG-19 (Rae et al., 2020) corpora, and find improvements in measured perplexity. In order to demonstrate that this translates into value for downstream applications we apply HSO to fewshot classification with the 1.5B parameter GPT-2, and find improvement in that setting as well. Finetuning a pretrained transformer language model (LM) (Va"
2021.findings-emnlp.346,2020.tacl-1.28,0,0.0288294,"ays to speed up HSO. Alternatives to finetuning. Our method is related to those that reduce the computational cost of finetuning by updating a smaller number of parameters or avoid finetuning altogether. Houlsby et al. (2019) introduce adapter modules which are finetuned in lieu of the full model. Li and Liang (2021) introduce prefix-tuning, which adds a fixed set of learnable vectors to the beginning of the input sequence. The latter is related to using prompts for contextual generation, which has gained popularity both to extract information from language models (e.g., Radford et al., 2019, Jiang et al., 2020) and perform tasks directly without updating any model parameters (Brown et al., 2020). Follow-up work has sought to understand the effectiveness of prompting (Le Scao and Rush, 2021) and automatically find or learn better prompts (Shin et al., 2020; Liu et al., 2021; Qin and Eisner, 2021). 3 Method Let f be a transformer language model computing the distribution for token xt given tokens x1:t−1 : pt = f (x1:t−1 ) In practice, one may cache the hidden states, ht ∈ R`×d , where ` is the number of layers and d is the embedding size. We represent this by factoring f into fh which computes hidden"
2021.findings-emnlp.346,2021.naacl-main.208,0,0.0889216,"Missing"
2021.findings-emnlp.346,2021.acl-long.353,0,0.0643922,"Missing"
2021.findings-emnlp.346,2020.findings-emnlp.219,0,0.0116644,"the LM loss to tune past hidden states to allow better prediction of unseen future tokens. They also only perform gradient updates to logits while we update hidden states. Plug-and-Play language models (PPLM; Dathathri et al., 2020) modify the behavior of pretrained LMs by updating hidden states at inference time, but with the goal of controllable generation (e.g., controlling sentiment) rather than improved fidelity. Unlike HSO, PPLMs require an attribute classifier which must be trained with labeled data. Several methods have been developed to more efficiently achieve the same goal as PPLM (Madotto et al., 2020; Krause et al., 2020), and these ideas could potentially be applied in analogous ways to speed up HSO. Alternatives to finetuning. Our method is related to those that reduce the computational cost of finetuning by updating a smaller number of parameters or avoid finetuning altogether. Houlsby et al. (2019) introduce adapter modules which are finetuned in lieu of the full model. Li and Liang (2021) introduce prefix-tuning, which adds a fixed set of learnable vectors to the beginning of the input sequence. The latter is related to using prompts for contextual generation, which has gained popula"
2021.findings-emnlp.346,N18-1202,0,0.0171185,"sing improves embedding quality, which suggests that doing so should improve performance on downstream tasks. We demonstrate HSO in the setting of language model evaluation on the WikiText-103 (Merity et al., 2017) and PG-19 (Rae et al., 2020) corpora, and find improvements in measured perplexity. In order to demonstrate that this translates into value for downstream applications we apply HSO to fewshot classification with the 1.5B parameter GPT-2, and find improvement in that setting as well. Finetuning a pretrained transformer language model (LM) (Vaswani et al., 2017; Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) is now the default method for attacking a task in modern NLP. Due to the high cost of pretraining, much research has been focused on how better to apply the pretrained models, rather than just improving pretraining itself. However, even finetuning can be too costly, especially for models such as the 175 2 Related Work billion parameter GPT-3 (Brown et al., 2020). As such, researchers have sought low cost alternatives, Learning during inference. HSO is related to such as finetuning a small set of auxiliary parame- methods that perform learning on the test set. One ters (H"
2021.findings-emnlp.346,2021.naacl-main.410,0,0.0198066,"ull model. Li and Liang (2021) introduce prefix-tuning, which adds a fixed set of learnable vectors to the beginning of the input sequence. The latter is related to using prompts for contextual generation, which has gained popularity both to extract information from language models (e.g., Radford et al., 2019, Jiang et al., 2020) and perform tasks directly without updating any model parameters (Brown et al., 2020). Follow-up work has sought to understand the effectiveness of prompting (Le Scao and Rush, 2021) and automatically find or learn better prompts (Shin et al., 2020; Liu et al., 2021; Qin and Eisner, 2021). 3 Method Let f be a transformer language model computing the distribution for token xt given tokens x1:t−1 : pt = f (x1:t−1 ) In practice, one may cache the hidden states, ht ∈ R`×d , where ` is the number of layers and d is the embedding size. We represent this by factoring f into fh which computes hidden states (possibly depending on past hidden states) and fp which computes output probabilities from the hidden states: ht = fh (xt , h1:t−1 ) (1) pt = fp (ht ) Given a loss function L which takes as arguments the ground truth next word and a distribution over word types, one can then compute"
2021.findings-emnlp.346,2020.emnlp-main.58,0,0.0635357,"Missing"
2021.findings-emnlp.346,2020.emnlp-main.346,0,0.0341193,"which are finetuned in lieu of the full model. Li and Liang (2021) introduce prefix-tuning, which adds a fixed set of learnable vectors to the beginning of the input sequence. The latter is related to using prompts for contextual generation, which has gained popularity both to extract information from language models (e.g., Radford et al., 2019, Jiang et al., 2020) and perform tasks directly without updating any model parameters (Brown et al., 2020). Follow-up work has sought to understand the effectiveness of prompting (Le Scao and Rush, 2021) and automatically find or learn better prompts (Shin et al., 2020; Liu et al., 2021; Qin and Eisner, 2021). 3 Method Let f be a transformer language model computing the distribution for token xt given tokens x1:t−1 : pt = f (x1:t−1 ) In practice, one may cache the hidden states, ht ∈ R`×d , where ` is the number of layers and d is the embedding size. We represent this by factoring f into fh which computes hidden states (possibly depending on past hidden states) and fp which computes output probabilities from the hidden states: ht = fh (xt , h1:t−1 ) (1) pt = fp (ht ) Given a loss function L which takes as arguments the ground truth next word and a distribut"
2021.findings-emnlp.346,D13-1170,0,0.00517612,"od Baseline DE4 HSO HSO-2 SST-2 2 4 6 8 53.9 58.3 57.9 58.4 52.2 (55.1) 55.6 (58.8) 56.2 (59.4) 59.9 (61.8) 59.5 63.1 68.0 70.2 64.0 66.5 69.2 70.2 AGNews 2 4 6 8 53.1 77.8 64.8 63.3 32.2 (35.0) 52.2 (55.2) — — 52.6 77.2 65.8 68.5 54.3 77.6 66.2 69.3 Table 4: Effect of updating hidden states on few-shot classification accuracy of GPT-2-XL on SST-2 and AGNews, where n is the number of examples per prompt. Neither hidden states or weights are updated for the baseline. HSO-2 is HSO with two gradient steps per window of text. method with the 1.5B parameter GPT-2-XL model. We use the binary SST-2 (Socher et al., 2013) and 4-way AGNews (Zhang et al., 2015) classification datasets. We follow choices made by Zhao et al. (2021), including their prompt formats, but we made several changes to their procedure to reduce computational requirements and variance. Most importantly, we resampled a class-balanced prompt for every test example (but kept the prompt fixed between the baseline and HSO) rather than using a fixed prompt.5 We used a learning rate of 0.01 and a window size of 10 tokens. Our experiments used a 24GB NVIDIA Quadro RTX 6000 GPU. We also test DE, as in contrast to the LM setting, the amount of fine-"
2021.findings-emnlp.346,2020.emnlp-demos.6,0,0.0610019,"Missing"
2021.naacl-main.259,D15-1075,0,0.0321276,"-based estimates for 5 Experiments the KL divergence is because our choices for the 5.1 Datasets prior preclude the possibility of a closed form for the KL. We consider two different approaches We consider four widely-used, publicly available when computing sample-based KLs: standard KL English datasets: the Penn Treebank (PTB) (Marand a modified one inspired by free bits (Li et al., cus et al., 1993; Bowman et al., 2016), Yahoo (Yang 2019; Pelsmaeker and Aziz, 2020; Kingma et al., et al., 2017; He et al., 2019), Yelp sentiment (Shen 2016), which we refer to as FB KL. et al., 2017), and SNLI (Bowman et al., 2015). 3245 5.2 Baselines Our baselines include standard VAE with linear KL annealing (Bowman et al., 2016); Cyc-VAE (Fu et al., 2019) in which the KL term is reweighted with a cyclical annealing schedule; Lag-VAE (He et al., 2019) which updates the encoder multiple times before each decoder update; VAE+FB (Kingma et al., 2016; Chen et al., 2017) which replaces the standard KL with FB (i.e., Eq. 9 with N = 1); and Pre-VAE+FB (Li et al., 2019) which initializes the VAE with a pretrained autoencoder and replaces standard KL with FB. We evaluated these baselines using their open source implementations"
2021.naacl-main.259,K16-1002,0,0.622135,"based on normalizing generation. However, the representation capacity flows (NF), specifically real NVP transformations of VAEs is still limited for modeling sentences due (Dinh et al., 2016). Using a real NVP prior entails to two main reasons. creating an invertible mapping from a simple base One is known as the posterior collapse problem, distribution to the prior distribution of the latent in which the posterior “collapses” to the prior and variable in a VAE. This choice allows a flexible the generator learns to ignore the latent variable prior distribution that is not constrained to a spe(Bowman et al., 2016). Many methods have been cific parametric family. The hope is that it would developed to address it: annealing (Fu et al., 2019), be better at modeling the data distribution. 3242 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3242–3258 June 6–11, 2021. ©2021 Association for Computational Linguistics We perform an empirical evaluation of priors and objective functions for training VAE sentence models on four standard datasets. We find the best performance overall when using the flow-based pri"
2021.naacl-main.259,D18-1020,1,0.843173,"− KL(qφ (z|x) ||pψ (z)) (1) priors while still using the standard Gaussian form for the approximate posterior. 3 Choices for Prior Families We now describe the three kinds of priors we will compare in our experiments. The first two are based on Gaussian mixtures (Sec. 3.1) and the third is based on normalizing flows (Sec. 3.2). We take these three prior families into consideration because they represent the three main categories of work in learning priors: simple Gaussian mixtures (usually as baselines), defining the prior as a function of the approximate posterior (Tomczak and Welling, 2018; Chen et al., 2018), and flow-based priors (Chen et al., 2017; Ziegler and Rush, 2019; Ma et al., 2019; Lee et al., 2020). Note that we do not make any changes to the approximate posterior distribution. That is, the approximate posterior follows a Gaussian distribution with a diagonal covariance matrix as in standard VAEs. 3.1 Gaussian Mixture Priors Our first choice is a uniform mixture of K Gaussians (MoG): K 1 X pψ (z) = f (z; µk , diag(σk2 )) K (2) k=1 where f (z; µ, Σ) is the density function of a ddimensional Gaussian with mean µ and covariance matrix Σ. The µk and σk are learnable parameter vectors with d"
2021.naacl-main.259,P19-1599,1,0.851192,"tion. original VAE objective, the evidence lower bound (ELBO), is not effective when learning priors. The 1 Introduction issue is not solely due to posterior collapse since it is not resolved by using modifications based on Variational autoencoders (VAEs; Kingma and free bits. To address this issue, we propose using a Welling, 2014) have been widely applied to many natural language processing tasks (Bowman et al., combined objective, adding to the ELBO a second objective (denoted MIS ) which is a different lower 2016; Zhang et al., 2016; Shen et al., 2017; Kim et al., 2018; Fang et al., 2019; Chen et al., 2019). bound on the log marginal likelihood obtained using importance sampling (Burda et al., 2016). VAEs provide statistical transparency in describing observations in a latent space and flexibility when Using the combination of the ELBO and MIS , we used in applications that require directly manipulat- compare multiple choices for the prior, including a ing the learned representation (Hu et al., 2017). Re- mixture of Gaussians, a prior based on a variational cent work (Li et al., 2020) has combined VAEs with mixture of posteriors (VampPrior; Tomczak and BERT/GPT in representation learning and gui"
2021.naacl-main.259,D19-1407,0,0.0370175,"Missing"
2021.naacl-main.259,N19-1021,0,0.120554,"till limited for modeling sentences due (Dinh et al., 2016). Using a real NVP prior entails to two main reasons. creating an invertible mapping from a simple base One is known as the posterior collapse problem, distribution to the prior distribution of the latent in which the posterior “collapses” to the prior and variable in a VAE. This choice allows a flexible the generator learns to ignore the latent variable prior distribution that is not constrained to a spe(Bowman et al., 2016). Many methods have been cific parametric family. The hope is that it would developed to address it: annealing (Fu et al., 2019), be better at modeling the data distribution. 3242 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3242–3258 June 6–11, 2021. ©2021 Association for Computational Linguistics We perform an empirical evaluation of priors and objective functions for training VAE sentence models on four standard datasets. We find the best performance overall when using the flow-based prior and the combined objective in the training objective. We refer to this setting as FlowPrior. The generation of prior samples"
2021.naacl-main.259,D18-1160,0,0.0184463,"ch model has richer usage of latent variables, we use selfBLEU to measure the diversity of a set of samples. We observe significant improvements in FlowPrior in Table 6, which implies a diverse latent representation and a better utilization of the latent variable. 7 Related Work When considering the parameterized family of VAE models, expressive latent components (i.e., posterior and prior) have been widely studied in computer vision (Dinh et al., 2015, 2016; Kingma and Dhariwal, 2018). However, multimodal priors have been seldom applied to language, with some exceptions (Serban et al., 2017; He et al., 2018; Ziegler and Rush, 2019; Ma et al., 2019; Lee et al., 2020). Chen et al. (2017) use autoregressive flow for the prior and posterior and experiment with images. Ziegler and Rush (2019) propose several autoregressive NF architectures and characterize performance on character-level language modeling. Ma et al. (2019) design priors using the Glow architecture to improve the performance of nonautoregressive neural machine translation. Lee et al. (2020) empirically characterize the performance of NF and simple Gaussian priors in token-level latent variable models, and observe that flexible priors y"
2021.naacl-main.259,W11-2123,0,0.0239861,".28 99.20 7.21 2 31 2.25 2.83 Table 1: Language modeling results on PTB dataset. MI: We follow Hoffman and Johnson (2016) and report estimated mutual information between the observation and its latent variable. AU: A dimension z in the latent variable is considered “active” if Covx (Ezvq(z|x) [z]) &gt;10−2 . AU is then the number of active latent dimensions (Burda et al., 2016). F-PPL and R-PPL: These metrics measure the correspondence between generated sentences from the model and the training corpus. We evaluate both F-PPL and R-PPL by estimating 5-gram language models using the KenLM toolkit (Heafield, 2011) with its default smoothing method. For FPPL, we estimate language models from the actual text and compute the perplexity of the generated samples. For R-PPL, we estimate language models from the generated samples and compute the perplexity of the actual text.5 Self-BLEU: The self-BLEU metric is one measure of the diversity of a set of samples (Zhu et al., 2018). It is calculated by averaging the BLEU scores computed between all pairs of samples. 6 6.1 Results Language Modeling We first perform language modeling tasks to characterize models’ efficacy at modeling texts in terms of modeling the"
2021.naacl-main.259,P19-1234,0,0.0211087,"d architecture for the prior, while they are using autoregressive NF. Also, we focus on models with a single latent variable for an entire sentence, while similar prior work has focused on token-level latent variables (Ziegler and Rush, 2019; Ma et al., 2019; Lee et al., 2020). Several others have employed NF for flexible modeling in NLP. Setiawan et al. (2020) present a variational translation model that uses NF in the approximate posterior while keeping the prior as Gaussian. Wang and Wang (2019) apply NF to a variational Wasserstein autoencoder in order to make the posterior more flexible. Jin et al. (2019) use transformed distributions via NF to model the emission density, which improves parsing performance as compared to Gaussian baselines. 8 Conclusion and adds the importance-sampled marginal likelihood (MIS ) as a second term to the standard VAE objective. Our empirical results show FlowPrior yields a substantial improvement in language modeling and generation tasks as compared to prior work. Adding MIS improves performance for other models as well, especially in settings when the prior parameters are being learned. Acknowledgments We would like to thank Sam Wiseman, Qingming Tang, and Mingd"
2021.naacl-main.259,2020.emnlp-main.378,0,0.0114931,") which is a different lower 2016; Zhang et al., 2016; Shen et al., 2017; Kim et al., 2018; Fang et al., 2019; Chen et al., 2019). bound on the log marginal likelihood obtained using importance sampling (Burda et al., 2016). VAEs provide statistical transparency in describing observations in a latent space and flexibility when Using the combination of the ELBO and MIS , we used in applications that require directly manipulat- compare multiple choices for the prior, including a ing the learned representation (Hu et al., 2017). Re- mixture of Gaussians, a prior based on a variational cent work (Li et al., 2020) has combined VAEs with mixture of posteriors (VampPrior; Tomczak and BERT/GPT in representation learning and guided Welling, 2018), and a prior based on normalizing generation. However, the representation capacity flows (NF), specifically real NVP transformations of VAEs is still limited for modeling sentences due (Dinh et al., 2016). Using a real NVP prior entails to two main reasons. creating an invertible mapping from a simple base One is known as the posterior collapse problem, distribution to the prior distribution of the latent in which the posterior “collapses” to the prior and variabl"
2021.naacl-main.259,D19-1437,0,0.358051,"e approximate posterior. 3 Choices for Prior Families We now describe the three kinds of priors we will compare in our experiments. The first two are based on Gaussian mixtures (Sec. 3.1) and the third is based on normalizing flows (Sec. 3.2). We take these three prior families into consideration because they represent the three main categories of work in learning priors: simple Gaussian mixtures (usually as baselines), defining the prior as a function of the approximate posterior (Tomczak and Welling, 2018; Chen et al., 2018), and flow-based priors (Chen et al., 2017; Ziegler and Rush, 2019; Ma et al., 2019; Lee et al., 2020). Note that we do not make any changes to the approximate posterior distribution. That is, the approximate posterior follows a Gaussian distribution with a diagonal covariance matrix as in standard VAEs. 3.1 Gaussian Mixture Priors Our first choice is a uniform mixture of K Gaussians (MoG): K 1 X pψ (z) = f (z; µk , diag(σk2 )) K (2) k=1 where f (z; µ, Σ) is the density function of a ddimensional Gaussian with mean µ and covariance matrix Σ. The µk and σk are learnable parameter vectors with dimensionality d (which is 32 in our experiments). This prior was used as a baseline"
2021.naacl-main.259,J93-2004,0,0.0763207,"Missing"
2021.naacl-main.259,2020.acl-main.646,0,0.2232,"viewed as a regularized autoencoder in which the first term is the negative reconstruction error and the second is the negative KL divergence between the approximate posterior qφ (z|x) and the latent variable prior pψ (z). It is common in practice K to fix the prior pψ (z) to be a standard Gaussian 1 X pψ (z) = qφ (z |uk ) (3) distribution and only learn θ and φ (Bowman et al., K k=1 2016; Yang et al., 2017; Shen et al., 2017). While constraining the prior to be a fixed stan- where K is the number of pseudo-inputs, each of dard Gaussian is common, it is not necessary for which is denoted uk . Pelsmaeker and Aziz (2020) tractability. Researchers have found benefit from applied this idea to text modeling and we follow using richer priors and posteriors (Rezende and their strategy for defining pseudo-inputs. That is, Mohamed, 2015; Kingma et al., 2016; Chen et al., each uk consists of a sequence of embeddings that 2017; Ziegler and Rush, 2019; Ma et al., 2019). have the same dimensionality as word embeddings. In this paper, we consider investigating alternative For each component k, the lengths of pseudo-inputs 3243 can vary; they are sampled based on the statistics of the lengths in the training set. We refer"
2021.naacl-main.259,D17-1066,0,0.0276585,"Missing"
2021.naacl-main.259,2020.acl-main.694,0,0.0279948,"variable models, and observe that flexible priors yield higher log-likelihoods but not better BLEU scores on machine translation tasks. Our work differs from that of Ziegler and Rush (2019) and Chen et al. (2017) as we are using a non-autoregressive flow-based architecture for the prior, while they are using autoregressive NF. Also, we focus on models with a single latent variable for an entire sentence, while similar prior work has focused on token-level latent variables (Ziegler and Rush, 2019; Ma et al., 2019; Lee et al., 2020). Several others have employed NF for flexible modeling in NLP. Setiawan et al. (2020) present a variational translation model that uses NF in the approximate posterior while keeping the prior as Gaussian. Wang and Wang (2019) apply NF to a variational Wasserstein autoencoder in order to make the posterior more flexible. Jin et al. (2019) use transformed distributions via NF to model the emission density, which improves parsing performance as compared to Gaussian baselines. 8 Conclusion and adds the importance-sampled marginal likelihood (MIS ) as a second term to the standard VAE objective. Our empirical results show FlowPrior yields a substantial improvement in language model"
2021.naacl-main.259,N19-1025,0,0.0344795,"Missing"
2021.naacl-main.259,D16-1050,0,0.0282834,"he sive prior with analysis and several forms of evaluation involving generation. original VAE objective, the evidence lower bound (ELBO), is not effective when learning priors. The 1 Introduction issue is not solely due to posterior collapse since it is not resolved by using modifications based on Variational autoencoders (VAEs; Kingma and free bits. To address this issue, we propose using a Welling, 2014) have been widely applied to many natural language processing tasks (Bowman et al., combined objective, adding to the ELBO a second objective (denoted MIS ) which is a different lower 2016; Zhang et al., 2016; Shen et al., 2017; Kim et al., 2018; Fang et al., 2019; Chen et al., 2019). bound on the log marginal likelihood obtained using importance sampling (Burda et al., 2016). VAEs provide statistical transparency in describing observations in a latent space and flexibility when Using the combination of the ELBO and MIS , we used in applications that require directly manipulat- compare multiple choices for the prior, including a ing the learned representation (Hu et al., 2017). Re- mixture of Gaussians, a prior based on a variational cent work (Li et al., 2020) has combined VAEs with mixture of po"
D09-1023,J00-1004,0,0.0318613,"at each target word aligned to at most one source word, which we also do here.6 Which translations are possible depends heavily on the configurations that the QDG permits. Formally, for a parent-child pair htτt (j) , tj i in τt , we consider the relationship between a(τt (j)) and a(j), the source-side words to which tτt (j) and tj align. If, for example, we require that, for all j, a(τt (j)) = τs (a(j)) or a(j) = 0, and that the root of τt must align to the root of τs or to NULL, then strict isomorphism must hold between τs and τt , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (“a(τt (j)) = τs (a(j))” corresponds to their “parent-child” configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in §2, f lex , f att , f val , and f dist can be easily incorp"
D09-1023,W05-0909,0,0.0233161,"decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only one target-side word (since we only allow a target word to al"
D09-1023,D08-1023,0,0.026862,"for clarity only the first five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, as defined in Eq. 1.7 (In §5 we will consider kbest and all-translations variations on this prob6 I.e., from here on, a : {1, . . . , m} → {0, . . . , n} where 0 denotes alignment to NULL. 7 Arguably, we seek argmaxt p(t |s), marginalizing out everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work. 222 Source: $ konnten sie es übersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:? ?:? übersetzen: translate übersetzen: translate sie:you sie:you konnten:could konnten:couldn konnten:might es:it sie:you übersetzen: translated übersetzen: translated sie:let ?:? es:it es:it es:it sie:them konnten:could NULL:to ... ... übersetzen: translate ... ... ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (oth"
D09-1023,P08-1024,0,0.183612,"features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, τt , a |s, τs ) = exp{θ > g(s, τs , a, t, τt )} > 0 0 0 a0 ,t0 ,τ 0 exp{θ g(s, τs , a , t , τt )} P Lexical Translations (2) t 4 There are two conventional definitions of feature functions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice. where the g are arbitrary feature functions and the θ are feature weights. If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this wor"
D09-1023,J93-2003,0,0.0122443,"ing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in τt (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in τs apart from a single recently aligned word. This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). This sacrifice is the result of our choice to use a conditional model (§2). The solution is to introduce a set of coverage features g cov (a). Here, these include: words: Of these, only f scov is local. 4.3 Non-Local Features The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them. Phrase lexicon features f phr , language model features f N for N > 1, and most coverage features are non-local with respect to our QDG. Recently Chiang (2007) introduced “cube pruning” as an approximate decoding method that extends a DP decoder with the ability to"
D09-1023,D08-1024,0,0.0502878,"ntly maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source sentence s and its parse τs , a QDG induc"
D09-1023,P05-1033,0,0.122116,"s are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed. In principle, we might include source-side parsing as part of decoding. 220 (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007). Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-ba"
D09-1023,J07-2003,0,0.820051,"emoved, allowing simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel comWe present a machine translation framework that can incorporate arb"
D09-1023,P09-1053,1,0.81996,"ndependence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source word to target words to which it may translate s = hs0 , . . . , sn i ∈ Σn source language sent"
D09-1023,P05-1067,0,0.0328551,"igned to at most one source word, which we also do here.6 Which translations are possible depends heavily on the configurations that the QDG permits. Formally, for a parent-child pair htτt (j) , tj i in τt , we consider the relationship between a(τt (j)) and a(j), the source-side words to which tτt (j) and tj align. If, for example, we require that, for all j, a(τt (j)) = τs (a(j)) or a(j) = 0, and that the root of τt must align to the root of τs or to NULL, then strict isomorphism must hold between τs and τt , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (“a(τt (j)) = τs (a(j))” corresponds to their “parent-child” configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in §2, f lex , f att , f val , and f dist can be easily incorporated into the QDG as d"
D09-1023,H05-1036,1,0.796739,"making the following approximation (Be10 Alignments could be supplied by automatic word alignment algorithms. We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses. 11 When the function’s value is computed by “inside” DP, the corresponding “outside” algorithm can be used to obtain the gradient. Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al. (2005). 5.2 Summing over τt and a For the summation over dependency trees and alignments given fixed t, required for p(τt | t, s, τs ), we perform “inside” lattice parsing with Gs,τs . The technique is the summing variant of the decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in prac"
D09-1023,1997.iwpt-1.10,0,0.088984,"h. Let the states be numbered 0 to `; states from bρ`c to ` are final states (for some ρ ∈ (0, 1)). For every position between consecutive states j − 1 and j (0 < j ≤ `), and for every word si in s, and for every word t ∈ Trans(si ), we instantiate an arc annotated with t and i. The weight of such an arc is exp{θ > f }, where f is the sum of feature functions that fire when si translates as t in target position j (e.g., f lex (si , t) and f dist (i, j)). Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms (Eisner, 1997). This decoder accounts for f lex , f att , f val , f dist , and f qg as local features. Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice representing possible translations. In each bundle, the arcs are listed in decreasing order according to weight and for clarity only the first five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, a"
D09-1023,P06-1121,0,0.0602962,"two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Sh"
D09-1023,W08-0302,1,0.868317,"often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source"
D09-1023,E09-1037,1,0.929278,"ng simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel comWe present a machine translation framework that can incorporate arbitrary features of both i"
D09-1023,2009.eamt-1.32,0,0.0300839,"rammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source sentence s and its"
D09-1023,P07-1019,0,0.0272789,"arious restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on"
D09-1023,N07-1008,0,0.0296098,"ble, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed. In principle, we might include source-side parsing as part of decoding. 220 (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007). Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is acco"
D09-1023,P04-1061,0,0.0334321,"carding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words × phrase conditional and “lexical smoothing” probabilities × two conditional directions. Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For our target-language syntactic features g syn , we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004). These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ). These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003). The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003). In total, there are 7 lexical and 7 word-class syntax features. For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i−j |whenever a(j) = i and i, j > 0. ("
D09-1023,N03-1017,0,0.464412,"decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sent"
D09-1023,P07-2045,0,0.0519099,"e that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007). In syntax-based systems, reordering is typically parameterized by grammar rules. For generality we permit these features to “see” all structures and denote them g reor (s, τs , a, t, τt ). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems of"
D09-1023,H05-1064,0,0.0228546,"0 0 k∈τt−1 (j) i =0 t ∈Trans(si0 ) >` a ´o f lex (si , t0 ) + f att ($, 0, t0 , k) + f qg (0, i, 0, k) (11) «ff  „ f lex (si0 , t0 ) + f att (t, j, t0 , k)+ (12) S(k, i0 , t0 ) × exp θ > −1 0 f val (t, j, τt (j)) + f qg (i, i , j, k) n ` ´o if τt−1 (j) = ∅ (13) S(j, i, t) = exp θ > f val (t, j, τt−1 (j)) Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. θ. Eqs. 11–13: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the function’s gradient (vector of first derivatives) with respect to θ.11 sag, 1975): p(t, τt |s, τs ) ≈ p(t |τt , s, τs ) × p(τt |t, s, τs ) Plugging this into Eq. 9, we arrive at Eq. 10 (Tab. 3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are ident"
D09-1023,P06-1096,0,0.0946841,"Missing"
D09-1023,E09-1061,0,0.0181354,"nisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Met"
D09-1023,W06-1606,0,0.0184383,"of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3"
D09-1023,P08-1023,0,0.031142,"of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq."
D09-1023,P02-1038,0,0.549002,"hrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences (1) In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, τt , a |s, τs ) = exp{θ > g(s, τs , a, t, τt )} > 0 0 0 a0 ,t0 ,τ 0 exp{θ g(s, τs , a , t , τt )} P Lexical Translations (2) t 4 There are two conventional definitions of feature functions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice. where the g are arbitrary feature functions and the θ are feature weights. If one or both parse trees or the word alignments are"
D09-1023,J03-1002,0,0.0201692,"e a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words × phrase conditional and “lexical smoothing” probabilities × two conditional directions. Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) wi"
D09-1023,P03-1021,0,0.0624178,", 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features. Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudolikelihood, as well as to compute their approximate gradients. Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways. The pseudolikelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA’s inner loop faster than MERT’s inner loop. 6 Experiments Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. 6.1 Data and Evaluation We use the Ge"
D09-1023,2001.mtsummit-papers.68,0,0.0214557,"with Gs,τs . The technique is the summing variant of the decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only o"
D09-1023,P92-1017,0,0.127686,"tures at a time. We must sum over target word sequences and word alignments (with fixed τt ), and separately over target trees and word alignments (with fixed t). Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j) |for all j, a fast “inside” DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn2 ) time and O(mn) space. 5.1 Summing over t and a The summation over target word sequences and alignments given fixed τt bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Let S(j, i, t) denote the sum of all translations rooted at position j in τt such that a(j) = i and tj = t. Tab. 3 gives the equations for this DP: Eq. 11 is the quantity of interest, Eq. 12 is the recursion, and Eq. 13 shows the base cases for leaves of τt . Letting q = max0≤i≤n |Trans(si )|, this algorithm runs in O(mn2 q 2 ) time and O(mnq) space. For efficiency we place a hard upper bound on q during training (details in §6). Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between th"
D09-1023,P05-1034,0,0.0656359,"). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features tha"
D09-1023,P08-1066,0,0.047245,"Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Gr"
D09-1023,W06-3104,0,0.102126,"st popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled e"
D09-1023,D09-1086,0,0.0414309,"ormalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source"
D09-1023,E09-1088,0,0.0224157,"five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, as defined in Eq. 1.7 (In §5 we will consider kbest and all-translations variations on this prob6 I.e., from here on, a : {1, . . . , m} → {0, . . . , n} where 0 denotes alignment to NULL. 7 Arguably, we seek argmaxt p(t |s), marginalizing out everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work. 222 Source: $ konnten sie es übersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:? ?:? übersetzen: translate übersetzen: translate sie:you sie:you konnten:could konnten:couldn konnten:might es:it sie:you übersetzen: translated übersetzen: translated sie:let ?:? es:it es:it es:it sie:them konnten:could NULL:to ... ... übersetzen: translate ... ... ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and"
D09-1023,D07-1003,1,0.951839,"imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source word to target words to which it may translate s = hs0 , . . . , sn i ∈ Σ"
D09-1023,P01-1067,0,0.18514,"controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mut"
D09-1023,D08-1022,0,\N,Missing
D09-1023,J03-4003,0,\N,Missing
D09-1023,P02-1040,0,\N,Missing
D09-1023,D08-1076,0,\N,Missing
D11-1044,W09-0437,0,0.0126704,"ish and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient"
D11-1044,C10-1007,0,0.0202267,"threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs"
D11-1044,D10-1117,0,0.0675722,"Missing"
D11-1044,J92-4003,0,0.0175609,"f extracted 3 For a monolingual task, Wu et al. (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. 477 phrase dependencies of the form hu, v, di, where u is the head phrase, v is the child phrase, and d ∈ {left, right} is the direction, we then estimate conditional probabilities p(v|u, d) using relative frequency estimation. Table 3 shows the most probable child phrases for an example parent phrase. To combat data sparseness, we perform the same procedure with each word replaced by its word cluster ID obtained from Brown clustering (Brown et al., 1992). We include a feature in the model for the sum of the scaled log-probabilities of each attachment: 0 m X i=1   max 0, C + log p(φi |φτφ (i) , d(i) (1) where d(i) = I[τφ (i) − i > 0] is the direction of the dependency arc. Although we use log-probabilities in this feature function, we first add a constant C to each to ensure they are all positive.4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity. Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a derivation compl"
D11-1044,D09-1021,0,0.0724346,"ncy syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score tran"
D11-1044,W08-0336,0,0.0420574,". Our training procedure requires two executions of MERT, and the second typically takes more iterations to converge (10 to 20 is typical) than the first due to the use of a larger feature set and increased possibility for search error due to the enlarged search space. 7 Experiments For experimental evaluation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and"
D11-1044,P05-1033,0,0.601758,"we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by depende"
D11-1044,P09-1053,1,0.853685,"ponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase depend"
D11-1044,D07-1079,0,0.0152683,"on (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith,"
D11-1044,N10-1141,0,0.020005,"a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use o"
D11-1044,W10-3801,0,0.0227672,"includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by"
D11-1044,H05-1036,1,0.889731,"= 400. Input: tuning set D = hS, T i, initial weights λ0 for coarse model, initial weights ψ 0 for additional features in fine model Output: coarse model learned weights: λM , fine model learned weights: hλ∗ , ψ ∗ i λM ← MERT (S, T , λ0 , 100, M OSES); LMERT ← GenerateLattices (S, λM ); LFB ← FBPrune (LMERT , λM ); hλ∗ , ψ ∗ i ← MERT (LFB , T , hλM , ψ 0 i, 200, QGD EP PARSE); return λM , hλ∗ , ψ ∗ i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type G OAL from the agenda, we are guaranteed to have the best parse."
D11-1044,C96-1058,0,0.109409,"., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires O(E 2 V ) time and O(E 2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So, we use approximate search based on coarse-to-fine decoding. We now discuss each step of this procedure; an outline is shown as Alg. 1. Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning"
D11-1044,P09-1087,0,0.327394,"ne. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together th"
D11-1044,N10-1140,0,0.0403402,"s described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tr"
D11-1044,P06-1121,0,0.0482079,"coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage stand"
D11-1044,D09-1023,1,0.861443,"are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches current"
D11-1044,2006.amta-papers.8,0,0.164723,"y parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel featu"
D11-1044,P04-1061,0,0.138389,"Missing"
D11-1044,N03-1017,0,0.356345,"d syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way"
D11-1044,2005.iwslt-1.8,0,0.0246777,"gh each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M"
D11-1044,P07-2045,0,0.0179078,"nition of QG. The alignment variable in QG links target tree nodes to source tree nodes. However, we never commit to a source phrase dependency tree, instead using a source lexical dependency tree output by a dependency parser, so our alignment variable a is a function from target tree nodes (phrases in φ) to source phrases in γ, which might not be source tree nodes. The features in our model may consider a large number of source phrase dependency trees as long as they are consistent with τs . 4 Features Our model contains all of the standard phrase-based features found in systems like Moses (Koehn et al., 2007), including four phrase table probability features, a phrase penalty feature, an n-gram language model, a distortion cost, six lexicalized reordering features, and a word penalty feature. We now describe in detail the additional features 476 $ ← said : $ ← said that $ ← is a $ ← will be $ ← it is $ ← this is $ ← we must the → united states the → development of the two → countries he → said : $ ← he said : $ ← we should $ ← has been - us → relations $ ← he said cross - strait → relations $ ← pointed out that , and → is the chinese → government $ ← is the $ ← said , one - china → principle sino"
D11-1044,P03-1056,0,0.0969451,"sed translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences. Only words that appeared at lea"
D11-1044,P09-1065,0,0.0164493,"a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. T"
D11-1044,C08-1064,0,0.0133262,"QPDG features described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown"
D11-1044,D08-1076,0,0.0253598,"ce s and its parse τs , i.e., finding the most probable derivation under the s/τs -specific grammar Gs,τs . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice par"
D11-1044,J93-2004,0,0.0369839,"Missing"
D11-1044,P09-1039,1,0.943789,"find the most liberal threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immedia"
D11-1044,J03-1006,0,0.0670421,", α = 100, and β = 400. Input: tuning set D = hS, T i, initial weights λ0 for coarse model, initial weights ψ 0 for additional features in fine model Output: coarse model learned weights: λM , fine model learned weights: hλ∗ , ψ ∗ i λM ← MERT (S, T , λ0 , 100, M OSES); LMERT ← GenerateLattices (S, λM ); LFB ← FBPrune (LMERT , λM ); hλ∗ , ψ ∗ i ← MERT (LFB , T , hλM , ψ 0 i, 200, QGD EP PARSE); return λM , hλ∗ , ψ ∗ i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type G OAL from the agenda, we are guaranteed t"
D11-1044,P02-1038,0,0.0870009,", thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar m"
D11-1044,J03-1002,0,0.00315822,"ation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used this baseline Moses system to generate phrase lattices"
D11-1044,P03-1021,0,0.0163051,"making this property no longer hold; as a result, G OAL items may be popped out of order from the agenda. Therefore, we use an approximation, simply popping G G OAL items from the agenda and then stopping. The items are sorted by their scores and the best is returned by the decoder (or the k best in the case of MERT). In our experiments, we set G = 4000. The combined strategy yields average decoding times in the range of 30 seconds per sentence, which is comparable to other syntax-based MT systems. 6 Training For tuning the coarse and fine parameters, we use minimum error rate training (MERT; Och, 2003) in a procedure shown as Alg. 2. We first use MERT to train parameters for the coarse phrase-based model used to generate phrase lattices. Then, after generating the lattices, we prune them and run MERT a 480 second time to tune parameters of the fine model, which includes all phrase-based and QPDG parameters. The arguments to MERT are a vector of source sentences (or lattices), a vector of target sentences, the initial parameter values, the size of the k-best list, and finally the decoder. We initialize λ to the default Moses feature weights and for ψ we initialize the two target phrase depen"
D11-1044,2001.mtsummit-papers.68,0,0.0892975,"Missing"
D11-1044,P05-1034,0,0.0590897,"that Related Work We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right"
D11-1044,P08-1066,0,0.287817,"ly at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string"
D11-1044,W06-3104,0,0.636635,"eristy Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract syntax. Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also inves"
D11-1044,D09-1086,0,0.148772,"ts. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependency grammars sis of the"
D11-1044,N03-1033,0,0.100675,"Missing"
D11-1044,D08-1065,0,0.252728,"i.e., finding the most probable derivation under the s/τs -specific grammar Gs,τs . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires"
D11-1044,W02-1021,0,0.13366,"ntages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. Our experiments demonstrate an average improvement of +0.65 BLEU in Chinese-English translation across three test sets and an improvement of +0.75 BLEU in Urdu-English translation over a phrase-based baseline. We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers, reporting promising results: usin"
D11-1044,D07-1003,1,0.897041,"and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the ph"
D11-1044,D09-1159,0,0.233739,"n} → {0, . . . , n} τφ : {1, . . . , m0 } → {0, . . . , m0 } a : {1, . . . , m0 } → {1, . . . , n0 } θ = hλ, ψi source language sentence target language sentence, translation of s segmentation of s into phrases segmentation of t into phrases dependency tree on source words s, where τs (i) is the index of the parent of word si (0 is the root, $) dependency tree on target phrases φ, where τφ (i) is the index of the parent of phrase φi one-to-one alignment from phrases in φ to phrases in γ parameters of the full model (λ = phrase-based, ψ = QPDG) Table 1: Key notation. have recently been used by Wu et al. (2009) for feature extraction for opinion mining. When used for translation modeling, they allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as long-distance relationships among the phrases in a sentence. We then define a quasi-synchronous phrase dependency grammar (QPDG) as a conditional model p(t, γ, φ, τφ , a |s, τs ) that induces a probabilistic monolingual phrase dependency grammar over sentences inspired by the source sentence and (lexical) dependency tree. The source and target sentences are segmented into phrases and the phrases are ali"
D11-1044,W06-3119,0,0.062679,"ach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alon"
D11-1044,C08-1144,0,0.136981,"Missing"
D11-1044,P02-1040,0,\N,Missing
D12-1124,D10-1124,1,0.87297,"Missing"
D12-1124,P11-1137,1,0.861191,"d. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al"
D12-1124,P07-1053,0,0.0338299,"on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readabil"
D12-1124,N10-1038,1,0.945812,"ustomer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and sentiment is an active area of investig"
D12-1124,N09-1031,1,0.950619,". charbroiled affect its price? When a customer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and"
D12-1124,C08-1060,0,0.027295,"cioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of"
D12-1124,D11-1055,1,0.907537,"Missing"
D13-1111,P11-1022,0,0.0364316,"w much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for"
D13-1111,J92-4003,0,0.0565547,"model of “bad” translations). This yielded 4 features. The procedure was then repeated using POS tags instead of words, for 8 features in total. Google 5-Grams (G OOG): Translations were compared to the Google 5-gram corpus (LDC2006T13) to compute: the number of 5-grams that matched, the number of 5-grams that missed, and a set of indicator features that fire if the fraction of 5grams that matched in the sentence was greater than {0.05, 0.1, 0.2, . . . , 0.9}, for a total of 12 features. Word Cluster LMs (WCLM): Using an implementation provided by Liang (2005), we performed Brown clustering (Brown et al., 1992) on 900k English sentences, including the NC corpus and random sentences from Gigaword. We clustered words that appeared at least twice, once with 300 clusters and again with 1000. We then replaced words with their clusters in a large corpus consisting of the WMT news data, Gigaword, and the NC data. An additional cluster label was used for unknown words. For each of the clusterings (300 and 1000), we estimated 5- and 7-gram LMs with Witten-Bell smoothing (Witten and Bell, 1991). We added 4 features to the reranker, one for the log-probability of the translation under each of the word cluster"
D13-1111,W13-2239,0,0.0691083,"t closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when consi"
D13-1111,W08-0336,0,0.0128042,"ata and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was"
D13-1111,P05-1022,0,0.0176181,"Missing"
D13-1111,D10-1059,0,0.0797957,"s are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity function ∆(·, ·) that specifies how two solutions differ. Our first contribution is a family of dissimilarity function"
D13-1111,J07-2003,0,0.0562394,"r x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as hym , hm i = argmax w φ(x, y, h) + hy,hi∈Tx m−1 X λj ∆(yj , y) Dissimilarity Functions for MT When designing a dissimilarity function ∆(·, ·) for MT, we want to consider variation both in individual word choice"
D13-1111,J05-1003,0,0.0155194,"Missing"
D13-1111,P09-1064,0,0.028759,"r n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Task"
D13-1111,N12-1059,0,0.402172,"bility to the negated count for each n-gram in previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems train"
D13-1111,D12-1065,0,0.0160887,"e to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Taskar, 2011) and discovery of topical threads in document collections (Gillenwater et al., 2012). Unfortunately, in the structured setting, DPPs make severely restric1102 tive assumptions on the scoring function, while our framework does not. 5 Experimental Setup We now embark on an extensive empirical evaluation of the framework presented above. We begin by analyzing our diverse sets of translations, showing how they differ from standard M -best lists (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminative reranking (Section 8), and a novel human postediting task (Section 9). In th"
D13-1111,2010.amta-papers.34,0,0.0181645,"veraged over the sentences in each quartile. As shown in the plot, the ranges of 20-diverse lists subsume those of 20-best lists, though the medians of diverse 3 The optimal values of λ were 0.005 for AR→EN and 0.01 for ZH→EN and DE→EN. Since these values depend on the scale of the weights learned by MERT, they are difficult to interpret in isolation. 1104 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE 1) to generate lists for system combination on TUNE 2 and TEST. We compare M -best lists, unique M -best lists, and M -diverse lists, with M ∈ {10, 15, 20}.5 For each choice of list type and M , we trained the system combiner on TUNE 2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists generated by the system combiner during tuning, k ∈ {300, 600}) were chosen to maximize"
D13-1111,W11-2123,0,0.00371833,"to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE 1. We used the learned parameters to generate M -best and diverse lists for TUNE 2 and TEST to use for subsequent experiments. 5.3 1 best 20 best 200 best 1000 best unique 20 best unique 200 best 20 diverse 20 div × 10 best 20 div × 50 best Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function ∆n (§3.2) and the λj weights on the dissimilarity terms in Eq. (2). Though"
D13-1111,2008.amta-srw.3,0,0.0500625,"ch sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used"
D13-1111,2009.iwslt-papers.4,0,0.0288142,"rom the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with mod"
D13-1111,D11-1125,0,0.0159599,", and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE 2 to choose the regularization parameter C from the set {0.01, 0.1, 1, 10}. We selected the value yielding the highest average"
D13-1111,P08-1067,0,0.0110543,"em makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions"
D13-1111,N03-1017,0,0.0799251,"Hx , where Hx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as hym , hm i = argmax w φ(x, y, h) + hy,hi∈Tx m−1 X λj ∆(yj , y) Dissimilarity Functions for MT When designing a dissimilarity function ∆(·, ·) for MT, we want to consi"
D13-1111,P07-2045,1,0.0187723,"03k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (St"
D13-1111,N10-1078,0,0.0440076,"t in typical M -best lists, thereby hindering its ability to correctly rank diverse lists at test time. These results suggest that part of the benefit of using diverse lists comes from seeing a larger portion of the output space during training. 9 Human Post-Editing Experiments We wanted to determine whether diverse translations could be helpful to users struggling to understand the output of an imperfect MT system. We consider a post-editing task in which users are presented with translation output without the source sentence, and are asked to improve it. This setting has been studied; e.g., Koehn (2010) presented evidence that monolingual speakers could often produce improved translations for this task, occasionally reaching the level of an expert translator. Here, we use a novel variation of this task in which multiple translations are shown to editors. We compare the use of entries from an M -best list and entries from a diverse list. Again, the original source sentence is not provided. Our goal is to determine whether multiple, diverse translations can help users to more accurately guess the meaning of the original sentence than entries from a standard M -best list. If so, commercial MT s"
D13-1111,N09-1046,1,0.452862,"0M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of"
D13-1111,W12-3123,0,0.0120141,"system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approach is its computa"
D13-1111,W06-1673,0,0.0863389,"of Diversity in Machine Translation Kevin Gimpel∗ Dhruv Batra† Chris Dyer‡ Gregory Shakhnarovich∗ ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637, USA † Virginia Tech, Blacksburg, VA 24061, USA ‡ Carnegie Mellon University, Pittsburgh, PA 15213, USA Corresponding author: kgimpel@ttic.edu Abstract Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT. 1 Introduction From the perspective of user interaction, the id"
D13-1111,N04-1022,0,0.0225903,"q. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over set"
D13-1111,P09-1019,1,0.418986,"est output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity function ∆(·, ·) that spec"
D13-1111,P03-1051,0,0.0141558,"ge pair has two tuning and one test set: TUNE 1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE 2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR→EN and ZH→EN and one for DE→EN. For AR→EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT20"
D13-1111,P09-1067,0,0.0139587,"at requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers divers"
D13-1111,2011.mtsummit-papers.58,0,0.199694,"ttings and do not show much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5,"
D13-1111,2011.eamt-1.12,0,0.0112461,"f the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approac"
D13-1111,2009.mtsummit-posters.20,0,0.038311,"he BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness"
D13-1111,C04-1072,0,0.0155123,"Missing"
D13-1111,N03-1033,0,0.0637169,"Missing"
D13-1111,D07-1105,0,0.0285552,"nal language model in ARPA format that sets the logprobability to the negated count for each n-gram in previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination"
D13-1111,D08-1065,0,0.0185506,"ate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity funct"
D13-1111,D08-1076,0,0.0130331,"s commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a gener"
D13-1111,P02-1038,0,0.108968,"of all strings in a source language. For an x ∈ X, let Yx denote the set of its possible translations y in the target language. MT models typically include a latent variable that captures the derivational structure of the translation process. Regardless of its specific form, we refer to this variable as a derivation h ∈ Hx , where Hx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a g"
D13-1111,J03-1002,0,0.016745,"ng us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for"
D13-1111,N04-1021,0,0.0168026,"ly determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the"
D13-1111,P03-1021,0,0.0466511,", discriminative reranking (Section 8), and a novel human postediting task (Section 9). In the remainder of this section, we describe details of our experimental setup. 5.1 Language Pairs and Datasets We use three language pairs: Arabic-to-English (AR→EN), Chinese-to-English (ZH→EN), and German-to-English (DE→EN). For AR→EN and DE→ EN , we used a phrase-based model (Koehn et al., 2003) and for ZH→EN we used a hierarchical phrase-based model (Chiang, 2007). Each language pair has two tuning and one test set: TUNE 1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE 2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR→EN and ZH→EN and one for DE→EN. For AR→EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens an"
D13-1111,P02-1040,0,0.10263,"sed for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE 1. We used the learned parameters to generate M -best and diverse lists for TUNE 2 and TEST to use for subsequent experiments. 5.3 1 best 20 best 200 best 1000 best unique 20 best unique 200 best 20 diverse 20 div × 10 best 20 div × 50 best Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function ∆n (§3.2) and the λj weights on the dissimilarity terms in Eq. (2). Though our framework permits different λj for each j, we use a single λ v"
D13-1111,P12-1101,0,0.0231153,"s the percentage of words whose lexical translation probability falls below a threshold. We also include versions of the first 2 features normalized by the translation length, for a total of 5 I NV M OD 1 features. Large LM (LLM): We created a large 4-gram LM by interpolating LMs from the WMT news data, Gigaword, Europarl, and the DE→EN news commentary (NC) corpus to maximize likelihood of a heldout development set (WMT08 test set). We used the average per-word log-probability as the single feature function in this category. Syntactic LM (S YN): We used the syntactic treelet language model of Pauls and Klein (2012) to compute two features: the translation log probability and the length-normalized log probability. Finite/Non-Finite Verbs (V ERB): We ran the Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) on each translation and added four features: the fraction of words tagged as finite/non-finite verbs, and the fraction of verbs that are finite/nonfinite.9 9 Words tagged as MD, VBP, VBZ, and VBD were counted 1106 Reranking features N/A (baseline) None + I NV M OD 1 + LLM, S YN + V ERB, D ISC + G OOG + WCLM AR → EN best div 50.1 50.5 50.7 50.3 50.8 50.5 51.1 50.4 51.3 50.7 51.3 51.2 51.8 ZH"
D13-1111,N07-1029,0,0.0239987,"Missing"
D13-1111,W03-0402,0,0.0324323,"e Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal be"
D13-1111,N04-1023,0,0.0100353,"LEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadolla"
D13-1111,2008.amta-papers.18,0,0.0323456,"field’s inception. It is the way we interact with commercial MT services (such as Google Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to"
D13-1111,W12-3102,0,\N,Missing
D13-1111,W13-2201,0,\N,Missing
D14-1139,afonso-etal-2002-floresta,0,0.00852921,"led to high POS tagging accuracy metrics. We call this voting scheme A LIGN. To see the benefit of A LIGN, we also compare to a simple scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to"
D14-1139,N10-1083,0,0.0490183,"Missing"
D14-1139,D10-1117,0,0.123885,"adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent struct"
D14-1139,P11-1087,0,0.245259,"rove learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out f"
D14-1139,erjavec-etal-2010-jos,0,0.0151633,"cheme A LIGN. To see the benefit of A LIGN, we also compare to a simple scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training"
D14-1139,D13-1205,0,0.0118045,"resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by building a model of the joint probability distribution pθ (x, y). We use a log-linear parameterization with feature vector f and weigh"
D14-1139,J92-4003,0,0.351774,"Missing"
D14-1139,P09-1042,0,0.0183092,"ributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn l"
D14-1139,W06-2920,0,0.00887404,"t Functions The output cost π should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a sufficient number of times until they were a similar size as the largest treebank. Then we counted gold POS tag unigrams and bigrams from the concatenation. tag unigram X NUM PRT ADV CONJ PRON DET ADJ ADP VERB . NOUN tag bigram DET PRT DET CONJ NUM ADV NOUN NOUN DET NOUN NOUN . count 50783 174613 179131 330210 436649 461880 615284 694685 906922 1018989 104266"
D14-1139,W11-2208,0,0.0341673,"Missing"
D14-1139,N09-1009,0,0.0296754,"over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1"
D14-1139,D11-1005,0,0.0132415,"to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work."
D14-1139,W12-1909,0,0.0241758,"Missing"
D14-1139,N10-1112,1,0.837539,"ammaticality. To address this, we introduce an observation cost function ∆ : X × X → R≥0 that indicates how much two observations differ. Using ∆, we define the following gain function γCCE1 (x(i) ) = n o X log exp θ > f (x(i) , y) − y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) + ∆(x(i) , x0 ) x0 ∈Ni y 0 ∈Y(x0 ) The function ∆ inflates the score of neighborhood entries with larger differences from the observed x(i) . This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function ∆ to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially inflated by ∆, learning pays more attention to them, choosing weights that penalize them more than the low"
D14-1139,N12-1023,1,0.81247,"AT LM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y (i) . Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y (i) , y), which requires the true output y (i) . We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function:   max θ > f (x(i) , y) − cost(y (i) , y) − y∈Y(x(i) )   θ > f (x(i) , y 0 ) + cost(y (i) , y 0 ) (1) max y 0 ∈Y(x(i) ) Maximizing this gain function for supervised learning corresponds to increasing the model score 1332 of outputs that have both high model score (θ > f ) and low cost, while decreasing the model score of outputs with high model score and high cost. For unsupervised learning, we do not have y (i) , so we simply drop y (i) from the cost function. The result is an output cost function π : Y → R≥0 which captures our a priori knowledge abou"
D14-1139,P07-1094,0,0.027513,"d CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised me"
D14-1139,P11-1061,0,0.0818384,"2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why th"
D14-1139,N06-1041,0,0.0842256,"z Z(θ) } The difficulty is the final term, log Z(θ), which requires summing over all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training = log (i) y∈Y(x(i) ) pθ (x , y) P P 0 0 x0 ∈Ni y 0 ∈Y(x0 ) pθ (x , y ) n o exp θ > f (x(i) , y) − X y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) x0 ∈Ni y 0 ∈Y(x0 ) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summation over pairs. Two desiderata govern the choice of N. One is to make the summation over its elements computationally tractable. If N(x) = X for all x ∈ X, we obtain EM, so a smaller neighborhood typically must be used in practice. The second consideration is to t"
D14-1139,D07-1031,0,0.164084,"SCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Sn"
D14-1139,P11-1042,0,0.025385,"Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by building a model of the joint probability distribution pθ (x, y). We use a log-linear parameterization with feature vector f and weight vector θ:  exp θ > f (x, y)  > pθ (x, y) = P 0 0 x0 ∈X,y 0 ∈Y(x0 ) exp θ f (x , y ) data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i) ), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE (x(i) ) = log Pr(x(i) |Ni ) P = log where the sum in the denominator ranges over all possible inputs and all valid outputs for them. In this paper, we consider ways of learning the paramet"
D14-1139,P04-1061,0,0.0631652,"e setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly"
D14-1139,2005.mtsummit-papers.11,0,0.00892963,"which we also compare to. When using both M AT LM and U NIV simultaneously, we first choose the best two α values by the LL criterion and the best two β values by the CE criterion when using only those individual costs. This gives us 4 pairs of values; we run experiments with these pairs and choose the pair to report using each of the model selection criteria. For system combination, we use the 4 system outputs resulting from these 4 pairs. For training bigram language models for the M AT LM cost, we use the language’s POS training data concatenated with its portion of the Europarl v7 corpus (Koehn, 2005) and the text of its type. For unknown words at test time, we use the UNK emission feature, the Brown cluster features with the special UNK cluster identifiers, and the word’s actual spelling features. 8 In subsequent experiments we tried C ∈ {0.01, 0.001} for the baseline CE setting and found minimal differences. 1336 neighborhood S HUFF 10 T RANS 1 mod. sel. none N/A CE M ATCH LL CE M AT LM LL none N/A CE M ATCH LL CE M AT LM LL cost DA M-1 1-1 45.0 38.0 48.9 31.5 49.9 34.4 49.1 34.3 50.2 40.0 58.5 42.7 58.5 42.5 58.8 42.8 59.4 43.5 58.7 42.8 NL M-1 1-1 55.1 45.7 56.5 46.4 56.5 46.4 59.6 50."
D14-1139,C10-2075,0,0.0258577,"ather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be im"
D14-1139,D12-1127,0,0.0254019,"Missing"
D14-1139,N09-1069,0,0.0766411,"Missing"
D14-1139,N06-1014,0,0.0530489,"of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typicall"
D14-1139,P05-1044,0,0.572934,"ion of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives. In this paper, we present a new objective function for weakly-supe"
D14-1139,J94-2001,0,0.481462,"ng structural preferences on the latent variable used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the ter"
D14-1139,P06-1072,0,0.50841,"text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 201"
D14-1139,D09-1086,0,0.0197016,"sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; th"
D14-1139,D10-1120,0,0.505272,"ion, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an output cost function in our framework. Posterior regularization (PR; Ganchev et al., 2010) is a general framework for declaratively specifying preferences on model outputs. Naseem et al. (2010) proposed universal syntactic rules for unsupervised dependency parsing and used them in a PR regime; we use analogous universal tag sequences in our cost function. Our output cost is similar to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging e"
D14-1139,P09-1009,0,0.460881,"07; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that"
D14-1139,nivre-etal-2006-talbanken05,0,0.0104745,"scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the π cost function. But we use M-1 and 1-1 accuracy to enable e"
D14-1139,N10-1116,0,0.100382,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,W10-2902,0,0.125561,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,petrov-etal-2012-universal,0,0.123056,"erm, defining our new gain function γCCE2 (x(i) ) = n o X log exp θ > f (x(i) , y) − π(y) − y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) + π(y 0 ) x0 ∈Ni y 0 ∈Y(x0 ) Gimpel (2012) found that using such “softened” versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training machine translation systems. 5.1 Output Cost Functions The output cost π should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a suff"
D14-1139,N09-1024,0,0.19186,"upervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by sw"
D14-1139,P09-1057,0,0.125057,"d task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das"
D14-1139,P04-1062,0,0.0392678,"9; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2."
D14-1139,P10-1130,0,0.103689,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,D11-1117,0,0.0681351,"mith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives."
D14-1139,W11-0303,0,0.0711019,"mith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives."
D14-1139,D13-1204,0,0.0138648,"into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired o"
D14-1139,Q13-1001,0,0.0298322,"Missing"
D14-1139,D09-1071,0,0.0607765,"Missing"
D14-1139,P10-2039,0,0.0204087,"Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the sam"
D14-1139,Q14-1005,0,0.0187655,"Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by build"
D14-1139,D11-1081,0,0.0239983,"e the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an outp"
D14-1139,D07-1096,0,\N,Missing
D15-1181,S12-1051,0,0.148195,"eling and similarity measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonst"
D15-1181,S12-1059,0,0.0108059,"Missing"
D15-1181,P14-1114,0,0.027304,"Missing"
D15-1181,S14-2114,0,0.0352034,"Missing"
D15-1181,D12-1050,0,0.0152731,"Missing"
D15-1181,P09-1053,0,0.0209443,"(1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional"
D15-1181,C04-1051,0,0.305727,"larities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most"
D15-1181,I05-5003,0,0.00955795,"Missing"
D15-1181,N13-1092,0,0.0723157,"Missing"
D15-1181,P12-1091,0,0.0163219,"hine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner"
D15-1181,D13-1090,0,0.08521,"roduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare loca"
D15-1181,S14-2131,0,0.122905,"Missing"
D15-1181,P14-1062,0,0.0453703,"Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent par"
D15-1181,D14-1181,0,0.0249844,"veloped a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse featu"
D15-1181,Q14-1017,0,0.106757,"Missing"
D15-1181,S14-2055,0,0.079445,"Missing"
D15-1181,N12-1019,0,0.0261517,"in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooli"
D15-1181,P14-5010,0,0.00337417,"We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task. Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dim g + Dim p + Dim k = 525-dimension vectors for each input word; fo"
D15-1181,S14-2001,0,0.125109,"measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits f"
D15-1181,D14-1162,0,0.113318,"bθk + ||θ||22 (6) m 2 m loss(θ) = k=1 where fbθ is the predicted distribution with model weight vector θ, f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS"
D15-1181,S12-1100,0,0.0426687,"Missing"
D15-1181,S12-1060,0,0.0122495,"Missing"
D15-1181,P15-1150,0,0.059497,"Missing"
D15-1181,U06-1019,0,0.0215901,"convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most previous work on modeling sentence similarity has focused on feature engineering. Several types of sparse features have been found useful, including: (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtai"
D15-1181,S12-1096,0,0.0315849,"Missing"
D15-1181,Q15-1025,1,0.0633854,", f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphra"
D15-1181,N15-1091,0,0.529578,"Missing"
D15-1181,S14-2044,0,0.212805,"Missing"
D15-1181,Q14-1034,0,\N,Missing
D16-1157,S12-1051,0,0.0144193,"the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages witho"
D16-1157,S13-1004,0,0.0200536,"sks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any m"
D16-1157,S14-2010,0,0.0216753,"ir similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Gan"
D16-1157,N06-2001,0,0.0153082,"morphology, and word choice. Inspection of embeddings of particular character ngrams reveals etymological links; e.g., die is close to mort. We release our resources to the community in the hope that CHARAGRAM can provide a strong baseline for subword-aware text representation. GRAM 2 Related Work We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memor"
D16-1157,D15-1041,0,0.0132114,"l architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (C"
D16-1157,D15-1075,0,0.00575546,"sequence. We were interested to see whether CHARAGRAM - PHRASE could handle negation, since it does model limited information about word order (via character n-grams that span multiple words). We made a list of “not” bigrams that could be represented by a single word, then embedded each bigram using both models and did a nearest-neighbor search over a working vocabulary.6 The results, in Table 8, show how the CHARAGRAM - PHRASE embeddings model negation. In all cases but one, the near6 This has all words in PPDB-XXL, our evaluations, and two other datasets: SST (Socher et al., 2013) and SNLI (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens. Word vehicals serious-looking near-impossible growths litered journeying babyyyyyy adirty Nearest Neighbors vehical, vehicles, vehicels, vehicular, cars, vehicle, automobiles, car serious, grave, acute, serious-minded, seriousness, gravity, serious-faced impossible, hard/impossible, audacious-impossible, impractical, unable growth, grow, growing, increases, grows, increase, rise, growls, rising liter, litering, lited, liters, literate, literature, literary, literal, lite, obliterated journey, journeys, voyage, trip, roadtrip, travel, tourney,"
D16-1157,P16-1139,0,0.00399984,"s outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1 1 Introduction Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded int"
D16-1157,P14-2111,0,0.047939,") used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic similarity model” or “deep structured semantic"
D16-1157,P16-1160,0,0.0220592,"models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015"
D16-1157,P16-2058,0,0.0518,"Missing"
D16-1157,W15-3904,0,0.0617296,"Missing"
D16-1157,D13-1146,0,0.0519918,"ing and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic simil"
D16-1157,P15-2076,0,0.048898,"Missing"
D16-1157,ganitkevitch-callison-burch-2014-multilingual,0,0.0247593,"014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014). Before training the CHARAGRAM model, we need to populate V , the vocabulary of character n-grams included in the model. We obtain these from the training data used for the final models in each setting, which is either the lexical or phrasal section of PPDB XXL. We tune over whether to include the full sets of character n-grams in these datasets or only those that appear more than once. When extracting n-grams, we include spaces and add an extra space before and after each word or phrase in the training and evaluation data to ensure that the beginning and end of each word is represented. We n"
D16-1157,N13-1092,0,0.0407243,"Missing"
D16-1157,D15-1181,1,0.802006,"xtual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation. This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a s"
D16-1157,J15-4004,0,0.0694913,"containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation. This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a summation is performed over the character n-grams in the sequence. We consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging. On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-ofthe-art performance on SimLex-999 (Hill et al., 2015). When evaluated on a large suite of sentencelevel semantic textual similarity tasks, CHARA GRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach similar performance, though CHARAGRAM converges fastest to high accuracy. 1504 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515, c Austin, Texas, November 1-5,"
D16-1157,N16-1162,0,0.112481,"Missing"
D16-1157,P15-1162,0,0.0128306,"Missing"
D16-1157,D14-1181,0,0.00208649,"populating a vector of length |V |with counts of character ngrams followed by a nonlinear transformation. We compare the CHARAGRAM model to two other models. First we consider LSTM architectures (Hochreiter and Schmidhuber, 1997) over the character sequence x, using the version from Gers et al. (2003). We use a forward LSTM over the characters in x, then take the final LSTM hidden vector as the representation of x. Below we refer to this model as “charLSTM.” We also compare to convolutional neural network (CNN) architectures, which we refer to below as “charCNN.” We use the architecture from Kim (2014) with a single convolutional layer followed by an optional fully-connected layer. We use filters of varying lengths of character n-grams, using two primary configurations of filter sets, one of which is identical to that used by Kim et al. (2015). Each filter operates over the entire sequence of character n-grams in x and we use max pooling for each filter. We tune over the choice of nonlinearity for both the convolutional filters and for the optional fullyconnected layer. We give more details below about filter sets, n-gram lengths, and nonlinearities. We note that using character n-gram conv"
D16-1157,P13-1149,0,0.0308746,"ord-aware text representation. GRAM 2 Related Work We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-le"
D16-1157,D15-1176,0,0.152769,"grams in the sequence. We consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging. On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-ofthe-art performance on SimLex-999 (Hill et al., 2015). When evaluated on a large suite of sentencelevel semantic textual similarity tasks, CHARA GRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach similar performance, though CHARAGRAM converges fastest to high accuracy. 1504 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We perform extensive analysis of our CHARA embeddings. We find large gains in performance on rare words, showing the empirical benefit of subword modeling. We also compare performance across different character n-gram vocabulary sizes, finding that the semantic tasks benefit far more from large vo"
D16-1157,P16-1100,0,0.0442846,"Missing"
D16-1157,W13-3512,0,0.235027,"tting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character"
D16-1157,S14-2001,0,0.0315977,"us on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the offic"
D16-1157,P15-2070,0,0.0506303,"Missing"
D16-1157,D14-1162,0,0.10436,"Missing"
D16-1157,P15-1094,0,0.058996,"Missing"
D16-1157,C14-1015,0,0.230275,"k We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translati"
D16-1157,K15-1026,0,0.0160069,"s are shown in Table 1. The CHARAGRAM model outperforms both the charLSTM and charCNN models, and also outperforms recent strong results on SL999. We also found that the charCNN and charLSTM models take far more epochs to converge than the CHARAGRAM model. We noted this trend across experiments and explore it further in Section 4.3. Comparison to Prior Work We found that performance of CHARAGRAM on word similarity tasks can be improved by using more character n-grams. This is explored in Section 4.4. Our best result from these experiments was obtained with the largest Model Hill et al. (2014) Schwartz et al. (2015) Faruqui and Dyer (2015) Wieting et al. (2015) CHARAGRAM (large) SL999 52 56 58 66.7 70.6 Table 2: Spearman’s ρ × 100 on SL999. refers to the CHARAGRAM CHARAGRAM (large) model described in Section 4.4. This model contains 173,881 character n-grams, more than the 100,283 in the CHARAGRAM model used in Table 1. model we considered, which contains 173,881 ngram embeddings. When using WS353 for model selection and training for 25 epochs, this model achieves 70.6 on SL999. To our knowledge, this is the best result reported on SL999 in this setting; Table 2 shows comparable recent results. Note that"
D16-1157,D13-1170,0,0.00784337,"mply averages the words in the sequence. We were interested to see whether CHARAGRAM - PHRASE could handle negation, since it does model limited information about word order (via character n-grams that span multiple words). We made a list of “not” bigrams that could be represented by a single word, then embedded each bigram using both models and did a nearest-neighbor search over a working vocabulary.6 The results, in Table 8, show how the CHARAGRAM - PHRASE embeddings model negation. In all cases but one, the near6 This has all words in PPDB-XXL, our evaluations, and two other datasets: SST (Socher et al., 2013) and SNLI (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens. Word vehicals serious-looking near-impossible growths litered journeying babyyyyyy adirty Nearest Neighbors vehical, vehicles, vehicels, vehicular, cars, vehicle, automobiles, car serious, grave, acute, serious-minded, seriousness, gravity, serious-faced impossible, hard/impossible, audacious-impossible, impractical, unable growth, grow, growing, increases, grows, increase, rise, growls, rising liter, litering, lited, liters, literate, literature, literary, literal, lite, obliterated journey, journeys, voyage, t"
D16-1157,N15-1186,0,0.0973462,"am embeddings. When using WS353 for model selection and training for 25 epochs, this model achieves 70.6 on SL999. To our knowledge, this is the best result reported on SL999 in this setting; Table 2 shows comparable recent results. Note that a higher SL999 number is reported by Mrkˇsi´c et al. (2016), but the setting is not comparable to ours as they started with embeddings tuned on SL999. Lastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection. We obtained a Spearman’s ρ of 47.1, which outperforms the 41.8 result from Soricut and Och (2015) and is competitive with the 47.8 reported by Pennington et al. (2014), which used a 42B-token corpus for training. 4.1.4 Sentence Embedding Experiments Training and Tuning We did initial training of our models using one pass through PPDB XL, which consists of 3,033,753 unique phrase pairs. Following Wieting et al. (2016), we use the annotated phrase pairs developed by Pavlick et al. (2015) as our validation set, using Spearman’s ρ to rank the models. We then take the highest performing models and train on the 9,123,575 unique phrase pairs in the phrasal section of PPDB XXL for 10 epochs. For"
D16-1157,W13-3204,0,0.0443586,"tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic similarity model” or “deep structured semantic model”) developed by Huang et al. (2013). For an information retrieval task, they represented words using feature vectors containing counts of character n-grams. Sperr et al. (2013) used a very similar technique to represent words in neural language models for machine translation. Our CHARAGRAM embeddings are based on this same idea. We show this strategy to be extremely effective when applied to both words and sentences, outperforming character LSTMs like those used by Ling et al. (2015a) and character CNNs like those from Kim et al. (2015). 3 Models We now describe models that embed textual sequences using their characters, including our CHARAGRAM model and the baselines that we compare to. We denote a character-based textual sequence by x = hx1 , x2 , ..., xm i, which"
D16-1157,P15-1150,0,0.0145383,"HARAGRAM embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1 1 Introduction Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This"
D16-1157,Q15-1025,1,0.846538,"is to produce embeddings for textual sequences such that the embeddings for paraphrases have high cosine similarity. Our third evaluation (Section 4.2) is a classification task, and follows the setup of the English part-of-speech tagging experiment from Ling et al. (2015a). 4.1 Word and Sentence Similarity We compare the ability of our models to capture semantic similarity for both words and sentences. We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al. (2015) and Wieting et al. (2016). More details are provided in the supplementary material. 4.1.1 Datasets For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to"
D16-1157,S15-2001,0,0.0627307,"the supplementary material. 4.1.1 Datasets For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or s"
D16-1241,P05-1045,0,0.0209727,"Missing"
D16-1241,P16-1086,0,0.230218,"Missing"
D16-1241,P03-1054,0,0.00625259,"Missing"
D16-1241,D13-1020,0,0.155425,"Missing"
D16-1241,P15-2115,1,0.45212,"Missing"
D16-1241,D15-1237,0,0.0518536,"Missing"
D17-1026,P11-2117,0,0.447553,"(Ganitkevitch et al., 2013). PPDB contains a large set of paraphrastic textual fragments extracted automatically from bilingual text (“bitext”), which is readily available for languages and domains. Versions of PPDB have been released for several languages (Ganitkevitch and Callison-Burch, 2014). However, more recent work has shown that the fragmental nature of PPDB’s pairs can be problematic, especially for recurrent networks (Wieting and Gimpel, 2017). Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011). While effective, this type of data is inherently limited in size and scope, and not available for languages other than English. PPDB is appealing in that it only requires bitext. We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs. We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs. In this paper, we build NMT systems for three Introduction Pretrained word embedd"
D17-1026,S14-2010,0,0.0507601,"Missing"
D17-1026,C04-1051,0,0.172726,"er work in learning general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). How"
D17-1026,I05-5002,0,0.0332126,"general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning senten"
D17-1026,S16-1081,0,0.0158621,"te the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to 2015 (Agirre et al., 2012, 2013, 2014, 2015), the SemEval 2015 Twitter task (Xu et al., 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. As our test set, we report the average Pearson’s r over these 22 sentence similarity tasks.4 As development data, we use the 2016 STS tasks (Agirre et al., 2016), where the tuning criterion is the average Pearson’s r over its 5 datasets. 5.2 GRAN AVG 67.2 65.8 64.5 65.8 65.5 65.4 66.5 65.1 67.2 65.1 67.3 66.1 67.8 65.7 67.4 65.9 67.0 65.2 66.5 66.2 67.2 65.6 66.5 64.7 Table 4: Test results (average Pearson’s r × 100 over 22 STS datasets) using a random selection of 24,000 examples from each data source. subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. We use PPDB XL in this paper, which consists of fairly high precision paraphrases. The other data source is th"
D17-1026,eisele-chen-2010-multiun,0,0.083257,"Missing"
D17-1026,S13-1004,0,0.031442,"Missing"
D17-1026,N15-1184,0,0.0202072,"Missing"
D17-1026,S12-1051,0,0.0332057,"ls, and the remaining 50,000 are used as train/validation/test data for the reference classification and language models described below. 277 5.1 Lang. Data SimpWiki PPDB CC CS EP News CC EP FR Giga News CC DE EP News Evaluation We evaluate the quality of a paraphrase dataset by using the experimental setting of Wieting et al. (2016b). We use the paraphrases as training data to create paraphrastic sentence embeddings, using the cosine of the embeddings as the measure of semantic relatedness, then evaluate the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to 2015 (Agirre et al., 2012, 2013, 2014, 2015), the SemEval 2015 Twitter task (Xu et al., 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. As our test set, we report the average Pearson’s r over these 22 sentence similarity tasks.4 As development data, we use the 2016 STS tasks (Agirre et al., 2016), where the tuning criterion is the average Pearson’s r over its 5 datasets. 5.2 GRAN AVG 6"
D17-1026,ganitkevitch-callison-burch-2014-multilingual,0,0.0883947,"re how neural machine translation output differs from humanwritten sentences, finding clear differences in length, the amount of repetition, and the use of rare words.1 1 Table 1: Illustrative examples of references (R) paired with back-translations (T). To learn their sentence embeddings, Wieting et al. used the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). PPDB contains a large set of paraphrastic textual fragments extracted automatically from bilingual text (“bitext”), which is readily available for languages and domains. Versions of PPDB have been released for several languages (Ganitkevitch and Callison-Burch, 2014). However, more recent work has shown that the fragmental nature of PPDB’s pairs can be problematic, especially for recurrent networks (Wieting and Gimpel, 2017). Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011). While effective, this type of data is inherently limited in size and scope, and not available for languages other than English. PPDB is appealing in that it only requires bitext. We would like to retain this property but develop a data resource with sentence pairs rath"
D17-1026,N13-1092,0,0.143558,"Missing"
D17-1026,W11-2123,0,0.0193471,"in the data. SimpWiki has a mean length of 24.2 and a standard deviation of 13.1. 5.6 Quality Filtering We also consider filtering based on several measures of the “quality” of the back-translation: • Translation Cost: We use the cost (negative log likelihood) of the translation from the NMT system, divided by the number of tokens in the translation. • Language Model: We train a separate language model for each language/data pair on 40,000 references that are separate from the 100,000 used for mining data. Due to the small data size, we train a 3-gram language model and use the KenLM toolkit (Heafield, 2011). • Reference/Translation Classification: We train binary classifiers to predict whether a given sentence is a reference or translation (described in Section 5.6.1). We use the probability of being a reference as the score for filtering. For translation cost, we tune the upper bound of the cost over the range [0.2, 1] using increments 279 Filtering Method None (Random) Translation Cost Language Model Reference Classification GRAN 66.9 66.6 66.7 67.0 AVG 65.5 65.4 65.5 65.5 Model Lang. Data CC CS EP News CC EP LSTM FR Giga News CC DE EP News CC CS EP News CC EP AVG FR Giga News CC DE EP News Ta"
D17-1026,P05-1074,0,0.255601,"kolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017) showed that PPDB is not as effective as sentential paraphrases, especially for recurrent networks. These results are intuitive because the phrases in PPDB are"
D17-1026,P01-1008,0,0.193712,"ll (Arora et al., 2017). Other work in learning general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wietin"
D17-1026,N16-1162,0,0.0106331,"Missing"
D17-1026,P15-1001,0,0.0255336,"nglish, and German→English. We used Groundhog3 as the implementation of the NMT systems for all experiments. We generally followed the settings and training procedure from previous work (Bahdanau et al., 2014; Sennrich et al., 2016a). As such, all networks have a hidden layer size of 1000 and an embedding layer size of 620. During training, we used Adadelta (Zeiler, 2012), a minibatch size of 80, and the training set was reshuffled between epochs. We trained a network for approximately 7 days on a single GPU (TITAN X), then the embedding layer was fixed and training continued, as suggested by Jean et al. (2015), for 12 hours. Additionally, the softmax was calculated over a filtered list of candidate translations. Following Jean et al. (2015), during decoding, we restrict the softmax layers’ output vocabulary to include: the 10000 most common words, the top 25 unigram translations, and the gold translations’ unigrams. All systems were trained on the available training data from the WMT15 shared translation task (15.7 million, 39.2 million, and 4.2 million sentence pairs for CS→EN, FR→EN, and DE→EN, 3 Available at https://github.com/sebastienj/LV_groundhog. 276 difference plays an important role in th"
D17-1026,W04-3219,0,0.412113,"mbeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieti"
D17-1026,P16-1009,0,0.0526135,"r recurrent networks (Wieting and Gimpel, 2017). Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011). While effective, this type of data is inherently limited in size and scope, and not available for languages other than English. PPDB is appealing in that it only requires bitext. We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs. We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs. In this paper, we build NMT systems for three Introduction Pretrained word embeddings have received a great deal of attention from the research community, but there is much less work on developing pretrained embeddings for sentences. Here we target sentence embeddings that are “paraphrastic” in the sense that two sentences with similar meanings are close in the embedding space. Wieting et al. (2016b) developed paraphrastic sentence embeddings that are useful for semantic textual sim"
D17-1026,P16-1162,0,0.0774275,"r recurrent networks (Wieting and Gimpel, 2017). Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011). While effective, this type of data is inherently limited in size and scope, and not available for languages other than English. PPDB is appealing in that it only requires bitext. We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs. We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured recently to yield strong performance especially in terms of producing grammatical outputs. In this paper, we build NMT systems for three Introduction Pretrained word embeddings have received a great deal of attention from the research community, but there is much less work on developing pretrained embeddings for sentences. Here we target sentence embeddings that are “paraphrastic” in the sense that two sentences with similar meanings are close in the embedding space. Wieting et al. (2016b) developed paraphrastic sentence embeddings that are useful for semantic textual sim"
D17-1026,2005.mtsummit-papers.11,0,0.248076,"Missing"
D17-1026,P07-2045,0,0.00496583,"Missing"
D17-1026,D16-1157,1,0.212157,"ieting,kgimpel}@ttic.edu, j.mallinson@ed.ac.uk R: We understand that has already commenced, but there is a long way to go. T: This situation has already commenced, but much still needs to be done. R: The restaurant is closed on Sundays. No breakfast is available on Sunday mornings. T: The restaurant stays closed Sundays so no breakfast is served these days. R: Improved central bank policy is another huge factor. T: Another crucial factor is the improved policy of the central banks. Abstract We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniq"
D17-1026,E17-1083,1,0.718386,"ned NMT systems and visualized part of the space of the source language encoder for their English→French system. Hill et al. (2016) evaluated the encoders of English-to-X NMT systems as sentence representations, finding them to 2 For example, CzEng 1.6 (Bojar et al., 2016) contains a billion words across its 8 domains. 275 Czech French German Europarl 650,000 2,000,000 2,000,000 Common Crawl 160,000 3,000,000 2,000,000 News Commentary 150,000 200,000 200,000 UN 12,000,000 109 French-English 22,000,000 CzEng 14,700,000 - perform poorly compared to several other methods based on unlabeled data. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. They used pairs of NMT systems, one to translate an English sentence into multiple foreign translations and the other to then translate back to English. Other work has used neural MT architectures and training settings to obtain better word embeddings (Hill et al., 2014a,b). Our approach differs in that we only use the NMT system to generate training data for training sentence embeddings, rather than use it as the source of the model. This permits us to decouple decisions made in designing the NMT archit"
D17-1026,S14-2001,0,0.0364352,"5.1 Lang. Data SimpWiki PPDB CC CS EP News CC EP FR Giga News CC DE EP News Evaluation We evaluate the quality of a paraphrase dataset by using the experimental setting of Wieting et al. (2016b). We use the paraphrases as training data to create paraphrastic sentence embeddings, using the cosine of the embeddings as the measure of semantic relatedness, then evaluate the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to 2015 (Agirre et al., 2012, 2013, 2014, 2015), the SemEval 2015 Twitter task (Xu et al., 2015), and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. As our test set, we report the average Pearson’s r over these 22 sentence similarity tasks.4 As development data, we use the 2016 STS tasks (Agirre et al., 2016), where the tuning criterion is the average Pearson’s r over its 5 datasets. 5.2 GRAN AVG 67.2 65.8 64.5 65.8 65.5 65.4 66.5 65.1 67.2 65.1 67.3 66.1 67.8 65.7 67.4 65.9 67.0 65.2 66.5 66.2 67.2 65.6 66.5 64.7 Table 4: Test results (av"
D17-1026,Q15-1025,1,0.693669,", 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017) showed that PPDB is not as effective as sentential paraphrases, especially for recurrent networks. These results are intuitive because the phrases in PPDB are short and often cut across constituent boundaries. For sentential paraphrases, Wieting and Gimpel (2017) used a dataset developed for text simplification by Coster and Kauchak (2011). It was created by aligning sentences from Simple English and standard English Wikipedia. We compare our data to both PPDB and this Wikipedia dataset. Related Work We describe related wo"
D17-1026,C12-1121,0,0.0282582,"k”) rather than rare words (“endangering”). 5.7 Diversity Filtering We consider several filtering criteria based on measures that encourage particular amounts of disparity between the reference and its backtranslation: • n-gram Overlap: Our n-gram overlap measures are calculated by counting n-grams of a given order in both the reference and translation, then dividing the number of shared n-grams by the total number of n-grams in the reference or translation, whichever has fewer. We use three n-gram overlap scores (n ∈ {1, 2, 3}). • BLEU Score: We use a smoothed sentencelevel BLEU variant from Nakov et al. (2012) that uses smoothing for all n-gram lengths and also smooths the brevity penalty. 6 Wikipedia was used to calculate the frequencies of the tokens. All tokens were lowercased. 7 This is noteworthy because the average sentence length of translations and references is not significantly different. 281 NMT SimpWiki Filtering Method GRAN AVG GRAN AVG Random 66.9 65.5 67.2 65.8 Unigram Overlap 66.6 66.1 67.8 67.4 Bigram Overlap 67.0 65.5 68.0 67.2 Trigram Overlap 66.9 65.4 67.8 66.6 BLEU Score 67.1 65.3 67.5 66.5 Data PPDB SimpWiki (100k/168k) CC-CS (24k) CC-CS (100k) CC-DE (24k) CC-DE (168k) GRAN 64"
D17-1026,P17-1190,1,0.463061,"le 1: Illustrative examples of references (R) paired with back-translations (T). To learn their sentence embeddings, Wieting et al. used the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). PPDB contains a large set of paraphrastic textual fragments extracted automatically from bilingual text (“bitext”), which is readily available for languages and domains. Versions of PPDB have been released for several languages (Ganitkevitch and Callison-Burch, 2014). However, more recent work has shown that the fragmental nature of PPDB’s pairs can be problematic, especially for recurrent networks (Wieting and Gimpel, 2017). Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (Coster and Kauchak, 2011). While effective, this type of data is inherently limited in size and scope, and not available for languages other than English. PPDB is appealing in that it only requires bitext. We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs. We turn to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Sennrich et al., 2016a), which has matured rece"
D17-1026,P15-1094,0,0.016573,"elop additional filtering methods, to experiment with richer architectures for sentence embeddings, and to further analyze the differences between neural machine translations and references. 2 and evaluated them using a suite of semantic textual similarity (STS) tasks and supervised semantic tasks. Others have begun to consider this setting as well (Arora et al., 2017). Other work in learning general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (200"
D17-1026,C10-1149,0,0.0260831,"utoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017"
D17-1026,P08-1089,0,0.0314263,"g frameworks (Le and Mikolov, 2014; Pham et al., 2015). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results. Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Zhao et al., 2010; Coster and Kauchak, 2011; Xu et al., 2014, 2015). The most relevant work uses bilingual corpora, e.g., Zhao et al. (2008) and Bannard and Callison-Burch (2005), the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages (Ganitkevitch and CallisonBurch, 2014) since it only relies on the availability of bilingual text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017) showed that PPDB is not as effective as sentential paraphrases, especially for recurrent networks. These results are intu"
D17-1026,S15-2001,0,\N,Missing
D18-1020,N16-1030,0,0.0237637,"he BiGRU is run over the input x1:T , where each xt is the concatenation of a word embedding and the concatenated final hidden states from a character-level BiGRU. The inference model qφ (zt |x1:T , t) is then a single layer feedforward neural network that uses ht as input. When parametrizing the posterior over latent variables in the following models below, we use this same procedure to produce hidden vectors with a BiGRU and then use them as input to feedforward networks. The structure of our inference model is similar to those used in previous state-of-the-art models for sequence labeling (Lample et al., 2016; Yang et al., 2017a). C1 (x1:T , lt ) = E [− log f (lt |yt )] (5) yt ∼qφ (·|x1:T ,t) The final loss for the model is L(x1:T , l1:T ) = T X [C1 (x1:T , lt ) − αU1 (x1:T , t)] t=1 (6) Where α is a trade-off hyperparameter. Similarly to the VSL-G model, qφ (zt |x1:T , t) and qφ (yt |x1:T , t) are parametrized by single layer feedforward neural networks using the hidden state ht as input. 3.5 Hierarchical Latent Variables We also explore hierarchical relationships among the latent variables. In particular, we introduce the VSL-GG-Hier model which has two Gaussian latent variables with the hierarc"
D18-1020,P08-1099,0,0.0380354,"an et al., 2016; Yang et al., 2017b), and sequence transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia)."
D18-1020,P11-2008,1,0.80619,"Missing"
D18-1020,P13-2017,0,0.0828262,"Missing"
D18-1020,P06-1027,0,0.0625951,"uage modeling (Bowman et al., 2016; Yang et al., 2017b), and sequence transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017;"
D18-1020,N04-1043,0,0.039809,"l methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learning interpretable latent representations. Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous a"
D18-1020,N13-1039,1,0.848097,"Missing"
D18-1020,P11-2009,0,0.0650038,"transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learnin"
D18-1020,N18-1162,0,0.0318899,"key novelties in our work compared to the prior work mentioned above are the proposed sequential variational labelers and the investigation of latent variable hierarchies within these models. The empirical effectiveness of latent hierarchical structure in variational modeling is a key contribution of this paper and may be applicable to the other applications discussed above. Recent work, contemporaneous with this submission, similarly showed the advantages of combining hierarchical latent variables and variational learning for conversational modeling, in the context of a non-sequential model (Park et al., 2018). 3 Proposed Methods We begin by describing variational autoencoders and the notation we will use in the following sections. We denote the input word sequence by x1:T , the corresponding label sequence by l1:T , the input words other than the word at position t by x−t , the generative model by pθ (·), and the posterior inference model by qφ (·). 3.1 Background: Variational Autoencoders We review variational autoencoders (VAEs) by describing a VAE for an input sequence x1:T . When using a VAE, we assume a generative model that generates an input using a latent variable z, typically assumed to f"
D18-1020,D14-1162,0,0.0901371,"Missing"
D18-1020,D10-1017,0,0.0325893,"al., 2017b), and sequence transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of"
D18-1020,P17-1161,0,0.0241059,"tent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learning interpretable latent representations. Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous and categorical latent variables. Hu et al. ("
D18-1020,E09-1088,0,0.0208267,"amount of work applying neural variational methods to NLP tasks, including document modeling (Mnih and Gregor, 2014; Miao et al., 2016; Serban et al., 2017), machine translation (Zhang et al., 2016), text generation (Bowman et al., 2016; Serban et al., 2017; Hu et al., 2017), language modeling (Bowman et al., 2016; Yang et al., 2017b), and sequence transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencod"
D18-1020,P16-2067,0,0.0265621,"sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learning interpretable latent representations. Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous and categorical latent variables. Hu et al. (2017) interpret a sentence as a combination of an unstructured latent code and a structured latent code, which can represent attributes of the sentence. There have been several efforts in combining variational autoencoders and recurrent networks (Gregor et al., 2015; Chung et al., 2015; Fr"
D18-1020,W03-0419,0,0.57525,"Missing"
D18-1020,P17-1194,0,0.0284363,"Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learning interpretable latent representations. Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous and categorical latent variables. Hu et al. (2017) interpret a sentence as a combination of an unstructured latent code and a structured latent code, which can represent attributes of the sentence. There have been several efforts in combining variational autoencoders and recurrent networks (Gregor et al., 2015; Chung et al., 2015; Fraccaro et al., 2016). While the details vary, these models typicall"
D18-1020,W17-4308,0,0.279171,"able for model training. In such cases regularization can be important, and it can be helpful to use additional unlabeled data. One approach for both regularization and semi-supervised training is to design latent-variable generative models and then develop neural variational methods for learning and inference (Kingma and Welling, 2014; Rezende and Mohamed, 2015). Neural variational methods have been quite successful for both generative modeling and representation learning, and have recently been applied to a variety of NLP tasks (Mnih and Gregor, 2014; Bowman et al., 2016; Miao et al., 2016; Serban et al., 2017; Zhou and Neubig, 2017; Hu et al., 2017). They are also very popular for semisupervised training; when used in such scenarios, they typically have an additional task-specific prediction loss (Kingma et al., 2014; Maale et al., 215 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 215–226 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics each time step in a sequence. This prior work mainly focused on ways of parameterizing the time dependence between the latent variables, which gives them more power in"
D18-1020,D16-1050,0,0.0398707,"ep, but we adopt stronger independence assumptions which leads to simpler models and inference procedures. Also, the models cited above were developed for modeling data distributions, rather than for supervised or semi-supervised learning, which is our focus here. chy, and find that the most effective structure also provides the cleanest separation of information in the latent space. 2 Related Work There is a growing amount of work applying neural variational methods to NLP tasks, including document modeling (Mnih and Gregor, 2014; Miao et al., 2016; Serban et al., 2017), machine translation (Zhang et al., 2016), text generation (Bowman et al., 2016; Serban et al., 2017; Hu et al., 2017), language modeling (Bowman et al., 2016; Yang et al., 2017b), and sequence transduction (Zhou and Neubig, 2017), but we are not aware of any such work for sequence labeling. Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al.,"
D18-1020,D17-1179,0,0.287709,"nce labeling (Quattoni et al., 2007; Sun and Tsujii, 2009). There has been a great deal of work on using variational autoencoders in semi-supervised settings (Kingma et al., 2014; Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b). Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Søgaard, 2011). The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017). Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data. Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Søgaard, 2017; Bingel and Søgaard, 2017; Rei, 2017, inter alia). Another related thread of work is learning interpretable latent representations. Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous and categorical latent variables. Hu et al. (2017) interpret a sentence as a"
D18-1020,P17-1029,0,0.327228,"ng. In such cases regularization can be important, and it can be helpful to use additional unlabeled data. One approach for both regularization and semi-supervised training is to design latent-variable generative models and then develop neural variational methods for learning and inference (Kingma and Welling, 2014; Rezende and Mohamed, 2015). Neural variational methods have been quite successful for both generative modeling and representation learning, and have recently been applied to a variety of NLP tasks (Mnih and Gregor, 2014; Bowman et al., 2016; Miao et al., 2016; Serban et al., 2017; Zhou and Neubig, 2017; Hu et al., 2017). They are also very popular for semisupervised training; when used in such scenarios, they typically have an additional task-specific prediction loss (Kingma et al., 2014; Maale et al., 215 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 215–226 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics each time step in a sequence. This prior work mainly focused on ways of parameterizing the time dependence between the latent variables, which gives them more power in modeling distributions"
D19-1040,D15-1075,0,0.0521416,"e author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction wit"
D19-1040,P18-1009,0,0.164166,"ddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent wo"
D19-1040,N18-1204,0,0.0267646,"es for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervised training to perform well on downstream ta"
D19-1040,L18-1269,0,0.391744,"er et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a). In this work, we focus on evaluating their capabilities in modeling entities. 3 EntEval We are interested in two approaches: contextualized entity representations (henceforth: CER) and descriptive entity representations (henceforth: DER), both encoding fixed-length vector representations for entities. The contextualized entity representations encodes an entity based on the context it appears regardless of whether the entity is seen before. The motivation behind contextualized entity representations is that we want an entity encoder that does not depend on entries in a kno"
D19-1040,D17-1284,0,0.554755,"n Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervise"
D19-1040,P18-1198,0,0.0855121,"Missing"
D19-1040,P17-1162,0,0.0147654,"ng natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity repr"
D19-1040,P13-2006,0,0.0645503,"v et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality"
D19-1040,D15-1103,0,0.017159,"ther than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity"
D19-1040,D13-1203,0,0.0254272,"l tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017"
D19-1040,D11-1072,0,0.187417,"Missing"
D19-1040,Q14-1037,0,0.0821294,"d representations. Recent work has sought to evaluate the knowledge acquired by pretrained language models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Conneau and Kiela, 2018; Wang et al., 2018; Liu et al., 2019a; Chen et al., 2019a, inter alia). In this work, we focus on evaluating their capabilities in modeling entities. Part of EntEval involves evaluating world knowledge about entities, relating them to fact 422 Logic was established as a discipline by Aristotle, who established its fundamental place in philosophy. efit the model (Durrett and Klein, 2014). Unlike other tasks, coreference typically involves longer context. To restrict the effect of broad context, we only keep two groups of coreference arcs from smaller context. One includes mentions that are in the same sentence (“same”) for examining the model capability of encoding local context. The other includes mentions that are in consecutive sentences (“next”) for the broader context. We create this task from the PreCo dataset (Chen et al., 2018), which has mentions annotated even when they are not part of coreference chains. We filter out instances in which both mentions are pronouns."
D19-1040,N16-1150,0,0.0452198,"Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings di"
D19-1040,2021.ccl-1.108,0,0.158509,"Missing"
D19-1040,P15-1001,0,0.010752,"defined by the ELMo parameters. In addition, we define the two bag-of-words reconstruction losses: X log q(xt |fELMo ([BOD]y1:Ty , 1, Ty )) lctx = − t ldesc = − X log q(yt |fELMo ([BOC]x1:Tx , i, j)) t where [BOD] and [BOC] are special symbols prepended to sentences to distinguish descriptions from contexts. The distribution q is parameterized by a linear layer that transforms the conditioning embedding into weights over the vocabulary. The final training loss is llang (x1:Tx ) + llang (y1:Ty ) + lctx + ldesc (1) Same as the original ELMo, each log loss is approximated with negative sampling (Jean et al., 2015). We write EntELMo to denote the model trained by Eq. (1). When using EntELMo for contextualized entity representations and descriptive entity representations, we use it analogously to ELMo. 5 5.1 5.2 Table 3 shows the performance of our models on the EntEval tasks. Our findings are detailed below: • Pretrained CWRs (ELMo, BERT) perform the best on EntEval overall, indicating that they capture knowledge about entities in contextual mentions or as entity descriptions. • BERT performs poorly on entity similarity and relatedness tasks. Since this task is zero-shot, it validates the recommended se"
D19-1040,P19-1598,0,0.059016,"atural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach"
D19-1040,D17-1195,0,0.0211293,"ng better entity representations by using natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better fo"
D19-1040,P19-1335,0,0.0538596,"database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webste"
D19-1040,P19-1066,0,0.0255656,"a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a)."
D19-1040,D17-1086,0,0.0276967,"h relation type to form our training set and 10 instances per type the for validation and test sets. We use Wikipedia descriptions for each entity in the pair whose relation we are predicting and we use descriptive entity representations for each entity with supervised training. 3.7 China_men&apos;s_nationa l_basketball_team Named Entity Disambiguation (NED) Named entity disambiguation is the task of linking a named-entity mention to its corresponding instance in a knowledge base such as Wikipedia. In this task, we consider CoNLL-YAGO (CoNLL; Hoffart et al., 2011) and Rare Entity Prediction (Rare; Long et al., 2017). For CoNLL-YAGO, following Hoffart et al. (2011) and Yamada et al. (2016), we used the 425 Practically, we encode the context using CER to be x1 , and encode each entity description using DER to be x2 , and pass [x1 , x2 , x1 x2 , |x1 − x2 |] to a linear model to predict whether it is the correct entity to fill in. The model is trained with cross entropy loss. 4 France won the match 4–2 to The France national football team claim their second World Cup represents France in international foot ball title. Figure 5: An example of hyperlinks in Wikipedia. “France” is linked to the Wikipedia page o"
D19-1040,P18-1148,0,0.0183762,"19; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that"
D19-1040,P19-2026,0,0.0153253,"al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and desc"
D19-1040,P19-1187,0,0.0237728,". Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given"
D19-1040,D17-1018,0,0.0176722,"Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al.,"
D19-1040,P16-1137,1,0.849658,"a Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of lit"
D19-1040,D18-1260,0,0.0138345,"nstitute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 201"
D19-1040,Q15-1023,0,0.0225531,"inh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather tha"
D19-1040,N19-1112,0,0.345972,"rsity, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this work, we propose EntEval: a test suite of diverse tasks tha"
D19-1040,W18-3026,0,0.0316874,"alized entity representations in this task. 3.5 Entity Similarity and Relatedness (ESR) Given two entities with their descriptions from Wikipedia, the task is to determine their similarity or relatedness. After the entity descriptions are encoded into vector representations, we compute their cosine similarity as predictions. We use the KORE (Hoffart et al., 2012) and Wik1. For each relationship, we replace an entity with similar negative entities based on cosine similarity of averaged GloVe embeddings (Pennington et al., 2014). 424 SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT. iSRS (Newman-Griffis et al., 2018) datasets in this task. Since the original datasets only provide entity names, we automatically add Wikipedia descriptions to each entity and manually ensure that every entity is matched to a Wikipedia description. We use Spearman’s rank correlation coefficient between our computed cosine similarity and the gold standard similarity/relatedness scores to measure the performance of entity representations. As KORE does not provide similarity scores of entity pairs, but simply ranks the candidate entities by their similarities to a target entity, we assign scores from 20 to 1 accordingly to each e"
D19-1040,N19-1087,0,0.0240893,"and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquire"
D19-1040,N19-1250,0,0.346171,"find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluat"
D19-1040,spitkovsky-chang-2012-cross,0,0.114517,"Missing"
D19-1040,D14-1162,0,0.0914542,"input representations to encode knowledge of entities based on the context. We use contextualized entity representations in this task. 3.5 Entity Similarity and Relatedness (ESR) Given two entities with their descriptions from Wikipedia, the task is to determine their similarity or relatedness. After the entity descriptions are encoded into vector representations, we compute their cosine similarity as predictions. We use the KORE (Hoffart et al., 2012) and Wik1. For each relationship, we replace an entity with similar negative entities based on cosine similarity of averaged GloVe embeddings (Pennington et al., 2014). 424 SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT. iSRS (Newman-Griffis et al., 2018) datasets in this task. Since the original datasets only provide entity names, we automatically add Wikipedia descriptions to each entity and manually ensure that every entity is matched to a Wikipedia description. We use Spearman’s rank correlation coefficient between our computed cosine similarity and the gold standard similarity/relatedness scores to measure the performance of entity representations. As KORE does not provide similarity scores of entity pairs, but simply ranks the candidate entiti"
D19-1040,N18-1202,0,0.685304,"ta Technological Institute at Chicago, IL, USA 4 Rutgers University, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this"
D19-1040,N19-1421,0,0.0158562,"available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015"
D19-1040,D18-1179,0,0.309682,"ta Technological Institute at Chicago, IL, USA 4 Rutgers University, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this"
D19-1040,N18-1074,0,0.0304412,"Missing"
D19-1040,P17-2052,0,0.0240077,"context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question a"
D19-1040,W14-2508,0,0.023988,"n in hyperlinks and improve ELMo-based CWRs on a variety of entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over enti"
D19-1040,P19-1487,0,0.0158614,"edings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et"
D19-1040,D19-1454,0,0.0212359,"ntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Ti"
D19-1040,W18-5446,0,0.0793014,"Missing"
D19-1040,D16-1159,0,0.0574611,"esolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a). In this work, we focus on evaluating their capabilities in modeling entities. 3 EntEval We are interested in two approaches: contextualized entity representations (henceforth: CER) and descriptive entity representations (henceforth: DER), both encoding fixed-length vector representations for entities. The contextualized entity representations encodes an entity based on the context it appears regardless of whether the entity is seen before. The motivation behind contextualized entity re"
D19-1040,P17-2067,0,0.0294836,"e ELMo-based CWRs on a variety of entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We giv"
D19-1040,Q18-1042,0,0.0199368,"2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kie"
D19-1040,N16-1114,0,0.0629429,"Missing"
D19-1040,D15-1083,0,0.059586,"Missing"
D19-1040,K16-1025,0,0.46592,"The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervised training to perform well on downstream tasks, while ELMo-based representations are more capable of performing zeroshot tasks. • In general, higher layers of ELMo and BERTbased CWRs are more transferable to EntEval tasks. To further improve contextuali"
D19-1040,Q17-1028,0,0.0839573,"Missing"
D19-1040,D18-1010,0,0.0233129,"entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Ent"
D19-1040,D18-1009,0,0.0197179,"Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 20"
D19-1040,P19-1472,0,0.0164685,"/github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al"
D19-1040,P19-1139,0,0.0279608,"essing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity r"
D19-1048,P93-1005,0,0.438063,"10 latent variable values causes them to capture the coarse-grained patterns we observe here. It is possible that more fine-grained differences would appear with a larger number of values. 6.2 7 Related Work Supervised Generative Models. Generative models have traditionally been used in supervised settings for many NLP tasks, including naive Bayes and other models for text classification (Maron, 1961; Yogatama et al., 2017), Markov models for sequence labeling (Church, 1988; Bikel et al., 1999; Brants, 2000; Zhou and Su, 2002), and probabilistic models for parsing (Magerrnan and Marcus, 1991; Black et al., 1993; Eisner, 1996; Collins, 1997; Dyer et al., 2016). Recent work in generative models for question answering (Lewis and Fan, 2019) learns to generate questions instead of directly penalizing prediction errors, which encourages the model to better understand the input data. Our work is directly inspired by that of Yogatama et al. (2017), who build RNN-based generative text classifiers and show scenarios where they can be empirically useful. Generation with Latent Variables Another advantage of generative models is that they can be used to generate data in order to better understand what they have"
D19-1048,N18-2116,1,0.897711,"Missing"
D19-1048,E17-1104,0,0.0274322,"e and the label. Instead of the soft mixture of discrete latent variable values that is used in classification (since we marginalize over the latent variable at test time), here we choose a single latent variable value when generating a textual sample. Text Classification. Traditionally, linear classifiers (McCallum and Nigam, 1998; Joachims, 1998; Fan et al., 2008) have been used for text classification. Recent work has scaled up text classification to larger datasets with models based on logistic regression (Joulin et al., 2017), convolutional neural networks (Kim, 2014; Zhang et al., 2015; Conneau et al., 2017), and recurrent neural networks (Xiao and Cho, 2016; Yogatama et al., 513 id description 1 future/progressive tenses 2 past/perfect tense 3 region names, locations 4 mixture 5 abbreviations 6 numbers, money-related 7 dates 8 country-oriented terms 9 mixure 10 symbols, links examples ... Commission is likely to follow opinion in the U.S. on the merger suit ... ... to increase computer software exports is beginning to show results ... A screensaver targeting spam-related websites appears to have been too successful . Universal has signed a handful of artists to a digital-only record label . .. N"
D19-1048,D14-1181,0,0.00313489,"s by setting the latent variable and the label. Instead of the soft mixture of discrete latent variable values that is used in classification (since we marginalize over the latent variable at test time), here we choose a single latent variable value when generating a textual sample. Text Classification. Traditionally, linear classifiers (McCallum and Nigam, 1998; Joachims, 1998; Fan et al., 2008) have been used for text classification. Recent work has scaled up text classification to larger datasets with models based on logistic regression (Joulin et al., 2017), convolutional neural networks (Kim, 2014; Zhang et al., 2015; Conneau et al., 2017), and recurrent neural networks (Xiao and Cho, 2016; Yogatama et al., 513 id description 1 future/progressive tenses 2 past/perfect tense 3 region names, locations 4 mixture 5 abbreviations 6 numbers, money-related 7 dates 8 country-oriented terms 9 mixure 10 symbols, links examples ... Commission is likely to follow opinion in the U.S. on the merger suit ... ... to increase computer software exports is beginning to show results ... A screensaver targeting spam-related websites appears to have been too successful . Universal has signed a handful of ar"
D19-1048,N19-1423,0,0.0642606,"Missing"
D19-1048,N16-1024,0,0.0223479,"the coarse-grained patterns we observe here. It is possible that more fine-grained differences would appear with a larger number of values. 6.2 7 Related Work Supervised Generative Models. Generative models have traditionally been used in supervised settings for many NLP tasks, including naive Bayes and other models for text classification (Maron, 1961; Yogatama et al., 2017), Markov models for sequence labeling (Church, 1988; Bikel et al., 1999; Brants, 2000; Zhou and Su, 2002), and probabilistic models for parsing (Magerrnan and Marcus, 1991; Black et al., 1993; Eisner, 1996; Collins, 1997; Dyer et al., 2016). Recent work in generative models for question answering (Lewis and Fan, 2019) learns to generate questions instead of directly penalizing prediction errors, which encourages the model to better understand the input data. Our work is directly inspired by that of Yogatama et al. (2017), who build RNN-based generative text classifiers and show scenarios where they can be empirically useful. Generation with Latent Variables Another advantage of generative models is that they can be used to generate data in order to better understand what they have learned, especially in seeking to understand lat"
D19-1048,C96-1058,0,0.507671,"alues causes them to capture the coarse-grained patterns we observe here. It is possible that more fine-grained differences would appear with a larger number of values. 6.2 7 Related Work Supervised Generative Models. Generative models have traditionally been used in supervised settings for many NLP tasks, including naive Bayes and other models for text classification (Maron, 1961; Yogatama et al., 2017), Markov models for sequence labeling (Church, 1988; Bikel et al., 1999; Brants, 2000; Zhou and Su, 2002), and probabilistic models for parsing (Magerrnan and Marcus, 1991; Black et al., 1993; Eisner, 1996; Collins, 1997; Dyer et al., 2016). Recent work in generative models for question answering (Lewis and Fan, 2019) learns to generate questions instead of directly penalizing prediction errors, which encourages the model to better understand the input data. Our work is directly inspired by that of Yogatama et al. (2017), who build RNN-based generative text classifiers and show scenarios where they can be empirically useful. Generation with Latent Variables Another advantage of generative models is that they can be used to generate data in order to better understand what they have learned, espe"
D19-1048,E91-1004,0,0.391351,"reland”. Our choice of only 10 latent variable values causes them to capture the coarse-grained patterns we observe here. It is possible that more fine-grained differences would appear with a larger number of values. 6.2 7 Related Work Supervised Generative Models. Generative models have traditionally been used in supervised settings for many NLP tasks, including naive Bayes and other models for text classification (Maron, 1961; Yogatama et al., 2017), Markov models for sequence labeling (Church, 1988; Bikel et al., 1999; Brants, 2000; Zhou and Su, 2002), and probabilistic models for parsing (Magerrnan and Marcus, 1991; Black et al., 1993; Eisner, 1996; Collins, 1997; Dyer et al., 2016). Recent work in generative models for question answering (Lewis and Fan, 2019) learns to generate questions instead of directly penalizing prediction errors, which encourages the model to better understand the input data. Our work is directly inspired by that of Yogatama et al. (2017), who build RNN-based generative text classifiers and show scenarios where they can be empirically useful. Generation with Latent Variables Another advantage of generative models is that they can be used to generate data in order to better under"
D19-1048,J93-2004,0,0.0721896,"Missing"
D19-1048,N16-1037,0,0.0210985,"” verdana , MS Sans Serif , arial , helvetica ” size = ” -2 ” color = ” # 666666 ” & gt ; & lt ; B & gt ; -washingtonpost.com & lt ; / B & gt ; & lt Table 6: Generated examples by controlling the latent variables and labels (world, sport, business, sci/tech) with our latent classifier trained on a small subset of the AGNews dataset. 2017), the latter of which is most closely-related to our models. et al., 2008). Recent work in neural networks has shown that introducing latent variables leads to higher representational capacity (Kingma and Welling, 2014; Chung et al., 2015; Burda et al., 2016; Ji et al., 2016). However, unlike variational autoencoders (Kingma and Ba, 2015) and related work that use continuous latent variables, Latent-variable Models. Latent variables have been widely used in both generative and discriminative models to learn rich structure from data (Petrov and Klein, 2007, 2008; Blunsom et al., 2008; Yu and Joachims, 2009; Morency 514 References our model is more similar to recent efforts that combine neural architectures with discrete latent variables and end-to-end training (Ji et al., 2016; Kim et al., 2017; Kong et al., 2017; Chen and Gimpel, 2018; Wiseman et al., 2018, inter"
D19-1048,E17-2068,0,0.0406336,"ent generative classifier to generate multiple samples by setting the latent variable and the label. Instead of the soft mixture of discrete latent variable values that is used in classification (since we marginalize over the latent variable at test time), here we choose a single latent variable value when generating a textual sample. Text Classification. Traditionally, linear classifiers (McCallum and Nigam, 1998; Joachims, 1998; Fan et al., 2008) have been used for text classification. Recent work has scaled up text classification to larger datasets with models based on logistic regression (Joulin et al., 2017), convolutional neural networks (Kim, 2014; Zhang et al., 2015; Conneau et al., 2017), and recurrent neural networks (Xiao and Cho, 2016; Yogatama et al., 513 id description 1 future/progressive tenses 2 past/perfect tense 3 region names, locations 4 mixture 5 abbreviations 6 numbers, money-related 7 dates 8 country-oriented terms 9 mixure 10 symbols, links examples ... Commission is likely to follow opinion in the U.S. on the merger suit ... ... to increase computer software exports is beginning to show results ... A screensaver targeting spam-related websites appears to have been too success"
D19-1048,C08-1106,0,0.10981,"Missing"
D19-1048,D14-1162,0,0.0820202,"Missing"
D19-1048,N07-1051,0,0.0901259,"Missing"
D19-1048,D18-1356,0,0.062464,"Missing"
D19-1048,P02-1060,0,0.0442024,"nted terms like “Indian”, “Russian”, “North Korea”, and “Ireland”. Our choice of only 10 latent variable values causes them to capture the coarse-grained patterns we observe here. It is possible that more fine-grained differences would appear with a larger number of values. 6.2 7 Related Work Supervised Generative Models. Generative models have traditionally been used in supervised settings for many NLP tasks, including naive Bayes and other models for text classification (Maron, 1961; Yogatama et al., 2017), Markov models for sequence labeling (Church, 1988; Bikel et al., 1999; Brants, 2000; Zhou and Su, 2002), and probabilistic models for parsing (Magerrnan and Marcus, 1991; Black et al., 1993; Eisner, 1996; Collins, 1997; Dyer et al., 2016). Recent work in generative models for question answering (Lewis and Fan, 2019) learns to generate questions instead of directly penalizing prediction errors, which encourages the model to better understand the input data. Our work is directly inspired by that of Yogatama et al. (2017), who build RNN-based generative text classifiers and show scenarios where they can be empirically useful. Generation with Latent Variables Another advantage of generative models"
D19-1060,S14-2010,0,0.0454692,"Missing"
D19-1060,S16-1081,0,0.0590142,"Missing"
D19-1060,S12-1051,0,0.0283078,"Additionally, we benchmark several popular pretrained sentence encoders on DiscoEval, including Skip-thought,4 InferSent (Conneau et al., 2017),5 DisSent (Nie et al., 2019),6 ELMo,7 and BERT.8 For ELMo, we use the averaged vector of all three layers and time steps as the sentence representations. For BERT, we use the averaged vector at the position of the “[CLS]” token across all layers. We also evaluate per-layer performance for both models in Section 6. When reporting results for SentEval, we compute the averaged Pearson correlations for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); ques"
D19-1060,P18-1198,0,0.0418821,"Missing"
D19-1060,J08-1001,0,0.105695,"e and we propose novel training signals based on document structure, including sentence position and section titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in"
D19-1060,I17-1001,0,0.0835728,"Missing"
D19-1060,D18-1465,0,0.0191306,"ed to their respective prior work, demonstrating the effectiveness of incorporating losses that make use of broader context. Through per-layer analysis, we also find that for both BERT and ELMo, deep layers consistently outperform shallower ones on DiscoEval, showing different trends from SentEval where the shallow layers have the best performance. et al., 2019, inter alia), much of it based on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpur"
D19-1060,N18-1016,0,0.0564081,"Missing"
D19-1060,N19-1423,0,0.0571218,"everal researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next sentences in the discourse context. Since Skip-thought performs well in downstream evaluation tasks, we use this neighboring-sentence objective as a starting point for our models. There is also work on incorporating discourse related objectives into the training of sentence representations. Jernite et al. (2017) propose binary sentenc"
D19-1060,W01-1605,0,0.770088,"discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the natural structure in structured document collections like Wikipedia. Empirically, we benchmark our models and several popular sentence encoders on DiscoEval and SentEval (Conneau and Kiela, 2018). We find that our proposed training objectives help the models capture different characteristics in the sentence represe"
D19-1060,C04-1051,0,0.0340211,"semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment results over all SentEval and DiscoEval tasks. Different models and training signals have complex effects when performing various downstream tasks. We summarize our findings below: • On DiscoEval, Skip-thought performs best on RST-DT. DisSent performs strongly for PDTB tasks but it requires discourse markers from PDTB for generating training data. BERT has the highest average by a large margin, but ELMo has competitive performance on multiple tasks. • The NL or SPP loss al"
D19-1060,S17-2001,0,0.0239801,"averaged vector of all three layers and time steps as the sentence representations. For BERT, we use the averaged vector at the position of the “[CLS]” token across all layers. We also evaluate per-layer performance for both models in Section 6. When reporting results for SentEval, we compute the averaged Pearson correlations for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment resu"
D19-1060,P08-1095,0,0.096885,"make use of broader context. Through per-layer analysis, we also find that for both BERT and ELMo, deep layers consistently outperform shallower ones on DiscoEval, showing different trends from SentEval where the shallow layers have the best performance. et al., 2019, inter alia), much of it based on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the"
D19-1060,D19-1040,1,0.768381,"r 3–7, 2019. 2019 Association for Computational Linguistics and Smith, 2017), and text generation (Bosselut et al., 2018). In this paper, we propose DiscoEval, a task suite designed to evaluate discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the natural structure in structured document collections like Wikipedia. Empirically, we benchmark our models and several p"
D19-1060,J10-3004,0,0.0844843,"Missing"
D19-1060,P19-1062,0,0.020742,"solute difference: [x1 , x2 , x1 x2 , |x1 − x2 |]. Discourse Relations As the most direct way to probe discourse knowledge, we consider the task of predicting annotated discourse relations among sentences. We use two human-annotated datasets: the RST Discourse Treebank (RST-DT; Carlson et al., 2001) and the Penn Discourse Treebank (PDTB; Prasad et al., 2008). They have different labeling schemes. PDTB provides discourse markers for adjacent sentences, whereas RST-DT offers document-level discourse trees, which recently was used to evaluate discourse knowledge encoded in document-level models (Ferracane et al., 2019). The difference allows us to see if the pretrained representations capture local or global information about discourse structure. More specifically, as shown in Figure 1, in RSTDT, text is segmented into basic units, elementary discourse units (EDUs), upon which a discourse tree is built recursively. Although a relation can take multiple units, we follow prior work (Ji and Eisenstein, 2014) to use right-branching trees for non-binary relations to binarize the tree structure and use the 18 coarse-grained relations defined by Carlson et al. (2001). When evaluating pretrained sentence encoders o"
D19-1060,L18-1269,0,0.103396,"tructure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the representation of a stand-alone sentence, such as its semantic roles, rather than the broader context in which it is situated. ∗ Equal contribution. Listed in alphabetical order. Data processing and evaluation scripts are available at https://github.com/ZeweiChu/DiscoEval. 1 649 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language"
D19-1060,D17-1254,0,0.043181,"sed on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next sentences in the discourse context. Since Skip-thought performs well in downstream evaluation tasks, we use this neighboring-sentence objective as a starting point for our models. There is also work on incorporating discourse related objectives into the training of s"
D19-1060,D17-1070,0,0.0310771,"36.6 37.3 36.5 38.0 37.7 37.7 37.8 39.3 37.9 36.9 38.2 57.0 56.2 57.1 56.4 56.7 57.4 56.2 55.8 54.5 53.7 54.6 54.4 55.1 54.2 54.1 54.1 Table 2: Results for SentEval and DiscoEval. The highest number in each column is boldfaced. The highest number for our models in each column is underlined. “All” uses all four losses. “avg.” is the averaged accuracy for all tasks in DiscoEval. probing tasks (Conneau et al., 2018), we report the average accuracy and report it as “Probing”. Additionally, we benchmark several popular pretrained sentence encoders on DiscoEval, including Skip-thought,4 InferSent (Conneau et al., 2017),5 DisSent (Nie et al., 2019),6 ELMo,7 and BERT.8 For ELMo, we use the averaged vector of all three layers and time steps as the sentence representations. For BERT, we use the averaged vector at the position of the “[CLS]” token across all layers. We also evaluate per-layer performance for both models in Section 6. When reporting results for SentEval, we compute the averaged Pearson correlations for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require"
D19-1060,D14-1168,0,0.0192421,"ties. Deeper discourse structures use more complex relations among sentences (e.g., tree-structured; see Figure 1). Theoretically, discourse structures have been approached through Centering Theory (Grosz et al., 1995) for studying distributions of entities across text and Rhetorical Structure Theory (RST; Mann and Thompson, 1988) for modelling the logical structure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the representation of a stand-alone sentence, such as its semantic rol"
D19-1060,W13-3214,0,0.0326406,"on document structure, including sentence position and section titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in later sections. We also experimented with adding h"
D19-1060,J95-2003,0,0.892439,"Missing"
D19-1060,P14-1065,0,0.0733621,"Missing"
D19-1060,N18-1149,0,0.0676301,"Missing"
D19-1060,N16-1162,0,0.0848488,"Missing"
D19-1060,P14-1002,0,0.0255845,"hemes. PDTB provides discourse markers for adjacent sentences, whereas RST-DT offers document-level discourse trees, which recently was used to evaluate discourse knowledge encoded in document-level models (Ferracane et al., 2019). The difference allows us to see if the pretrained representations capture local or global information about discourse structure. More specifically, as shown in Figure 1, in RSTDT, text is segmented into basic units, elementary discourse units (EDUs), upon which a discourse tree is built recursively. Although a relation can take multiple units, we follow prior work (Ji and Eisenstein, 2014) to use right-branching trees for non-binary relations to binarize the tree structure and use the 18 coarse-grained relations defined by Carlson et al. (2001). When evaluating pretrained sentence encoders on RST-DT, we first encode EDUs into vectors, then use averaged vectors of EDUs of subtrees as the representation of the subtrees. The target prediction is the label of nodes in discourse trees and the input to the classifier is [xleft , xright , xleft xright , |xleft − xright |], where xleft and xright are vec651 1. These functions include fast and synchronized response to environmental chan"
D19-1060,Q15-1024,0,0.0168708,"sentence position and section titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in later sections. We also experimented with adding hidden layers to the Disco"
D19-1060,D17-1019,0,0.0288234,"ction titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in later sections. We also experimented with adding hidden layers to the DiscoEval classification mod"
D19-1060,P17-1092,0,0.0480299,"Missing"
D19-1060,C02-1150,0,0.0868934,"016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment results over all SentEval and DiscoEval tasks. Different models and training signals have complex effects when performing various downstream tasks. We summarize our findings below: • On DiscoEval, Skip-thought performs best on RST-DT. DisSent performs strongly for PDTB tasks but it requires discourse markers from PDTB for generating training data. BERT has the highest average by a large margin, but ELMo has competitive perform"
D19-1060,P19-1410,0,0.070228,"Missing"
D19-1060,N18-1164,0,0.0131186,"both BERT and ELMo, deep layers consistently outperform shallower ones on DiscoEval, showing different trends from SentEval where the shallow layers have the best performance. et al., 2019, inter alia), much of it based on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next sentences in the discourse context. Since Skip-thoug"
D19-1060,D09-1036,0,0.1581,"Missing"
D19-1060,P18-1040,0,0.0296178,"human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in later sections. We also experimented with adding hidden layers to the DiscoEval classification models. However, we find simpler linear c"
D19-1060,I17-1062,0,0.0133481,"sis, we also find that for both BERT and ELMo, deep layers consistently outperform shallower ones on DiscoEval, showing different trends from SentEval where the shallow layers have the best performance. et al., 2019, inter alia), much of it based on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next sentences in the discourse conte"
D19-1060,N19-1112,0,0.122654,"49–662, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Smith, 2017), and text generation (Bosselut et al., 2018). In this paper, we propose DiscoEval, a task suite designed to evaluate discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the natural structure in structured document collections like Wikipedia. Empirically, w"
D19-1060,N16-1098,0,0.0330411,"we call Sentence Position. It can be seen as way to probe the knowledge of linearly-structured discourse, where the ordering corresponds to the timings of events. When constructing this dataset, we take five consecutive sentences from a corpus, randomly move one of these five sentences to the first position, and ask models to predict the true position of the first sentence in the modified sequence. We create three versions of this task, one for each of the following three domains: the first five sentences of the introduction section of a Wikipedia article (Wiki), the ROC Stories corpus (ROC; Mostafazadeh et al., 2016), and the first 5 sentences in the abstracts of arXiv papers (arXiv; Chen et al., 2016). Figure 4 shows an example of this task for the ROC Stories domain. The first sentence should be in the fourth position among these sentences. To make correct predictions, the model needs to be aware of both typical orderings of events as well as how events are described in language. In the example shown, Bonnie’s excitement comes from her imagination so it must happen after she picked up the jeans and tried them on but right before she realized the actual size. To train classifiers for these tasks, we do t"
D19-1060,Q18-1005,0,0.0203683,"here the structure is manifested in the timing of introducing entities. Deeper discourse structures use more complex relations among sentences (e.g., tree-structured; see Figure 1). Theoretically, discourse structures have been approached through Centering Theory (Grosz et al., 1995) for studying distributions of entities across text and Rhetorical Structure Theory (RST; Mann and Thompson, 1988) for modelling the logical structure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the"
D19-1060,P15-1121,0,0.0222218,"erstood as sentence ordering, where the structure is manifested in the timing of introducing entities. Deeper discourse structures use more complex relations among sentences (e.g., tree-structured; see Figure 1). Theoretically, discourse structures have been approached through Centering Theory (Grosz et al., 1995) for studying distributions of entities across text and Rhetorical Structure Theory (RST; Mann and Thompson, 1988) for modelling the logical structure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understandin"
D19-1060,2021.ccl-1.108,0,0.0890067,"Missing"
D19-1060,P19-1442,0,0.118426,"tence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next sentences in the discourse context. Since Skip-thought performs well in downstream evaluation tasks, we use this neighboring-sentence objective as a starting point for our models. There is also work on incorporating discourse related objectives into the training of sentence representations. Jernite et al. (2017) propose binary sentence ordering, conjunction prediction (requiring manually-defined conjunction groups), and next sentence prediction. Similarly, Sileo et al. (2019) and Nie et al. (2019) create training datasets automatically based on discourse relations provided in the Penn Discourse Treebank (PDTB; Lin et al., 2009). Our work differs from prior work in that we propose a general-purpose pretrained sentence embedding evaluation suite that covers multiple aspects of discourse knowledge and we propose novel training signals based on document structure, including sentence position and section titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to"
D19-1060,W15-4640,0,0.0301519,"ugh per-layer analysis, we also find that for both BERT and ELMo, deep layers consistently outperform shallower ones on DiscoEval, showing different trends from SentEval where the shallow layers have the best performance. et al., 2019, inter alia), much of it based on recovering linguistic annotations of discourse structure. Several researchers have defined tasks related to discourse structure, including sentence ordering (Chen et al., 2016; Logeswaran et al., 2016; Cui et al., 2018), sentence clustering (Wang et al., 2018b), and disentangling textual threads (Elsner and Charniak, 2008, 2010; Lowe et al., 2015; Mehri and Carenini, 2017; Jiang et al., 2018; Kummerfeld et al., 2019). There is a great deal of prior work on pretrained representations (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016; Wieting et al., 2016; McCann et al., 2017; Gan et al., 2017; Peters et al., 2018a; Logeswaran and Lee, 2018; Devlin et al., 2019; Tang and de Sa, 2019; Yang et al., 2019; Liu et al., 2019b, inter alia). Skip-thought vectors form an effective architecture for generalpurpose sentence embeddings. The model encodes a sentence to a vector representation, and then predicts the previous and next senten"
D19-1060,P18-1091,0,0.028527,"manifested in the timing of introducing entities. Deeper discourse structures use more complex relations among sentences (e.g., tree-structured; see Figure 1). Theoretically, discourse structures have been approached through Centering Theory (Grosz et al., 1995) for studying distributions of entities across text and Rhetorical Structure Theory (RST; Mann and Thompson, 1988) for modelling the logical structure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the representation of a"
D19-1060,S14-2001,0,0.0349165,"e representations. For BERT, we use the averaged vector at the position of the “[CLS]” token across all layers. We also evaluate per-layer performance for both models in Section 6. When reporting results for SentEval, we compute the averaged Pearson correlations for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment results over all SentEval and DiscoEval tasks. Different models and train"
D19-1060,P04-1035,0,0.012179,"for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment results over all SentEval and DiscoEval tasks. Different models and training signals have complex effects when performing various downstream tasks. We summarize our findings below: • On DiscoEval, Skip-thought performs best on RST-DT. DisSent performs strongly for PDTB tasks but it requires discourse markers from PDTB for generating"
D19-1060,D13-1170,0,0.0144604,"Missing"
D19-1060,P05-1015,0,0.0249347,"performance for both models in Section 6. When reporting results for SentEval, we compute the averaged Pearson correlations for Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We refer to the average as unsupervised semantic similarity (USS) since those tasks do not require training data. We compute the averaged results for the STS Benchmark (Cer et al., 2017), textual entailment, and semantic relatedness (Marelli et al., 2014) and refer to the average as supervised semantic similarity (SSS). We compute the average accuracy for movie review (Pang and Lee, 2005); customer review (Hu and Liu, 2004); opinion polarity (Wiebe et al., 2005); subjectivity classification (Pang and Lee, 2004); Stanford sentiment treebank (Socher et al., 2013); question classification (Li and Roth, 2002); and paraphrase detection (Dolan et al., 2004), and refer to it as sentence classification (SC). For the rest of the linguistic 5.2 Results Table 2 shows the experiment results over all SentEval and DiscoEval tasks. Different models and training signals have complex effects when performing various downstream tasks. We summarize our findings below: • On DiscoEval, Skip-thought"
D19-1060,P19-1397,0,0.0233119,"Missing"
D19-1060,N18-1202,0,0.401796,"ing and the 9th International Joint Conference on Natural Language Processing, pages 649–662, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Smith, 2017), and text generation (Bosselut et al., 2018). In this paper, we propose DiscoEval, a task suite designed to evaluate discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the"
D19-1060,D18-1179,0,0.136509,"ing and the 9th International Joint Conference on Natural Language Processing, pages 649–662, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Smith, 2017), and text generation (Bosselut et al., 2018). In this paper, we propose DiscoEval, a task suite designed to evaluate discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the"
D19-1060,P16-1027,0,0.018755,"studying distributions of entities across text and Rhetorical Structure Theory (RST; Mann and Thompson, 1988) for modelling the logical structure of natural language via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the representation of a stand-alone sentence, such as its semantic roles, rather than the broader context in which it is situated. ∗ Equal contribution. Listed in alphabetical order. Data processing and evaluation scripts are available at https://github.com/ZeweiChu/DiscoEval. 1 649 Proceedings of"
D19-1060,W18-5441,0,0.0535862,"Missing"
D19-1060,W18-5446,0,0.0937236,"Missing"
D19-1060,prasad-etal-2008-penn,0,0.610029,"ledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distributional semantics of text. These objectives depend only on the natural structure in structured document collections like Wikipedia. Empirically, we benchmark our models and several popular sentence encoders on DiscoEval and SentEval (Conneau and Kiela, 2018). We find that our proposed training objectives help the models capture different characteristics in the sentence representations. Additionall"
D19-1060,D18-1175,0,0.169664,"age via discourse trees. Researchers have found modelling discourse useful in a range of tasks (Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Liu and Lapata, 2018; Pan et al., 2018), including summarization (Gerani et al., 2014), text classification (Ji Introduction Pretrained sentence representations have been found useful in various downstream tasks such as visual question answering (Tapaswi et al., 2016), script inference (Pichotta and Mooney, 2016), and information retrieval (Le and Mikolov, 2014; Palangi et al., 2016). Benchmark datasets (Adi et al., 2017; Conneau and Kiela, 2018; Wang et al., 2018a, 2019) have been proposed to evaluate the encoded knowledge, where the focus has been primarily on natural language understanding capabilities of the representation of a stand-alone sentence, such as its semantic roles, rather than the broader context in which it is situated. ∗ Equal contribution. Listed in alphabetical order. Data processing and evaluation scripts are available at https://github.com/ZeweiChu/DiscoEval. 1 649 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages"
D19-1060,D16-1159,0,0.0175693,"Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 649–662, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Smith, 2017), and text generation (Bosselut et al., 2018). In this paper, we propose DiscoEval, a task suite designed to evaluate discourse-related knowledge in pretrained sentence representations. DiscoEval comprises 7 task groups covering multiple domains, including Wikipedia, stories, dialogues, and scientific literature. The tasks are probing tasks (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Poliak et al., 2018; Tenney et al., 2019; Liu et al., 2019a; Ettinger, 2019; Chen et al., 2019, inter alia) based on sentence ordering, annotated discourse relations, and discourse coherence. The data is either generated semi-automatically or based on human annotations (Carlson et al., 2001; Prasad et al., 2008; Lin et al., 2009; Kummerfeld et al., 2019). We also propose a set of novel multi-task learning objectives building upon standard pretrained sentence encoders, which rely on the assumption of distribu"
D19-1060,D18-1116,0,0.0574171,"Missing"
D19-1060,N19-1351,0,0.0398907,"Missing"
D19-1060,C10-2172,0,0.0356198,"ning signals based on document structure, including sentence position and section titles, without requiring additional human annotation. 2 We propose DiscoEval, a test suite of 7 tasks to evaluate whether sentence representations include semantic information relevant to discourse processing. Below we describe the tasks and datasets, as well as the evaluation framework. We closely follow the SentEval sentence embedding evaluation suite, in particular its supervised sentence and 3 Related Work Discourse modelling and discourse parsing have a rich history (Marcu, 2000; Barzilay and Lapata, 2008; Zhou et al., 2010; Kalchbrenner and Blunsom, 2013; Ji and Eisenstein, 2015; Li and Jurafsky, 2017; Wang et al., 2018c; Liu et al., 2018; Lin 650 Discourse Evaluation 1. In any case, the brokerage firms are clearly moving faster to create new ads than they did in the fall of 1987. 2. [But] it remains to be seen whether their ads will be any more effective. label: Comparison.Contrast sentence pair classification tasks, which use predefined neural architectures with slots for fixeddimensional sentence embeddings. All DiscoEval tasks are modelled by logistic regression unless otherwise stated in later sections. We"
D19-5605,P98-1013,0,0.218741,"han one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for generating continuations. Frame Semantics. A story is composed of a sequence of meaningful events (Chatman, 1980), often following particular patterns described in various terms such as scripts (Schank and Abelson, 1977) and frames. FrameNet (Baker et al., 1998) is an inventory of semantic frames, which are semantic abstractions describing universal categories of events, concepts, and relationships. We consider frame semantics as another control attribute in our framework. In order to get a frame semantic representation for a continuation, we use SEMAFOR (Das et al., 2014). SEMAFOR automatically produces a frame-semantic parse for a sentence, which consists of spans that evoke particular frames in FrameNet as well as annotations Table 2: Generated continuations from our framework with different control attribute values. Boldface indicates attribute v"
D19-5605,D17-1168,0,0.0183918,"nteresting story endings, albeit without control variables. In stronger relevance to our work, Clark et al. (2018b) explore a creative writing setting with a machine in the loop, albeit with mixed results in terms of the quality of system suggestions. Predicting and controlling with frame values suggests a new way of interacting with collaborative writing systems, as long as frames can be communicated to users in ways they can easily understand. Recently, Clark et al. (2018a) proposed a neural text generation method that explicitly represents and tracks entities. In addition, event sequences (Chaturvedi et al., 2017; Liu et al., 2018) are important elements in narrative texts but under-explored for story generation. 5 The BS and TS baselines do not use control variables. We remove results from 10-question sets where more than half of the questions were answered with the “neither” option, as we were concerned that these annotators did not fully understand the task or did not spend enough time studying the continuations. This occurred in roughly one third of question sets. 6 51 References These and related characteristics of creative writing could be incorporated into our framework as control attributes in"
D19-5605,N18-1204,0,0.160133,"pare the diversity of story continuations controlled by different sentence attributes and find 44 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 44–58 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d where vi is the vector representation of word xi , si ∈ Rd is the hidden state at time i, and fe1 and fe2 are the forward and backward RNN functions. frames to generate continuations. One potential use case of controllable, diverse story generation is collaborative writing applications (Clark et al., 2018b). We conduct a human evaluation to assess the utility of providing multiple suggestions from our models in this setting, demonstrating promising results for the potential of controllable generation for collaborative writing. 2 Decoder. Our decoder uses an RNN with the general global attention scheme from Luong et al. (2015). An additional input zdec is fed to the decoder at each time step to reflect the characteristics of the control variable: Task Description and Definitions hj = fd ([yj−1 ; zdec ], hj−1 ) Given a story context and a control attribute value, our goal is to generate a story"
D19-5605,W18-2501,0,0.0134655,"ted summaries with a desired length (Kikuchi et al., 2016; Fan et al., 2018a). We similarly use length of the continuation as a control attribute. Instead of using an embedding for each integer length value, we group the lengths into a small number of bins (details are provided below). zenc and zdec are fixed one-hot vectors for each bin. Verbal Predicates. Semantic role labeling (SRL) is a form of shallow semantic parsing that annotates predicates and their arguments in sentences. We consider predicates from a semantic role labeling as control attributes. We use the SRL system from AllenNLP (Gardner et al., 2018) to automatically obtain predicates for the continuations in our training set. Then, a predicate vector is obtained by first summing up 100-dimensional GloVe embeddings (Pennington et al., 2014) of the predicted predicates (if there is more than one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for g"
D19-5605,W04-1013,0,0.0194067,"frames. For example, in the sentence “Roa’s advice made Emma a lot happier in her life!”, “a lot” evokes the Quantity frame while “Emma a lot happier” evokes the Effect frame. The frame set variable z is computed by summing embeddings for the frames in the set: X z = R(l) = Rj (2) “continuation”) given the previous four sentences. We use the 10k most frequent words in the training set as our vocabulary. A special token hunk i is introduced for unknown words. 5.2 Previous work evaluates generation tasks with automatic metrics, such as perplexity (PPL), BLEU (Papineni et al., 2002),3 and ROUGE (Lin, 2004). We adopt these in our evaluation and add three more metrics using the pretrained story scorer from Sagarkar et al. (2018). The scorer rates a generated continuation given its context along three dimensions: relevance (R), interestingness (I), and overall quality (O). The story scorer does not use a gold standard continuation. In addition, to evaluate the diversity of the generation, we use Max-BLEU4 and Max-ROUGE. First, we compute BLEU and ROUGE scores over a set of outputs (y1 , y2 , ..., yn ) with different attribute values given the same story context, then we compute the max scores: j∈l"
D19-5605,P02-1040,0,0.107184,"Missing"
D19-5605,D14-1162,0,0.0823943,"er length value, we group the lengths into a small number of bins (details are provided below). zenc and zdec are fixed one-hot vectors for each bin. Verbal Predicates. Semantic role labeling (SRL) is a form of shallow semantic parsing that annotates predicates and their arguments in sentences. We consider predicates from a semantic role labeling as control attributes. We use the SRL system from AllenNLP (Gardner et al., 2018) to automatically obtain predicates for the continuations in our training set. Then, a predicate vector is obtained by first summing up 100-dimensional GloVe embeddings (Pennington et al., 2014) of the predicted predicates (if there is more than one), then reducing the dimension to 64 using principal component analysis.1 We wish to clarify that we do not use the argument structure from the SRL system. We restrict our focus to simply the set of verbal predicates in the SRL structure; this would presumably be simpler to use in interactive settings where users would specify attribute values for generating continuations. Frame Semantics. A story is composed of a sequence of meaningful events (Chatman, 1980), often following particular patterns described in various terms such as scripts ("
D19-5605,D17-1228,0,0.159581,"Missing"
D19-5605,N12-1059,0,\N,Missing
D19-5605,D15-1166,0,\N,Missing
D19-5605,P10-1158,0,\N,Missing
D19-5605,C98-1013,0,\N,Missing
D19-5605,J14-1002,0,\N,Missing
D19-5605,D13-1170,0,\N,Missing
D19-5605,D13-1111,1,\N,Missing
D19-5605,N16-1098,0,\N,Missing
D19-5605,S18-2024,1,\N,Missing
D19-5605,P19-1254,0,\N,Missing
D19-5605,W18-2706,0,\N,Missing
D19-5605,W16-0202,0,\N,Missing
D19-5605,N16-1014,0,\N,Missing
D19-5614,K18-1031,0,0.0242861,"a combination of all three unsupervised metrics. Stopping criteria are rarely discussed in prior work on textual transfer. 3.2 Fluency (“PP”): Transferred sentences can exhibit high Acc and Sim while still being ungrammatical. So we add a third unsupervised metric to target fluency. We compute perplexity (“PP”) of the transferred corpus, using a language model pretrained on the concatenation of X0 and X1 . We note that perplexity is distinct from fluency. However, certain measures based on perplexity have been shown to correlate with sentence-level human fluency judgments (Gamon et al., 2005; Kann et al., 2018). Furthermore, as discussed in Section 3.3, we punish abnormally small perplexities, as transferred texts with such perplexities typically consist entirely of words and phrases that do not result in meaningful sentences. Our summary metric, described in Section 3.3, can be tailored by practitioners for various datasets and tasks which may require more or less weight on semantic preservation. Unsupervised Evaluation Metrics We now describe our proposals. We validate the metrics with human judgments in Section 6.3. Post-transfer classification accuracy (“Acc”): This metric was mentioned above. W"
D19-5614,D14-1181,0,0.00907797,"Missing"
D19-5614,D17-1230,0,0.0248515,"desired content and style are conditioning contexts. Li et al. (2018) used a feature-based approach that deletes characteristic words from the original sentence, retrieves similar sentences in the target corpus, and generates based on the original sentence and the characteristic words from the retrieved sentences. Xu et al. (2018) integrated reinforcement learning into the textual transfer problem. Another way to address the lack of parallel data is to use learning frameworks based on adversarial objectives (Goodfellow et al., 2014); several have done so for textual transfer (Yu et al., 2017; Li et al., 2017; Yang et al., 2018a; Shen et al., 2017; Fu et al., 2018). Recent work uses target-domain language models as discriminators to provide more stable feedback in learning (Yang et al., 2018b). To preserve semantics more explicitly, Fu et al. (2018) use a multi-decoder model to learn content representations that do not reflect styles. Shetty et al. (2017) use a cycle constraint that penalizes L1 distance between input and round-trip transfer reconstruction. Our cycle consistency loss is inspired by Shetty et al. (2017), together with the idea of back translation in unsupervised neural machine tran"
D19-5614,N18-1169,0,0.115453,"Missing"
D19-5614,W14-3348,0,0.0445363,"df(q) = log(|C |· |{s ∈ C : q ∈ s}|−1 ) (q is a word, s is a sentence, C = X0 ∪ X1 ). We use 300-dimensional GloVe word embeddings (Pennington et al., 2014). Then, Sim is the average of the cosine similarities over all original/transferred sentence pairs. Though this metric is quite simple, we show empirically that it is effective in capturing semantic similarity. Simplicity in evaluation metrics is beneficial for computational efficiency and widespread adoption. The quality of transfer evaluations will be significantly boosted with even such a simple metric. We also experimented with METEOR (Denkowski and Lavie, 2014). However, given that we found it to be strongly correlated with Sim (shown in supplemental materials), we adopt Sim due to its computational efficiency and simplicity. Different textual transfer tasks may require different degrees of semantic preservation. Our summary metric, described in Section 3.3, can be tailored by practitioners for various datasets and tasks which may require more or less weight on semantic preservation. #ep Acc Sim Sentence original input the host that walked us to the table and left without a word . 0.5 0.87 0.65 the food is the best and the food is the . 3.3 0.72 0.7"
D19-5614,W17-4912,0,0.0170296,"ic preservation. To demonstrate the effectiveness of our metrics, we experiment with textual transfer models discussed above, using both their Yelp polarity dataset and a new literature dataset that we propose. Across model variants, our metrics correlate well with human judgments, at both the sentencelevel and system-level. 2 Textual Transfer Models In terms of generating the transferred sentences, to address the lack of parallel data, Hu et al. (2017) used variational autoencoders to generate content representations devoid of style, which can be converted to sentences with a specific style. Ficler and Goldberg (2017) used conditional language models to generate sentences where the desired content and style are conditioning contexts. Li et al. (2018) used a feature-based approach that deletes characteristic words from the original sentence, retrieves similar sentences in the target corpus, and generates based on the original sentence and the characteristic words from the retrieved sentences. Xu et al. (2018) integrated reinforcement learning into the textual transfer problem. Another way to address the lack of parallel data is to use learning frameworks based on adversarial objectives (Goodfellow et al., 2"
D19-5614,P14-5010,0,0.00244063,"We consider two corpora of literature. The first corpus contains works of Charles Dickens collected from Project Gutenberg. The second corpus is comprised of modern literature from the Toronto Books Corpus (Zhu et al., 2015). Sentences longer than 25 words are removed. Unlike the Yelp dataset, the two corpora have very different vocabularies. This dataset poses challenges for the textual transfer task, and it provides diverse data for assessing quality of our evaluation system. Given the different and sizable vocabulary, we preprocess by using the named entity recognizer in Stanford CoreNLP (Manning et al., 2014) to replace names and locations with PERSON - and - LOCATION - tags, respectively. We also use byte-pair encoding (BPE), commonly used in generation tasks (Sennrich et al., 2016b). We only use sentences with lengths between 6 and 25. The resulting dataset has 156K, 5K, 5K Dickens training, development, and testing sentences, respectively, and 165K/5K/5K modern literature sentences. 5.2 Shen et al. (2017) M0+para M0+cyc M0+cyc+lang M0+cyc+para M0+cyc+para+lang M0+cyc+2d M6+para+lang Acc Sim PP GM 0.818 0.819 0.813 0.807 0.798 0.804 0.805 0.818 0.719 0.734 0.770 0.796 0.783 0.785 0.817 0.805 37."
D19-5614,2005.eamt-1.15,0,0.075528,"e much better under a combination of all three unsupervised metrics. Stopping criteria are rarely discussed in prior work on textual transfer. 3.2 Fluency (“PP”): Transferred sentences can exhibit high Acc and Sim while still being ungrammatical. So we add a third unsupervised metric to target fluency. We compute perplexity (“PP”) of the transferred corpus, using a language model pretrained on the concatenation of X0 and X1 . We note that perplexity is distinct from fluency. However, certain measures based on perplexity have been shown to correlate with sentence-level human fluency judgments (Gamon et al., 2005; Kann et al., 2018). Furthermore, as discussed in Section 3.3, we punish abnormally small perplexities, as transferred texts with such perplexities typically consist entirely of words and phrases that do not result in meaningful sentences. Our summary metric, described in Section 3.3, can be tailored by practitioners for various datasets and tasks which may require more or less weight on semantic preservation. Unsupervised Evaluation Metrics We now describe our proposals. We validate the metrics with human judgments in Section 6.3. Post-transfer classification accuracy (“Acc”): This metric wa"
D19-5614,N19-1049,0,0.0499402,"therefore unavailable for new textual transfer tasks. There is a great deal of recent work in textual transfer (Yang et al., 2018b; Santos et al., 2018; Zhang et al., 2018; Logeswaran et al., 2018; Nikolov and Hahnloser, 2018), but all either lack certain categories of unsupervised metric or lack human validation of them, which we contribute. Moreover, the textual transfer community lacks discussion of early stopping criteria and methods of holistic model comparison. We propose a one-number summary for transfer quality, which can be used to select and compare models. In contemporaneous work, Mir et al. (2019) similarly proposed three types of metrics for style transfer tasks. There are two main differences compared to our work: (1) They use a stylekeyword masking procedure before evaluating semantic similarity, which works on the Yelp dataset (the only dataset Mir et al. (2019) test on) but does not work on our Literature dataset or similarly complicated tasks, because the masking procedure goes against preserving content-specific nonstyle-related words. (2) They do not provide a 3 3.1 Evaluation Issues with Most Existing Methods Prior work in automatic evaluation of textual transfer has focused o"
D19-5614,P02-1040,0,0.107642,"Chicago, IL 60637, USA yzpang@nyu.edu, kgimpel@ttic.edu Abstract facing text generation applications such as dialogue (Ritter et al., 2011) and writing assistance (Heidorn, 2000). It can also improve NLP systems via data augmentation and domain adaptation. However, one factor that makes textual transfer difficult is the lack of parallel corpora. Advances have been made in developing transfer methods that do not require parallel corpora (see Section 2), but issues remain with automatic evaluation metrics. Li et al. (2018) used crowdsourcing to obtain manually-written references and used BLEU (Papineni et al., 2002) to evaluate sentiment transfer. However, this approach is costly and difficult to scale for arbitrary textual transfer tasks. Researchers have thus turned to unsupervised evaluation metrics that do not require references. The most widely-used unsupervised evaluation uses a pretrained style classifier and computes the fraction of times the classifier was convinced of transferred style (Shen et al., 2017). However, relying solely on this metric leads to models that completely distort the semantic content of the input sentence. Table 1 illustrates this tendency. We address this deficiency by ide"
D19-5614,P18-1090,0,0.0413808,"dress the lack of parallel data, Hu et al. (2017) used variational autoencoders to generate content representations devoid of style, which can be converted to sentences with a specific style. Ficler and Goldberg (2017) used conditional language models to generate sentences where the desired content and style are conditioning contexts. Li et al. (2018) used a feature-based approach that deletes characteristic words from the original sentence, retrieves similar sentences in the target corpus, and generates based on the original sentence and the characteristic words from the retrieved sentences. Xu et al. (2018) integrated reinforcement learning into the textual transfer problem. Another way to address the lack of parallel data is to use learning frameworks based on adversarial objectives (Goodfellow et al., 2014); several have done so for textual transfer (Yu et al., 2017; Li et al., 2017; Yang et al., 2018a; Shen et al., 2017; Fu et al., 2018). Recent work uses target-domain language models as discriminators to provide more stable feedback in learning (Yang et al., 2018b). To preserve semantics more explicitly, Fu et al. (2018) use a multi-decoder model to learn content representations that do not"
D19-5614,D14-1162,0,0.0822865,"ecause the masking procedure goes against preserving content-specific nonstyle-related words. (2) They do not provide a 3 3.1 Evaluation Issues with Most Existing Methods Prior work in automatic evaluation of textual transfer has focused on post-transfer classification accuracy (“Acc”), computed by using a pretrained classifier to measure classification accuracy of transferred texts (Hu et al., 2017; Shen et al., 2017). However, there is a problem with 139 where idf(q) = log(|C |· |{s ∈ C : q ∈ s}|−1 ) (q is a word, s is a sentence, C = X0 ∪ X1 ). We use 300-dimensional GloVe word embeddings (Pennington et al., 2014). Then, Sim is the average of the cosine similarities over all original/transferred sentence pairs. Though this metric is quite simple, we show empirically that it is effective in capturing semantic similarity. Simplicity in evaluation metrics is beneficial for computational efficiency and widespread adoption. The quality of transfer evaluations will be significantly boosted with even such a simple metric. We also experimented with METEOR (Denkowski and Lavie, 2014). However, given that we found it to be strongly correlated with Sim (shown in supplemental materials), we adopt Sim due to its co"
D19-5614,P18-1080,0,0.0521667,"penalizes L1 distance between input and round-trip transfer reconstruction. Our cycle consistency loss is inspired by Shetty et al. (2017), together with the idea of back translation in unsupervised neural machine translation (Artetxe et al., 2017; Lample et al., 2017), and the idea of cycle constraints in image generation by Zhu et al. (2017). Related Work Textual Transfer Evaluation Recent work has included human evaluation of the three categories (post-transfer style accuracy, semantic preservation, fluency), but does not propose automatic evaluation metrics for all three (Li et al., 2018; Prabhumoye et al., 2018; Chen et al., 2018; Zhang et al., 2018). There have been recent proposals for supervised evaluation metrics (Li et al., 2018), but these require annotation and are therefore unavailable for new textual transfer tasks. There is a great deal of recent work in textual transfer (Yang et al., 2018b; Santos et al., 2018; Zhang et al., 2018; Logeswaran et al., 2018; Nikolov and Hahnloser, 2018), but all either lack certain categories of unsupervised metric or lack human validation of them, which we contribute. Moreover, the textual transfer community lacks discussion of early stopping criteria and m"
D19-5614,N18-1122,0,0.0337215,"Missing"
D19-5614,N18-1012,0,0.0402088,"vel and system-level. Automatic and manual evaluation also show large improvements over the baseline method of Shen et al. (2017). We hope that our proposed metrics can speed up system development for new textual transfer tasks while also encouraging the community to address our three complementary aspects of transfer quality. 1 Introduction We consider textual transfer, which we define as the capability of generating textual paraphrases with modified attributes or stylistic properties, such as politeness (Sennrich et al., 2016a), sentiment (Hu et al., 2017; Shen et al., 2017), and formality (Rao and Tetreault, 2018). An effective transfer system could benefit a range of user§ Work completed while the author was a student at the University of Chicago and a visiting student at Toyota Technological Institute at Chicago. 138 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 138–147 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d way of aggregating three metrics for the purpose of model selection and overall comparison. We address these two problems, and we also propose metrics that are simple in addi"
D19-5614,D11-1054,0,0.0290943,"Missing"
D19-5614,P18-2031,0,0.0193377,"tion by Zhu et al. (2017). Related Work Textual Transfer Evaluation Recent work has included human evaluation of the three categories (post-transfer style accuracy, semantic preservation, fluency), but does not propose automatic evaluation metrics for all three (Li et al., 2018; Prabhumoye et al., 2018; Chen et al., 2018; Zhang et al., 2018). There have been recent proposals for supervised evaluation metrics (Li et al., 2018), but these require annotation and are therefore unavailable for new textual transfer tasks. There is a great deal of recent work in textual transfer (Yang et al., 2018b; Santos et al., 2018; Zhang et al., 2018; Logeswaran et al., 2018; Nikolov and Hahnloser, 2018), but all either lack certain categories of unsupervised metric or lack human validation of them, which we contribute. Moreover, the textual transfer community lacks discussion of early stopping criteria and methods of holistic model comparison. We propose a one-number summary for transfer quality, which can be used to select and compare models. In contemporaneous work, Mir et al. (2019) similarly proposed three types of metrics for style transfer tasks. There are two main differences compared to our work: (1) They use"
D19-5614,D18-1138,0,0.0122539,"d-trip transfer reconstruction. Our cycle consistency loss is inspired by Shetty et al. (2017), together with the idea of back translation in unsupervised neural machine translation (Artetxe et al., 2017; Lample et al., 2017), and the idea of cycle constraints in image generation by Zhu et al. (2017). Related Work Textual Transfer Evaluation Recent work has included human evaluation of the three categories (post-transfer style accuracy, semantic preservation, fluency), but does not propose automatic evaluation metrics for all three (Li et al., 2018; Prabhumoye et al., 2018; Chen et al., 2018; Zhang et al., 2018). There have been recent proposals for supervised evaluation metrics (Li et al., 2018), but these require annotation and are therefore unavailable for new textual transfer tasks. There is a great deal of recent work in textual transfer (Yang et al., 2018b; Santos et al., 2018; Zhang et al., 2018; Logeswaran et al., 2018; Nikolov and Hahnloser, 2018), but all either lack certain categories of unsupervised metric or lack human validation of them, which we contribute. Moreover, the textual transfer community lacks discussion of early stopping criteria and methods of holistic model comparison. We"
D19-5614,N16-1005,0,0.142472,"emonstrate that our metrics correlate well with human judgments, at both the sentence-level and system-level. Automatic and manual evaluation also show large improvements over the baseline method of Shen et al. (2017). We hope that our proposed metrics can speed up system development for new textual transfer tasks while also encouraging the community to address our three complementary aspects of transfer quality. 1 Introduction We consider textual transfer, which we define as the capability of generating textual paraphrases with modified attributes or stylistic properties, such as politeness (Sennrich et al., 2016a), sentiment (Hu et al., 2017; Shen et al., 2017), and formality (Rao and Tetreault, 2018). An effective transfer system could benefit a range of user§ Work completed while the author was a student at the University of Chicago and a visiting student at Toyota Technological Institute at Chicago. 138 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 138–147 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d way of aggregating three metrics for the purpose of model selection and overall com"
D19-5614,P16-1162,0,0.123902,"emonstrate that our metrics correlate well with human judgments, at both the sentence-level and system-level. Automatic and manual evaluation also show large improvements over the baseline method of Shen et al. (2017). We hope that our proposed metrics can speed up system development for new textual transfer tasks while also encouraging the community to address our three complementary aspects of transfer quality. 1 Introduction We consider textual transfer, which we define as the capability of generating textual paraphrases with modified attributes or stylistic properties, such as politeness (Sennrich et al., 2016a), sentiment (Hu et al., 2017; Shen et al., 2017), and formality (Rao and Tetreault, 2018). An effective transfer system could benefit a range of user§ Work completed while the author was a student at the University of Chicago and a visiting student at Toyota Technological Institute at Chicago. 138 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 138–147 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d way of aggregating three metrics for the purpose of model selection and overall com"
D19-5614,1983.tc-1.13,0,0.354744,"Missing"
D19-5614,P18-1042,1,0.85406,"ht + (1 − i )h t Uniform([0, 1]) is sampled for each training instance. The adversarial loss is based on Arjovsky et al. (2017),2 with the exception that we use the hidden states of the decoder instead of word distributions as inputs to Dt0 , similar to Eq. (3). We choose WGAN in the hope that its differentiability properties can help avoid vanishing gradient and mode collapse problems. We expect the generator to receive helpful gradients even if the discriminators perform well. This approach leads to much better outputs, as shown below. For paraphrase pairs, we use the ParaNMT-50M dataset (Wieting and Gimpel, 2018).1 4.4 Language Modeling Loss We attempt to improve fluency (our third metric) and assist transfer with a loss based on matching a pretrained language model for the target style. The loss is the cross entropy (CE) between the probability distribution from this language model and the distribution from the decoder: P  P Llang (θE , θG ) = 1t=0 Ext i CE(lt,i , gt,i ) (6) 4.6 where lt,i and gt,i are distributions over the vocabulary defined as follows: We iteratively update (1) θD0 , θD1 , θD00 , and θD10 by gradient descent on Ladv 0 , Ladv 1 , Ladv 00 , and Ladv 01 , respectively, and (2) θE ,"
E09-1037,D08-1017,1,0.840978,". There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each"
E09-1037,E06-1011,0,0.0427657,", though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings. 3 The theorem indexing scheme might be based on a topological ordering given by the proof structure, but is not important for our purposes. 320 proof, normally at the first opportunity. Any feature function can be expressed this way. For local features, we can go farther; we define a function top(`) that returns the proof string corresponding to the antecedents and consequent of the last inference step in `. Local features have the property: hloc m (x, y) = Pt i=1 fm (x, top(`i )) McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. 3 Approximate Decoding Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is an approximate technique for decoding (Eq. 4); it is used widely in machine translation. Given proof locality, it is essentially an efficient implementation of the k-best proof semiring. Cube pruning goes farther in that it permits nonlocal features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate. We describe the two approximations cube pruning makes,"
E09-1037,P92-1017,0,0.191961,"Missing"
E09-1037,W04-2401,0,0.0348634,"2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each value. If k = ∞, the algorithm performs exact decoding with non-local features (at obviously formidable expense in combinatorial problems). Approximation 2: lazy computation. Cube pruning exploits the fact that k < ∞ to use lazy computation. When combining the k-best proof lists of d theorems’ values"
E09-1037,D08-1016,0,0.0423917,"a fully connected graphical model). Sometimes, it is possible to achieve proof locality by adding more indices to the DP variables (for example, consider modifying the bigram HMM Viterbi algorithm for trigram HMMs). This increases the number of variables and hence computational cost. In general, it leads to exponential-time inference in the worst case. There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in"
E09-1037,P07-1095,1,0.848712,"d for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each value. If k = ∞, the algorithm performs"
E09-1037,D07-1003,1,0.894163,"Missing"
E09-1037,D08-1023,0,0.0653066,"ph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a For a proof list u ¯ , we uk to denote the sum P use k¯ of all proof scores, i:hui ,Ui i∈¯u ui . The aggregation operator over operands 8 {ui }N i=1 , all such that uis = 0, is defined by: LN (14) (12) S  N + Res u ¯ i=1 i , S  E N max-k u ¯ , g , 0 0 i=1 i DP N i=1 ui0 6 Algebraic structures are typically defined with binary operators only, so we were unable to find a suitable term for this structure in the literature. 7 Blunsom and Osborne (2008) described a related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here. 8 We assume that operands ui to ⊕cs will never be such that uis = 1 (non-local feature functions). This is reasonable in the widely used log-linear model setting we have adopted, where weights λm are factors in a proof’s product score. 9 The bottom-up agenda algorithm in Eisner et al. (2005) might possibly be generalized so that associativity, distributivity, and binary operators are not required (John Blatz, p.c.). 10 This data structur"
E09-1037,P08-1024,0,0.113899,"Missing"
E09-1037,J07-2003,0,0.395095,"ing over discrete structures with non-local features, which we relate to cube pruning (§4). We discuss implementation (§5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (§6). We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assump"
E09-1037,H05-1036,1,0.612539,"99) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure. For example, in Goodman’s framework, the forward algorithm and the Viterbi algorithm are comprised of the same logic program with different semirings. Goodman defined other semirings, including ones we will use here. This formal framework was the basis for the Dyna programming language, which permits a declarative specification of the logic program and compiles it into an efficient, agendabased, bottom-up procedure (Eisner et al., 2005). For our purposes, a DP consists of a set of recursive equations over a set of indexed variables. For example, the probabilistic CKY algorithm (run on sentence w1 w2 ...wn ) is written as CX,i−1,i = pX→wi CX,i,k = Semirings define these values and define two operators over them, called “aggregation” (max in Eq. 1) and “combination” (× in Eq. 1). Goodman and Eisner et al. assumed that the values of the variables are in a semiring, and that the equations are defined solely in terms of the two semiring operations. We will often refer to the “probability” of a proof, by which we mean a nonnegativ"
E09-1037,P02-1001,0,0.134031,"from each node to the nodes it depends on; ⊕ vertices depend on ⊗ vertices, which depend on ⊕ and axiom vertices. Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in backpropagation algorithms. Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al. (2005). This is desirable when carrying out the optimization problems involved in parameter estimation. Another differentiation technique, implemented within the semiring, is given by Eisner (2002). Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations. Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them. If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning. Then, the computational requirements for approximate summing are nearly equivalent"
E09-1037,P05-1045,0,0.0193977,"imes, it is possible to achieve proof locality by adding more indices to the DP variables (for example, consider modifying the bigram HMM Viterbi algorithm for trigram HMMs). This increases the number of variables and hence computational cost. In general, it leads to exponential-time inference in the worst case. There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth"
E09-1037,J99-4004,0,0.170178,"ique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference algorithms tractable. Doing so limits the features that are available to our models, requiring features to be structurally local. Yet many problems in NLP—machine translation, parsing, named-entity recognition, and others—have benefited from the addition of non-local features that break classical independence assumptions. D"
E09-1037,W05-1506,0,0.0554184,"⊗ vertices, which depend on ⊕ and axiom vertices. Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in backpropagation algorithms. Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al. (2005). This is desirable when carrying out the optimization problems involved in parameter estimation. Another differentiation technique, implemented within the semiring, is given by Eisner (2002). Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations. Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them. If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning. Then, the computational requirements for approximate summing are nearly equivalent to cube pruning, but the approximation is less accurate. 6 i k-best proof ("
E09-1037,P07-1019,0,0.212081,"ete structures with non-local features, which we relate to cube pruning (§4). We discuss implementation (§5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (§6). We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference a"
E09-1037,P08-1067,0,0.0703983,"ns within the semiring value. Recall that values in the k-best proof semiring fall in Ak = (R≥0 ×L)≤k . For cube decoding, we use a different set Acd defined as ..., huk g 0 (Uk ), Uk ii Here, max-k is simply used to re-sort the k-best proof list following function evaluation. The semiring properties fail to hold when introducing non-local features in this way. In particular, ⊗cd is not associative when 1 < k < ∞. For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as “NGramTree” features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j + 1 when two constituents CY,i,j and CZ,j,k are combined. The semiring value associated with such a feature is u = hhi, NGramTree π (), 1i (for a specific path π), and we rewrite Eq. 1 as follows (where ranges for summation are omitted for space): L CX,i,k = cd pX→Y Z ⊗cd CY,i,j ⊗cd CZ,j,k ⊗cd u Acd = (R≥0 × L)≤k ×G × {0, 1} {z } | Ak where the binary variable indicates whether the value contains a k-best list (0, which we call an “ordinary” value) or a non-local feature function in G (1, which we call a"
E09-1037,W01-1812,0,0.07274,"al, the approach is exact. With non-local features, the k-best list may not contain the k-best proofs, and the residual score, while including all possible proofs, may not include all of the non-local features in all of those proofs’ probabilities. Acs = R≥0 × (R≥0 × L)≤k × G × {0, 1} A value u ∈ Acs is defined as u = hu0 , hhu1 , U1 i, hu2 , U2 i, ..., huk , Uk ii, gu , us i {z } | u ¯ i=1 ui = 5 Implementation We have so far viewed dynamic programming algorithms in terms of their declarative specifications as semiring-weighted logic programs. Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al. (2005). Because Goodman’s and Eisner et al.’s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodman’s algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a For a proof list u ¯ , we uk to denote the sum P use k¯ of all proof scores, i:hui ,Ui i∈¯u ui . The aggregation operator over operands 8 {ui }N i=1 , all such that uis = 0, is defin"
E09-1037,H05-1064,0,0.153915,"Missing"
E17-2009,P16-1223,0,0.115827,"). 2.1 Neural Readers Hermann et al. (2015) developed the CNN/Daily Mail comprehension tasks and introduced ques52 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 52–57, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tion answering models based on neural networks. Many others have been developed since. We refer to these models as “neural readers”. While a detailed survey is beyond our scope, we briefly describe the neural readers used in our experiments: the Stanford (Chen et al., 2016), Attention Sum (Kadlec et al., 2016), and GatedAttention (Dhingra et al., 2016) Readers. These neural readers use attention based on the question and passage to choose an answer from among the words in the passage. We use d for the context word sequence, q for the question (with a blank to be filled), A for the candidate answer list, and V for the vocabulary. We describe neural readers in terms of three components: att function is an inner product in the Attention Sum Reader and a bilinear product in the Stanford Reader. The computed attentions are then passed through a softmax function to fo"
E17-2009,P16-1086,0,0.133454,". (2015) developed the CNN/Daily Mail comprehension tasks and introduced ques52 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 52–57, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tion answering models based on neural networks. Many others have been developed since. We refer to these models as “neural readers”. While a detailed survey is beyond our scope, we briefly describe the neural readers used in our experiments: the Stanford (Chen et al., 2016), Attention Sum (Kadlec et al., 2016), and GatedAttention (Dhingra et al., 2016) Readers. These neural readers use attention based on the question and passage to choose an answer from among the words in the passage. We use d for the context word sequence, q for the question (with a blank to be filled), A for the candidate answer list, and V for the vocabulary. We describe neural readers in terms of three components: att function is an inner product in the Attention Sum Reader and a bilinear product in the Stanford Reader. The computed attentions are then passed through a softmax function to form a probability distribution. The Ga"
E17-2009,D16-1241,1,0.704858,"Missing"
E17-2009,P16-1144,0,0.329197,"e a appears in context d. The RNNs use either gated recurrent units (Cho et al., 2014) or long short-term memory (Hochreiter and Schmidhuber, 1997). 2.2 2. Attention: The readers then compute attention weights on positions of h using g. In general, we define αi = softmax(att(hi , g)), where i ranges over positions in h. The Training Data Construction Each LAMBADA instance is divided into a context (4.6 sentences on average) and a target sentence, and the last word of the target sentence is the target word to be predicted. The LAMBADA dataset consists of development (DEV) and test (TEST) sets; Paperno et al. (2016) also provide 1 We overload the e function to operate on sequences and denote the embedding of d and q as matrices e(d) and e(q). 53 a control dataset (CONTROL), an unfiltered sample of instances from the BookCorpus. We construct a new training dataset from the BookCorpus. We restrict it to instances that contain the target word in the context. This decision is natural given our use of neural readers that assume the answer is contained in the passage. We also ensure that the context has at least 50 words and contains 4 or 5 sentences and we require the target sentences to have more than 10 wor"
E17-2009,D13-1020,0,0.0423847,"Missing"
J14-2005,2008.amta-srw.1,0,0.0249847,"ical machine translation (Yamada and Knight 2001). Syntax-based translation models are diverse, using different grammatical formalisms and features. Some use a parse tree for the source sentence (“tree-to-string”), others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: o"
J14-2005,W09-0434,0,0.05099,"Missing"
J14-2005,D10-1117,0,0.0243122,"ees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised mo"
J14-2005,C10-1011,0,0.0351016,"we obtained from the Stanford Chinese segmenter.15 For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact of using unsupervised parsers, we also performed experiments in which we replaced supervised parsers for Chinese and English with unsupervised counterparts. We now describe how we trained unsupervised parsers for these four languages. 15 More dependency parsers have been made available by the research community since we began this research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automa"
J14-2005,J92-4003,0,0.360252,"es between long phrases, we expect to face problems of data sparseness. Long phrases do not occur very often, so pairs of long phrases will occur less often still. One way to address this is to also extract rules that use part-of-speech (POS) tags in place of words. However, since words can have multiple POS tags, we would then need to infer POS tags for the words in order to determine which rule is applicable. So we instead use hard word clusters, which provide a deterministic mapping from words to cluster identifiers. Furthermore, certain types of hard word clusters, such as Brown clusters (Brown et al. 1992), have been shown to correspond well to POS tag categories (Christodoulopoulos, Goldwater, and Steedman 2010). We chose Brown clusters for this reason. Brown clustering uses a bigram hidden Markov model (HMM) in which states are hard cluster labels and observations are words. The emission distributions are constrained such that each word has a nonzero emission probability from at most one cluster label. Clusters can be obtained efficiently through a greedy algorithm that approximately maximizes the HMM’s log-likelihood by alternately proposing new clusters and merging existing ones. This proce"
J14-2005,W06-2920,0,0.0397563,"for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication: 23 June 2013. doi:10.1162/COLI a 00175 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 2007). The availability of these parsers, and gains in their accuracy, triggered research interest in syntax-based statistical machine translatio"
J14-2005,D09-1021,0,0.0397908,"Missing"
J14-2005,W08-0336,0,0.038351,"Missing"
J14-2005,P05-1022,0,0.0104731,"ath from xj to xk , minDirPathLen returns ∞. Adding these two features gives us a total of 88 QPD features. Along with the 14 phrase-based features there are a total of 102 features in our model. 6. Decoding For our model, decoding consists of solving Equation (1)—that is, finding the highestscoring tuple hy , π, φ, τφ , b i for an input sentence x and its parse τx . This is a challenging search problem, because it is at least as hard as the search problem for phrase-based models, which is intractable (Koehn, Och, and Marcu 2003). Because of this we use a coarse-to-fine strategy for decoding (Charniak and Johnson 2005; Petrov 2009). Coarseto-fine inference is a general term for procedures that make two (or more) passes over the search space, pruning the space with each pass. Typically, feature complexity is increased in each pass, as richer features can often be computed more easily in the smaller search space. One simple coarse-to-fine procedure for our model would start by generating a k-best list of derivations using a phrase-based decoder. This “coarse model” would account for all of the phrase-based features. Then we could parse each derivation to incorporate the QPD features and rerank the k-best lis"
J14-2005,P05-1033,0,0.463927,"pervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation"
J14-2005,J07-2003,0,0.285541,"maximum of 15 iterations. We used k-best lists of size 150 and a fixed, untuned value of λ = 0.1 for all experiments. 6.5 Comparison to Earlier Work The decoder described above represents some advances over those presented in earlier papers. Our original decoder was designed for a lexical dependency model; we used lattice dependency parsing on lattices in which each edge contained a single sourcetarget word pair (Gimpel and Smith 2009b). Inference was approximated using cube decoding (Gimpel and Smith 2009a), an algorithm that incorporates non-local features in a way similar to cube pruning (Chiang 2007). After developing our QPD model, we moved to phrase lattices but still approximated inference using an agenda algorithm (Nederhof 2003; Eisner, Goldlust, and Smith 2005) with pre-pruning of dependency edges in a coarse pass (Gimpel and Smith 2011). 14 We used OOQP (Gertz and Wright 2003) to solve the quadratic program in the inner loop, which uses HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk). 380 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features All decoders used lattice dependency parsing, but our current decode"
J14-2005,P10-1146,0,0.0239339,"others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed s"
J14-2005,D10-1056,0,0.0277978,"Missing"
J14-2005,D11-1005,1,0.903197,"Missing"
J14-2005,W06-1628,0,0.0564866,"Missing"
J14-2005,2009.eamt-1.10,0,0.0238875,"words must be present between both the source and target phrase pairs. We note that this rule says nothing about what fills the gap. In particular, the gap-filling material does not have to be translationally equivalent, and indeed in the given sentence pair it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten methodik ist . i thi"
J14-2005,P09-1053,1,0.8273,"l and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been used in most previous applications of QG, including word alignment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside from translation, QG has been used for a variety of applications involving relationships among sentences, including question answering (Wang, Smith, and Mitamura 2007), paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence simplification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith 2011), and supervised parsing from multiple treebanks with different annotation conventions (Li, Liu, and Che 2012). 2.3 Dependency Syntax and Machine Translation Many syntactic theories have been applied to translation modeling, but we focus in this article on dependency syntax (Tesni`ere 1959). Dependency syntax is a lightweight formalism that builds trees consisting of a set of direct"
J14-2005,D11-1018,0,0.0585073,"rzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based model of Chiang (2005) induces a synchronous grammar from parallel text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011) showed that using a supervised POS tagger to label these synchronous rules can improve performance up to the level of a model that uses a supervised full syntactic pars"
J14-2005,P05-1067,0,0.485025,"-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between the two trees."
J14-2005,J94-4004,0,0.0604105,"vantages of phrase-based and syntax-based translation. We report statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submissio"
J14-2005,P10-4002,0,0.0156596,"mpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-free grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can be used to sample projective dependency trees by representing a projective dependency grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer et al. 2010) to sample projective dependency trees uniformly at random for each sentence.16 We only compared the random parser for source-side parsing. Swapping parsers for the target language requires parsing the target side of the parallel corpus, rerunning rule extraction and feature computation with the new parses, and finally re-tuning to learn new feature weights. By contrast, changing the source-side parser only requires re-parsing the source side of the tuning and test sets and re-tuning. 7.2 Results We now present our main results, shown in Tables 6–9. We see that enlarging the search space resul"
J14-2005,C96-1058,0,0.0301186,"hs explored during the beam search. 6.2 Lattice Dependency Parsing Each path in a phrase lattice corresponds to a tuple hy , π, φ, b i for the input x . To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples hy , π, φ, b , τφ i. Lattice parsing jointly maximizes over paths through a lattice and parse structures on those paths. Because we use an arc-factored phrase dependency model (Equation (3)), the lattice dependency parsing algorithm we use is a straightforward generalization of the arcfactored dynamic programming algorithm from Eisner (1996). The algorithm is shown in Figure 10. It is shown as a set of recursive equations in which shapes are used in place of function names and shape indices are used in place of function arguments. The equations ground out in functions edgeScore and arcScore that score individual lattice edges and phrase dependency arcs, respectively.13 A semiring-generic format is used; for decoding, the semiring “plus” operator (⊕) would be defined as max and the semiring “times” operator (⊗) would be defined as +. The entry point when executing the algorithm is to build G OAL, which in turn requires building th"
J14-2005,P03-2041,0,0.314358,"lassic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures"
J14-2005,H05-1036,1,0.812842,"Missing"
J14-2005,W05-1504,1,0.879634,"yi’ yj’ yl’ yk’ xi xl xj xk yi’ yj’ yl’ yk’ Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. 371 Computational Linguistics Volume 40, Number 2 5.2.2 Dependency Length Features. Related to the string-to-tree configurations are features that score source- and target-side lengths (i.e., number of words crossed) of target-side phrase dependencies. These lengths can also be useful for hard constraints to speed up inference; we return to this in Section 6. These features and constraints are similar to those used in vine grammar (Eisner and Smith 2005). We first include a feature that counts the number of source-side words between the 0 j0 aligned source phrases in each attachment in τφ . Letting πc0 = x i0 and πd0 = x lk0 :       fsrc = I dir(c0 , d0 ) = left k0 − (j0 + 1) + I dir(c0 , d0 ) = right i0 − (l0 + 1) vine (34) Although this feature requires the segmentation of the source sentence in order to determine the number of source words crossed, the actual identities of those words are not needed, so the feature does not depend on x. We would expect this feature’s weight to be negative for most language pairs, encouraging closenes"
J14-2005,W02-1039,0,0.56847,"phrase-based and syntax-based translation. We report statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received"
J14-2005,P09-1087,0,0.313606,"ependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the n"
J14-2005,P09-1042,0,0.0551068,"Missing"
J14-2005,D11-1079,0,0.0422617,"Missing"
J14-2005,P03-1011,0,0.219752,"ch to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible"
J14-2005,W08-0302,1,0.829259,"nt of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the noise in automatic word aligners and parsers. Second, not all dependencies are preserved in hand-aligned data, so we would need to be able to handle non-isomorphic structure even if we did have perfect tools. T"
J14-2005,E09-1037,1,0.941369,"sed an alternative to synchronous grammar— quasi-synchronous grammar (QG)—that exploits this fact for increased flexibility in translation modeling. A QG assumes the source sentence and a parse are given and scores possible translations of the source sentence along with their parses. That is, a quasi-synchronous grammar is a monolingual grammar that derives strings in the target language. The strings’ derivations are scored using feature functions on an alignment from nodes in the target tree to nodes in the source tree. The quasi-synchronous dependency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007) and the phrase dependency model we present in Section 3. wir wollen keinen . we do not want one . wir durchleben keine wiederholung des jahres 1938 . we are not living a replay of 1938 . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar. 352 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be ins"
J14-2005,D09-1023,1,0.927912,"Missing"
J14-2005,D11-1044,1,0.873755,"Missing"
J14-2005,N12-1069,1,0.943037,"only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation, reporting statistically significant improvements over our baselines. These initial results offer promise for researchers to apply syntactic translation models to the thousands of languages for which we do not have manually annotated corpora, and naturally suggest future resea"
J14-2005,N12-1023,1,0.875806,"Missing"
J14-2005,D11-1138,0,0.0133818,"al. 2005) or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009). If we do not have parsers for either language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based mo"
J14-2005,W11-1011,0,0.012807,"”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typ"
J14-2005,W11-2123,0,0.0151211,"riments. Language Models. Language models were trained using the target side of the parallel corpus in each case augmented with 24,760,743 lines (601,052,087 tokens) of randomly selected sentences from the Gigaword v4 corpus (excluding the New York Times and Los Angeles Times). The minimum count cutoff for unigrams, bigrams, and trigrams was one and the cutoff for fourgrams and fivegrams was three. Language models were estimated using the SRI Language Modeling toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Chen and Goodman 1998). Language model inference was performed using KenLM (Heafield 2011) within Moses. For EN→MG, we estimated a 5-gram language model using only the target side of the parallel corpus, which contained 89,107 lines with 2,031,814 tokens. We did not use any additional Malagasy data for estimating the EN→MG language models in order to explore a scenario in which target-language text is limited or expensive to obtain. Word Clustering. Brown clusters (Brown et al. 1992) were generated using code provided by Liang (2005). For each language pair, 100 word clusters were generated for the target language. The implementation allows the use of a token count cutoff, which ca"
J14-2005,P08-1067,0,0.240556,"ass, as richer features can often be computed more easily in the smaller search space. One simple coarse-to-fine procedure for our model would start by generating a k-best list of derivations using a phrase-based decoder. This “coarse model” would account for all of the phrase-based features. Then we could parse each derivation to incorporate the QPD features and rerank the k-best list with the modified scores; this is the “fine model.” The advantage of this approach is its simplicity, but other research has shown that k-best lists for structured prediction tend to have very little diversity (Huang 2008), and we expect even less diversity in cases like machine translation where latent variables are almost always present. Instead, we generate a phrase lattice (Ueffing, Och, and Ney 2002) in a coarse pass and perform lattice dependency parsing as the fine pass. The remainder of this section is laid out as follows. We begin by reviewing phrase lattices in Section 6.1. In Section 6.2 we present our basic lattice dependency parsing algorithm. We give three ways to speed it up in Section 6.3; one enables a more judicious search without affecting the search space, and the other two prune the search"
J14-2005,W05-1506,0,0.0127642,"hrase-based model used to generate phrase lattices. Then, after generating the lattices, we prune them (Section 6.3.2) and use a second round of tuning to learn parameters of the fine model, which includes all phrase-based and QPD feature weights. We initialized the phrase-based feature weights using the default Moses weights. For the QPD features, we initialized the phrase dependency probability feature weights to 0.002 and the weights for all other features to 0. For tuning, we need the k-best outputs, for which efficient dynamic programming algorithms are available. We use Algorithm 3 from Huang and Chiang (2005), which lazily finds the k best derivations efficiently. In preliminary testing, we found that the k-best lists tended to be dominated by repeated translations with different derivations, so we used the technique presented by Huang, Knight, and Joshi (2006), which finds a unique k-best list, returning the highest-scoring derivation for each of k unique translations. This modification requires the maintenance of additional data structures to store all of the previously found string yields for each item built during parsing. This incurs additional overhead but allows us to obtain a far more dive"
J14-2005,2006.amta-papers.8,0,0.218632,"Missing"
J14-2005,P07-1022,0,0.0187182,"procedure is described in Gimpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-free grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can be used to sample projective dependency trees by representing a projective dependency grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer et al. 2010) to sample projective dependency trees uniformly at random for each sentence.16 We only compared the random parser for source-side parsing. Swapping parsers for the target language requires parsing the target side of the parallel corpus, rerunning rule extraction and feature computation with the new parses, and finally re-tuning to learn new feature weights. By contrast, changing the source-side parser only requires re-parsing the source side of the tuning and test sets and re-tuning. 7.2 Results We now present our main results, shown in Tables 6–9. We see that"
J14-2005,N07-1018,0,0.0550626,"Missing"
J14-2005,D11-1017,0,0.0138224,"her language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based model of Chiang (2005) induces a synchronous grammar from parallel text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011) sh"
J14-2005,P02-1017,0,0.126625,"ish), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, le"
J14-2005,P04-1061,0,0.312071,"Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation, reporting statistically significant improvements over our baselines. These initial results offer promise for researchers to apply syntactic translation models to the thousands of languages for which we do not have manually"
J14-2005,W04-3250,0,0.0637841,"give examples. We conclude in Section 7.4 with a runtime analysis of our decoder and show the impact of decoding constraints on speed and translation quality. 7.1 Experimental Setup In this section we describe details common to the experiments reported in this section. Details about decoding and learning were described in Section 6. Full details about language pairs, data sets, and baseline systems are given in Appendix A and Appendix B. We repeat important details here. We use case-insensitive IBM BLEU (Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap (Koehn 2004) with 100,000 samples (p ≤ 0.05). 7.1.1 Language Pairs. We consider German→English (DE→EN), Chinese→English (ZH→EN), Urdu→English (UR→EN), and English→Malagasy (EN→MG) translation. These four languages exhibit a range of syntactic divergence from English. They also vary in the availability of resources like parallel data, monolingual target-language data, and treebanks. It is standard practice to evaluate unsupervised parsers on languages that do actually have treebanks, which are used for evaluation. We consider this case as well, comparing supervised parsers for English and Chinese to our un"
J14-2005,P07-2045,0,0.124468,"ased flexibility in translation modeling. A QG assumes the source sentence and a parse are given and scores possible translations of the source sentence along with their parses. That is, a quasi-synchronous grammar is a monolingual grammar that derives strings in the target language. The strings’ derivations are scored using feature functions on an alignment from nodes in the target tree to nodes in the source tree. The quasi-synchronous dependency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007) and the phrase dependency model we present in Section 3. wir wollen keinen . we do not want one . wir durchleben keine wiederholung des jahres 1938 . we are not living a replay of 1938 . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar. 352 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been"
J14-2005,N03-1017,0,0.0829903,"Missing"
J14-2005,P04-1060,0,0.0282907,"f parallel text is available and we have a parser for one of the languages: The parallel text can be word-aligned and the annotations can be projected across the word alignments (Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentoswki 2001). The projected parses can be improved by applying manually written rules (Hwa et al. 2005) or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009). If we do not have parsers for either language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine"
J14-2005,P03-1056,0,0.285639,"opinion mining and previously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest,"
J14-2005,W08-0402,0,0.0129461,"d thus far. So, every node in the lattice is annotated with the coverage vector of all paths that end there. This is shown for three of the nodes in the figure. The lattice is constructed such that all features in the model are locally computable on individual lattice edges. To make n-gram language model features local, all paths leading to a given node must end in the same n − 1 words.12 In the example, there are two nodes with equivalent coverage vectors that are separated because they end in 12 In practice, this state replication can be reduced by exploiting sparsity in the language model (Li and Khudanpur 2008). 375 Computational Linguistics Volume 40, Number 2 different words (you vs. could). Decoders like Moses can output phrase lattices like these; the lattice simply encodes the paths explored during the beam search. 6.2 Lattice Dependency Parsing Each path in a phrase lattice corresponds to a tuple hy , π, φ, b i for the input x . To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples hy , π, φ, b , τφ i. Lattice parsing jointly maximizes over paths through a lattice and parse structures on those paths. Because we use an arc-factored"
J14-2005,P12-1071,0,0.0370512,"Missing"
J14-2005,C04-1072,0,0.0341904,"translation y when describing the reranker and omit the additional input and output variables τx , π, φ, τφ , and b , but they are always present and used for computing features. We R assume a tuning set with N source sentences: {x i }N i=1 . Let Yi be the set of reference (1) (k) translations for source sentence x i . Let Yi = {y i . . . y i } denote the set of k candidate translations (outputs of our lattice dependency parsing decoder) for x i . Let y ∗i denote the highest-quality translation in the set, that is, y ∗i = argminy ∈Yi `(YRi , y ), where `(YRi , y ) is the negated BLEU+1 score (Lin and Och 2004) of y evaluated against references YRi . 379 Computational Linguistics Volume 40, Number 2 We use the following cost function for sentence i and candidate translation y : L(YRi , y ) = `(YRi , y ) − `(YRi , y ∗i ) (38) that is, the negated BLEU+1 score of translation y i relative to that of the best translation (yy∗i ) in the set. Yadollahpour, Batra, and Shakhnarovich (2013) formulate the reranking learning problem as an L2 -regularized slack-rescaled structured support vector machine (SSVM; Tsochantaridis et al. 2005). The feature weights θ for the fine model are learned by solving the follo"
J14-2005,C04-1090,0,0.265499,"parses across word alignments in order to model dependency syntax on phrase pairs. $ konnten sie es übersetzen ? $ could you translate it ? Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using"
J14-2005,P07-1089,0,0.021249,"decrease of 0.5 BLEU. When pairing unsupervised English parsing with supervised Chinese parsing, we see an average drop of just 0.2 BLEU compared to the fully supervised case. When both parsers are unsupervised, BLEU scores drop further but are still above the best Moses baseline on average. One idea that we have not explored is to parse our parallel corpus using each parser (unsupervised and supervised), then extract rules consistent with any of the parses. This might give us some of the benefits of forest-based rule extraction, which has frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language pairs, we could pool the rules extracted from all parallel corpora for computing targetsyntactic features. For example, adding the English phrase dependency rules from the DE→EN corpus could improve performance of our ZH→EN and UR→EN systems. Moving beyond translation, we could use the pool of extracted rules from all systems (and using all parsers) to build monolingual phrase dependency parsers for use in other applications (Wu et al. 2009). 7.2.2 Feature Ablation. We performed feature abla"
J14-2005,J93-2004,0,0.0466416,"Missing"
J14-2005,P09-1039,1,0.885898,"Missing"
J14-2005,D10-1004,1,0.883885,"ation. Our model organizes phrases into a tree structure inspired by dependency syntax (Tesni`ere 1959). Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. The result captures phenomena like local reordering and idiomatic translations within phrases, as well as long-distance relationships among the phrases in a sentence. We use the term phrase dependency tree when referring to this type of dependency tree; phrase dependencies have also been used by Wu et al. (2009) for opinion mining and previously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009)."
J14-2005,N03-1021,0,0.0297769,"is summarized in Table 1. 351 Computational Linguistics Volume 40, Number 2 Table 1 Notation used in this article. i, j, k, l x, y xi j xi [i] |x | integers vectors entry i in vector x sequence from entry i to entry j (inclusive) in vector x the set containing the first i positive integers length of vector x 2.2 Synchronous and Quasi-Synchronous Grammars To model syntactic transformations, researchers have developed powerful grammatical formalisms, many of which are variations of synchronous grammars. The most widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005; Melamed 2003), an extension of context-free grammar to a bilingual setting where two strings are generated simultaneously with a single derivation. Synchronous context-free grammars are computationally attractive but researchers have shown that they cannot handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). Figure 1 shows two such examples of word alignment patterns in German–English data. These patterns were called “crossserial discontinuous translation units” (CDTUs) by Søgaard and Kuhn (2009). CDTUs cannot even be handled by the mo"
J14-2005,D08-1022,0,0.0257376,"vised English parsing with supervised Chinese parsing, we see an average drop of just 0.2 BLEU compared to the fully supervised case. When both parsers are unsupervised, BLEU scores drop further but are still above the best Moses baseline on average. One idea that we have not explored is to parse our parallel corpus using each parser (unsupervised and supervised), then extract rules consistent with any of the parses. This might give us some of the benefits of forest-based rule extraction, which has frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language pairs, we could pool the rules extracted from all parallel corpora for computing targetsyntactic features. For example, adding the English phrase dependency rules from the DE→EN corpus could improve performance of our ZH→EN and UR→EN systems. Moving beyond translation, we could use the pool of extracted rules from all systems (and using all parsers) to build monolingual phrase dependency parsers for use in other applications (Wu et al. 2009). 7.2.2 Feature Ablation. We performed feature ablation experiments for UR→EN translation, shown"
J14-2005,P08-1023,0,0.142212,"Missing"
J14-2005,D10-1120,0,0.0229544,", the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntac"
J14-2005,J03-1006,0,0.0952783,"perator (⊕) would be defined as max and the semiring “times” operator (⊗) would be defined as +. The entry point when executing the algorithm is to build G OAL, which in turn requires building the other structures. We use a simple top–down implementation with memoization. Our style of specifying dynamic programming algorithms is similar to weighted deduction, but additionally specifies indices and ranges of iteration, which are useful for a top–down implementation. Top–down dynamic programming avoids the overhead of maintaining a priority queue that is required by bottom–up agenda algorithms (Nederhof 2003; Eisner, Goldlust, and Smith 2005). The disadvantage of top–down dynamic programming is that wasted work can be done; structures can be built that are never used in any full parse. This problem appears when parsing with context-free grammars, and so the CKY algorithm works bottom– up, starting with the smallest constituents and incrementally building larger ones. This is because context-free grammars may contain rules with only non-terminals. Top– down execution may consider the application of such rules in sequence, producing long derivations of non-terminals that never “ground out” in any s"
J14-2005,P03-1021,0,0.00908061,"we used the technique presented by Huang, Knight, and Joshi (2006), which finds a unique k-best list, returning the highest-scoring derivation for each of k unique translations. This modification requires the maintenance of additional data structures to store all of the previously found string yields for each item built during parsing. This incurs additional overhead but allows us to obtain a far more diverse k-best list given a fixed time and memory budget. For the first round of tuning, we use R AMPION (Gimpel and Smith 2012b), which performs competitively with minimum error rate training (Och 2003) but is more stable. For training the fine model, however, we found that R AMPION did not lead to substantial improvements over the output of the coarse phrase-based model alone. We found better performance by using a fine learner designed for the k-best reranking setting, in particular the structured support vector machine reranker described by Yadollahpour, Batra, and Shakhnarovich (2013). Though we are doing lattice reranking rather than k-best reranking, the learning problem for our fine model is similar to that for k-best reranking in that the decoder is exact (i.e., there is no pruning t"
J14-2005,J03-1002,0,0.00722327,"Missing"
J14-2005,P02-1040,0,0.0911232,"iments in Section 7.2.2. We present the results of a manual evaluation in Section 7.3 and give examples. We conclude in Section 7.4 with a runtime analysis of our decoder and show the impact of decoding constraints on speed and translation quality. 7.1 Experimental Setup In this section we describe details common to the experiments reported in this section. Details about decoding and learning were described in Section 6. Full details about language pairs, data sets, and baseline systems are given in Appendix A and Appendix B. We repeat important details here. We use case-insensitive IBM BLEU (Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap (Koehn 2004) with 100,000 samples (p ≤ 0.05). 7.1.1 Language Pairs. We consider German→English (DE→EN), Chinese→English (ZH→EN), Urdu→English (UR→EN), and English→Malagasy (EN→MG) translation. These four languages exhibit a range of syntactic divergence from English. They also vary in the availability of resources like parallel data, monolingual target-language data, and treebanks. It is standard practice to evaluate unsupervised parsers on languages that do actually have treebanks, which are used for evaluation. We consider t"
J14-2005,P05-1034,0,0.17418,"Missing"
J14-2005,W08-1006,0,0.182932,"iously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use"
J14-2005,N06-1032,0,0.0655531,"Missing"
J14-2005,seeker-kuhn-2012-making,0,0.0172191,"e segmenter.15 For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact of using unsupervised parsers, we also performed experiments in which we replaced supervised parsers for Chinese and English with unsupervised counterparts. We now describe how we trained unsupervised parsers for these four languages. 15 More dependency parsers have been made available by the research community since we began this research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automatic POS tags for dependency grammar induction"
J14-2005,P08-1066,0,0.140416,"Missing"
J14-2005,C90-3045,0,0.695122,"ndled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphi"
J14-2005,H05-1095,0,0.0345308,"indicate that other words must be present between both the source and target phrase pairs. We note that this rule says nothing about what fills the gap. In particular, the gap-filling material does not have to be translationally equivalent, and indeed in the given sentence pair it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten"
J14-2005,A97-1014,0,0.0579414,"ee-to-tree divergence in German–English data.1 We consider the German– English parallel corpus used in our experiments (and described in Appendix A). We parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art dependency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head rules (Yamada and Matsumoto 2003). We parsed the German side using the factored model in the Stanford parser (Rafferty and Manning 2008), which is trained from the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser’s source code defines a set of head rules for converting the phrase-structure parse output to dependencies.2 The first example is shown in Figure 3. The bold words illustrate a “sibling” relationship, meaning that the source words aligned to the parent and child in the English sentence have the same parent on the German side. Many sibling configurations appear when the English dependency is DET→N within a PP. By convention, the NEGRA treebank uses flat structures for PPs like “P DET N” rather than using a separate NP for DET N. When the parser converts this to a dependenc"
J14-2005,W06-3104,0,0.351833,"Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between the two trees. A weighted QG uses feature functions to softly penalize or encourage particular types of syntactic divergence. In this article, we present a"
J14-2005,D09-1086,0,0.133319,"Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been used in most previous applications of QG, including word alignment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside from translation, QG has been used for a variety of applications involving relationships among sentences, including question answering (Wang, Smith, and Mitamura 2007), paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence simplification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith 2011), and supervised parsing from multiple treebanks with different annotation conventions (Li, Liu, and Che 2012). 2.3 Dependency Syntax and Machine Translation Many syntactic theories have been applied to translation modeling, but we focus in this article on dependency syntax (Tesni`ere 1959). Dependency syntax is a lightweight formalism that builds trees consisting of a set of directed arcs from words to their syntactic heads (also called “"
J14-2005,P09-1009,0,0.195419,"Missing"
J14-2005,W09-2303,0,0.0847503,"rt statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; acc"
J14-2005,D11-1118,0,0.054708,"2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automatic POS tags for dependency grammar induction can work as well as or better than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011) showed that unsupervised tags could work as well as those from a supervised POS tagger. For Urdu and Malagasy, we use fully unsupervised POS tagging, using the approach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the “direct gradient” version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we use the gold standard POS tags from their respective treebanks for training the pa"
J14-2005,N10-1116,0,0.0218812,"Missing"
J14-2005,N03-1033,0,0.0830797,"s for dependency grammar induction can work as well as or better than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011) showed that unsupervised tags could work as well as those from a supervised POS tagger. For Urdu and Malagasy, we use fully unsupervised POS tagging, using the approach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the “direct gradient” version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we use the gold standard POS tags from their respective treebanks for training the parser, then use the Stanford POS tagger (Toutanova et al. 2003) to tag the parallel data, tuning, and test sets. As our dependency parsing model, we use the dependency model with valence (Klein and Manning 2004) initialized with a convex initializer (Gimpel and Smith 2012a). The training procedure is described in Gimpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-fr"
J14-2005,D08-1065,0,0.0176142,"check reachability of the item endpoints and only proceed if one can reach the other. We modified the algorithm to output maximum lengths because we use the maximum lengths to compute the target-side vine grammar features and constraints, as mentioned in Section 5.2.2. In particular we use a feature ftgt that is a target-side vine analog to fsrc but using the Floyd-Warshall maximum path lengths in place of the actual vine lengths. 6.3.2 Lattice Pruning. To reduce phrase lattice sizes, we prune lattice edges using forward–backward pruning (Sixtus and Ortmanns 1999), which has also been used by Tromble et al. (2008). This pruning method computes the max-marginal for each lattice edge, which is the score of the best full path that uses that edge, then prunes edges whose max-marginal is below a certain fraction of the best path score in the lattice. Maxmarginals have been used for other coarse-to-fine learning frameworks (Weiss, Sapp, and Taskar 2010) and offer the advantage that the best path in the lattice is preserved during pruning. We only use the score contribution from the phrase-based features when computing these max-marginals. For each lattice, we use a grid search to find the most liberal thresh"
J14-2005,C10-1123,0,0.0171536,"en to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the noise in automatic word aligners and parsers. Second, not"
J14-2005,W02-1021,0,0.0258444,"Missing"
J14-2005,D07-1003,1,0.871521,"Missing"
J14-2005,P06-1123,0,0.0621487,"Missing"
J14-2005,D10-1050,0,0.0532535,"Missing"
J14-2005,D11-1038,0,0.0243739,"Missing"
J14-2005,J97-3002,0,0.262601,"ive integers as [k]. This notation is summarized in Table 1. 351 Computational Linguistics Volume 40, Number 2 Table 1 Notation used in this article. i, j, k, l x, y xi j xi [i] |x | integers vectors entry i in vector x sequence from entry i to entry j (inclusive) in vector x the set containing the first i positive integers length of vector x 2.2 Synchronous and Quasi-Synchronous Grammars To model syntactic transformations, researchers have developed powerful grammatical formalisms, many of which are variations of synchronous grammars. The most widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005; Melamed 2003), an extension of context-free grammar to a bilingual setting where two strings are generated simultaneously with a single derivation. Synchronous context-free grammars are computationally attractive but researchers have shown that they cannot handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). Figure 1 shows two such examples of word alignment patterns in German–English data. These patterns were called “crossserial discontinuous translation units” (CDTUs) by Søgaard and Kuhn (2009"
J14-2005,D09-1159,0,0.053068,"Missing"
J14-2005,D11-1020,0,0.133979,"Missing"
J14-2005,W07-0706,0,0.487515,"Missing"
J14-2005,W03-3023,0,0.0447235,"servation across languages by using dependencies on phrases—flat multi-word units—rather than words. To motivate these choices, we now give two frequently occurring examples of dependency tree-to-tree divergence in German–English data.1 We consider the German– English parallel corpus used in our experiments (and described in Appendix A). We parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art dependency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head rules (Yamada and Matsumoto 2003). We parsed the German side using the factored model in the Stanford parser (Rafferty and Manning 2008), which is trained from the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser’s source code defines a set of head rules for converting the phrase-structure parse output to dependencies.2 The first example is shown in Figure 3. The bold words illustrate a “sibling” relationship, meaning that the source words aligned to the parent and child in the English sentence have the same parent on the German side. Many sibling configurations appear when the English dependency is DET"
J14-2005,P01-1067,0,0.747174,"ivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication: 23 June 2013. doi:10.1162/COLI a 00175 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 2007). The availability of these parsers, and gains in their accuracy, triggered research interest in syntax-based statistical machine translation (Yamada and Knight 2001). Syntax-based translation models are diverse, using different grammatical formalisms and features. Some use a parse tree for the source sentence (“tree-to-string”), others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed tha"
J14-2005,N01-1026,0,0.110426,"Missing"
J14-2005,H01-1035,0,0.020934,"Missing"
J14-2005,D11-1019,0,0.0392025,"Missing"
J14-2005,W06-3119,0,0.0418071,"it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten methodik ist . i think that it is consequently a question of the appropriate methodologies . dass es ... ist that it is (b) abschließend möchte ich herrn langen herzlich für seinen bericht danken ,... finally , mr president , i would like to thank mr langen warmly for his report ,... für"
J14-2005,C08-1144,0,0.175247,"e 4 Example of a sentence pair containing a frequently-observed “grandparent-grandchild” relationship in German–English data: the English parent and child words in the until←recently dependency are aligned to German words in a grandparent-grandchild relationship. 355 Computational Linguistics Volume 40, Number 2 On the other hand, models that use rules employing syntax (Yamada and Knight 2001) or syntax-like representations (Chiang 2005) handle long-distance reordering better than phrase-based systems (Birch, Blunsom, and Osborne 2009), and therefore perform better for certain language pairs (Zollmann et al. 2008). In order to better handle syntactic divergence and obtain the benefits of these two types of models, we use rules that combine phrases and syntax. In particular, our rules use dependencies between phrases rather than words; we call them phrase dependencies. When adding in source syntax, we eschew the constraints of synchronous grammar in favor of the feature-based approach of quasi-synchronous grammar. So we call our model a quasi-synchronous phrase dependency (QPD) translation model. In Section 3.1, we define phrase dependency trees and in Section 3.2 we present our model. We discuss rule e"
J14-2005,P11-1001,0,0.293962,"and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English par"
J14-2005,N10-1140,0,\N,Missing
J14-2005,N10-1083,0,\N,Missing
J14-2005,C10-2136,0,\N,Missing
J14-2005,N03-1031,0,\N,Missing
J14-2005,P09-1063,0,\N,Missing
J14-2005,D07-1096,0,\N,Missing
J14-2005,W90-0102,0,\N,Missing
N10-1038,P07-1053,0,0.00905126,"ge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across dif"
N10-1038,N09-1031,1,0.594354,"used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across different media outlets and"
N10-1038,W02-1011,0,0.0198514,"did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They used the aggregate sentiment scores and mention counts of each movie in news articles as predictors. While there has been substantial prior work on using critics’ reviews, to our knowledge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify"
N10-1038,W00-1308,0,0.035303,"o our pool of features in the following order: whether the 4.2 Text Features We extract three types of text features (described below). We only included feature instances that occurred in at least five different movies’ reviews. We stem and downcase individual word components in all our features. I. n-grams. We considered unigrams, bigrams, and trigrams. A 25-word stoplist was used; bigrams and trigrams were only filtered if all words were stopwords. II. Part-of-speech n-grams. As with words, we added unigrams, bigrams, and trigrams. Tags were obtained from the Stanford part-of-speech tagger (Toutanova and Manning, 2000). III. Dependency relations. We used the Stanford parser (Klein and Manning, 2003) to parse the critic reviews and extract syntactic dependencies. The dependency relation features consist of just the relation part of a dependency triple hrelation, head word, modifier wordi. We consider three ways to combine the collection of reviews for a given movie. The first (“−”) simply concatenates all of a movie’s reviews into a single document before extracting features. The second (“+”) conjoins each feature with the source site (e.g., New York Times) from whose review it was extracted. A third version"
N10-1112,W02-1001,0,0.173616,"θ &gt; f (x(i) , y)} &gt; (i) 0 y 0 ∈Y(x(i) ) exp{θ f (x , y )} cost(y (i) , y) P X −θ &gt; f (x(i) , y (i) ) + log i=1 (4) (5) exp{θ &gt; f (x(i) , y) + cost(y (i) , y)} (6) y∈Y(x(i) ) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 (3) y∈Y(x(i) ) i=1 y∈Y("
N10-1112,H05-1087,0,0.0135,"Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard”P m"
N10-1112,D07-1033,0,0.0107519,"(i) ) + max   θ &gt; f (x(i) , y) + cost(y (i) , y) y∈Y(x(i) ) X exp{θ &gt; f (x(i) , y)} &gt; (i) 0 y 0 ∈Y(x(i) ) exp{θ f (x , y )} cost(y (i) , y) P X −θ &gt; f (x(i) , y (i) ) + log i=1 (4) (5) exp{θ &gt; f (x(i) , y) + cost(y (i) , y)} (6) y∈Y(x(i) ) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented"
N10-1112,W04-3250,0,0.0137351,"Missing"
N10-1112,D09-1005,0,0.0354659,"nce of the North American Chapter of the ACL, pages 733–736, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics on training data: Pn P i=1 (i) (i) y∈Y(x(i) ) pθ (y|x )cost(y , y) (2) With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax"
N10-1112,P03-1021,0,0.07487,"ser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max"
N10-1112,P06-2101,0,0.0647648,"th respect to the conditional distribution pθ (y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics on training data: Pn P i=1 (i) (i) y∈Y(x(i) ) pθ (y|x )cost(y , y) (2) With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wi"
N10-1112,P06-1028,0,0.0223626,"tructures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard”P maximum of max-margin with the “softmax” (log exp) from CLL; hence we use the name “softmax-margin.” Like"
N10-1112,W03-0419,0,0.177423,"Missing"
N10-1112,J96-1002,0,\N,Missing
N10-1112,D08-1076,0,\N,Missing
N10-1112,P02-1062,0,\N,Missing
N12-1023,2007.mtsummit-papers.3,0,0.314589,"Missing"
N12-1023,D08-1023,0,0.0193604,"Missing"
N12-1023,P08-1024,0,0.105614,"Missing"
N12-1023,W08-0304,0,0.27423,"the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ ("
N12-1023,W08-0336,0,0.0631996,"Missing"
N12-1023,N12-1047,0,0.320612,"recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes  ˜ ˜ the following  ( form: losscost X, Y , θ = )  N (i) ˜   cost Y , argmax score(x , y, h; θ) hy,hi∈T(x(i) ) i=1 (3) 2 We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost(hy, hi, hy 0 , h0 i) = cost(y, hy 0 , h0 i) = cost(y, y 0 ). 3 Cherry and Foster (2012) have concurrently performed a similar analysis. MERT directly minimizes the corpus-level cost function of the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning."
N12-1023,D08-1024,0,0.840805,"pplied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) M"
N12-1023,N09-1025,0,0.713561,"Missing"
N12-1023,P05-1033,0,0.0719136,". Risk minimization corresponds to choosing argminθ∈Θ Ep(X,Y ) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1 For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). 222 the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an ac˜ Y˜ i, where X ˜ = {x(i) }N and tual corpus pair hX, i=1 (i) N Y˜ = {y }i=1 , we instead consider regularized empirical risk minimization: ˜ Y˜ , θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 norm, but many other choices are possible. In this paper, we use `2 . Models are evaluat"
N12-1023,J07-2003,0,0.253188,"Missing"
N12-1023,P11-2031,1,0.0916464,".e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{score(x, y, h; θ)} (4) ˜ Y˜ , θ) = The log l"
N12-1023,W02-1001,0,0.135437,"titute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are"
N12-1023,W09-0439,0,0.207542,"t any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{score(x, y, h; θ)} (4)"
N12-1023,N10-1112,1,0.11129,"Missing"
N12-1023,D11-1125,0,0.298817,"Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that the corr"
N12-1023,N03-1017,0,0.0188096,"anslations, and the model parameters to a real value indicating the quality of the parameters. Risk minimization corresponds to choosing argminθ∈Θ Ep(X,Y ) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1 For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). 222 the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an ac˜ Y˜ i, where X ˜ = {x(i) }N and tual corpus pair hX, i=1 (i) N Y˜ = {y }i=1 , we instead consider regularized empirical risk minimization: ˜ Y˜ , θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 n"
N12-1023,P07-2045,0,0.0115244,"Missing"
N12-1023,P03-1051,0,0.0369964,"Missing"
N12-1023,D09-1005,0,0.024833,"Missing"
N12-1023,P06-1096,0,0.794861,"Missing"
N12-1023,C04-1072,0,0.360718,"Missing"
N12-1023,D08-1076,0,0.170118,"Missing"
N12-1023,C08-1074,0,0.0293292,"from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{"
N12-1023,P02-1038,0,0.212031,"loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and"
N12-1023,J03-1002,0,0.00451832,"Missing"
N12-1023,P03-1021,0,0.711819,"to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We"
N12-1023,2001.mtsummit-papers.68,0,0.0457285,", θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 norm, but many other choices are possible. In this paper, we use `2 . Models are evaluated using a task-specific notion of error, here encoded as a cost function, cost : YN × YN → R≥0 , such that the worse a translation is, the higher its cost. The cost function will typically make use of an automatic evaluation metric for machine translation; e.g., cost might be 1 minus the BLEU score (Papineni et al., 2001).2 We note that our analysis in this paper is applicable for understanding the loss function being optimized given a fixed set of k-best lists.3 However, most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predi"
N12-1023,P06-2101,0,0.0414099,"Missing"
N12-1023,D07-1080,0,0.796037,"Missing"
N12-1023,D07-1055,0,0.211629,"Missing"
N12-1023,P02-1040,0,\N,Missing
N12-1069,N10-1083,0,0.110527,"Missing"
N12-1069,D10-1117,0,0.132022,"U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 it"
N12-1069,D10-1118,0,0.0216064,"at we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify C CV 1 to restrict the summation over parents to exclude e00 if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by C CV 2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal is to develop concave models, Brody employed Bayesian nonparametrics in his version of Model 1, which makes the model non-concave. 4 Experiments We ran experiments to determine how well our concave grammar induction models C CV 1 and C CV 2 can perform on their own and when used as initializers for the DMV (Klein and Manning, 2004). The DMV is a generative model of POS tag sequences and projective dependency trees over them. It is the foundation of most state-of-the-art unsupervised grammar"
N12-1069,J93-2003,0,0.158755,"alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function maximized by EM is non-concave. As a result, researchers are obligated to consider initialization in addition to model design (Klein and Manning, 2004; Goldberg et al., 2008). For example, consider the dependency grammar induction results shown in Table 1 when training the 1 It i"
N12-1069,W06-2920,0,0.0851498,"4 37.5 / 30.9 43.7 / 35.5 el hu 37/32 23/18 50/41 23/20 51/45 32/28 50/45 60/46 avg. log-likelihood -15.05 -14.84 -14.93 -14.45 Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ≤ 10 words and second is for all sentences. For training, sentences ≤ 10 words from each treebank were used. In order, languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish. Data We use data prepared for the CoNLL 2006/07 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).4 We follow standard practice in removing punctuation and using short sentences (≤ 10 or ≤ 20 words) for training. For all experiments, we train on separate data from that used for testing and use gold POS tags for both training and testing. We report accuracy on (i) test set sentences ≤ 10 words and (ii) all sentences from the test set. Results Results for English are shown in Tab. 1. We train on §2–21 and test on §23 in the Penn Treebank. The constraint on sentence roots helps a great deal, as C CV 2 by itself is competitive with the DMV when testing on short sentences."
N12-1069,N09-1009,1,0.848241,". 2 579 Model ATT R IGHT C CV 1 C CV 2 DMV Shared LN L-EVG Feature DMV LexTSG-DMV Posterior Reg. Punc/UTags Init. N/A U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initia"
N12-1069,P08-1085,0,0.0690685,"Missing"
N12-1069,N09-1012,0,0.205543,"Missing"
N12-1069,P04-1061,0,0.514135,". Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We investigate models for unsupervised learning with concave log-likelihood functions. We begin with the most well-known example, IBM Model 1 for word alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function max"
N12-1069,W11-3901,0,0.255875,"Missing"
N12-1069,D10-1120,0,0.332351,"the conditions observed. This conditioning information may innecessary to develop concave models for two tasks. clude the direction of the edge, its distance, and any properties about the words in the sentence. We 3.1 Part-of-Speech Tagging found that conditioning on direction improved perConsider a standard first-order hidden Markov formance: we rewrite the c distributions as c(ej | model for POS tagging. Letting y denote the tag e0i , sign(j − i)) and denote this model by C CV 1. 578 We note that we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify C CV 1 to restrict the summation over parents to exclude e00 if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by C CV 2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal i"
N12-1069,petrov-etal-2012-universal,0,0.0450502,"d in our concave models. The DMV also has multinomial distributions for deciding whether to stop or continue generating children in each direction considering whether any children have already been generated in that direction. The majority of researchers use the original initializer from Klein and Manning (2004), denoted here K&M. K&M is a deterministic harmonic initializer that sets parent-child token affinities inversely ˇ This is similar to the rule used by Mareˇcek and Zabokrtsk´ y (2011) with empirical success. 3 As verbs, we take all tags that map to V in the universal tag mappings from Petrov et al. (2012). Thus, to apply this constraint to a new language, one would have to produce a similar tag mapping or identify verb tags through manual inspection. 2 579 Model ATT R IGHT C CV 1 C CV 2 DMV Shared LN L-EVG Feature DMV LexTSG-DMV Posterior Reg. Punc/UTags Init. N/A U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English"
N12-1069,N10-1116,0,0.434287,"Missing"
N12-1069,D11-1118,0,0.116615,"Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and"
N12-1069,W11-0303,0,0.097129,"Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and"
N12-1069,P11-2081,0,0.0361349,"Missing"
N12-1069,D07-1096,0,\N,Missing
N13-1039,P11-1087,0,0.0199181,"of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” 7 http://www.ark.c"
N13-1039,D11-1052,0,0.403712,": u = “you”) and prepositions (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many"
N13-1039,J92-4003,0,0.437604,"Missing"
N13-1039,E03-1009,0,0.019282,"lusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the"
N13-1039,P11-1137,1,0.142556,"g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as"
N13-1039,N13-1037,0,0.383435,"rs corresponding to some of these words. This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. Introduction Online conversational text, typified by microblogs, chat, and text messages,1 is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text"
N13-1039,D11-1142,0,0.0448156,"g minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and §6.2. 6 Experiments We are primarily concerned with performance on our annotated datasets described"
N13-1039,P11-2008,1,0.727026,"Missing"
N13-1039,P11-1038,0,0.422239,"to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexic"
N13-1039,P08-1068,0,0.420847,"t for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha ha"
N13-1039,P11-1037,0,0.0938106,"e are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for b"
N13-1039,P12-3005,0,0.246426,"Missing"
N13-1039,J93-2004,0,0.061082,"Missing"
N13-1039,N10-1004,0,0.161793,"regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: • an open-source part-of-"
N13-1039,W96-0213,0,0.684593,"community: • an open-source part-of-speech tagger for online conversational text (§2); • unsupervised Twitter word clusters (§3); • an improved emoticon detector for conversational text (§4); 380 Proceedings of NAACL-HLT 2013, pages 380–390, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1 , and is parameterized by a multiclass logistic regression: p(yt = k |y t−1 , x, t; β) ∝   P (obs) (trans) exp βyt−1 ,k + j βj,k fj (x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K"
N13-1039,D11-1141,0,0.44769,"tactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results i"
N13-1039,P05-1044,1,0.328937,"demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about t"
N13-1039,N12-1052,0,0.100512,"s—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa aha"
N13-1039,P10-1040,0,0.242904,"analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha"
N13-1039,P02-1053,0,0.00928357,"y inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and"
N13-1039,petrov-etal-2012-universal,0,\N,Missing
N15-1028,N09-1003,0,0.0553204,"ing the word aligner in cdec (Dyer et al., 2010) run on the WMT0610 news commentary corpora and Europarl. After training, we apply the learned CCA/DCCA projection mappings to the original English word embeddings (180K words) and use these transformed embeddings for our evaluation tasks. 3.1 Evaluation Tasks We compare our DCCA-based embeddings to the original word vectors and to CCA-based em1 www.statmt.org/wmt11/ beddings on several tasks. We use WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs with human similarity ratings. It is divided into WS-SIM and WS-REL by Agirre et al. (2009) to measure similarity and relatedness. We also use SimLex-999 (Hill et al., 2014), a new similarity-focused dataset consisting of 666 noun pairs, 222 verb pairs, and 111 adjective pairs. Finally, we use the bigram similarity dataset from Mitchell and Lapata (2010) which has 3 subsets, adjective-noun (AN), noun-noun (NN), and verbobject (VN), and dev and test sets for each. For the bigram task, we simply add the word vectors output by CCA or DCCA to get bigram vectors.2 All task datasets contain pairs with human similarity ratings. To evaluate embeddings, we compute cosine similarity between t"
N15-1028,N12-1095,1,0.859014,"e idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonline"
N15-1028,P14-2131,1,0.807197,"this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 20"
N15-1028,D13-1167,0,0.00836244,"thout using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data. Acknowledgments We are grateful to Manaal Faruqui for sharing resources, and to Chris Dyer, David Sontag, Lyle Ungar, and anonymous reviewers for helpful input"
N15-1028,P02-1033,0,0.16235,"012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) tec"
N15-1028,P10-4002,0,0.0115371,"CCA and KCCA on a speech recognition task. 3 Experiments We use English and German as our two languages. Our original monolingual word vectors are the same as those used by Faruqui and Dyer (2014). They are 640-dimensional and are estimated via latent semantic analysis on the WMT 2011 monolingual news corpora.1 We use German-English translation pairs as the input to CCA and DCCA, using the same set of 36K pairs as used by Faruqui and Dyer. These pairs contain, for each of 36K English word types, the single most frequently aligned German word. They were obtained using the word aligner in cdec (Dyer et al., 2010) run on the WMT0610 news commentary corpora and Europarl. After training, we apply the learned CCA/DCCA projection mappings to the original English word embeddings (180K words) and use these transformed embeddings for our evaluation tasks. 3.1 Evaluation Tasks We compare our DCCA-based embeddings to the original word vectors and to CCA-based em1 www.statmt.org/wmt11/ beddings on several tasks. We use WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs with human similarity ratings. It is divided into WS-SIM and WS-REL by Agirre et al. (2009) to measure similarity and r"
N15-1028,E14-1049,0,0.74059,"similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonlinear transformations of two languages’ embeddings that a"
N15-1028,P08-1088,0,0.0870681,"d´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks"
N15-1028,J14-1004,0,0.012743,"om two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002;"
N15-1028,N13-1056,0,0.0166863,"r and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, co"
N15-1028,C12-1089,0,0.0603216,"ingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data. Acknowledgments We are grateful to Manaal Faruqui for sharing resources, and to Chris Dyer, David Sontag, Lyle Ungar, and anonymous review"
N15-1028,P14-2037,0,0.344264,"Missing"
N15-1028,W02-0902,0,0.103506,"models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and"
N15-1028,P08-1068,0,0.0314638,"example via canonical correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational"
N15-1028,W13-3512,0,0.0293337,"Missing"
N15-1028,N04-1043,0,0.0307805,"e their quality, for example via canonical correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that ad"
N15-1028,N10-1135,0,0.151153,"Missing"
N15-1028,P00-1054,0,0.0867822,"se-like similarity and thereby discourage the similarity of hypernym-hyponym pairs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008);"
N15-1028,N12-1052,0,0.0465392,"Missing"
N15-1028,P10-1040,0,0.140012,"cal correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in b"
N15-1028,D13-1168,0,0.091007,"hao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embedd"
N15-1028,P06-2124,0,0.00841971,"irs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used"
N15-1028,W05-0804,0,0.0353416,"de them less similar than the original vectors, and DCCA made them less similar still. This matches our intuition that bilingual information should encourage paraphrase-like similarity and thereby discourage the similarity of hypernym-hyponym pairs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens,"
N15-1028,D13-1141,0,0.555013,"entations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonlinear transformations"
N15-1028,J15-4004,0,\N,Missing
N18-1007,N16-1024,0,0.0388215,"ay have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the prosody modeling com"
N18-1007,1993.eamt-1.1,0,0.608238,"Missing"
N18-1007,Q13-1006,0,0.0167593,"an improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency"
N18-1007,P06-1055,0,0.121059,"Missing"
N18-1007,P06-1021,0,0.716489,"r, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demonstrate improvements"
N18-1007,H91-1073,1,0.371999,"rical Engineering, University of Washington 2 Toyota Technological Institute at Chicago 3 Department of Computer Science, UNC Chapel Hill {ttmt001, ostendor}@uw.edu, mbansal@cs.unc.edu, {shtoshni, kgimpel, klivescu}@ttic.edu Abstract Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation. Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991). Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994). However, most speech parsing systems in practice take little advantage of these cues. Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment,"
N18-1007,D13-1013,0,0.587645,"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input fe"
N18-1007,Q14-1011,0,0.616409,"parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input features to a linearized parse"
N18-1007,H05-1030,1,0.848641,"s in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demons"
N18-1007,D17-1296,0,0.24349,"dule; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1). However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model). Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017). Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement. a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had> my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal. Further, the"
N18-1007,P15-1113,0,0.0224504,"ror suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the"
N18-1007,N04-4032,1,0.423253,"act the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody. Not included in this analysis are sentence boundary errors, which also change the “gold” parse. Thus, prosody may be more useful than results here indicate. 6 It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing. A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007). Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks. The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies. (In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.) Our analyses of the impact of prosody also extends prior work. Related Work Related work on parsing conversational speech has mainly addressed four probl"
N18-1007,D16-1109,0,0.187986,"n interruption point. Repairs often involve parallel grammatical Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012). In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had 76 obtained smaller gains. Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural"
N18-1007,D12-1096,0,0.0498214,"r benefit from prosody in the fluent set. tachment errors. Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment. Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser. Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word. Types of errors. We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences. The two models differ in the types of error reductions they provide. Including pause information gives largest improvements on PP attachment and Modifier atEffect of transcription errors. The results and analyses so far have assumed that we have reliable transcripts. In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio"
N18-1170,P05-1074,0,0.748798,"inal test sets (§5). Together these results not only establish the first general purpose syntactically controlled paraphrase approach, but also suggest that this general paradigm could be used for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nemat"
N18-1170,N03-1003,0,0.138616,"Missing"
N18-1170,D08-1021,0,0.0149106,"Missing"
N18-1170,2005.mtsummit-ebmt.3,0,0.272082,"Missing"
N18-1170,D17-1091,0,0.091119,"Missing"
N18-1170,W17-5401,0,0.101857,"Missing"
N18-1170,D12-1139,0,0.0195225,"odel is trained to produce s2 . To overcome learned biases of the NMT system, we also include reversed pairs hs2 , s1 i during training. 2.2.1 Syntactic templates To provide syntactic control, we linearize the bracketed parse structure without leaf nodes (i.e., tokens). For example, the corresponding linearized parse 2 Syntactic diversity was measured by the entropy of the top two levels of parse trees in the corpora. 3 Similar automated filtering could be used to produce data for many other transformations, such as tense changes, pointof-view shifts, and even stylometric pattern differences (Feng et al., 2012). This is an interesting area for future work. 4 Because of the large dataset size, we use the faster but less accurate shift-reduce parser written by John Bauer. 1876 tree for the sentence “She drove home.” is (S(NP(PRP))(VP(VBD)(NP(NN)))(.)). A system that requires a complete linearized target parse at test-time is unwieldy; how do we go about choosing the target parse? To simplify test-time usage, we relax the target syntactic form to a parse template, which we define as the top two levels of the linearized parse tree (the level immediately below the root along with the root); the prior exa"
N18-1170,W17-4912,0,0.0524541,"Missing"
N18-1170,D17-1215,0,0.169957,"nguage processing datasets often suffer from a dearth of linguistic variation, which can hurt the generalization of models trained on them. Recent work has shown it is possible to easily “break” many learned models by evaluating them on adversarial examples (Goodfellow et al., 2015), which are generated by manually introducing lexical, pragmatic, and syntactic variation not seen in the training set (Ettinger et al., 2017). Robustness to such adversarial examples can potentially be improved by augmenting the training data, as shown by prior work that introduces rulebased lexical substitutions (Jia and Liang, 2017; FAuthors contributed equally. The man is standing in the water at the base of a waterfall Liang et al., 2017). However, more complex transformations, such as generating syntactically adversarial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the"
N18-1170,N10-1017,0,0.045833,"22.3 18.3 17.7 Table 1: A crowdsourced paraphrase evaluation on a three-point scale (0 = no paraphrase, 1 = ungrammatical paraphrase, 2 = grammatical paraphrase) shows both that NMT- BT and SCPN produce mostly grammatical paraphrases. Feeding parse templates to SCPN instead of full parses does not impact its quality. 4.1 Intrinsic Experiments w/ full parses w/ templates NMT- BT SCPN 2 Paraphrase quality & grammaticality To measure paraphrase quality and grammaticality, we perform a crowdsourced experiment in which workers are asked to rate a paraphrase pair hs, gi on the three-point scale of Kok and Brockett (2010), where s is the source sentence and g is the generated sentence. A 0 on this scale indicates no paraphrase relationship, while 1 means that g is an ungrammatical paraphrase of s and 2 means that g is a grammatical paraphrase of s. We select 100 paraphrase pairs from the development set of our PARA NMT-50M split (after the postprocessing steps detailed in Section 3.3) and have three workers rate each pair.7 To focus the evaluation on the effect of syntactic manipulation on quality, we minimum paraphrastic similarity to 0.7. 7 We use the Crowdflower platform for our experiments. 1878 only selec"
N18-1170,P16-1094,0,0.0138315,"is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effective way of generating adversarial examples. Using a parser, we"
N18-1170,J10-3003,0,0.0423198,"ency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this dataset. Shen et al. (2006) extend this to deeper syntactic analysis. All of these approaches use syntax to 7 Related Work Paraphrase generation (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) has been tackled using many different methods, including those based on hand-crafted rules (McKeown, 1983), synonym substitution (Bolshakov and Gelbukh, 2004), machine translation (Quirk et al., 2004), and, most recently, deep learning (Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017). Our syntactically controlled setting also relates to controlled language generation tasks in which one desires to generate or rewrite a sentence with particular characteristics. We review related work in both 13 A configuration without the copy mechanism copies input syntax even more, with a 47."
N18-1170,E17-1083,0,0.0546964,"d for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nematus NMT system (Sennrich et al., 2017). The training data of this system includes four sources: Common Crawl, CzEng 1.6, Europarl, and News Commentary. The CzEng corpus is the larges"
N18-1170,S14-2001,0,0.00800909,"he problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification proposed by Li et al. (2016b) for NMT- BT but found that it dr"
N18-1170,P06-2096,0,0.106453,"Missing"
N18-1170,J83-1001,0,0.445484,"rial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the target. General purpose syntactically controlled paraphrase generation is a challenging task. Approaches that rely on handcrafted rules and grammars, such as the question generation system of McKeown (1983), support only a limited number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exist"
N18-1170,N16-3013,0,0.122415,"Missing"
N18-1170,D17-1299,0,0.0213328,"Missing"
N18-1170,D13-1170,0,0.0143659,"hrases for adversarial example generation. To formalize the problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification pro"
N18-1170,N03-1024,0,0.454247,"Missing"
N18-1170,P17-3007,0,0.0644294,"Missing"
N18-1170,D14-1162,0,0.12426,"k and ignore all phrase-level labels (because our paraphrase models are trained on only sentences). Table 4 shows that for both datasets, SCPN breaks many more examples than NMT- BT. Moreover, as shown in Table 5, NMT- BT’s paraphrases differ from the original example mainly by lexical substitutions, while SCPN often produces dramatically different syntactic structures. 5.3 Are the adversarial examples valid? We have shown that we can break pretrained models with controlled paraphrases, but are these paraon the three-point scale. 10 We initialize both models using pretrained GloVe embeddings (Pennington et al., 2014) and set the LSTM hidden dimensionality to 300. 11 Since the SICK development dataset is tiny, we additionally generate adversarial examples on its test set. phrases actually valid adversarial examples? After all, it is possible that the syntactic modifications cause informative clauses or words (e.g., negations) to go missing. To measure the validity of our adversarial examples, we turn again to crowdsourced experiments. We ask workers to choose the appropriate label for a given sentence or sentence pair (e.g., positive or negative for SST), and then we compare the worker’s judgment to the or"
N18-1170,P15-1150,0,0.0186117,"ime comes . ” you said . can i get a good burglar when the time comes ? look at the time the thief comes . Table 3: Syntactically controlled paraphrases generated by SCPN for two examples from the PARA NMT-50M development set. For each input sentence, we show the outputs of four different templates; the fourth template is a failure case (highlighted in green) exhibiting semantic divergence and/or ungrammaticality, which occurs when the target template is unsuited for the input. for which to generate adversarial examples; we use the bidirectional LSTM baseline for both SST and SICK outlined in Tai et al. (2015) since it is a relatively simple architecture that has proven to work well for a variety of problems.10 Since the SICK task involves characterizing the relationship between two sentences, for simplicity we only generate adversarial examples for the first sentence and keep the second sentence fixed to the ground truth. 5.2 Breaking pretrained models For each dataset, we generate paraphrases for held-out examples and then run a pretrained model over them.11 We consider a development example x broken if the original prediction yx is correct, but the prediction yx0 for at least one paraphrase x0 i"
N18-1170,C16-1275,0,0.301174,"Missing"
N18-1170,D17-1228,0,0.0560075,"Missing"
N18-1170,W04-3219,0,0.398798,"paraphrase generation, noting two primary families: template-based and translationbased. The first family includes approaches that use hand-crafted rules (McKeown, 1983), thesaurus-based substitution (Bolshakov and Gelbukh, 2004; Zhang and LeCun, 2015), lattice matching (Barzilay and Lee, 2003), and templatebased “shake & bake” paraphrasing (Carl et al., 2005). These methods often yield grammatical outputs but they can be limited in diversity. The second family includes methods that rewrite the input using methods based on parallel text (Bannard and Callison-Burch, 2005), machine translation (Quirk et al., 2004; Napoles et al., 2016; Suzuki et al., 2017), or related statistical techniques (Zhao et al., 2009). Of particular relevance to our work are methods that incorporate syntax to improve fluency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this"
N18-1170,P17-1099,0,0.0157426,"either parse templates or full parses depending on their desired level of control. 3 Syntactically Controlled Paraphrase Networks The SCPN encoder-decoder architecture is built from standard neural modules, as we describe in this section. 3.1 Neural controlled paraphrase generation Given a sentential paraphrase pair hs1 , s2 i and a corresponding target syntax tree p2 for s2 , we encode s1 using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and our decoder is a two-layer LSTM augmented with soft attention over the encoded states (Bahdanau et al., 2014) as well as a copy mechanism (See et al., 2017). Following existing work in NMT (Sennrich et al., 2015), we preprocess s1 and s2 into subword units using byte pair encoding, and we perform decoding using beam search. For all attention computations, we use a bilinear product with a learned parameter matrix W: given vectors u and v, we score them by uT Wv. We incorporate the target syntax p2 into the generation process by modifying the inputs to the decoder. In particular, a standard decoder LSTM receives two inputs at every time step: (1) the embedding wt−1 of the ground-truth previous word in s2 , and (2) an attention-weighted average at o"
N18-1170,D17-1026,1,0.789198,"ed number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exists publicly, we follow Wieting et al. (2017) and automatically generate millions of paraphrase pairs using neural backtranslation. Backtranslation naturally injects linguistic variation between the original sentence and its backtranslated counterpart. By running the process at a very large scale and testing for the specific variations we want to produce, we can gather ample input-output pairs for a wide range of phenomena. Our focus is on syntactic transformations, which we define using templates derived from linearized constituency parses (§2). Given such parallel data, we can easily train an encoder-decoder model that takes a sentence"
N18-1170,E17-3017,0,0.0397283,"Missing"
N18-1170,D11-1038,0,0.00804775,"ation than existing uncontrolled paraphrase generation systems, instead preferring purely syntactic modifications. It is capable of generating adversarial examples that fool pretrained NLP models. Furthermore, by training on such examples, we increase the robustness of these models to syntactic variation. Controlled language generation There is growing interest in generating language with the ability to influence the topic, style, or other properties of the output. Most related to our methods are those based on syntactic transformations, like the tree-to-tree sentence simplification method of Woodsend and Lapata (2011) based on quasi-synchronous grammar (Smith and Eisner, 2006). Our method is more general since we do not require a grammar and there are only soft constraints. Perhaps the closest to the proposed method is the conditioned recurrent language model of Ficler and Goldberg (2017), which produces language with user-selected properties such as sentence length and formality but is incapable of generating paraphrases. For machine translation output, Niu et al. (2017) 8 Conclusion Acknowledgments We thank the reviewers for their insightful comments. We would also like to thank Mark Yatskar for many use"
N18-1170,N16-1005,0,0.00722262,"generator to produce viable full parses. improve grammaticality, which is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effect"
N18-1170,P09-1094,0,0.169719,"Missing"
N18-1170,W06-3104,0,\N,Missing
N18-1170,P14-5010,0,\N,Missing
N18-2116,D14-1181,0,0.0115152,"ntly outperforms other models under various size budgets. 2015), and the IMDB movie review dataset (Maas et al., 2011). We randomly sample 5,000 instances from the training set to use as development data for all datasets except for IMDB, where we sample 2,000. Table 1 shows dataset statistics. For IMDB, to make our results comparable to Shu and Nakayama (2018), we follow their experimental setup: We tokenize and lowercase the IMDB data using NLTK and truncate each review to be at most 400 words. For the other datasets, we lowercase and tokenize the sentences using regular expressions based on Kim (2014). For optimization, we use Adam (Kingma and Ba, 2015) with learning rate 0.001. Embedding matrices are randomly initialized for all models. To reduce the hyperparameter search space, the LSTM hidden vector size is set to 50 for all experiments and the Gumbel-Softmax temperature is fixed to 0.9. When a single result is reported, all other hyperparameters (vocabulary size v, embedding dimension m, number of clusters k, and number of unique vectors u) are tuned based on the development sets. Our code is implemented in TensorFlow (Abadi et al., 2015) and is available at https://github.com/mingdach"
N18-2116,P11-1015,0,0.0698539,"t arg max aij 0.05 1≤j≤k 0.10 0.15 (c) Yelp Full where the function one hot returns a one-hot vector of length k with a 1 in the index given by its argument. These argmax operations can be precomputed for all words in V , permitting us to discard the vk cluster probabilities and instead just store a cluster pointer for each word, each of which will only take O(log2 k) space. 0.20 0.05 0.10 0.15 0.20 (d) Yelp Polarity Figure 2: Development accuracy vs model size (MB) on four datasets. ME consistently outperforms other models under various size budgets. 2015), and the IMDB movie review dataset (Maas et al., 2011). We randomly sample 5,000 instances from the training set to use as development data for all datasets except for IMDB, where we sample 2,000. Table 1 shows dataset statistics. For IMDB, to make our results comparable to Shu and Nakayama (2018), we follow their experimental setup: We tokenize and lowercase the IMDB data using NLTK and truncate each review to be at most 400 words. For the other datasets, we lowercase and tokenize the sentences using regular expressions based on Kim (2014). For optimization, we use Adam (Kingma and Ba, 2015) with learning rate 0.001. Embedding matrices are rando"
N18-2116,D17-1309,0,0.0313803,"l size budgets. Our results demonstrate that clustering can maintain or improve performance while offering extremely small deployed models. 739 Proceedings of NAACL-HLT 2018, pages 739–745 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work rameters. In our experiments, we limit the vocabulary to various sizes v, always keeping the most frequent v words and replacing the rest with an unknown word symbol. Several methods have been proposed for reducing the memory requirements of models that use word embeddings. One is based on quantization (Botha et al., 2017; Han et al., 2016), which changes the way parameters are stored. In particular, it seeks to find shared weights among embedding vectors and only keeps scale factors for each word. Another family of methods uses hashing functions to replace dictionaries (Tito Svenstrup et al., 2017; Joulin et al., 2017). This can save storage space, but still requires the model to have roughly the same size embedding matrix. Network pruning has also been used to compress neural networks. Han et al. (2015) pruned weights iteratively by removing weights below a threshold and then retraining the network. Our work"
N18-2116,J92-4003,0,0.441784,"Missing"
N18-2116,D14-1162,0,0.107041,"Missing"
N19-1089,P16-1000,0,0.182494,"Missing"
N19-1089,D10-1049,0,0.0715998,"elated Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just sta"
N19-1089,W05-0909,0,0.0288252,"entified using word overlap). 4.2 Experiments We experiment with two types of encoder/decoder modules: bidirectional LSTMs, and transform831 ers (Vaswani et al., 2017). We use a vocabulary of size 50K, truncate the maximum input sequence length to 500, and use a batch size of 32 in all experiments. To help models distinguish between claims and context we demarcate claim fields with special &lt;claim&gt;, &lt;key&gt;, and &lt;value&gt; tokens. We train all the models for 150k steps, and evaluate on the validation dataset every 10k steps. Evaluation is performed using the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) translation metrics, and Precision, Recall and F1 score of the predicted bag-of-words (omitting stopwords). The model with the highest F1 score on the validation set is used during test time. and sentence lengths to examine the impact of the such variables. Discussion of Quantitative Results Our results contain a few key findings. The first is that knowing the relevant claims is critical to obtaining stateof-the-art performance; even knowing only oracle claims is sufficient to perform better than all of the other baselines, although there is a still a large improvement when context is additio"
N19-1089,H05-1042,0,0.0268991,"and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported"
N19-1089,W03-1016,0,0.0121642,"iers over the ones written by professional journalists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Targe"
N19-1089,P16-1154,0,0.053028,"ration We move our focus to the main task of postmodifier generation. 4.1 Attention Methods At its core, post-modifier generation involves producing a variable-length sequence output conditioned on two variable-length inputs: the words in the current and previous sentence (e.g. the context), and the collection of claims about the entity. Accordingly, the sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014) is a natural fit for the task — we use it as the foundation for all of our baseline models. Since research has shown that attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016) consistently improve seq2seq model performance, we use these in our baselines as well. One choice that must be made when using this framework is how to combine the different inputs. The default approach we use is to concatenate the claim and context into a linear sequence of tokens during preprocessing (shown in Figure 4a). We also experiment with encoding the claims and each of the context sentences separately, then concatenating their vector representations before decoding. We refer to this as the tri-encoder approach (shown in Figure 4b). As discussed earlier, selecting relevant claims is"
N19-1089,C10-2062,0,0.100165,"a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just started , but we actually"
N19-1089,P83-1022,0,0.302777,"ted post-modifiers over the ones written by professional journalists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 201"
N19-1089,D16-1128,0,0.0312256,"Missing"
N19-1089,P09-1011,0,0.015688,"of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s"
N19-1089,D11-1149,0,0.021912,"us work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just started , but we actually opened the seaso"
N19-1089,P14-5010,0,0.00397954,"es reliable ways to identify text to remove and sources of information that can be used to generate the text. Here we describe a pipeline for generating such a dataset for our task. 2.1 Dataset We construct the PoMo dataset using three different news corpora: NYTimes (Sandhaus, 2008), CNN and DailyMail (Hermann et al., 2015). We use Wikidata to collect facts about entities.2 2 1 CNN Wikidata dump from https://www.wikidata. org/wiki/Wikidata:Database_download (Dump date: 2018/06/25) https://stonybrooknlp.github.io/PoMo/ 827 2.1.1 Post-Modifier and Entity Identification We use Stanford CoreNLP (Manning et al., 2014) to parse each sentence in the news articles and to identify named entities. We extract post-modifiers by finding noun phrases that share an appos relation3 with any recognized named entity in the sentence. In this work, we only consider postmodifiers for people. In the future, we plan to expand PoMo to include more post-modifiers for other targets, such as organizations. We extract only one such pair from a given sentence to reduce the possible noise in the extraction process. In our running example from Figure 1, Noam Chomsky is recognized as a person entity. The word “professor” is an appos"
N19-1089,N16-1086,0,0.0160505,"ed relevant during dataset curation are prefaced with a +. In the first example, knowing the relevant claims helps the Oracle model produce an output that closely matches the Target, however lack of temporal information causes the model to miss the word former. In the second example, the All Claims and Oracle models produce the same post-modifier. Although it is similar to the Target in meaning, it receives a low score using our evaluation metrics. Futhermore, our data curation method fails to identify relevant claims. Modern approaches employ neural networks to solve this problem end-to-end. Mei et al. (2016) utilize an encoder-decoder framework to map weather conditions to a weather forecast. Ahn et al. (2016) and Yang et al. (2017) introduce a new class of language models which are capable of entity coreference and copying facts from an external knowledge base. Building upon these models, Wiseman et al. (2017) introduce an auxiliary reconstruction loss which use the hidden states of the decoder to recover the facts used to generate the text. Liu et al. (2018) introduce a hierarchical attention model for fact selection, with the higher level focusing on which records in the table to select and th"
N19-1089,P02-1040,0,0.10381,"ting (since relevant claims were identified using word overlap). 4.2 Experiments We experiment with two types of encoder/decoder modules: bidirectional LSTMs, and transform831 ers (Vaswani et al., 2017). We use a vocabulary of size 50K, truncate the maximum input sequence length to 500, and use a batch size of 32 in all experiments. To help models distinguish between claims and context we demarcate claim fields with special &lt;claim&gt;, &lt;key&gt;, and &lt;value&gt; tokens. We train all the models for 150k steps, and evaluate on the validation dataset every 10k steps. Evaluation is performed using the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) translation metrics, and Precision, Recall and F1 score of the predicted bag-of-words (omitting stopwords). The model with the highest F1 score on the validation set is used during test time. and sentence lengths to examine the impact of the such variables. Discussion of Quantitative Results Our results contain a few key findings. The first is that knowing the relevant claims is critical to obtaining stateof-the-art performance; even knowing only oracle claims is sufficient to perform better than all of the other baselines, although there is a still a lar"
N19-1089,N18-1012,0,0.0260431,"Missing"
N19-1089,P98-2209,0,0.199591,"lists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims"
N19-1089,N07-1022,0,0.00933399,"with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secret"
N19-1089,D17-1197,0,0.0252433,"model produce an output that closely matches the Target, however lack of temporal information causes the model to miss the word former. In the second example, the All Claims and Oracle models produce the same post-modifier. Although it is similar to the Target in meaning, it receives a low score using our evaluation metrics. Futhermore, our data curation method fails to identify relevant claims. Modern approaches employ neural networks to solve this problem end-to-end. Mei et al. (2016) utilize an encoder-decoder framework to map weather conditions to a weather forecast. Ahn et al. (2016) and Yang et al. (2017) introduce a new class of language models which are capable of entity coreference and copying facts from an external knowledge base. Building upon these models, Wiseman et al. (2017) introduce an auxiliary reconstruction loss which use the hidden states of the decoder to recover the facts used to generate the text. Liu et al. (2018) introduce a hierarchical attention model for fact selection, with the higher level focusing on which records in the table to select and the lower level focusing on which cells in a particular row to pay attention to. In order to train complex neural models, the que"
N19-1254,S14-2010,0,0.1692,"Missing"
N19-1254,S16-1081,0,0.105927,"Missing"
N19-1254,S12-1051,0,0.414197,"e the induced representations on both standard semantic similarity tasks and on several novel syntactic similarity tasks. We use a deep generative model consisting of von Mises Fisher (vMF) and Gaussian priors on the semantic and syntactic latent variables (respectively) and a deep bag-of-words decoder that conditions on these latent variables. Following much recent work, we learn this model by optimizing the ELBO with a VAE-like (Kingma and Welling, 2014; Rezende et al., 2014) approach. Our learned semantic representations are evaluated on the SemEval semantic textual similarity (STS) tasks (Agirre et al., 2012; Cer et al., 2017). Because there has been less work on evaluating syntactic representations of sentences, we propose several new syntactic evaluation tasks, which involve predicting the syntactic analysis of an unseen sentence to be the syntactic analysis of its nearest neighbor (as determined by the latent syntactic representation) in a large set of annotated sentences. In order to improve the quality and disentanglement of the learned representations, we incorporate simple additional losses in our training, which are designed to force the latent representations to capture different informa"
N19-1254,S13-1004,0,0.0835053,"Missing"
N19-1254,P17-2054,0,0.0332706,"As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic and syntactic information from sentence representations. To achieve this, we introduce the vMFGaussian Variational Autoencoder (VGVAE). As shown in Figure 1, VGVAE assumes a sentence is generated by conditioning on two independent variables: semantic variable y and syntactic variable z. In particular, our model gives ris"
N19-1254,W18-3403,0,0.0425585,"develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic and syntactic information from sentence representations. To achieve this, we introduce the vMFGaussian Variational Autoencoder (VGVAE). As shown in Figure 1, VGVAE assumes a sentence is generated by conditioning on two independent variables: semantic variable y and syntactic variable z. In particular, our model gives rise to the following join"
N19-1254,K16-1002,0,0.654965,"stigate the effect of moving from bagof-words to recurrent neural network modules. We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks. Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.1 1 Introduction As generative latent variable models, especially of the continuous variety (Kingma and Welling, 2014; Goodfellow et al., 2014), have become increasingly important in natural language processing (Bowman et al., 2016; Gulrajani et al., 2017), there has been increased interest in learning models where the latent representations are disentangled (Hu et al., 2017). Much of the recent NLP work on learning disentangled representations of text has focused on disentangling the representation of attributes such as sentiment from the representation of content, typically in an effort to better control text generation (Shen et al., 2017; Zhao et al., 2017; Fu et al., 2018). In this work, we instead focus on learning sentence representations that disentangle the syntax and the semantics of a sentence. We are moreover"
N19-1254,S17-2001,0,0.184506,"ntations on both standard semantic similarity tasks and on several novel syntactic similarity tasks. We use a deep generative model consisting of von Mises Fisher (vMF) and Gaussian priors on the semantic and syntactic latent variables (respectively) and a deep bag-of-words decoder that conditions on these latent variables. Following much recent work, we learn this model by optimizing the ELBO with a VAE-like (Kingma and Welling, 2014; Rezende et al., 2014) approach. Our learned semantic representations are evaluated on the SemEval semantic textual similarity (STS) tasks (Agirre et al., 2012; Cer et al., 2017). Because there has been less work on evaluating syntactic representations of sentences, we propose several new syntactic evaluation tasks, which involve predicting the syntactic analysis of an unseen sentence to be the syntactic analysis of its nearest neighbor (as determined by the latent syntactic representation) in a large set of annotated sentences. In order to improve the quality and disentanglement of the learned representations, we incorporate simple additional losses in our training, which are designed to force the latent representations to capture different information. In particular"
N19-1254,D18-1020,1,0.939359,"ork There is a growing amount of work on learning interpretable or disentangled latent representations both in machine learning (Tenenbaum and Freeman, 2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or s"
N19-1254,D10-1056,0,0.0152122,"ated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphrastic sentences, but our focus here is on learning disentangled representations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic"
N19-1254,D17-1070,0,0.0910589,"Missing"
N19-1254,D18-1354,0,0.0312758,"supervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic and syntactic information from sentence representations. To achieve this, we introduce the vMFGaussian Variational Autoencoder (VGVAE). As shown in Figure 1, VGVAE assumes a sentence is generated by conditioning on two independent variables: semantic variable y and syntactic variable z. In particular, our model gives rise to the following joint likelihood pθ (x, y, z) = pθ (y)pθ (z)pθ (x|y, z) = pθ (y)pθ (z) T Y p(xt |y, z), t=1 where xt is the tth word of x, T is the sentence length, and p(xt |y, z) is"
N19-1254,Q18-1031,0,0.0343211,"∼qφ (z|x) def − KL(qφ (y|x)kpθ (y)) == ELBO (1) 3.1 Parameterizations VGVAE uses two distribution families in defining the posterior over latent variables, namely, the von Mises-Fisher (vMF) distribution and the Gaussian distribution. vMF Distribution. vMF can be regarded as a Gaussian distribution on a hypersphere with two parameters: µ and κ. µ ∈ Rm is a normalized vector (i.e. kµk2 = 1 ) defining the mean direction. κ ∈ R≥0 is often referred to as a concentration parameter analogous to the variance in a Gaussian distribution. vMF has been used for modeling similarity between two sentences (Guu et al., 2018), which is particularly suited to our purpose here, since we will evaluate our semantic representations in the context of modeling paraphrases (See Sections 4.1 and 4.2 for more details). Therefore, we assume qφ (y|x) follows vMF(µα (x), κα (x)) and the prior pθ (y) follows the uniform distribution vMF(·, 0). With this choice of prior and posterior distribution, the KL(qφ (y|x)kpθ (y)) appearing in the ELBO can be computed in closed-form: κα Im/2 (κα ) + (m/2 − 1) log κα − Im/2−1 (κα ) (m/2) log(2π) − log Im/2−1 (κα )+ m m log π + log 2 − log Γ( ), 2 2 (2) i 2 In preliminary experiments, we ob"
N19-1254,N18-1170,1,0.814728,"o propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical dependencies (Mitchell, 2016), finding that decomposing syntactic and semantic information can lead to improved performance on semantic tasks. We find similar trends in our results, but at the level of sentence representations. A similar idea has been explored for text generation (Iyyer et al., 2018), where adversarial examples are generated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphrastic sentences, but our focus here is on learning disentangled representations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manni"
N19-1254,C88-1057,0,0.6919,"P applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical dependencies (Mitchell, 2016), finding that decomposing syntactic and semantic information can lead to improved performance on semantic tas"
N19-1254,P04-1061,0,0.13015,"r et al., 2018), where adversarial examples are generated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphrastic sentences, but our focus here is on learning disentangled representations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Propo"
N19-1254,P14-5010,0,0.00897471,"rmance given the training set. Since computing tree edit distance is time consuming, we subsample 100 test instances and compute the minimum tree edit distance for each sampled instance. Thus, this number can be seen as the approximated upper bound performance for this task given the training set. To use a more standard metric for these syntactic similarity tasks, we must be able to retrieve training examples with the same number of words as the sentence we are trying to parse. We accordingly parse and tag the five million paraphrase subset of the ParaNMT training data using Stanford CoreNLP (Manning et al., 2014). To form a test set, we group sentences in terms of sentence length and subsample 300 sentences for each sentence length. After removing the paraphrases of the sentences in the test set, we use the rest of the training set as candidate sentences for nearest neighbor search, and we restrict nearest neighbors to have the same sentence length as the sentence we are attempting to parse or tag, which allows us to use standard metrics like labeled F1 score and tagging accuracy for evaluation. Results As shown in Table 2, the syntactic variables and semantic variables demonstrate similar trends acro"
N19-1254,J93-2004,0,0.0645673,"e initial stages are of low quality. To overcome this issue, DPL is included starting at the second epoch of training so that the models can have a warm start. 6 Experiments 6.1 Setup as our training set. We use SemEval semantic textual similarity (STS) task 2017 (Cer et al., 2017) as a development set. For semantic similarity evaluation, we use the STS tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016) and the STS benchmark test set (Cer et al., 2017). For evaluating syntactic similarity, we propose several evaluations. One uses the gold parse trees from the Penn Treebank (Marcus et al., 1993), and the others are based on automatically tagging and parsing five million paraphrases from ParaNMT50M; we describe these tasks in detail below. For hyperparameters, the dimensions of the latent variables are 50. The dimensions of word embeddings are 50. We use cosine similarity as similarity metric for all of our experiments. We tune the weights for PRL and reconstruction loss from 0.1 to 1 in increments of 0.1 based on the development set performance. We use one sample from each latent variable during training. When evaluating VGVAE based models on STS tasks, we use the mean direction of t"
N19-1254,W16-1615,0,0.0225748,"syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical dependencies (Mitchell, 2016), finding that decomposing syntactic and semantic information can lead to improved performance on semantic tasks. We find similar trends in our results, but at the level of sentence representations. A similar idea has been explored for text generation (Iyyer et al., 2018), where adversarial examples are generated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphrastic sentences, but our focus here is on le"
N19-1254,P15-1126,0,0.0207986,"Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical dependencies (Mitchell, 2016), finding that decomposing syntactic and semantic information can lead to improved performance on semantic tasks. We find similar trends in our results, but at the level of sentence representations. A similar idea has been explored for text generation (Iyyer et al., 2018), where adversarial examples are generated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphr"
N19-1254,D14-1162,0,0.0899817,"M AVG uses the averaged hidden states of a bidirectional LSTM as the sentence representation, where forward and backward hidden states are concatenated. These models use 50 dimensional word embeddings and 50 dimensional LSTM hidden vectors per direction. These baselines are trained with DPL only. Additionally, we scramble the input sentence for BLSTM AVG since it has been reported beneficial for its performance in semantic similarity tasks (Wieting and Gimpel, 2017). We also benchmark several pretrained embeddings on both semantic similarity and syntactic similarity datasets, including GloVe (Pennington et al., 2014),3 SkipThought (Kiros et al., 2015),4 3 We subsampled half a million paraphrase pairs from ParaNMT-50M (Wieting and Gimpel, 2018) We use 300 dimensional Common Crawl embeddings available at nlp.stanford.edu/projects/glove 4 github.com/ryankiros/skip-thoughts 2457 semantic var. bm avg GloVe 39.0 48.7 SkipThought 42.1 42.0 67.8 61.0 InferSent ELMo 57.7 60.3 BERT 4.5 15.0 W ORDAVG 71.9 64.8 BLSTM AVG 71.4 64.4 VGVAE 45.5 42.7 51.5 49.3 VGVAE + WPL VGVAE + DPL 68.4 58.2 67.9 57.8 VGVAE + PRL VGVAE + PRL + WPL 69.8 61.3 VGVAE + PRL + DPL 71.2 64.2 71.0 63.5 VGVAE + DPL + WPL ALL 72.3 65.1 72.5 65.1"
N19-1254,N18-1202,0,0.150896,"Missing"
N19-1254,P16-2067,0,0.0346358,"disentangled representations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic and syntactic information from sentence representations. To achieve this, we introduce the vMFGaussian Variational Autoencoder (VGVAE). As shown in Figure 1, VGVAE assumes a sentence is generated by conditioning on two independent variables: semantic variable y and syntactic varia"
N19-1254,P17-1194,0,0.0264324,"entations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic representations to choose nearest neighbors. There is a great deal of work on applying multitask learning to various NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia) and, recently, as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). 3 Proposed Approach Our goal is to extract the disentangled semantic and syntactic information from sentence representations. To achieve this, we introduce the vMFGaussian Variational Autoencoder (VGVAE). As shown in Figure 1, VGVAE assumes a sentence is generated by conditioning on two independent variables: semantic variable y and syntactic variable z. In p"
N19-1254,W17-4308,0,0.0598036,"nenbaum and Freeman, 2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedma"
N19-1254,D18-1356,1,0.822729,"lid lines indicate generative model. Related Work There is a growing amount of work on learning interpretable or disentangled latent representations both in machine learning (Tenenbaum and Freeman, 2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads lear"
N19-1254,D18-1480,0,0.0284075,"2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the extent to which this knowledge leads learned representations to favor syntactic or semantic information from a sentence. There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical"
N19-1254,P18-1070,0,0.0191367,"d lines indicate inference model. Solid lines indicate generative model. Related Work There is a growing amount of work on learning interpretable or disentangled latent representations both in machine learning (Tenenbaum and Freeman, 2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particular multi-task losses and measure the e"
N19-1254,P17-1061,0,0.155064,"els, especially of the continuous variety (Kingma and Welling, 2014; Goodfellow et al., 2014), have become increasingly important in natural language processing (Bowman et al., 2016; Gulrajani et al., 2017), there has been increased interest in learning models where the latent representations are disentangled (Hu et al., 2017). Much of the recent NLP work on learning disentangled representations of text has focused on disentangling the representation of attributes such as sentiment from the representation of content, typically in an effort to better control text generation (Shen et al., 2017; Zhao et al., 2017; Fu et al., 2018). In this work, we instead focus on learning sentence representations that disentangle the syntax and the semantics of a sentence. We are moreover interested in disentangling these representa1 Code and data are available at github.com/ mingdachen/disentangle-semantics-syntax tions not for the purpose of controlling generation, but for the purpose of calculating semantic or syntactic similarity between sentences (but not both). To this end, we propose a generative model of a sentence which makes use of both semantic and syntactic latent variables, and we evaluate the induced r"
N19-1254,P17-1029,0,0.191749,"Figure 1: Graphical model of VGVAE. Dashed lines indicate inference model. Solid lines indicate generative model. Related Work There is a growing amount of work on learning interpretable or disentangled latent representations both in machine learning (Tenenbaum and Freeman, 2000; Reed et al., 2014; Makhzani et al., 2015; Mathieu et al., 2016; Higgins et al., 2016; Chen et al., 2016; Hsu et al., 2017) and in various NLP applications, including sentence sentiment and style transfer (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018, inter alia), morphological reinflection (Zhou and Neubig, 2017), semantic parsing (Yin et al., 2018), text generation (Wiseman et al., 2018), and sequence labeling (Chen et al., 2018). Another related thread of work is text-based variational autoencoders (Miao et al., 2016; Bowman et al., 2016; Serban et al., 2017; Xu and Durrett, 2018). In terms of syntax and semantics in particular, there is a rich history of work in analyzing their interplay in sentences (Jurafsky, 1988; van Valin, Jr., 2005). We do not intend to claim that the two can be entirely disentangled in distinct representations. Rather, our goal is to propose modica of knowledge via particula"
N19-1254,P17-1190,1,0.866956,"ting and Gimpel, 2018). Specifically, W ORDAVG takes the average over the word embeddings in the input sequence to obtain the sentence representation. BLSTM AVG uses the averaged hidden states of a bidirectional LSTM as the sentence representation, where forward and backward hidden states are concatenated. These models use 50 dimensional word embeddings and 50 dimensional LSTM hidden vectors per direction. These baselines are trained with DPL only. Additionally, we scramble the input sentence for BLSTM AVG since it has been reported beneficial for its performance in semantic similarity tasks (Wieting and Gimpel, 2017). We also benchmark several pretrained embeddings on both semantic similarity and syntactic similarity datasets, including GloVe (Pennington et al., 2014),3 SkipThought (Kiros et al., 2015),4 3 We subsampled half a million paraphrase pairs from ParaNMT-50M (Wieting and Gimpel, 2018) We use 300 dimensional Common Crawl embeddings available at nlp.stanford.edu/projects/glove 4 github.com/ryankiros/skip-thoughts 2457 semantic var. bm avg GloVe 39.0 48.7 SkipThought 42.1 42.0 67.8 61.0 InferSent ELMo 57.7 60.3 BERT 4.5 15.0 W ORDAVG 71.9 64.8 BLSTM AVG 71.4 64.4 VGVAE 45.5 42.7 51.5 49.3 VGVAE + W"
N19-1254,P18-1042,1,0.913391,"There has been prior work with similar goals for representations of words (Mitchell and Steedman, 2015) and bilexical dependencies (Mitchell, 2016), finding that decomposing syntactic and semantic information can lead to improved performance on semantic tasks. We find similar trends in our results, but at the level of sentence representations. A similar idea has been explored for text generation (Iyyer et al., 2018), where adversarial examples are generated by controlling syntax. Some of our losses use sentential paraphrases, relating them to work in paraphrase modeling (Wieting et al., 2016; Wieting and Gimpel, 2018). Deudon (2018) recently proposed a variational framework for modeling paraphrastic sentences, but our focus here is on learning disentangled representations. As part of our evaluation, we develop novel syntactic similarity tasks for sentence representations learned without any syntactic supervision. These evaluations relate to the broad range of work in unsupervised parsing (Klein and Manning, 2004) and part-of-speech tagging (Christodoulopoulos et al., 2010). However, our evaluations differ from previous evaluations in that we employ k-nearestneighbor syntactic analyzers using our syntactic"
N19-1335,D18-1020,1,0.838997,". The benefits of inference networks may be coming in part from multi-task training; Edunov et al. (2018) similarly found benefit from combining tokenlevel and sequence-level losses. We focused on structured prediction in this paper, but inference networks are useful in other settings as well. For example, it is common to use a particular type of inference network to approximate posterior inference in neural approaches to latent-variable probabilistic modeling, such as variational autoencoders (Kingma and Welling, 2013) and, more closely related to this paper, variational sequential labelers (Chen et al., 2018). In such settings, Kim et al. (2018) have found benefit with instance-specific updating of inference network parameters, which is related to our instancelevel fine-tuning. There are also connections between structured inference networks and amortized structured inference (Srikumar et al., 2012) as well as methods for neural knowledge distillation and model compression (Hinton et al., 2015; Ba and Caruana, 2014; Kim and Rush, 2016). Gradient descent is used for inference in several settings, e.g., structured prediction energy networks (Belanger and McCallum, 2016), image generation application"
N19-1335,D17-1014,0,0.167088,"one and then use gradient descent to solve the following optimization problem: argmin EΘ (x, y) y∈YR (x) where YR is the relaxed continuous output space. For sequence labeling, YR (x) consists of length|x |sequences of probability distributions over output labels. To obtain a discrete labeling for evaluation, the most probable label at each position is returned. There are multiple settings in which gradient descent has been used for structured inference, e.g., image generation (Johnson et al., 2016), structured prediction energy networks (Belanger and McCallum, 2016), and machine translation (Hoang et al., 2017). Gradient descent has the advantage of simplicity. Standard autodifferentiation toolkits can be used to compute gradients of the energy with respect to the output once the output space has been relaxed. However, one challenge is maintaining constraints on the variables being optimized. Therefore, we actually perform gradient descent in an even more relaxed output space YR′ (x) which consists of length-|x |sequences of vectors, where each vector yt ∈ RL . When computing the energy, we use a softmax transformation on each yt , solving the following optimization problem with gradient descent: ar"
N19-1335,hockenmaier-steedman-2002-acquiring,0,0.132917,"Missing"
N19-1335,N18-1033,0,0.135381,"well, but are outperformed slightly by seq2seq models across all three tasks. Using the Viterbi algorithm for exact inference yields the best performance for NER but is not best for the other two tasks. It may be surprising that an inference network trained to mimic Viterbi would outperform Viterbi in terms of accuracy, which we find for the CNN for POS tagging and the seq2seq inference network for CCG. We suspect this occurs for two reasons. One is due to the addition of the local loss in the inference network objective; the inference networks may be benefiting from this multi-task training. Edunov et al. (2018) similarly found benefit from a combination of token-level and sequence-level losses. The other potential reason is beneficial inductive bias with the inference network architecture. For POS tagging, the CNN architecture is clearly well-suited to this task given the strong performance of the local CNN baseline. Nonetheless, the CNN inference network is able to improve upon both the CNN baseline and Viterbi. 86 84 Baseline(CNNs) Baseline(BLSTM) Baseline(seq2seq) InfNet(CNNs) InfNet(BLSTM) InfNet(seq2seq) 91.5 91 90.5 90 82 89.5 80 0 100 200 89 300 200 400 Hidden size 600 800 1000 Hidden size (a"
N19-1335,P14-1062,0,0.0279142,"ils on how ℓtoken is defined for different inference network families below. It is also the loss used in our non-structured baseline models. 4.1 Inference Network Architectures We now describe options for inference network architectures for sequence labeling. For each, we optionally include the modeling improvements described in Section 2.1. When doing so, we append “+” to the setting’s name to indicate this (e.g., infnet+). 4.1.1 Convolutional Neural Networks CNNs are frequently used in NLP to extract features based on symbol subsequences, whether words or characters (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; 3315 Kim et al., 2016; Zhang et al., 2015). CNNs use filters that are applied to symbol sequences and are typically followed by some sort of pooling operation. We apply filters over a fixed-size window centered on the word being labeled and do not use pooling. The feature maps fn (x, t) for (2n + 1)gram filters are defined: fn (x, t) = g(Wn [vxt−n ; ...; vxt+n ] + bn ) where g is a nonlinearity, vxt is the embedding of word xt , and Wn and bn are filter parameters. We consider two CNN configurations: one uses n = 0 and n = 1 and the other uses n = 0 and n = 2. For each, we concate"
N19-1335,D14-1181,0,0.00316949,"d for different inference network families below. It is also the loss used in our non-structured baseline models. 4.1 Inference Network Architectures We now describe options for inference network architectures for sequence labeling. For each, we optionally include the modeling improvements described in Section 2.1. When doing so, we append “+” to the setting’s name to indicate this (e.g., infnet+). 4.1.1 Convolutional Neural Networks CNNs are frequently used in NLP to extract features based on symbol subsequences, whether words or characters (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; 3315 Kim et al., 2016; Zhang et al., 2015). CNNs use filters that are applied to symbol sequences and are typically followed by some sort of pooling operation. We apply filters over a fixed-size window centered on the word being labeled and do not use pooling. The feature maps fn (x, t) for (2n + 1)gram filters are defined: fn (x, t) = g(Wn [vxt−n ; ...; vxt+n ] + bn ) where g is a nonlinearity, vxt is the embedding of word xt , and Wn and bn are filter parameters. We consider two CNN configurations: one uses n = 0 and n = 1 and the other uses n = 0 and n = 2. For each, we concatenate the tw"
N19-1335,P11-2008,1,0.78127,"Missing"
N19-1335,D16-1139,0,0.0498683,"ariable probabilistic modeling, such as variational autoencoders (Kingma and Welling, 2013) and, more closely related to this paper, variational sequential labelers (Chen et al., 2018). In such settings, Kim et al. (2018) have found benefit with instance-specific updating of inference network parameters, which is related to our instancelevel fine-tuning. There are also connections between structured inference networks and amortized structured inference (Srikumar et al., 2012) as well as methods for neural knowledge distillation and model compression (Hinton et al., 2015; Ba and Caruana, 2014; Kim and Rush, 2016). Gradient descent is used for inference in several settings, e.g., structured prediction energy networks (Belanger and McCallum, 2016), image generation applications (Mordvintsev et al., 2015; Gatys et al., 2015), finding adversarial examples (Goodfellow et al., 2015), learning paragraph embeddings (Le and Mikolov, 2014), and machine translation (Hoang et al., 2017). Gradient descent has started to be replaced by inference networks in some of these settings, such as image transformation (Johnson et al., 2016; Li and Wand, 2016). Our results provide more evidence that gradient descent can be r"
N19-1335,N16-1030,0,0.760425,"networks and gradient descent, using the former to provide a warm start for the latter.1 1 Introduction Structured prediction models commonly involve complex inference problems for which finding exact solutions is intractable (Cooper, 1990). There are generally two ways to address this difficulty. One is to restrict the model family to those for which inference is feasible. For example, state-ofthe-art methods for sequence labeling use structured energies that decompose into label-pair potentials and then use rich neural network architectures to define the potentials (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016, inter alia). Exact dynamic programming algorithms like the Viterbi algorithm can be used for inference. 1 Code is available at github.com/lifu-tu/ BenchmarkingApproximateInference The second approach is to retain computationally-intractable scoring functions but then use approximate methods for inference. For example, some researchers relax the structured output space from a discrete space to a continuous one and then use gradient descent to maximize the score function with respect to the output (Belanger and McCallum, 2016). Another approach is to train a neural network ("
N19-1335,N13-1039,1,0.648333,"Missing"
N19-1335,D14-1162,0,0.0807797,"Missing"
N19-1335,D12-1102,0,0.0293941,"Missing"
N19-1335,D17-1283,0,0.0302302,"aster because it does not require updating inference network parameters. 8 Related Work The most closely related prior work is that of Tu and Gimpel (2018), who experimented with RNN inference networks for sequence labeling. We compared three architectural families, showed the relationship between optimal architectures and downstream tasks, compared inference networks to gradient descent, and proposed novel variations. We focused in this paper on sequence labeling, in which CRFs with neural network potentials have emerged as a state-of-the-art approach (Lample et al., 2016; Ma and Hovy, 2016; Strubell et al., 2017; Yang et al., 2018). Our results suggest that inference networks can provide a feasible way to speed up test-time inference over Viterbi without much loss in performance. The benefits of inference networks may be coming in part from multi-task training; Edunov et al. (2018) similarly found benefit from combining tokenlevel and sequence-level losses. We focused on structured prediction in this paper, but inference networks are useful in other settings as well. For example, it is common to use a particular type of inference network to approximate posterior inference in neural approaches to late"
N19-1335,D15-1166,0,0.0605993,"engio et al., 2015) during training because it works better for our tasks. In our experiments, the forward and backward encoder LSTMs use hidden dimension H, as does the LSTM decoder. Thus the model becomes similar to the BLSTM tagger except with conditioning on previous labeling decisions in a left-to-right manner. We also experimented with the use of beam search for both the seq2seq baseline and inference networks and did not find much difference in the results. Also, as alternatives to the deterministic position-based attention described above, we experimented with learned local attention (Luong et al., 2015) and global attention, but they did not work better on our tasks. 4.2 Methods to Improve Inference Networks To further improve the performance of an inference network for a particular test instance x, we propose two novel approaches that leverage the strengths of inference networks to provide effective starting points and then use instance-level fine-tuning in two different ways. 4.1.3 Sequence-to-Sequence Models Sequence-to-sequence (seq2seq; Sutskever et al. 2014) models have been successfully used for many sequential modeling tasks. It is common to augment models with an attention mechanism"
N19-1335,P16-1101,0,0.462926,"t descent, using the former to provide a warm start for the latter.1 1 Introduction Structured prediction models commonly involve complex inference problems for which finding exact solutions is intractable (Cooper, 1990). There are generally two ways to address this difficulty. One is to restrict the model family to those for which inference is feasible. For example, state-ofthe-art methods for sequence labeling use structured energies that decompose into label-pair potentials and then use rich neural network architectures to define the potentials (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016, inter alia). Exact dynamic programming algorithms like the Viterbi algorithm can be used for inference. 1 Code is available at github.com/lifu-tu/ BenchmarkingApproximateInference The second approach is to retain computationally-intractable scoring functions but then use approximate methods for inference. For example, some researchers relax the structured output space from a discrete space to a continuous one and then use gradient descent to maximize the score function with respect to the output (Belanger and McCallum, 2016). Another approach is to train a neural network (an “inference netwo"
N19-1335,W17-2632,1,0.791786,"Missing"
N19-1335,N16-1025,0,0.0476909,"Missing"
P11-2008,C10-2005,0,0.591162,"toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T"
P11-2008,W10-0713,0,0.446157,"enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate t"
P11-2008,J93-2004,0,0.0664899,"erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. In this paper, we produce an English POS tagger that is designed especially for Twitter data. Our contributions are as follows: • we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we prov"
P11-2008,petrov-etal-2012-universal,1,0.297019,"Missing"
P11-2008,N10-1020,0,0.140487,"yD licenseN and& #2$ notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a"
P11-2008,N10-1100,0,0.180483,"notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour"
P11-2008,N03-1033,0,0.210842,"Missing"
P11-2008,P10-1040,0,0.263127,"butional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ USVT , where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. M ETAPH : Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7 1 α = 100 , C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8 Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (War"
P14-2131,P11-2125,0,0.0198849,"Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP, w = 10 S KIP DEP S IM"
P14-2131,W13-3520,0,0.0657293,"Missing"
P14-2131,P14-2133,0,0.431574,"Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (T URIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (1[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently, Andreas and Klein (2014) investigate the use of embeddings in constituent parsing. There are several differences: we work on dependency parsing, use clustering-based features, and tailor our embeddings to dependency-style syntax; their work additionally studies vocabulary expansion and relating in-vocabulary words via embeddings. Web results: Table 6 shows our main Web results.12 Here, we see that the S ENNA, B ROWN, and S KIP DEP embeddings perform the best on average (and are statistically indistinguishable, except S ENNA vs. S KIP DEP on the reviews domain). They yield statistically significant UAS improvements ov"
P14-2131,P12-1092,0,0.796086,"pendency Parsing Mohit Bansal Kevin Gimpel Karen Livescu Toyota Technological Institute at Chicago, IL 60637, USA {mbansal, kgimpel, klivescu}@ttic.edu Abstract tinuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To th"
P14-2131,J14-1004,0,0.184327,"rsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and"
P14-2131,J92-4003,0,0.294466,"e of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from con2 Continuous Word Representations There are many ways to train continuous representations; in this paper, we"
P14-2131,W07-2416,0,0.0430778,"s, specifically its secondorder projective model.9 We remove all features that occur only once in the training data. For WSJ parsing, we use the standard train(0221)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use Comparing bucket and bit string features: In Table 4, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do. This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously. Their prefixes also naturally define features at multiple levels o"
P14-2131,P08-1068,0,0.907992,"both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time. Finally, a simple parser ensemble on all the representations achieves the best results, suggesting their complementarity for dependency parsing. Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare sever"
P14-2131,P09-1116,0,0.0387301,"n MSTParser, using bit string prefixes of the head, argument, sibling, intermediate words, etc., to augment or replace the POS and lexical identity information. We tried various sets of prefix lengths on the development set and found the best setting to use prefixes of length 4, 6, 8, and 12.5 3.2 Continuous Representation Features We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding 4 A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). 5 See Koo et al. (2008) for the exact feature templates. They used the full string in place of the length-12 prefixes, but that setting worked slightly worse for us. Note that the baseline parser used by Koo et al. (2008) is different from the second-order MSTParser that we use here; their parser allows grandparent interactions in addition to the sibling interactions in ours. We use their clusters, available at http://people. 3 For clustering, we use k-means with k = 1000 and initialize by placing centroids on the 1000 most-frequent words. csail.mit.edu/maestro/papers/bllip-clusters.gz. 811"
P14-2131,J93-2004,0,0.0490206,"ontinuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on al"
P14-2131,E06-1011,0,0.123145,"imilarity (S IM): One widely-used evaluation compares distances in the continuous space to human judgments of word similarity using the 353-pair dataset of Finkelstein et al. (2002). We compute cosine similarity between the two vectors in each word pair, then order the word pairs by similarity and compute Spearman’s rank correlation coefficient (ρ) with the gold similarities. Embeddings with high ρ capture similarity in terms of paraphrase and topical relationships. 3 Dependency Parsing Features We now discuss the features that we add to our baseline dependency parser (second-order MSTParser; McDonald and Pereira, 2006) based on discrete and continuous representations. 3.1 Clustering-based tagging accuracy (M-1): Intuitively, we expect embeddings to help parsing the most if they can tell us when two words are similar syntactically. To this end, we use a metric based on unsupervised evaluation of POS taggers. We perform clustering and map each cluster to one POS tag so as to maximize tagging accuracy, where multiple clusters can map to the same tag. We cluster vectors corresponding to the tokens in PTB WSJ sections 00-21.3 Table 2 shows these metrics for representations used in this paper. The B ROWN clusters"
P14-2131,P08-1109,0,0.0313486,"Missing"
P14-2131,S13-1035,0,0.00746473,"we parse the BLLIP corpus (minus PTB) using our baseline dependency parser, then build a corpus in which each line contains a single child word c, its parent word p, its grandparent g, and the dependency label ` of the hc, pi link: “`&lt;L&gt; g&lt;G&gt; p c `&lt;L&gt; ”, that is, both the dependency label and grandparent word are subscripted with a special token to avoid collision with words.2 We train the S KIP model on this corpus of tuples with window size w = 1, denoting the result S KIP DEP . Note that this approach needs a parsed corpus, but there also already exist such resources (Napoles et al., 2012; Goldberg and Orwant, 2013). Syntactically-tailored Representations We train word embeddings using the continuous bag-of-words (CBOW) and skip-gram (S KIP) models described in Mikolov et al. (2013a; 2013b) as implemented in the open-source toolkit word2vec. These models avoid hidden layers in the neural network and hence can be trained in only minutes, compared to days or even weeks for the others, as shown in Table 1.1 We adapt these embeddings to be more useful for dependency parsing in two ways, described next. 2.1.1 Smaller Context Windows The CBOW model learns vectors to predict a word given its set of surrounding"
P14-2131,N04-1043,0,0.0744049,"Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester e"
P14-2131,W13-3511,0,0.0352405,"of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP, w = 10 S KIP DEP S IM – 49.8 29.5 62.6 34.7"
P14-2131,W03-3023,0,0.105048,"ith real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well. 8 We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set. 9 We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopunc decode-type:proj 10 Our setup is different from SANCL 2012 (Petrov and McDonald, 2012) because the exact splits and test data were only available to participants. 11 We find similar improvements under labeled attachment score (LAS). We ignore punctuation : , “ ” . in our evaluation (Yamada and Matsumoto, 2003; McDonald et al., 2005). 812 System Baseline B ROWN S ENNA T URIAN H UANG CBOW S KIP S KIP DEP ans 82.6 83.4 83.7 83.0 83.1 82.9 83.1 83.3 eml nwg rev blog Avg 81.2 84.3 83.8 85.5 83.5 81.7 85.2 84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average."
P14-2131,W12-3018,0,0.0120023,"Missing"
P14-2131,N13-1039,1,0.269497,"Missing"
P14-2131,J07-2002,0,0.0898319,"Missing"
P14-2131,W09-1119,0,0.013559,"84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, H"
P14-2131,W96-0213,0,0.172904,"at occur only once in the training data. For WSJ parsing, we use the standard train(0221)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use Comparing bucket and bit string features: In Table 4, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do. This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously. Their prefixes also naturally define features at multiple levels of granularity. WSJ results: Table 5 shows our main WSJ results. Alt"
P14-2131,D11-1141,0,0.0144767,"2.9 83.1 83.3 eml nwg rev blog Avg 81.2 84.3 83.8 85.5 83.5 81.7 85.2 84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 201"
P14-2131,W09-3829,0,0.0230055,"Intrinsic Evaluation of Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP,"
P14-2131,D11-1118,0,0.0282083,"Missing"
P14-2131,N12-1052,0,0.0728699,"Missing"
P14-2131,D11-1116,0,0.0124489,"ts ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (T URIA"
P14-2131,P10-1040,0,0.913842,"loring Continuous Word Representations for Dependency Parsing Mohit Bansal Kevin Gimpel Karen Livescu Toyota Technological Institute at Chicago, IL 60637, USA {mbansal, kgimpel, klivescu}@ttic.edu Abstract tinuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in te"
P14-2131,P07-1031,0,\N,Missing
P15-2115,P98-1013,0,0.340159,"efine a frame semantic parse as a tuple hT, F, R, Li. We define six features based on two parsed sentences hT 1 , F 1 , R1 , L1 i and hT 2 , F 2 , R2 , L2 i: • f1 : # frame label matches: |{hs, ti : s ∈ F 1 , t ∈ F 2 , s = t}| • f2 : # argument label matches: |{hs, ti : s ∈ L1 , t ∈ L2 , s = t}|. • f3 : # target matches, ignoring frame labels: |{hs, ti : s ∈ T 1 , t ∈ T 2 , s = t}|. Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three frames are identified. The target words pulled, all, and shelves have respective frame labels C AUSE MOTION, Q UANTITY, and NATU - • f4 : # argument matches, ignoring arg."
P15-2115,N10-1138,0,0.00955749,"es B+D+F B+D+F+S Bwc + D + Fc + Swc Experiments MCTest splits its stories into train, development, and test sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer λ and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts w and c to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Noce"
P15-2115,P14-2131,1,0.646995,"stion, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector fw+ as the vector summation of all words inside sentence w and fw× as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concate+ and f × . For the nate q and a, then calculate fqa qa bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use + , f + ) and cos(f × , f × ) as features, where cos(fqa w qa w cos is cosine similarity. For syntactic features, where τw is the bag of dependencies of w and τq"
P15-2115,D13-1160,0,0.0111299,"achan et al., 2015). We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic te"
P15-2115,J14-1002,0,0.0681273,"ed with an argument label in the parse. We denote the bag of argument labels in the parse by L. For each phrase r ∈ R, there is an argument label denoted Lr ∈ L. We define a frame semantic parse as a tuple hT, F, R, Li. We define six features based on two parsed sentences hT 1 , F 1 , R1 , L1 i and hT 2 , F 2 , R2 , L2 i: • f1 : # frame label matches: |{hs, ti : s ∈ F 1 , t ∈ F 2 , s = t}| • f2 : # argument label matches: |{hs, ti : s ∈ L1 , t ∈ L2 , s = t}|. • f3 : # target matches, ignoring frame labels: |{hs, ti : s ∈ T 1 , t ∈ T 2 , s = t}|. Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three fram"
P15-2115,N10-1145,0,0.0124901,"= a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the"
P15-2115,D14-1067,0,0.019125,"We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroo"
P15-2115,W01-1201,0,0.0210934,"Missing"
P15-2115,P99-1042,0,0.537395,"Missing"
P15-2115,D14-1082,0,0.0279241,"t sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer λ and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts w and c to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond D"
P15-2115,J13-4004,0,0.0239521,"Missing"
P15-2115,W14-2416,0,0.0265491,"Missing"
P15-2115,P07-1098,0,0.0294712,"ollowing three categories: (1) v = r and u = a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent eac"
P15-2115,P15-1121,0,0.360924,"Missing"
P15-2115,D13-1020,0,0.627704,"t vector θ with an entry for each feature, the prediction a ˆ for a new P and q is given by: Figure 1: Example output from SEMAFOR. a ˆ = arg max max θ&gt; f (P, w, q, a) RAL FEATURES . a∈A w∈W Given triples {hP i , q i , ai i}ni=1 , we minimize an `2 -regularized max-margin loss function: n  X min λ||θ||2 + − max θ&gt; f (P i , w, q i , ai ) θ i=1  + max a∈A &gt; w∈W i 0 i i  max θ f (P , w , q , a) + ∆(a, a ) w0 ∈W where λ is the weight of the `2 term and ∆(a, ai ) = 1 if a 6= ai and 0 otherwise. The latent variable w makes the loss function non-convex. 3 Features We start with two features from Richardson et al. (2013). Our first feature corresponds to their sliding window similarity baseline, which measures weighted word overlap between the bag of words constructed from the question/answer and the bag of words in the window. We call this feature B. The second feature corresponds to their word distance baseline, and is the minimal distance between two word occurrences in the passage that are also contained in the question/answer pair. We call this feature D. Space does not permit a detailed description. 3.1 Each frame has its own set of arguments; e.g., the C AUSE MOTION frame has the labeled Agent, Theme,"
P15-2115,P15-1024,0,0.43581,"Missing"
P15-2115,P10-1040,0,0.0293802,"let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector fw+ as the vector summation of all words inside sentence w and fw× as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concate+ and f × . For the nate q and a, then calculate fqa qa bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use + , f + ) and cos(f × , f × ) as features, where cos(fqa w qa w cos is cosine similarity. For syntactic features, where τw is the"
P15-2115,D07-1003,0,0.00989106,"s: (1) v = r and u = a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dim"
P15-2115,C98-1013,0,\N,Missing
P16-1137,W13-3515,0,0.586494,"ould be used as terms. To expand frame relationships in FrameNet, tuples can draw relations from the frame relation types (e.g., “is causative of”) and terms can be frame lexical units or their definitions. Several researchers have used commonsense knowledge to improve language technologies, including sentiment analysis (Cambria et al., 2012; Agarwal et al., 2015), semantic similarity (Caro et al., 2015), and speech recognition (Lieberman et al., 2005). Our hope is that our models can enable many other NLP applications to benefit from commonsense knowledge. Our work is most similar to that of Angeli and Manning (2013). They also developed methods to assess the plausibility of new facts based on a training set of facts, considering commonsense data from ConceptNet in one of their settings. Like us, they can handle an unbounded set of terms by using (simple) composition functions for novel terms, which is rare among work in KBC. One key difference is that their best method requires iterating over the KB at test time, which can be computationally expensive with large KBs. Our models do not require iterating over the training set. We compare to several baselines inspired by their work, and we additionally eval"
P16-1137,D14-1059,0,0.146072,"z et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Manning, 2014), while others have developed curated resources of commonsense knowledge via manual annotation (Lenat and Guha, 1989; Speer and Havasi, 2012) or games with a purpose (von Ahn et al., 2006). Curated resources typically have high precision but suffer from a lack of coverage. For cerWe improve the coverage of commonsense resources by formulating the problem as one of knowledge base completion. We focus on a particular curated commonsense resource called ConceptNet (Speer and Havasi, 2012). ConceptNet contains tuples consisting of a left term, a relation, and a right term. The relations come from"
P16-1137,P98-1013,0,0.0601106,"tml. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, AnalogySpace can only return a confidence score for a pair of terms drawn from the training set. Our models can assign scores to tuples that contain novel terms (as long as they consist of words in our vocabulary). Though we use ConceptNet, similar techniques can be applied to other curated resources like WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). For WordNet, tuples can contain lexical entries that are linked via synset relations (e.g., “hypernym”). WordNet contains many multiword entries (e.g., “cold sweat”), which can be modeled compositionally by our term models; alternatively, entire glosses could be used as terms. To expand frame relationships in FrameNet, tuples can draw relations from the frame relation types (e.g., “is causative of”) and terms can be frame lexical units or their definitions. Several researchers have used commonsense knowledge to improve language technologies, including sentiment analysis (Cambria et al., 2012"
P16-1137,P14-2131,1,0.284641,"rackets surround terms. We replace the bracketed portions with their corresponding terms and insert the relation between them: “The effect of soak in a hotspring C AUSES get pruny skin”. We do this for all training tuples.3 We used the word2vec (Mikolov et al., 2013) toolkit to train skip-gram word embeddings on this data. We trained for 20 iterations, using a dimensionality of 200 and a window size of 5. We refer to these as “CN-trained” embeddings for the remainder of this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated"
P16-1137,D14-1067,0,0.0207423,"s medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gord"
P16-1137,D11-1142,0,0.015575,"utanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsense facts to use for train1 Available at http://ttic.uchicago.edu/ kgimpel/commonsense.html. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, AnalogySpace can only return a confidence score for a pair of terms drawn from the training set. Our models can assig"
P16-1137,D14-1044,0,0.023207,"fastest. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with en"
P16-1137,W12-3023,0,0.060375,"al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsense facts to use for train1 Available at http://ttic.uchicago.edu/ kgimpel/commonsense.html. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, Ana"
P16-1137,D15-1038,0,0.0156161,"r ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V E"
P16-1137,J15-4004,0,0.0181414,"am tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That is, (R = R0 ) ∧ (t1 = t01 ) ∧ (head (t2 ) = head (t02 )), or (R = R0 ) ∧ (t2 = t02 ) ∧ ("
P16-1137,P03-1054,0,0.0377736,"the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That is, (R = R0 ) ∧ (t1 = t01 ) ∧ (head (t2 ) = head (t02 )), or (R = R0 ) ∧ (t2 = t02 ) ∧ (head (t1 ) = head (t01 )). The head word for a term was obtained by running the Stanford Parser (Klein and Manning, 2003) on the term. This baseline does not use word embeddings. • Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for t1 and t2 , ignoring the relation. Vectors for t1 and t2 are obtained by word averaging. • Max Similarity (MaxSim): For tuple τ in an evaluation set, this baseline outputs the maximum similarity between τ and any tuple in the training set. The similarity is computed by concatenating the vectors for t1 , R, and t2 , then computing cosine similarity. As in ArgSim, we obtain vectors for terms by averaging their words. We only consider R when usi"
P16-1137,D11-1049,0,0.0202937,"y-generated negative examples performs best while also being fastest. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce"
P16-1137,D15-1176,0,0.027443,"train models that define a function score(t1 , R, t2 ) that provides a quality score for an arbitrary tuple ht1 , R, t2 i. These models will be evaluated by their ability to distinguish true heldout tuples from false ones. We describe two model families for scoring tuples. We assume that we have embeddings for words and define models that use these word embeddings to score tuples. So our models are limited to tuples in which terms consist of words in the word embedding vocabulary, though future work could consider character-based architectures for open-vocabulary modeling (Huang et al., 2013; Ling et al., 2015). 3.1 Bilinear Models We first consider bilinear models, since they have been found useful for KBC in past work (Nickel et al., 2011; Jenatton et al., 2012; Garc´ıa-Dur´an et al., 2014; Yang et al., 2014). A bilinear model has the following form for a tuple ht1 , R, t2 i: where a is a nonlinear activation function (tuned among ReLU, tanh, and logistic sigmoid) and where we have introduced additional parameters W (B) and b(B) . This gives us the following model: scorebilinear (t1 , R, t2 ) = u> 1 MR u2 When using the LSTM, we tune the decision about how to produce the final term vectors to pass"
P16-1137,P09-1113,0,0.345086,"tuples from ConceptNet. 1 right term relax relaxation your muscle be sore go to spa get pruny skin change into swim suit conf. 3.3 2.6 2.3 2.0 1.6 1.6 Table 1: ConceptNet tuples with left term “soak in hotspring”; final column is confidence score. tain resources, researchers have developed methods to automatically increase coverage by inferring missing entries. These methods are commonly categorized under the heading of knowledge base completion (KBC). KBC is widelystudied for knowledge bases like Freebase (Bollacker et al., 2008) which contain large sets of entities and relations among them (Mintz et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Man"
P16-1137,N15-1054,0,0.0131238,"ing the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsen"
P16-1137,P15-1016,0,0.0109296,"ur resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction sys"
P16-1137,N03-1033,0,0.0405368,"Missing"
P16-1137,D15-1174,0,0.00593326,"task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) a"
P16-1137,Q15-1025,1,0.776827,"f this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That"
P16-1137,D14-1162,0,0.115252,"terations, using a dimensionality of 200 and a window size of 5. We refer to these as “CN-trained” embeddings for the remainder of this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02"
P16-1137,N13-1008,0,0.129197,"x relaxation your muscle be sore go to spa get pruny skin change into swim suit conf. 3.3 2.6 2.3 2.0 1.6 1.6 Table 1: ConceptNet tuples with left term “soak in hotspring”; final column is confidence score. tain resources, researchers have developed methods to automatically increase coverage by inferring missing entries. These methods are commonly categorized under the heading of knowledge base completion (KBC). KBC is widelystudied for knowledge bases like Freebase (Bollacker et al., 2008) which contain large sets of entities and relations among them (Mintz et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Manning, 2014), while others have developed c"
P16-1137,speer-havasi-2012-representing,0,0.463962,"icago, Chicago, IL, 60637, USA University of Illinois at Chicago, Chicago, IL, 60607, USA ‡ Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA † lix1@uchicago.edu, ataher2@uic.edu, {lifu,kgimpel}@ttic.edu relation M OTIVATED B Y G OAL U SED F OR M OTIVATED B Y G OAL H AS P REREQUISITE C AUSES H AS P REREQUISITE Abstract We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set. However, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as mediumconfidence tuples from ConceptNet. 1 right term relax relaxation your muscle be sore go to spa get pruny skin change in"
P16-1137,C98-1013,0,\N,Missing
P17-1190,S14-2010,0,0.153248,"Missing"
P17-1190,S16-1081,0,0.043704,"mber of tokens is approximately the same as the number of tokens in the SimpWiki sentences.3 We use PARAGRAM - SL 999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix (Ww ) for all models. For all experiments, we fix the mini-batch size to 100, and λc to 0. We tune the margin δ over {0.4, 0.6, 0.8} and λw over {10−4 , 10−5 , 10−6 , 10−7 , 10−8 , 0}. We train AVG for 7 epochs, and the LSTM for 3, since it converges much faster and does not benefit from 7 epochs. For optimization we use Adam (Kingma and Ba, 2015) with a learning rate of 0.001. We use the 2016 STS tasks (Agirre et al., 2016) for model selection, where we average the Pearson’s r over its 5 datasets. We refer to this type of model selection as test. For evaluation, we report the average Pearson’s r over the 22 other sentence similarity tasks. The results are shown in Table 1. We first note that, when training on PPDB, we find the same result as Wieting et al. (2016b): AVG outperforms the LSTM by more than 13 points. However, when training both on sentence pairs, the gap shrinks to about 9 points. It appears that part of the inferior performance for the LSTM in prior work was due to training on phrase pairs rather t"
P17-1190,S13-1004,0,0.0943184,"Missing"
P17-1190,S12-1051,0,0.321693,"ing them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1 1 Introduction Modeling sentential compositionality is a fundamental aspect of natural language semantics. Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications. Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function 1 Trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine similarity in the embedding space. In particular, we focus on the setting of Wieting et al. (2016b), in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks. Surprisingly, Wieting et al. found that simple embedding functions—those based on averaging word vectors—outperform more powerful ar"
P17-1190,P01-1008,0,0.0769469,"rk, the GATED RECURRENT AVER AGING NETWORK , that improves upon both AVG and LSTMs for these tasks, and we release our code and trained models. Furthermore, we analyzed the different errors produced by AVG and the recurrent methods and found that the recurrent methods were learning composition that wasn’t being captured by AVG. We also investigated the GRAN in order to better understand the compositional phenomena it was learning by analyzing the L1 norm of its gate over various inputs. Future work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs. Our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings. Acknowledgments Table 11: Average L1 norms for words with the tag VBG with selected dependency labels. We thank the anonymous reviewers for their valuable comments. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Sc"
P17-1190,D12-1050,0,0.0206072,"r, but we mention popular functional families: neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; ˙Irsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012). Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity. An exception is th"
P17-1190,P11-2117,0,0.429259,"ound that simple embedding functions—those based on averaging word vectors—outperform more powerful architectures based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). In this paper, we revisit their experimental setting and present several techniques that together improve the performance of the LSTM to be superior to word averaging. We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia (Coster and Kauchak, 2011). Even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence embeddings, outperforming much larger sets of examples from PPDB. We then show how we can modify and regularize the LSTM to further improve its performance. The main modification is to simply average the hidden states instead of using the final one. For regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence. We find that these techniques help in the transfer learning setting and on two sup"
P17-1190,C04-1051,0,0.719353,"VG and LSTMs for these tasks, and we release our code and trained models. Furthermore, we analyzed the different errors produced by AVG and the recurrent methods and found that the recurrent methods were learning composition that wasn’t being captured by AVG. We also investigated the GRAN in order to better understand the compositional phenomena it was learning by analyzing the L1 norm of its gate over various inputs. Future work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs. Our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings. Acknowledgments Table 11: Average L1 norms for words with the tag VBG with selected dependency labels. We thank the anonymous reviewers for their valuable comments. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357"
P17-1190,ganitkevitch-callison-burch-2014-multilingual,0,0.038889,".2 Experiments with Data Sources We first investigate how different sources of training data affect the results. We try two data sources. The first is phrase pairs from the Paraphrase Database (PPDB). PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014). The second source of data is a set of sentence pairs automatically extracted from Simple English Wikipedia and English Wikipedia articles by Coster and Kauchak (2011). This data was extracted for developing text simplification AVG PPDB 67.7 SimpWiki 68.4 LSTM LSTMAVG 54.2 64.2 59.3 67.5 Table 1: Test results on SemEval semantic textual similarity datasets (Pearson’s r × 100) when training on different sources of data: phrase pairs from PPDB or simple-to-standard English Wikipedia sentence pairs from Coster and Kauchak (2011). systems, where each instance pairs a simple and complex sentence r"
P17-1190,N13-1092,0,0.255342,"Missing"
P17-1190,N16-1162,0,0.100015,"Missing"
P17-1190,P15-1162,0,0.0573939,"Missing"
P17-1190,P14-1062,0,0.225195,"ssively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the GATED RECURRENT AVER AGING NETWORK , that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1 1 Introduction Modeling sentential compositionality is a fundamental aspect of natural language semantics. Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications. Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function 1 Trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine similarity in the embedding space. In particular, we focus on the setting of Wieting et al. (2016b), in which models are trai"
P17-1190,D14-1181,0,0.0154765,"transfer and supervised learning settings, forming a promising new recurrent architecture for semantic modeling. 2 Related Work Modeling sentential compositionality has received a great deal of attention in recent years. A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; ˙Irsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012). Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher"
P17-1190,D15-1176,0,0.0122851,"eling. 2 Related Work Modeling sentential compositionality has received a great deal of attention in recent years. A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; ˙Irsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012). Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017;"
P17-1190,D12-1110,0,0.04883,"Missing"
P17-1190,D13-1170,0,0.0152245,"Missing"
P17-1190,D15-1280,0,0.00409219,"rk Modeling sentential compositionality has received a great deal of attention in recent years. A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; ˙Irsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012). Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al.,"
P17-1190,P14-5010,0,0.00237374,"atively rare, and those that we found did not have any systematic patterns. 5.2 GRAN Gate Analysis We also investigate what is learned by the gating function of the GATED RECURRENT AVERAGING NETWORK . We are interested to see whether its estimates of importance correlate with those of traditional syntactic and (shallow) semantic analysis. We use the oracle trained GATED RECURRENT AVERAGING NETWORK from Table 3 and calculate the L1 norm of the gate after embedding 10,000 sentences from English Wikipedia.8 We also automatically tag and parse these sentences using the Stanford dependency parser (Manning et al., 2014). We then compute the average gate L1 norms for particular part-of-speech tags, dependency arc labels, and their conjunction. Table 9 shows the highest/lowest average norm tags and dependency labels. The network prefers nouns, especially proper nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence. Analyzing the dependency relations, we find 8 We selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing. 2085 POS top 10 bot. 10 NNP TO NNPS WDT CD POS NNS DT VBG WP NN IN JJ CC UH PRP VBN EX JJS WR"
P17-1190,P15-1150,0,0.0377388,"Missing"
P17-1190,S14-2001,0,0.0191616,"e same techniques to improve performance in the supervised setting, described in Section 4.3. In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM. 4.1 Transfer Learning 4.1.1 Datasets and Tasks We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. We report the average Pearson’s r over these 22 sentence similarity tasks. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Further details are provided in the official task descriptions (Agirre et al., 2012, 2013, 2014, 2015). 4.1.2 Ex"
P17-1190,P08-1028,0,0.185806,"s to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the GATED RECURRENT AVER AGING NETWORK , that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1 1 Introduction Modeling sentential compositionality is a fundamental aspect of natural language semantics. Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications. Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function 1 Trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine similarity in the embedding space. In particular, we focus on the setting of"
P17-1190,P15-1094,0,0.0856321,"ral networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012). Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity. An exception is the work of Wieting et al. (2016b). We closely follow their experimental setup and directly address some outstanding questions in their experimental results. Here we briefly summarize their main findings and their attempts at explaining them. They made the surprising discovery that word averaging outperforms LSTMs by a wide margin in the transfer learning settin"
P17-1190,D16-1157,1,0.291512,"Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications. Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function 1 Trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine similarity in the embedding space. In particular, we focus on the setting of Wieting et al. (2016b), in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks. Surprisingly, Wieting et al. found that simple embedding functions—those based on averaging word vectors—outperform more powerful architectures based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). In this paper, we revisit their experimental setting and present several techniques that together improve the performance of the LSTM to be superior to word averaging. We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database"
P17-1190,Q15-1025,1,0.523712,"is all zeros and the second is all ones throughout the sequence, the model is equivalent to averaging the LSTM states. Further analysis of these models is included in Section 4. 2 We tried a variant of this model without the gate. We obtain at from f (Wx xt + Wh ht + b), where f is a nonlinearity, tuned over tanh and ReLU. The performance of the model is significantly worse than the GRAN in all experiments. Our experiments are designed to address the empirical question posed by Wieting et al. (2016b): why do LSTMs underperform AVG for transfer 3.2 Training We follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016b), described below. The training data consists of a set S of phrase or sentence pairs hs1 , s2 i from either the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) or the aligned Wikipedia sentences (Coster and Kauchak, 2011) where s1 and s2 are assumed to be paraphrases. We optimize a margin-based loss: min Wc ,Ww 1 |S| X hs1 ,s2 i∈S max(0, δ − cos(g(s1 ), g(s2 )) + cos(g(s1 ), g(t1 ))) + max(0, δ − cos(g(s1 ), g(s2 ))  + cos(g(s2 ), g(t2 ))) + λc kWc k2 + λw kWwinitial − Ww k2 (2) where g is the model in use (e.g., AVG or LSTM), δ is the margin, λc and λw are re"
P17-1190,S15-2001,0,0.0234044,"gap between the two models in the transfer setting. We then apply these same techniques to improve performance in the supervised setting, described in Section 4.3. In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM. 4.1 Transfer Learning 4.1.1 Datasets and Tasks We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. We report the average Pearson’s r over these 22 sentence similarity tasks. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Further details are provided in the o"
P17-1190,N18-1049,0,\N,Missing
P17-2097,D15-1166,0,0.0119775,"atenation of all sentences. We also consider the possibility of including an endingoriented attention mechanism (ATT). For training, we use a simple supervised hinge loss objective. 3.1 |wi | 1 X (j) Si = hi |wi | (1) j=1 We define this function from word sequence wi to sentence representation Si by ENC W ORDS(wi ). 3.1.2 Adding Attention Attention mechanisms (Bahdanau et al., 2015; Mnih et al., 2014) have yielded considerable performance gains for machine comprehension (Hermann et al., 2015; Sukhbaatar et al., 2015; Chen et al., 2016), parsing (Vinyals et al., 2015), and machine translation (Luong et al., 2015). After generating the representation e = S5 = E NC W ORDS(w5 ) for candidate ending w5 , we use it to compute the attention over the individual hidden vectors of each sentence to compute modified sentence representations Si† . That is: Encoders (j) αi Our encoders encode text sequences into representations. When using our H IER model, we use a hierarchical recurrent neural network (RNN) (Li et al., 2015) with two levels. The first RNN en(j) = e> M hi Si† = |wi | X j=1 617 (j) βi (j) (j) βi hi (j) ∝ exp{αi } (2) (j) where hi is the jth entry of hi and M is a bilinear attention matrix.1 Figure"
P17-2097,W17-0908,0,0.0655963,"Missing"
P17-2097,W17-0913,0,0.0162223,"gram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index sentences in the story or plot, and j as a superscript to index individual words in sentences. E.g., we use wi to indicate the ith sentence of the story/plot"
P17-2097,P16-1223,0,0.037702,"ntations, and a non-hierarchical encoder (F LAT) that simply encodes the concatenation of all sentences. We also consider the possibility of including an endingoriented attention mechanism (ATT). For training, we use a simple supervised hinge loss objective. 3.1 |wi | 1 X (j) Si = hi |wi | (1) j=1 We define this function from word sequence wi to sentence representation Si by ENC W ORDS(wi ). 3.1.2 Adding Attention Attention mechanisms (Bahdanau et al., 2015; Mnih et al., 2014) have yielded considerable performance gains for machine comprehension (Hermann et al., 2015; Sukhbaatar et al., 2015; Chen et al., 2016), parsing (Vinyals et al., 2015), and machine translation (Luong et al., 2015). After generating the representation e = S5 = E NC W ORDS(w5 ) for candidate ending w5 , we use it to compute the attention over the individual hidden vectors of each sentence to compute modified sentence representations Si† . That is: Encoders (j) αi Our encoders encode text sequences into representations. When using our H IER model, we use a hierarchical recurrent neural network (RNN) (Li et al., 2015) with two levels. The first RNN en(j) = e> M hi Si† = |wi | X j=1 617 (j) βi (j) (j) βi hi (j) ∝ exp{αi } (2) (j)"
P17-2097,N16-1098,0,0.104998,"h a correct ending and an incorrect ending. This means that the task is one of outlier detection: systems must estimate the density of correct endings in the training data and 2 Task and Datasets We refer to a 5-sentence sequence as a story, the incomplete 4-sentence sequence as a plot, and the 616 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 616–622 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2097 fifth sentence as an ending. The ROC story corpus (Mostafazadeh et al., 2016) contains training, validation, and test sets. The training set contains 5-sentence stories. The validation and test sets contain 4-sentence plots followed by two candidate endings, with only one correct. Mostafazadeh et al. (2016) evaluated several methods for solving the task. Since the training set does not contain incorrect endings, their methods are based on computing similarity between the plot and ending. Their best results were obtained with the Deep Structured Semantic Model (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network la"
P17-2097,W17-0909,0,0.0331459,"Deep Structured Semantic Model (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index sentences in the story or plot, and j as a superscript to i"
P17-2097,W17-0912,0,0.0115492,"ts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index sentences in the story or plot, and j as a superscript to index individual words in sentences. E.g., we use wi to indicate the ith"
P17-2097,W17-0906,0,0.161385,"alidation and test sets contain 4-sentence plots followed by two candidate endings, with only one correct. Mostafazadeh et al. (2016) evaluated several methods for solving the task. Since the training set does not contain incorrect endings, their methods are based on computing similarity between the plot and ending. Their best results were obtained with the Deep Structured Semantic Model (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN"
P17-2097,W11-2123,0,0.0247395,"Missing"
P17-2097,D14-1162,0,0.106067,"function on each layer of the feed-forward network and tune the numbers of hidden layers and the layer widths. 3.3 4 Experimental Setup We shuffle and split the validation set into 5 folds and do 5-fold cross validation. For modeling decisions, we tune based on the average accuracy of the held-out folds. For final experiments, we choose the fold with the best held-out accuracy and report its test set accuracy. We use Adam (Kingma and Ba, 2015) for optimization with learning rate 0.001 and mini-batch size 50. We use pretrained 300-dimensional GloVe embeddings trained on Wikipedia and Gigaword (Pennington et al., 2014) and keep them fixed during training. We use L2 regularization for the score feed-forward network, which has a single hidden layer of size 512. We use 300 for the LSTM hidden vector dimensionality for both encoders. Training Since we are training on the validation set which contains both correct and incorrect endings, we minimize the following hinge loss: L = max(0, −score(D + ) + score(D − ) + δ) where D + is the representation of the correct story, D − is the representation of the incorrect story, and δ = 1 is the margin. 1 In preliminary experiments we found bilinear attention to work bette"
P17-2097,W17-0911,0,0.0281416,"3) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index sentences in the story or plot, and j as a superscript to index individual words in sentences. E.g., we use w"
P17-2097,W17-0910,0,0.0328308,"l (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index sentences in the story or plot, and j as a superscript to index individual words in se"
P17-2097,K17-1004,0,0.36721,"ing. Their best results were obtained with the Deep Structured Semantic Model (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index senten"
P17-2097,W17-0907,0,0.234402,"ing. Their best results were obtained with the Deep Structured Semantic Model (DSSM) (Huang et al., 2013) which represents texts using character trigram counts followed by neural network layers and a similarity function. Concurrently with our work, the LSDSem 2017 shared task was held (Mostafazadeh et al., 2017), focusing on the ROC story cloze task. Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings (Schwartz et al., 2017a,b; Bugert et al., 2017; Flor and Somasundaran, 2017; Schenk and Chiarcos, 2017; Roemmele et al., 2017; Goel and Singh, 2017; Mihaylov and Frank, 2017). 3 codes the sequence of words in a sentence; the same RNN is used for sentences in the plot and for each candidate ending. The second RNN encodes the sequence of sentence representations in a plot or story. When using our F LAT model, we only use the first RNN described above; the only change is that the input becomes the concatenation of multiple sentences (separated by sentence boundary tokens). Below we use i as a subscript to index senten"
P17-2097,D13-1170,0,\N,Missing
P18-1042,S14-2010,0,0.22708,"Missing"
P18-1042,S16-1081,0,0.248113,"Missing"
P18-1042,S13-1004,0,0.0912717,"Missing"
P18-1042,S12-1051,0,0.152782,"escribed in Section 3. We use 100K samples from each corpus and trained 3 different models on each: W ORD, T RIGRAM, and LSTM. Table 4 shows that CzEng provides the best training data for all models, so we used it to create PARA NMT-50M and for all remaining experiments. Experiments We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings. 5.1 W ORD T RIGRAM LSTM 80.9 80.2 79.1 83.6 81.5 82.5 78.9 78.0 80.4 80.2 78.2 80.5 Evaluation We evaluate sentence embeddings using the SemEval semantic textual similarity (STS) tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016) and the STS Benchmark (Cer et al., 2017). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 means they are completely equivalent. As our test set, we report the average Pearson’s r 5 As in our prior work (Wieting and Gimpel, 2017), we found that scrambling significantly improves results, even with our much larger training set. But while we previously used a scrambling rate of 0.5, we found that a smaller rate of 0.3 worked better when training on PARA NMT-50M, pres"
P18-1042,P05-1074,0,0.777795,"form all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1 1 Introduction While many approaches have been developed for generating or finding paraphrases (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Dolan et al., 2004), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), which was created automatically from bilingual text by pivoting over the non-English language (Bannard and Callison-Burch, 2005). PPDB has been used to improve word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, PPDB is less useful for learning sentence embeddings (Wieting and Gimpel, 2017). In this paper, we describe the creation of a dataset containing more than 50 million sentential 1 Dataset, code, and embeddings are available at https: //www.cs.cmu.edu/˜jwieting. 451 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 451–462 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics find lexical and phrasa"
P18-1042,D17-1254,0,0.050896,"ntation function and as a general similarity metric. 2 Sentence embeddings. Our learning and evaluation setting is the same as that of our recent work that seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks (Wieting et al., 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017). We trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. Prior work in learning general sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015; Gan et al., 2017), and other sources of supervision and learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017; Conneau et al., 2017). Related Work We discuss work in automatically building paraphrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions. Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a"
P18-1042,P01-1008,0,0.779741,"a large parallel corpus, following Wieting et al. (2017). Our hope is that PARA NMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARA NMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1 1 Introduction While many approaches have been developed for generating or finding paraphrases (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Dolan et al., 2004), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), which was created automatically from bilingual text by pivoting over the non-English language (Bannard and Callison-Burch, 2005). PPDB has been used to improve word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, PPDB is less useful for learning sentence embeddings (Wieting and Gimpel, 2017). In this paper, we describe the creation of a dataset contain"
P18-1042,ganitkevitch-callison-burch-2014-multilingual,0,0.0260282,"n of a dataset containing more than 50 million sentential 1 Dataset, code, and embeddings are available at https: //www.cs.cmu.edu/˜jwieting. 451 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 451–462 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics find lexical and phrasal paraphrases in parallel text. Ganitkevitch et al. (2013) scaled up these techniques to produce the Paraphrase Database (PPDB). Our goals are similar to those of PPDB, which has likewise been generated for many languages (Ganitkevitch and Callison-Burch, 2014) since it only needs parallel text. In particular, we follow the approach of Wieting et al. (2017), who used NMT to translate the non-English side of parallel text to get English-English paraphrase pairs. We scale up the method to a larger dataset, produce state-of-the-art paraphrastic sentence embeddings, and release all of our resources. of negative examples. In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work (Kiros et al., 2015; Conneau et al., 2017), finding our embeddings to perform competitively. Finally, in Section 6, we briefly report result"
P18-1042,N13-1092,0,0.227573,"Missing"
P18-1042,S17-2001,0,0.314378,"IGRAM, etc.), δ is the margin, and t is a “negative example” taken from a mini-batch during optimization. The intuition is that we want the two texts to be more similar to each other than to their negative examples. To select t we choose the most similar sentence in some set. For simplicity we use the mini-batch for this set, i.e., t= argmax Table 4: Pearson’s r × 100 on STS2017 when training on 100k pairs from each back-translated parallel corpus. CzEng works best for all models. over each year of the STS tasks from 2012-2016. We use the small (250-example) English dataset from SemEval 2017 (Cer et al., 2017) as a development set, which we call STS2017 below. The supplementary material contains a description of a method to obtain a paraphrase lexicon from PARA NMT-50M that is on par with that provided by PPDB 2.0. We also evaluate our sentence embeddings on a range of additional tasks that have previously been used for evaluating sentence representations (Kiros et al., 2015). cos(g(s), g(t0 )) t0 :ht0 ,·i∈Sb {hs,s0 i} where Sb ⊆ S is the current mini-batch. Modification: mega-batching. By using the mini-batch to select negative examples, we may be limiting the learning procedure. That is, if all"
P18-1042,D17-1070,0,0.280794,"similar to those of PPDB, which has likewise been generated for many languages (Ganitkevitch and Callison-Burch, 2014) since it only needs parallel text. In particular, we follow the approach of Wieting et al. (2017), who used NMT to translate the non-English side of parallel text to get English-English paraphrase pairs. We scale up the method to a larger dataset, produce state-of-the-art paraphrastic sentence embeddings, and release all of our resources. of negative examples. In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work (Kiros et al., 2015; Conneau et al., 2017), finding our embeddings to perform competitively. Finally, in Section 6, we briefly report results showing how PARA NMT-50M can be used for paraphrase generation. A standard encoderdecoder model trained on PARA NMT-50M can generate paraphrases that show effects of “canonicalizing” the input sentence. In other work, fully described by Iyyer et al. (2018), we used PARA NMT-50M to generate paraphrases that have a specific syntactic structure (represented as the top two levels of a linearized parse tree). We release the PARA NMT-50M dataset, our trained sentence embeddings, and our code. PARA NMT"
P18-1042,N16-1162,0,0.0678644,"Missing"
P18-1042,P11-2117,0,0.0800647,"ons. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017). The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to 3 The PARA NMT-50M Dataset To create our dataset, we used back-translation of bitext (Wieting et al., 2017). We used a CzechEnglish NMT system to translate Czech sentences 452 Dataset Common Crawl CzEng 1.6 Europarl News Commentary Avg. Length Avg. IDF Avg."
P18-1042,W14-3348,0,0.110228,"Missing"
P18-1042,C04-1051,0,0.89913,"(2017). Our hope is that PARA NMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARA NMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1 1 Introduction While many approaches have been developed for generating or finding paraphrases (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Dolan et al., 2004), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), which was created automatically from bilingual text by pivoting over the non-English language (Bannard and Callison-Burch, 2005). PPDB has been used to improve word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, PPDB is less useful for learning sentence embeddings (Wieting and Gimpel, 2017). In this paper, we describe the creation of a dataset containing more than 50 million sentential 1 Datas"
P18-1042,N18-1170,1,0.820469,"dataset, produce state-of-the-art paraphrastic sentence embeddings, and release all of our resources. of negative examples. In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work (Kiros et al., 2015; Conneau et al., 2017), finding our embeddings to perform competitively. Finally, in Section 6, we briefly report results showing how PARA NMT-50M can be used for paraphrase generation. A standard encoderdecoder model trained on PARA NMT-50M can generate paraphrases that show effects of “canonicalizing” the input sentence. In other work, fully described by Iyyer et al. (2018), we used PARA NMT-50M to generate paraphrases that have a specific syntactic structure (represented as the top two levels of a linearized parse tree). We release the PARA NMT-50M dataset, our trained sentence embeddings, and our code. PARA NMT-50M is the largest collection of sentential paraphrases released to date. We hope it can motivate new research directions and be used to create powerful NLP models, while adding a robustness to existing ones by incorporating paraphrase knowledge. Our paraphrastic sentence embeddings are state-of-the-art by a significant margin, and we hope they can be u"
P18-1042,I05-5002,0,0.222149,"Cann et al., 2017). Hill et al. (2016) evaluated the encoders of Englishto-X NMT systems as sentence representations. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017). The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to 3 The PARA NMT-50M Dataset To create our dataset, we used back-translation of bitext (Wieting et al., 2017). We used a CzechEnglish NMT system"
P18-1042,P17-2017,0,0.0160821,"e sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017). The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to 3 The PARA NMT-50M Dataset To create our dataset, we used back-translation of bitext (Wieting et al., 2017). We used a CzechEnglish NMT system to translate Czech sentences 452 Dataset Common Crawl CzEng 1.6 Europarl News Commentary Avg. Length Avg. IDF Avg. Para. Score Vocab. Entropy Parse Entropy Total Size 24.0±3"
P18-1042,D17-1126,0,0.106376,"Missing"
P18-1042,D14-1162,0,0.0833938,"Missing"
P18-1042,P15-1094,0,0.361152,"tion setting is the same as that of our recent work that seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks (Wieting et al., 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017). We trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. Prior work in learning general sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015; Gan et al., 2017), and other sources of supervision and learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017; Conneau et al., 2017). Related Work We discuss work in automatically building paraphrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions. Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings (Faruqui et al., 2015; Wieting et al., 2015"
P18-1042,E17-1083,0,0.0891174,"learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016). NMT architectures and training settings have been used to obtain better embeddings for words (Hill et al., 2014a,b) and words-in-context (McCann et al., 2017). Hill et al. (2016) evaluated the encoders of Englishto-X NMT systems as sentence representations. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), c"
P18-1042,W04-3219,0,0.405062,"t al. (2016) evaluated the encoders of Englishto-X NMT systems as sentence representations. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017). The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to 3 The PARA NMT-50M Dataset To create our dataset, we used back-translation of bitext (Wieting et al., 2017). We used a CzechEnglish NMT system to translate Czech se"
P18-1042,E17-3017,0,0.0843082,"Missing"
P18-1042,P14-5010,0,0.00406311,"l data, we randomly sampled 100K English reference translations from each data source and computed statistics. Table 1 shows the average sentence length, the average inverse document frequency (IDF) where IDFs are computed using Wikipedia sentences, and the average paraphrase score for the two sentences. The paraphrase score is calculated by averaging PARAGRAM - PHRASE embeddings (Wieting et al., 2016b) for the two sentences in each pair and then computing their cosine similarity. The table also shows the entropies of the vocabularies and constituent parses obtained using the Stanford Parser (Manning et al., 2014).2 Europarl exhibits the least diversity in terms of 3.2 Manual Evaluation We conducted a manual analysis of our dataset in order to quantify its noise level and assess how the 2 To mitigate sparsity in the parse entropy, we used only the top two levels of each parse tree. 453 Para. Score # Avg. Tri. Paraphrase Fluency Range (M) Overlap 1 2 3 1 2 3 (-0.1, 0.2] 4.0 0.00±0.0 92 6 2 1 5 94 (0.2, 0.4] 3.8 0.02±0.1 53 32 15 1 12 87 (0.4, 0.6] 6.9 0.07±0.1 22 45 33 2 9 89 (0.6, 0.8] 14.4 0.17±0.2 1 43 56 11 0 89 (0.8, 1.0] 18.0 0.35±0.2 1 13 86 3 0 97 cator of quality. At the low ranges, we inspecte"
P18-1042,S17-2016,0,0.0331008,"Missing"
P18-1042,N16-1018,0,0.0335231,"Missing"
P18-1042,P17-3007,0,0.0279439,"tion. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001), using distributional similarity to find similar dependency paths (Lin and Pantel, 2001), using comparable articles from multiple news sources (Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004), aligning sentences between standard and Simple English Wikipedia (Coster and Kauchak, 2011), crowdsourcing (Xu et al., 2014, 2015; Jiang et al., 2017), using diverse MT systems to translate a single source sentence (Suzuki et al., 2017), and using tweets with matching URLs (Lan et al., 2017). The most relevant prior work uses bilingual corpora. Bannard and Callison-Burch (2005) used methods from statistical machine translation to 3 The PARA NMT-50M Dataset To create our dataset, we used back-translation of bitext (Wieting et al., 2017). We used a CzechEnglish NMT system to translate Czech sentences 452 Dataset Common Crawl CzEng 1.6 Europarl News Commentary Avg. Length Avg. IDF Avg. Para. Score Vocab. Entropy Parse Entropy Total Size 24.0±34.7 7.7±1.1 0.83±0.16 7.2 3.5 0.16M 13.3±19.3 7.4±1.2 0.84±0.16 6.8 4.1 51.4M 26.1±15."
P18-1042,P15-1150,0,0.129408,"Missing"
P18-1042,D16-1157,1,0.683246,"side of a large Czech-English parallel corpus. We pair the English translations with the English references to form paraphrase pairs. We call this dataset PARA NMT-50M. It contains examples illustrating a broad range of paraphrase phenomena; we show examples in Section 3. PARA NMT-50M has the potential to be useful for many tasks, from linguistically controlled paraphrase generation, style transfer, and sentence simplification to core NLP problems like machine translation. We show the utility of PARA NMT-50M by using it to train paraphrastic sentence embeddings using the learning framework of Wieting et al. (2016b). We primarily evaluate our sentence embeddings on the SemEval semantic textual similarity (STS) competitions from 2012-2016. Since so many domains are covered in these datasets, they form a demanding evaluation for a general purpose sentence embedding model. Our sentence embeddings learned from PARA NMT-50M outperform all systems in every STS competition from 2012 to 2016. These tasks have drawn substantial participation; in 2016, for example, the competition attracted 43 teams and had 119 submissions. Most STS systems use curated lexical resources, the provided supervised training data wit"
P18-1042,Q15-1025,1,0.900994,"4; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017; Conneau et al., 2017). Related Work We discuss work in automatically building paraphrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions. Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016). NMT architectures and training settings have been used to obtain better embeddings for words (Hill et al., 2014a,b) and words-in-context (McCann et al., 2017). Hill et al. (2016) evaluated the encoders of Englishto-X NMT systems as sentence representations. Mallinson et al. (2017) adapted trained NMT models to produce sentence similarity scores in semantic evaluations. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material (Barzilay and McKeown, 2001"
P18-1042,P17-1190,1,0.906961,"s have been developed for generating or finding paraphrases (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Dolan et al., 2004), there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), which was created automatically from bilingual text by pivoting over the non-English language (Bannard and Callison-Burch, 2005). PPDB has been used to improve word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, PPDB is less useful for learning sentence embeddings (Wieting and Gimpel, 2017). In this paper, we describe the creation of a dataset containing more than 50 million sentential 1 Dataset, code, and embeddings are available at https: //www.cs.cmu.edu/˜jwieting. 451 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 451–462 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics find lexical and phrasal paraphrases in parallel text. Ganitkevitch et al. (2013) scaled up these techniques to produce the Paraphrase Database (PPDB). Our goals are similar to those of PPDB, which has like"
P18-1042,D17-1026,1,0.322899,"ncoding each sentence independently using our models and computing cosine similarity between their embeddings. We experiment with several compositional architectures and find them all to work well. We find benefit from making a simple change to learning (“mega-batching”) to better leverage the large training set, namely, increasing the search space We describe PARA NMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the nonEnglish side of a large parallel corpus, following Wieting et al. (2017). Our hope is that PARA NMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARA NMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1 1 Introduction While many approaches have been developed for generating or finding paraphrases (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Dolan e"
P18-1042,S15-2001,0,0.0607953,"Missing"
P19-1180,J90-1003,0,0.402579,"ardly be induced by VG-NSL. Here we evaluate models on the MSCOCO test set, which is well-matched to the training domain; we leave the extension of our work to more abstract domains to future work. We apply Benepar (Kitaev and Klein, 2018),3 an off-the-shelf constituency parser 1846 3 https://pypi.org/project/benepar with state-of-the-art performance (95.52 F1 score) on the WSJ test set,4 to parse the captions in the MSCOCO test set as gold constituency parse trees. We evaluate all of the investigated models using the F1 score compared to these gold parse trees.5 pointwise mutual information (Church and Hanks, 1990) between adjacent words as the syntactic distance. We compose constituency parse trees based on the distances in the same way as PRPN and ON-LSTM. 4.2 Syntax acquisition from downstream tasks. Choi et al. (2018) propose to compose binary constituency parse trees directly from downstream tasks using the Gumbel softmax trick (Jang et al., 2017). We integrate a Gumbel tree-based caption encoder into the visual semantic embedding approach (Kiros et al., 2014). The model is trained on the downstream task of image-caption retrieval. Baselines We compare VG-NSL with various baselines for unsupervised"
P19-1180,N09-1009,0,0.0176942,"y parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplementary material. There has been some prior work on improving unsupervised parsing by leveraging extra signals, such as parallel text (Snyder et al., 2009), annotated data in another language with parallel text (Ganchev et al., 2009), annotated data in other languages without parallel text (Cohen et al., 2011), or non-parallel text from multiple languages (Cohen and Smith, 2009). We leave the integration of other grounding signals as future work. Grounded language acquisition. Grounded language acquisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures with no humandefined labels or rules. Meanwhile, learning visual-semantic representations in a joint embedding space (Ngiam et al., 2011) is a widely s"
P19-1180,D11-1005,0,0.0222117,"rast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplementary material. There has been some prior work on improving unsupervised parsing by leveraging extra signals, such as parallel text (Snyder et al., 2009), annotated data in another language with parallel text (Ganchev et al., 2009), annotated data in other languages without parallel text (Cohen et al., 2011), or non-parallel text from multiple languages (Cohen and Smith, 2009). We leave the integration of other grounding signals as future work. Grounded language acquisition. Grounded language acquisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures with no humandefined labels or rules. Meanwhile, learning visual-semantic represe"
P19-1180,N19-1116,0,0.182078,"score, which emerges during the matching between constituents and images, correlates well with a similar measure defined by linguists. Finally, VG-NSL can be easily extended to multiple languages, which we evaluate on the Multi30K data set (Elliott et al., 2016, 2017) consisting of German and French image captions. 2 Related Work Linguistic structure induction from text. Recent work has proposed several approaches for inducing latent syntactic structures, including constituency trees (Choi et al., 2018; Yogatama et al., 2017; Maillard and Clark, 2018; Havrylov et al., 2019; Kim et al., 2019; Drozdov et al., 2019) and dependency trees (Shi et al., 2019), from the distant supervision of downstream tasks. However, most of the methods are not able to produce linguistically sound structures, or even consistent ones with fixed data and hyperparameters but different random initializations (Williams et al., 2018). A related line of research is to induce latent syntactic structure via language modeling. This approach has achieved remarkable performance on unsupervised constituency parsing (Shen et al., 2018a, 2019), especially in identifying the boundaries of higher-level (i.e., larger) constituents. To our kn"
P19-1180,W17-4718,0,0.0204156,"ecifically, the variance of avg. F1 is always less than 0.6 while the self F1 is greater than 80. Note that the PRPN and ON-LSTM models are not tuned using self F1 , since these models are usually trained for hundreds or thousands of epochs and thus it is computationally expensive to evaluate self F1 . We leave the efficient tuning of these baselines by self F1 as a future work. 4.8 Extension to Multiple Languages We extend our experiments to the Multi30K data set, which is built on the Flickr30K data set (Young et al., 2014) and consists of English, German (Elliott et al., 2016), and French (Elliott et al., 2017) captions. For Multi30K, there are 29,000 images in the training set, 1,014 in the development set and 1,000 in the test set. Each image is associated with one caption in each language. We compare our models to PRPN and ONLSTM in terms of overall F1 score (Table 4). VGNSL with the head-initial inductive bias consis6 5 Discussion We have proposed a simple but effective model, the Visually Grounded Neural Syntax Learner, for visually grounded language structure acquisition. VG-NSL jointly learns parse trees and visually grounded textual representations. In our experiments, we find that this appr"
P19-1180,W16-3210,0,0.0767938,"Missing"
P19-1180,P09-1042,0,0.0200864,"duces dependency parse trees based on automatically induced pseudo tags. In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplementary material. There has been some prior work on improving unsupervised parsing by leveraging extra signals, such as parallel text (Snyder et al., 2009), annotated data in another language with parallel text (Ganchev et al., 2009), annotated data in other languages without parallel text (Cohen et al., 2011), or non-parallel text from multiple languages (Cohen and Smith, 2009). We leave the integration of other grounding signals as future work. Grounded language acquisition. Grounded language acquisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures wit"
P19-1180,N10-1115,0,0.020555,"kens, VG-NSL builds a latent constituency parse tree, and recursively composes representations for every constituent. Next, it matches textual representations with visual inputs, such as the paired image with the constituents. Both modules are jointly optimized from natural supervision: the model acquires constituency structures, composes textual representations, and links them with visual scenes, by looking at images and reading paired captions. 3.1 Textual Representations and Structures VG-NSL starts by composing a binary constituency structure of text, using an easy-first bottom-up parser (Goldberg and Elhadad, 2010). The composition of the tree from a caption of length n consists (t) (t) (t) of n−1 steps. Let X(t) = (x1 , x2 , · · · , xk ) denote the textual representations of a sequence of constituents after step t, where k = n − t. For simplicity, we use X(0) to denote the word embeddings for all tokens (the initial representations). At step t, a score function score(·; Θ), parameterized by Θ, is evaluated on all pairs of consecutive constituents, resulting in a vector score(X(t−1) ; Θ) of length n − t: score(X(t−1) ; Θ)j h i  (t−1) (t−1) , score xj , xj+1 ; Θ . We implement score(·; Θ) as a two-laye"
P19-1180,D17-1176,0,0.0553525,"ding-Predict Network (PRPN; Shen et al., 2018a) and the Ordered Neuron LSTM (ON-LSTM; Shen et al., 2019) currently produce the best fully unsupervised constituency parsing results. One issue with PRPN, however, is that it tends to produce meaningless parses for lower-level (smaller) constituents (Phu Mon Htut et al., 2018). Over the last two decades, there has been extensive study targeting unsupervised constituency parsing (Klein and Manning, 2002, 2004, 2005; Bod, 2006a,b; Ponvert et al., 2011) and dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2010; Han et al., 2017). However, all of these approaches are based on linguistic annotations. Specifically, they operate on the part-of-speech tags of words instead of word tokens. One exception is Spitkovsky et al. (2011), which produces dependency parse trees based on automatically induced pseudo tags. In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplem"
P19-1180,N19-1115,0,0.0287252,"g captions. In addition, the concreteness score, which emerges during the matching between constituents and images, correlates well with a similar measure defined by linguists. Finally, VG-NSL can be easily extended to multiple languages, which we evaluate on the Multi30K data set (Elliott et al., 2016, 2017) consisting of German and French image captions. 2 Related Work Linguistic structure induction from text. Recent work has proposed several approaches for inducing latent syntactic structures, including constituency trees (Choi et al., 2018; Yogatama et al., 2017; Maillard and Clark, 2018; Havrylov et al., 2019; Kim et al., 2019; Drozdov et al., 2019) and dependency trees (Shi et al., 2019), from the distant supervision of downstream tasks. However, most of the methods are not able to produce linguistically sound structures, or even consistent ones with fixed data and hyperparameters but different random initializations (Williams et al., 2018). A related line of research is to induce latent syntactic structure via language modeling. This approach has achieved remarkable performance on unsupervised constituency parsing (Shen et al., 2018a, 2019), especially in identifying the boundaries of higher-lev"
P19-1180,W18-2903,0,0.0410368,"Missing"
P19-1180,P14-2135,0,0.0368252,"?) ?2 ?(?) Image Embedding Visual-Semantic Embeddings Image Figure 2: VG-NSL consists of two modules: a textual module for inferring structures and representations for captions, and a visual-semantic module for matching constituents with images. VG-NSL induces constituency parse trees of captions by looking at images and reading paired captions. word-level concreteness estimation based on text (Turney et al., 2011; Hill et al., 2013), human judgments (Silberer and Lapata, 2012; Hill and Korhonen, 2014a; Brysbaert et al., 2014), and multimodal data (Hill and Korhonen, 2014b; Hill et al., 2014; Kiela et al., 2014; Young et al., 2014; Hessel et al., 2018; Silberer et al., 2017; Bhaskar et al., 2017). As with Hessel et al. (2018) and Kiela et al. (2014), our model uses multi-modal data to estimate concreteness. Compared with them, we define concreteness for spans instead of words, and use it to induce linguistic structures. 3 Visually Grounded Neural Syntax Learner Given a set of paired images and captions, our goal is to learn representations and structures for words and constituents. Toward this goal, we propose the Visually Grounded Neural Syntax Learner (VGNSL), an approach for the grounded acquisit"
P19-1180,C18-1315,1,0.940281,"uisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures with no humandefined labels or rules. Meanwhile, learning visual-semantic representations in a joint embedding space (Ngiam et al., 2011) is a widely studied approach, and has achieved remarkable results on image-caption retrieval (Kiros et al., 2014; Faghri et al., 2018; Shi et al., 2018a), image caption generation (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Ma et al., 2015), and visual question answering (Malinowski et al., 2015). In this work, we borrow this idea to match visual and textual representations. Concreteness estimation. Turney et al. (2011) define concrete words as those referring to things, events, and properties that we can perceive directly with our senses. Subsequent work has studied 1843 A cat is on the ground Caption Structure and Representation Inference (Score-Sample-Combine) Constituency Parse Tree Embeddings of Constituents (?) Image Encoder ?3 (?"
P19-1180,D18-1492,1,0.926387,"uisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying visual attributes or actions. Instead, our model induces syntax structures with no humandefined labels or rules. Meanwhile, learning visual-semantic representations in a joint embedding space (Ngiam et al., 2011) is a widely studied approach, and has achieved remarkable results on image-caption retrieval (Kiros et al., 2014; Faghri et al., 2018; Shi et al., 2018a), image caption generation (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Ma et al., 2015), and visual question answering (Malinowski et al., 2015). In this work, we borrow this idea to match visual and textual representations. Concreteness estimation. Turney et al. (2011) define concrete words as those referring to things, events, and properties that we can perceive directly with our senses. Subsequent work has studied 1843 A cat is on the ground Caption Structure and Representation Inference (Score-Sample-Combine) Constituency Parse Tree Embeddings of Constituents (?) Image Encoder ?3 (?"
P19-1180,D12-1130,0,0.0344708,"ucture and Representation Inference (Score-Sample-Combine) Constituency Parse Tree Embeddings of Constituents (?) Image Encoder ?3 (?) ?1 ?(?) (?) ?2 ?(?) Image Embedding Visual-Semantic Embeddings Image Figure 2: VG-NSL consists of two modules: a textual module for inferring structures and representations for captions, and a visual-semantic module for matching constituents with images. VG-NSL induces constituency parse trees of captions by looking at images and reading paired captions. word-level concreteness estimation based on text (Turney et al., 2011; Hill et al., 2013), human judgments (Silberer and Lapata, 2012; Hill and Korhonen, 2014a; Brysbaert et al., 2014), and multimodal data (Hill and Korhonen, 2014b; Hill et al., 2014; Kiela et al., 2014; Young et al., 2014; Hessel et al., 2018; Silberer et al., 2017; Bhaskar et al., 2017). As with Hessel et al. (2018) and Kiela et al. (2014), our model uses multi-modal data to estimate concreteness. Compared with them, we define concreteness for spans instead of words, and use it to induce linguistic structures. 3 Visually Grounded Neural Syntax Learner Given a set of paired images and captions, our goal is to learn representations and structures for words"
P19-1180,P06-1072,0,0.0194056,") constituents. To our knowledge, the Parsing-Reading-Predict Network (PRPN; Shen et al., 2018a) and the Ordered Neuron LSTM (ON-LSTM; Shen et al., 2019) currently produce the best fully unsupervised constituency parsing results. One issue with PRPN, however, is that it tends to produce meaningless parses for lower-level (smaller) constituents (Phu Mon Htut et al., 2018). Over the last two decades, there has been extensive study targeting unsupervised constituency parsing (Klein and Manning, 2002, 2004, 2005; Bod, 2006a,b; Ponvert et al., 2011) and dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2010; Han et al., 2017). However, all of these approaches are based on linguistic annotations. Specifically, they operate on the part-of-speech tags of words instead of word tokens. One exception is Spitkovsky et al. (2011), which produces dependency parse trees based on automatically induced pseudo tags. In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparis"
P19-1180,P09-1009,0,0.582847,"instead of word tokens. One exception is Spitkovsky et al. (2011), which produces dependency parse trees based on automatically induced pseudo tags. In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related works in the supplementary material. There has been some prior work on improving unsupervised parsing by leveraging extra signals, such as parallel text (Snyder et al., 2009), annotated data in another language with parallel text (Ganchev et al., 2009), annotated data in other languages without parallel text (Cohen et al., 2011), or non-parallel text from multiple languages (Cohen and Smith, 2009). We leave the integration of other grounding signals as future work. Grounded language acquisition. Grounded language acquisition has been studied for imagecaption data (Christie et al., 2016a), video-caption data (Siddharth et al., 2014; Yu et al., 2015), and visual reasoning (Mao et al., 2019). However, existing approaches rely on human labels or rules for classifying"
P19-1180,D11-1118,0,0.0885202,"Missing"
P19-1180,N10-1116,0,0.0201771,"nowledge, the Parsing-Reading-Predict Network (PRPN; Shen et al., 2018a) and the Ordered Neuron LSTM (ON-LSTM; Shen et al., 2019) currently produce the best fully unsupervised constituency parsing results. One issue with PRPN, however, is that it tends to produce meaningless parses for lower-level (smaller) constituents (Phu Mon Htut et al., 2018). Over the last two decades, there has been extensive study targeting unsupervised constituency parsing (Klein and Manning, 2002, 2004, 2005; Bod, 2006a,b; Ponvert et al., 2011) and dependency parsing (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2010; Han et al., 2017). However, all of these approaches are based on linguistic annotations. Specifically, they operate on the part-of-speech tags of words instead of word tokens. One exception is Spitkovsky et al. (2011), which produces dependency parse trees based on automatically induced pseudo tags. In contrast to these existing approaches, we focus on inducing constituency parse trees with visual grounding. We use parallel data from another modality (i.e., paired images and captions), instead of linguistic annotations such as POS tags. We include a detailed comparison between some related w"
P19-1180,D11-1063,0,0.360918,"actions. Instead, our model induces syntax structures with no humandefined labels or rules. Meanwhile, learning visual-semantic representations in a joint embedding space (Ngiam et al., 2011) is a widely studied approach, and has achieved remarkable results on image-caption retrieval (Kiros et al., 2014; Faghri et al., 2018; Shi et al., 2018a), image caption generation (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Ma et al., 2015), and visual question answering (Malinowski et al., 2015). In this work, we borrow this idea to match visual and textual representations. Concreteness estimation. Turney et al. (2011) define concrete words as those referring to things, events, and properties that we can perceive directly with our senses. Subsequent work has studied 1843 A cat is on the ground Caption Structure and Representation Inference (Score-Sample-Combine) Constituency Parse Tree Embeddings of Constituents (?) Image Encoder ?3 (?) ?1 ?(?) (?) ?2 ?(?) Image Embedding Visual-Semantic Embeddings Image Figure 2: VG-NSL consists of two modules: a textual module for inferring structures and representations for captions, and a visual-semantic module for matching constituents with images. VG-NSL induces const"
P19-1180,Q18-1019,0,0.148579,"Missing"
P19-1180,J93-2004,0,\N,Missing
P19-1180,W06-2912,0,\N,Missing
P19-1180,P14-2118,0,\N,Missing
P19-1180,D14-1032,0,\N,Missing
P19-1180,P02-1017,0,\N,Missing
P19-1180,P06-1109,0,\N,Missing
P19-1180,Q14-1006,0,\N,Missing
P19-1180,Q14-1023,0,\N,Missing
P19-1180,D16-1156,0,\N,Missing
P19-1180,P18-1108,0,\N,Missing
P19-1180,N19-1114,0,\N,Missing
P19-1427,S14-2010,0,0.0425502,"Missing"
P19-1427,S16-1081,0,0.075813,"Missing"
P19-1427,S13-1004,0,0.0543439,"Missing"
P19-1427,S12-1051,0,0.0491652,"entum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation being considered, we select models with the highest performance on the validation set. 4 Experiments 4.1 where u is a candidate hypothesis, U(x) is a set of candidate hypotheses, and t is the reference. 7 Evaluation is on the SemEval Semantic Textual Similarity (STS) datasets from 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). In the SemEval STS competitions, teams create models that need to work well on domains both represented in the training data and hidden domains revealed at test time. Our model and those of Wieting and Gimpel (2018), in contrast to the best performing STS systems, do not use any manually-labeled training examples nor any other linguistic resources beyond the ParaNMT corpus (Wieting and Gimpel, 2018). 8 Available at https://github.com/pytorch/ fairseq. Data Training models with minimum risk is expensive, but we wanted to evaluate in a difficult, realistic setting usin"
P19-1427,2014.iwslt-evaluation.4,0,0.0200873,"hat nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when"
P19-1427,W11-2103,0,0.0721467,"Missing"
P19-1427,N10-1080,0,0.0160871,"scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted metrics to optimize during training or combinations of metrics and optimizers, given a fixed SMT system. The 2011 results showed that nearly all metrics p"
P19-1427,D17-1070,0,0.0180748,"the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we a"
P19-1427,W14-3348,0,0.0229286,"s evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding), comparing the generated translation"
P19-1427,N18-1033,0,0.107196,"sentence is about 20 times faster than METEOR when code is executed on GPU (NVIDIA GeForce GTX 1080). 6 We used the segment level data from newstest2015 and newstest2016 available at http://statmt.org/ wmt18/metrics-task.html. The former contains 7 language pairs and the latter 5. 4346 close, but the semantic similarity correlations7 in Table 1 are not, suggest that the difference between METEOR and SIM largely lies in fluency. However, not capturing fluency is something that can be ameliorated by adding a down-weighted maximum-likelihood (MLE) loss to the minimum risk loss. This was done by Edunov et al. (2018) and we use this in our experiments as well. 3 Lang. cs-en de-en ru-en tr-en Objective Functions. Following (Edunov et al., 2018), we first train models with maximum-likelihood with label-smoothing (LTokLS ) (Szegedy et al., 2016; Pereyra et al., 2017). We set the confidence penalty of label smoothing to be 0.1. Next, we fine-tune the model with a weighted average of minimum risk training (LRisk ) (Shen et al., 2015) and (LTokLS ), where the expected risk is defined as: u∈U (x) Test 2,983 2,998 3,000 3,000 Therefore, our fine-tuning objective becomes: LWeighted = γLTokLS + (1 − γ)LRisk Archite"
P19-1427,N16-1162,0,0.0688546,"Missing"
P19-1427,D11-1125,0,0.0338379,"ain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted met"
P19-1427,W16-2303,0,0.0303766,"Missing"
P19-1427,C04-1072,0,0.0918735,"mber of sentence pairs in the training/validation/test sets for all four languages. Machine Translation Preliminaries X Train 218,384 284,286 235,159 207,678 p(u|x) cost(t, u) P 0 u0 ∈U (x) p(u |x) We tune γ from the set {0.2, 0.3} in our experiments. In minimum risk training, we aim to minimized the expected cost. In our case that is 1 − BLEU(t, h) or 1 − S IMI L E(t, h) where t is the target and h is the generated hypothesis. As is commonly done, we use a smoothed version of BLEU by adding 1 to all n-gram counts except unigram counts. This is to prevent BLEU scores from being overly sparse (Lin and Och, 2004). We generate candidates for minimum risk training from n-best lists with 8 hypotheses without and do not include the reference in the candidates. Optimization. We optimize our models using Nesterov’s accelerated gradient method (Sutskever et al., 2013) using a learning rate of 0.25 and momentum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation be"
P19-1427,D11-1035,0,0.0238807,"mong metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in"
P19-1427,P13-2067,0,0.0266082,"2011 results showed that nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the"
P19-1427,P16-1159,0,0.0403668,"n terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in which tuning to optimize a metric leads to the best performance on that metric (Och, 2003). Edunov et al. (2018) compared structured losses for NMT, also using sentence-level BLEU. They found risk to be an effective and robust choice, so we use risk as well in this paper. 9 Conclusion We have proposed S IMI L E, an alternative to BLEU for use as a reward in minimum risk tra"
P19-1427,N19-4007,1,0.804774,"0.50 0.55 0.45 0.15 0.08 0.03 0.33 0.24 0.27 0.34 1.48 2.50 0.66 0.13 0.25 0.34 0.63 0.65 Table 6: F1 score for various buckets of words. The values in the table are the difference between F1 for that specific language type and bucket between training using S IMI L E and BLEU (positive values means S IM I L E had a higher F1). The first part of the table shows F1 scores across bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found"
P19-1427,P06-2101,0,0.06992,"do not know how to explain it - it is really unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015"
P19-1427,P03-1021,0,0.454799,"he case and it’s even possible for less accurate translations to have higher scores than more accurate ones. The bottom half of the table shows examples where the difference in BLEU scores is large, but the difference in SIM scores is small. From thexe examples we can see that when BLEU scores are very different, the semantics of the sentence can still be preserved. However, we observe that often in these cases, the SIM scores of the sentences tend to be similar. 8 Related Work The seminal work on training machine translation systems to optimize particular evaluation measures was performed by Och (2003), who intro4351 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Workers are beginning to clean up workers . Workers have begun to clean up in Rszke. In Rszke, workers are beginning to clean up. 3.2 -26.3 All that stuff sure does take a toll. None of this takes a toll . All of this is certain to take its toll . 7.1 -22.7 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Another advantage is that they have fewer enemies. Another benefit: they have less enemies. Another advantage: they have fewer enemies. -33.8 -9.6 I d"
P19-1427,W15-3032,0,0.0355517,"Missing"
P19-1427,P02-1040,0,0.10718,"c results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding"
P19-1427,P15-1094,0,0.112474,"rrect but lexically different translations. Moreover, since the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT train"
P19-1427,D07-1080,0,0.0565854,"lly unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In the"
P19-1427,P17-1190,1,0.867686,"s bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found that when training semantic embeddings using an averaging function, embeddings that bear the most information regarding the meaning have larger norms. We also see that these same parts-of-speech (nouns, proper nouns, numbers) have the largest difference in F1 scores between S IMI L E and BLEU. Other parts-of-speech like SYM and INTJ have high F1 scores as well, and words belongin"
P19-1427,P18-1042,1,0.892666,"timization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we add a length penalty to avoid short translations, and compose the embeddings of subword units, ra"
P19-1427,P19-1453,1,0.81423,"s, s0 i and we use a margin-based loss: `(s, s0 ) = max(0, δ − cos(g(s), g(s0 )) + cos(g(s), g(t))) 2 In semantic textual similarity the goal is to produce scores that correlate with human judgments on the degree to which two sentences have the same semantics. In embedding based models, including the models used in this paper, the score is produced by the cosine of the two sentence embeddings. 3 We use SentencePiece which is available at https:// github.com/google/sentencepiece. 4 We use 16.77 million paraphrase pairs extracted from the ParaNMT corpus (Wieting and Gimpel, 2018). Recently, in (Wieting et al., 2019) it has been shown that strong performance on semantic similarity tasks can also be achieved using bitext directly without the need for backtranslation. 4345 Model SIM S IMI L E Wieting and Gimpel (2018) BLEU METEOR STS 1st Place STS 2nd Place STS 3rd Place 2012 69.3 70.1 67.8 39.2 53.4 64.8 63.4 64.1 2013 64.1 59.8 62.8 29.5 47.6 62.0 59.1 58.3 2014 77.2 74.7 76.9 42.8 63.7 74.3 74.2 74.3 2015 80.3 79.4 79.8 49.8 68.8 79.0 78.0 77.8 2016 78.6 77.8 76.8 47.4 61.8 77.7 75.7 75.7 tion). However, we found that this favored short sentences. We instead penalize a generated sentence if its length di"
P19-1427,D16-1137,0,0.0755019,"Missing"
P19-1453,S14-2010,0,0.192289,"Missing"
P19-1453,S16-1081,0,0.10707,"Missing"
P19-1453,S13-1004,0,0.309609,"Missing"
P19-1453,S12-1051,0,0.146732,"overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the same meaning as measured by human judges. The evaluation metric is Pearson’s r with the gold labels. We use the small STS English-English dataset from Cer et al. (2017) for model selection. Second, we compare our best model, SP, on two semantic crosslingual tasks: the 2017 SemEval STS task (Cer et al., 2017) which consists of monolingual and cross-lingual datasets and the 2018 Building and Using Parallel Corpora (BUCC) shared bitext mining task (Zweigenbaum et al., 2018). 3.1 Hyperparameters and Optimization Unless"
P19-1453,N06-1003,0,0.0530907,"semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at http"
P19-1453,S17-2001,0,0.187252,"e additional benefit of creating cross-lingual representations that are useful for tasks such as mining or filtering parallel data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006;"
P19-1453,D17-1070,0,0.039622,"ets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly be"
P19-1453,P11-2117,0,0.0379629,"he resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence"
P19-1453,C04-1051,0,0.257736,"data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018"
P19-1453,N13-1092,0,0.0967382,"Missing"
P19-1453,C18-1122,0,0.06052,"Missing"
P19-1453,W18-6317,0,0.237718,"In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest me"
P19-1453,N16-1162,0,0.095929,"Missing"
P19-1453,D18-2012,0,0.0362043,"current pair. However, in the bilingual case, negative examples are only selected from the sentences in the batch from the opposing language. To select difficult negative examples that aid training, we use the mega-batching procedure of Wieting and Gimpel (2018), which aggregates M mini-batches to create one mega-batch and selects negative examples therefrom. Once each pair in the megabatch has a negative example, the mega-batch is split back up into M mini-batches for training. Encoders. Our primary sentence encoder simply averages the embeddings of subword units generated by sentencepiece (Kudo and Richardson, 2018); we refer to it as SP. This means that the sentence piece embeddings themselves are the only learned parameters of this model. As baselines we explore averaging character trigrams (T RIGRAM) (Wieting et al., 2016a) and words (W ORD). SP provides a compromise between averaging words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as t"
P19-1453,D17-1126,0,0.0250789,"ross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015"
P19-1453,L16-1147,0,0.0418628,"ell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedemann, 2016) and made a plot of their average STS performance on the 2012-2016 English datasets compared to their SP overlap6 and language distance.7 We segmented the languages separately and trained the models for 10 epochs using the 2017 STS task for model selection. The plot, shown in Figure 1, shows that SentencePieces (SP) overlap is highly correlated with STS score. There are also two clusters in the plot, languages that have a similar alphabet to English and those that do not. In each cluster we find that performance is negatively correlated with language distance. Therefore, languages similar to E"
P19-1453,E17-2002,0,0.141406,"ng that SP is hundreds of times faster. 4605 Model All Lang. Lang. (SP Ovl. ≤ 0.3) Lang. (SP Ovl. &gt; 0.3) Language Similarity Vs. Performance 0.75 indmsa Avg. STS Pearson&apos;s r 0.64 0.63 0.62 0.61 0.60 0.59 tha kor zho 0.70 hun glgron deu swedan ita spa boscat vie turpolnldpor fra norhrv srp ces est slk fin slv sqi lit eus isl lav bul ell heb rus ukr ara fas sinjpn mkd mal kat 0.65 Language Distance 0.65 0.60 0.55 Lang. Distance -22.8 -63.8 -34.2 Table 5: Spearman’s ρ × 100 between average performance on the 2012-2016 STS tasks compared to SP overlap (SP Ovl.) and language distance as defined by Littell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedema"
P19-1453,P15-1094,0,0.0479643,"Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature ha"
P19-1453,W17-2619,0,0.0335664,"and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when app"
P19-1453,P18-2035,0,0.017455,"s. 2 In fact, we show that for monolingual similarity, we can devise random encoders that outperform some of this work. 4602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4602–4608 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Learning Sentence Embeddings We first describe our objective function and then describe our encoder, in addition to several baseline encoders. The methodology proposed here borrows much from past work (Wieting and Gimpel, 2018; Guo et al., 2018; Gr´egoire and Langlais, 2018; Singla et al., 2018), but this specific combination has not been explored and, as we show in experiments, is surprisingly effective. Training. The training data consists of a sequence of parallel sentence pairs (si , ti ) in source and target languages respectively. For each sentence pair, we randomly choose a negative target sentence t0i during training that is not a translation of si . Our objective is to have source and target sentences be more similar than source and negative target examples by a margin δ: i Xh min δ−fθ (si , ti ) + fθ (s, t0i )) . θsrc ,θtgt i + The similarity function is defined as:   fθ"
P19-1453,D16-1157,1,0.904892,"ese intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest method to date that is able to achieve state-of-the-art results on multiple monolingual and cross-lingual semantic textual similarity (STS) and parallel corpora mining tasks.2 Lastly, since bitext is available for so many language"
P19-1453,P17-1190,1,0.866482,"words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as the input tokens. For all encoders, when the vocabularies of source and target languages overlap, the corresponding encoder embedding parameters are shared. As a result, languages pairs with more lexical overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the s"
P19-1453,P18-1042,1,0.918867,"irs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from n"
P19-1453,D17-1026,1,0.872931,"asets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on model"
P19-1453,P18-2037,0,0.407958,"their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these repr"
P19-1599,P17-2054,0,0.0202285,"oncurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological"
P19-1599,W05-0909,0,0.0613396,"PL VGVAE + LC VGVAE + LC + WPL VGVAE + WN VGVAE + WN + WPL VGVAE + LC + WN + WPL 3.5 4.5 3.3 5.9 13.0 13.2 13.6 24.8 26.5 24.0 29.1 43.2 43.4 44.7 Prior work using supervised parsers SCPN + template SCPN + full parse 17.8 19.2 47.9 50.4 22.8 26.1 Table 1: Test results. The final metric (ST) measures the syntactic match between the output and the reference. automatic evaluation metrics, designed to capture different components of the task. To measure roughly the amount of semantic content that matches between the predicted output and the reference, we report BLEU score (BL), METEOR score (MET; Banerjee and Lavie, 2005) and three ROUGE scores, including ROUGE-1 (R1), ROUGE-2 (R-2) and ROUGE-L (R-L). Even though these metrics are not purely based on semantic matching, we refer to them in this paper as “semantic metrics” to differentiate them from our second metric category, which we refer to as a “syntactic metric”. For the latter, to measure the syntactic similarity between generated sentences and the reference, we report the syntactic tree edit distance (ST). To compute ST, we first parse the sentences using Stanford CoreNLP (Manning et al., 2014), and then compute the tree edit distance (Zhang and Shasha,"
P19-1599,W18-3403,0,0.0167439,"o use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and"
P19-1599,K16-1002,0,0.561572,"ck of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g., its sentiment or formality) (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017; Fu et al., 2018; Zhao et al., 2018; Fan et al., 2018, inter alia), there is also recent work which attempts to control structural aspects of the generation, such as its l"
P19-1599,P18-1015,0,0.0512825,"the content from one image and the style from another (Gatys et al., 2016). In particular, in our setting, we seek to generate a sentence that combines the semantics from one sentence with the syntax from another, and so we only require a pair of (unparsed) sentences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x"
P19-1599,S17-2001,0,0.0185673,"esumably caused by the fact that LSTMs have sequence information, making the optimization of WPL trivial. We also observe that adding WPL to both the encoder and decoder brings the largest improvement. 7.2 Encoder Analysis To investigate what has been learned in the encoder, we evaluate qφ (y|x) and qφ (z|x) on both semantic similarity tasks and syntactic similarity tasks and also inspect the latent codes. Semantic Similarity. We use cosine similarity between two variables encoded by the inference networks as the predictions and then compute Pearson correlations on the STS Benchmark test set (Cer et al., 2017). As shown in Table 4, the semantic variable y always outperforms the syntactic variable z by a large margin, suggesting that different variables have captured different information. Every time when we add WPL the differences in performance between the two variables increases. Moreover, the differences between these two variables are correlated with the performance of models in Table 1, showing that a better generation system has a more disentangled latent representation. Syntactic Similarity. We use the syntactic evaluation tasks from Chen et al. (2019) to evaluate the syntactic knowledge enc"
P19-1599,N18-2116,1,0.830018,"rNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2naEPwFl9eJs1qxbuoVO8vy7WbPI4CHMMJnIEHV1CDO6hDAxgM4Ble4c2Rzovz7nzMW1ecfOYI/sD5/AG3041s&lt;/latexit&gt; Latent Codes for Syntactic Encoder Since what we want from the syntactic encoder is only the syntactic structure of a sentence, using standard word embeddings tends to mislead the syntactic encoder to believe the syntax is manifested by the exact word tokens. An example is that the generated sentence often preserves the exact pronouns or function words in the syntactic input instead of making necessary changes based on the semantics. To alleviate this, we follow Chen and Gimpel (2018) to represent each word with a latent code (LC) for word clusters within the word embedding layer. Our goal is for this to create a bottleneck layer in the word embeddings, thereby forcing the syntactic encoder to learn a more abstract representation of the syntax. However, since our purpose is not to reduce model size (unlike Chen and Gimpel, 2018), we marginalize out the latent code to get the embeddings during both training and testing. That is, ew = y1 x2 t+1 decoder and concatenate the semantic variable y with the hidden vector output by the decoder for predicting the word at the next tim"
P19-1599,D18-1020,1,0.940251,"ed multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and Neubig, 2017), sequence labeling (Chen et al., 2018), and sentence representations (Chen et al., 2019). 3 Methods Given two sentences X and Y , our goal is to generate a sentence Z that follows the syntax of Y and the semantics of X. We refer to X and Y as the semantic template and syntactic template, respectively. To solve this problem, we follow Chen et al. (2019) and take an approach based on latentvariable probabilistic modeling, neural variational inference, and multi-task learning. In particular, we assume a generative model that has two latent variables: y for semantics and z for syntax (as depicted in Figure 2). We refer to our model as"
P19-1599,N19-1254,1,0.934808,"uality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and Neubig, 2017), sequence labeling (Chen et al., 2018), and sentence representations (Chen et al., 2019). 3 Methods Given two sentences X and Y , our goal is to generate a sentence Z that follows the syntax of Y and the semantics of X. We refer to X and Y as the semantic template and syntactic template, respectively. To solve this problem, we follow Chen et al. (2019) and take an approach based on latentvariable probabilistic modeling, neural variational inference, and multi-task learning. In particular, we assume a generative model that has two latent variables: y for semantics and z for syntax (as depicted in Figure 2). We refer to our model as a vMF-Gaussian Variational Autoencoder (VGVAE). F"
P19-1599,D17-1091,0,0.190135,"tic knowledge, bringing consistent and sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct"
P19-1599,D18-1354,0,0.0222362,"entential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and Neubig, 2017), sequence labeling (Chen et al., 2018), and sentence representations (Chen et al., 2019). 3 Methods Given two sentences X and Y , our goal is to generate a sentence Z that follow"
P19-1599,W18-2706,0,0.126383,"eristics.1 1 Introduction Controllable text generation has recently become an area of intense focus in the natural language processing (NLP) community. Recent work has focused both on generating text satisfying certain stylistic requirements such as being formal or exhibiting a particular sentiment (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017), as well as on generating text meeting structural requirements, such as conforming to a particular template (Iyyer et al., 2018; Wiseman et al., 2018). These systems can be used in various application areas, such as text summarization (Fan et al., 2018), adversarial example generation (Iyyer et al., 2018), dialogue (Niu and Bansal, 2018), and data-to-document generation (Wiseman et al., 2018). However, prior work on controlled generation has typically assumed a known, finite set of values that the controlled attribute can take on. In this work, we are interested instead in the novel setting where the generation is controlled 1 Code and data are available at github.com/ mingdachen/syntactic-template-generation through an exemplar sentence (where any syntactically valid sentence is a valid exemplar). We will focus in particular on using a sent"
P19-1599,W17-4912,0,0.193644,"fically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.1 1 Introduction Controllable text generation has recently become an area of intense focus in the natural language processing (NLP) community. Recent work has focused both on generating text satisfying certain stylistic requirements such as being formal or exhibiting a particular sentiment (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017), as well as on generating text meeting structural requirements, such as conforming to a particular template (Iyyer et al., 2018; Wiseman et al., 2018). These systems can be used in various application areas, such as text summarization (Fan et al., 2018), adversarial example generation (Iyyer et al., 2018), dialogue (Niu and Bansal, 2018), and data-to-document generation (Wiseman et al., 2018). However, prior work on controlled generation has typically assumed a known, finite set of values that the controlled attribute can take on. In this work, we are interested instead in the novel setting w"
P19-1599,Q18-1031,0,0.0363813,"mputer vision, in which an image is generated that combines the content from one image and the style from another (Gatys et al., 2016). In particular, in our setting, we seek to generate a sentence that combines the semantics from one sentence with the syntax from another, and so we only require a pair of (unparsed) sentences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018)."
P19-1599,N18-1170,1,0.871423,"ning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.1 1 Introduction Controllable text generation has recently become an area of intense focus in the natural language processing (NLP) community. Recent work has focused both on generating text satisfying certain stylistic requirements such as being formal or exhibiting a particular sentiment (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017), as well as on generating text meeting structural requirements, such as conforming to a particular template (Iyyer et al., 2018; Wiseman et al., 2018). These systems can be used in various application areas, such as text summarization (Fan et al., 2018), adversarial example generation (Iyyer et al., 2018), dialogue (Niu and Bansal, 2018), and data-to-document generation (Wiseman et al., 2018). However, prior work on controlled generation has typically assumed a known, finite set of values that the controlled attribute can take on. In this work, we are interested instead in the novel setting where the generation is controlled 1 Code and data are available at github.com/ mingdachen/syntactic-template-generation through"
P19-1599,P18-1139,0,0.0190804,"on is controlled 1 Code and data are available at github.com/ mingdachen/syntactic-template-generation through an exemplar sentence (where any syntactically valid sentence is a valid exemplar). We will focus in particular on using a sentential exemplar to control the syntactic realization of a generated sentence. This task can benefit natural language interfaces to information systems by suggesting alternative invocation phrases for particular types of queries (Kumar et al., 2017). It can also bear on dialogue systems that seek to generate utterances that fit particular functional categories (Ke et al., 2018; Li et al., 2019). To address this task, we propose a deep generative model with two latent variables, which are designed to capture semantics and syntax. To achieve better disentanglement between these two variables, we design multi-task learning objectives that make use of paraphrases and word order information. To further facilitate the learning of syntax, we additionally propose to train the syntactic component of our model with word noising and latent word-cluster codes. Word noising randomly replaces word tokens in the syntactic inputs based on a part-of-speech tagger used only at train"
P19-1599,D18-1421,0,0.174402,"nd sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g."
P19-1599,W04-1013,0,0.0367084,"damage in this area seems to be quite minimal. Z: the capacity of this office needs to be reinforced even further. Figure 1: Examples from our annotated evaluation dataset of paraphrase generation using semantic input X (red), syntactic exemplar Y (blue), and the reference output Z (black). similarity between the syntactic inputs and the references, and (3) syntactic variation between the semantic input and references. Examples are shown in Figure 1. This dataset allows us to evaluate different approaches quantitatively using standard metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). As the success of controllability of generated sentences also largely depends on the syntactic similarity between the syntactic exemplar and the reference, we propose a “syntactic similarity” metric based on evaluating tree edit distance between constituency parse trees of these two sentences after removing word tokens. Empirically, we benchmark the syntacticallycontrolled paraphrase network (SCPN) of Iyyer et al. (2018) on this novel dataset, which shows strong performance with the help of a supervised parser at test-time but also can be sensitive to the quality of the parse predictor. We s"
P19-1599,N18-1018,0,0.073569,"ging consistent and sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of ge"
P19-1599,E17-1083,0,0.108056,"ely characterizes syntactic knowledge, bringing consistent and sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks"
P19-1599,P14-5010,0,0.00475446,"reference, we report BLEU score (BL), METEOR score (MET; Banerjee and Lavie, 2005) and three ROUGE scores, including ROUGE-1 (R1), ROUGE-2 (R-2) and ROUGE-L (R-L). Even though these metrics are not purely based on semantic matching, we refer to them in this paper as “semantic metrics” to differentiate them from our second metric category, which we refer to as a “syntactic metric”. For the latter, to measure the syntactic similarity between generated sentences and the reference, we report the syntactic tree edit distance (ST). To compute ST, we first parse the sentences using Stanford CoreNLP (Manning et al., 2014), and then compute the tree edit distance (Zhang and Shasha, 1989) between constituency parse trees after removing word tokens. 6.3 Baselines We report results for three baselines. The first two baselines directly output the corresponding syntactic or semantic input for each instance. For the last baseline, we consider SCPN (Iyyer et al., 2018). As SCPN requires parse trees for both the syntactic and semantic inputs, we follow the process in their paper and use the Stanford shiftreduce constituency parser (Manning et al., 2014) to parse both, then use the parsed sentences as inputs to SCPN. We"
P19-1599,Q18-1027,0,0.0232701,"of intense focus in the natural language processing (NLP) community. Recent work has focused both on generating text satisfying certain stylistic requirements such as being formal or exhibiting a particular sentiment (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017), as well as on generating text meeting structural requirements, such as conforming to a particular template (Iyyer et al., 2018; Wiseman et al., 2018). These systems can be used in various application areas, such as text summarization (Fan et al., 2018), adversarial example generation (Iyyer et al., 2018), dialogue (Niu and Bansal, 2018), and data-to-document generation (Wiseman et al., 2018). However, prior work on controlled generation has typically assumed a known, finite set of values that the controlled attribute can take on. In this work, we are interested instead in the novel setting where the generation is controlled 1 Code and data are available at github.com/ mingdachen/syntactic-template-generation through an exemplar sentence (where any syntactically valid sentence is a valid exemplar). We will focus in particular on using a sentential exemplar to control the syntactic realization of a generated sentence. This tas"
P19-1599,P18-1123,0,0.0277648,"erated that combines the content from one image and the style from another (Gatys et al., 2016). In particular, in our setting, we seek to generate a sentence that combines the semantics from one sentence with the syntax from another, and so we only require a pair of (unparsed) sentences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing"
P19-1599,P02-1040,0,0.105484,"the agency’s capacities. Y : the damage in this area seems to be quite minimal. Z: the capacity of this office needs to be reinforced even further. Figure 1: Examples from our annotated evaluation dataset of paraphrase generation using semantic input X (red), syntactic exemplar Y (blue), and the reference output Z (black). similarity between the syntactic inputs and the references, and (3) syntactic variation between the semantic input and references. Examples are shown in Figure 1. This dataset allows us to evaluate different approaches quantitatively using standard metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). As the success of controllability of generated sentences also largely depends on the syntactic similarity between the syntactic exemplar and the reference, we propose a “syntactic similarity” metric based on evaluating tree edit distance between constituency parse trees of these two sentences after removing word tokens. Empirically, we benchmark the syntacticallycontrolled paraphrase network (SCPN) of Iyyer et al. (2018) on this novel dataset, which shows strong performance with the help of a supervised parser at test-time but also can be sensitive to the quality of the"
P19-1599,N19-1263,0,0.0446104,"ne image and the style from another (Gatys et al., 2016). In particular, in our setting, we seek to generate a sentence that combines the semantics from one sentence with the syntax from another, and so we only require a pair of (unparsed) sentences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figur"
P19-1599,D14-1162,0,0.0806882,"Missing"
P19-1599,P16-2067,0,0.0222054,"ntences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled represe"
P19-1599,W18-5713,0,0.0292258,"which an image is generated that combines the content from one image and the style from another (Gatys et al., 2016). In particular, in our setting, we seek to generate a sentence that combines the semantics from one sentence with the syntax from another, and so we only require a pair of (unparsed) sentences. We also note recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluatio"
P19-1599,C16-1275,0,0.168097,"position loss effectively characterizes syntactic knowledge, bringing consistent and sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllabl"
P19-1599,W04-3219,0,0.309887,"that using our word position loss effectively characterizes syntactic knowledge, bringing consistent and sizeable improvements over syntactic-related evaluation. The latent code module learns interpretable latent representations. Additionally, all of our models can achieve improvements over baselines. Qualitatively, we show that our models do suffer from the lack of an abstract syntactic representation, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas mu"
P19-1599,P17-1194,0,0.0246,"e recent, concurrent work that attempts to use sentences as exemplars in controlling generation (Wang et al., 2019) in the context of data-to-document generation (Wiseman et al., 2017). Another related line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in"
P19-1599,D17-1066,0,0.0315723,"on, though we also show that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g., its sentiment or formality) (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017; Fu et al., 2018; Zhao et al., 2018; Fan et al., 2018, inter alia), there is also recent work which attempts to control structural aspects of the generation, such as its latent (Wiseman et al., 2018) or syntactic ("
P19-1599,W17-4308,0,0.0284307,"that SCPN and our models exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g., its sentiment or formality) (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017; Fu et al., 2018; Zhao et al., 2018; Fan et al., 2018, inter alia), there is also recent work which attempts to control structural aspects of the generation, such as its latent (Wiseman et al., 2018) or syntactic (Iyyer et al., 2018) t"
P19-1599,P19-1200,0,0.0122027,"tifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g., its sentiment or formality) (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017; Fu et al., 2018; Zhao et al., 2018; Fan et al., 2018, inter alia), there is also recent work which attempts to control structural aspects of the generation, such as its latent (Wiseman et al., 2018) or syntactic (Iyyer et al., 2018) template. Our work is closely related to th"
P19-1599,N03-1033,0,0.122971,"; Bowman et al., 2016; Higgins et al., 2016), the weight of the KL divergence in Equation 1 can be important when learning with latent variables. We attach weights to the KL divergence in Equation 1 and tune them based on development set performance. 5.2 Word Noising via Part-of-Speech Tags In practice, we often observe that the syntactic encoder tends to remember word types instead of learning syntactic structures. To provide a more flexible notion of syntax, we add word noising (WN) based on part-of-speech (POS) tags. More specifically, we tag the training set using the Stanford POS tagger (Toutanova et al., 2003). Then we group the word types based on the top two most frequent tags for each word type. During training, as shown in Figure 5, we noise the syntactic inputs by randomly replacing word tokens based on the groups and tags we obtained. This provides our framework many examples of word interchangeability based on POS tags, and discourages the syntactic encoder from memorizing the Experiments Training Setup For training with the PRL, we require a training set of sentential paraphrase pairs. We use ParaNMT (Wieting and Gimpel, 2018), a dataset of approximately 50 million paraphrase pairs. To ensu"
P19-1599,P18-1042,1,0.828678,"ecifically, we tag the training set using the Stanford POS tagger (Toutanova et al., 2003). Then we group the word types based on the top two most frequent tags for each word type. During training, as shown in Figure 5, we noise the syntactic inputs by randomly replacing word tokens based on the groups and tags we obtained. This provides our framework many examples of word interchangeability based on POS tags, and discourages the syntactic encoder from memorizing the Experiments Training Setup For training with the PRL, we require a training set of sentential paraphrase pairs. We use ParaNMT (Wieting and Gimpel, 2018), a dataset of approximately 50 million paraphrase pairs. To ensure there is enough variation between paraphrases, we filter out paraphrases with high BLEU score (Papineni et al., 2002) between the two sentences in each pair, which leaves us with around half a million paraphrases as our training set. All hyperparameter tuning is based on the BLEU score on the development set (see appendix for more details). 6.2 Evaluation Dataset and Metrics To evaluate models quantitatively, we manually annotate 1300 instances based on paraphrase pairs from ParaNMT independent from our training set. Each inst"
P19-1599,D18-1356,1,0.917304,"he proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.1 1 Introduction Controllable text generation has recently become an area of intense focus in the natural language processing (NLP) community. Recent work has focused both on generating text satisfying certain stylistic requirements such as being formal or exhibiting a particular sentiment (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017), as well as on generating text meeting structural requirements, such as conforming to a particular template (Iyyer et al., 2018; Wiseman et al., 2018). These systems can be used in various application areas, such as text summarization (Fan et al., 2018), adversarial example generation (Iyyer et al., 2018), dialogue (Niu and Bansal, 2018), and data-to-document generation (Wiseman et al., 2018). However, prior work on controlled generation has typically assumed a known, finite set of values that the controlled attribute can take on. In this work, we are interested instead in the novel setting where the generation is controlled 1 Code and data are available at github.com/ mingdachen/syntactic-template-generation through an exemplar sentence (w"
P19-1599,D18-1480,0,0.0192422,"els exhibit similar artifacts. 2 Related Work We focus primarily on the task of paraphrase generation, which has received significant recent attention (Quirk et al., 2004; Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017; Ma et al., 2018; Li et al., 2018). In order to disentangle the syntactic and semantic aspects of paraphrase generation we learn an explicit latent variable model using a variational autoencoder (VAE) (Kingma and Welling, 2014), which is now commonly applied to text generation (Bowman et al., 2016; Miao et al., 2016; Semeniuta et al., 2017; Serban et al., 2017; Xu and Durrett, 2018; Shen et al., 2019). In seeking to control generation with exemplars, our approach relates to recent work in controllable text generation. Whereas much work on controllable text generation seeks to control distinct attributes of generated text (e.g., its sentiment or formality) (Hu et al., 2017; Shen et al., 2017; Ficler and Goldberg, 2017; Fu et al., 2018; Zhao et al., 2018; Fan et al., 2018, inter alia), there is also recent work which attempts to control structural aspects of the generation, such as its latent (Wiseman et al., 2018) or syntactic (Iyyer et al., 2018) template. Our work is c"
P19-1599,P17-1061,0,0.023995,"d line of work builds generation upon sentential exemplars (Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) in order to improve the quality of the generation itself, rather than to allow for control over syntactic structures. There has been a great deal of work in applying multi-task learning to improve performance on NLP tasks (Plank et al., 2016; Rei, 2017; Augenstein and Søgaard, 2017; Bollmann et al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and Neubig, 2017), sequence labeling (Chen et al., 2018), and sentence representations (Chen et al., 2019). 3 Methods Given two sentences X and Y , our goal is"
P19-1599,P17-1029,0,0.134057,"al., 2018, inter alia). Some recent work used multi-task learning as a way of improving the quality or disentanglement of learned representations (Zhao et al., 2017; Goyal et al., 2017; Du et al., 2018; John et al., 2018). Part of our evaluation involves assessing the dif5973 z x x log pθ (x) ≥ y Figure 2: Graphical model. Dashed lines indicate the inference model. Solid lines indicate the generative model. − log = ferent characteristics captured in the semantic and syntactic encoders, relating them to work on learning disentangled representations in NLP, including morphological reinflection (Zhou and Neubig, 2017), sequence labeling (Chen et al., 2018), and sentence representations (Chen et al., 2019). 3 Methods Given two sentences X and Y , our goal is to generate a sentence Z that follows the syntax of Y and the semantics of X. We refer to X and Y as the semantic template and syntactic template, respectively. To solve this problem, we follow Chen et al. (2019) and take an approach based on latentvariable probabilistic modeling, neural variational inference, and multi-task learning. In particular, we assume a generative model that has two latent variables: y for semantics and z for syntax (as depicted"
Q15-1005,S07-1002,0,0.0410802,"the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are Ng global context words (wg ) and N` local context words (w` ), a"
Q15-1005,P14-2131,1,0.815957,"esented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of e"
Q15-1005,S13-2050,0,0.121632,"012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´ero"
Q15-1005,S07-1060,0,0.00951226,"2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context i"
Q15-1005,D07-1109,0,0.00754963,"ge that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element"
Q15-1005,E09-1013,0,0.500978,"lly conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep the local context of each ambiguous word, discarding the global context. However, the topical information contained in the broader context, though it may not determine the sense directly, might still be useful for narrowing down the likely senses of the ambiguous word. Consider the ambi"
Q15-1005,J93-2003,0,0.0664727,"uations. 62 the model. We later include an empirical comparison to justify some of our modeling choices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation dis"
Q15-1005,D07-1108,0,0.0103102,"(HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous"
Q15-1005,P05-1048,0,0.0329534,", expensive, and subject to poor 60 inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it ca"
Q15-1005,D07-1007,0,0.0209944,"se a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional w"
Q15-1005,P11-1098,0,0.0420272,"Missing"
Q15-1005,E03-1020,0,0.0393647,"aseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are Ng global context words"
Q15-1005,D09-1046,0,0.023234,"on (WSI) is the task of automatically discovering all senses of an ambiguous word in a corpus. The inputs to WSI are instances of the ambiguous word with its surrounding context. The output is a grouping of these instances into clusters corresponding to the induced senses. WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep"
Q15-1005,P09-1002,0,0.0144135,"Missing"
Q15-1005,ide-suderman-2004-american,0,0.0506654,", all of which are observed. There is one latent variable (“topic” tg ) for the wg and two latent variables (“topic” t` and “sense” s` ) for the w` . Each instance has topic mixing proportions θt and sense mixing proportions θs . For clarity, not all variables are shown. The complete figure with all variables is given in Appendix A. This is a dependency network, not a directed graphical model, as shown by the directed arrows between t` and s` ; see text for details. dataset released for SemEval-2013 Task 13 (Jurgens and Klapaftis, 2013), collected from the Open American National Corpus (OANC; Ide and Suderman, 2004).2 It includes 50 target words: 20 verbs, 20 nouns, and 10 adjectives. There are a total of 4,664 instances across all target words. Each instance contains only one sentence, with a minimum length of 22 and a maximum length of 100. The gold standard for the dataset was prepared by multiple annotators, where each annotator labeled instances based on the sense inventories in WordNet 3.1. For each instance, they rated all senses of a target word on a Likert scale from one to five. 4 A Sense-Topic Model for WSI We now present our sense-topic model, shown in plate notation in Figure 1. It generates"
Q15-1005,S13-2049,0,0.165256,"arate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language model"
Q15-1005,N13-1062,0,0.0118433,"automatically discovering all senses of an ambiguous word in a corpus. The inputs to WSI are instances of the ambiguous word with its surrounding context. The output is a grouping of these instances into clusters corresponding to the induced senses. WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep the local conte"
Q15-1005,P02-1017,0,0.0184036,"ices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation distribution: Pr(w` |t` = j, s` = k) = Pψtj(w` |t` = j)Pψsk(w` |s` = k) Ztj ,sk where Ztj ,sk is a normali"
Q15-1005,E12-1060,0,0.237265,"formance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full disc"
Q15-1005,S13-2051,0,0.0614558,"c model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic"
Q15-1005,P10-1116,0,0.00968942,"ly discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplicati"
Q15-1005,D07-1038,0,0.0120413,"lating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation distribution: Pr(w` |t` = j, s` = k) = Pψtj(w` |t` = j)Pψsk(w` |s` = k) Ztj ,sk where Ztj ,sk is a normalization factor, and we"
Q15-1005,P10-2041,0,0.0212012,"Missing"
Q15-1005,passonneau-etal-2010-word,0,0.0283751,"deling to WSI as well as other approaches that use word embeddings and clustering algorithms. WSD and WSI: WSI is related to but distinct from word sense disambiguation (WSD). WSD seeks to assign a particular sense label to each target word instance, where the sense labels are known and usually drawn from an existing sense inventory like WordNet (Miller et al., 1990). Although extensive research has been devoted to WSD, WSI may be more useful for downstream tasks. WSD relies on sense inventories whose construction is time-intensive, expensive, and subject to poor 60 inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis,"
Q15-1005,D09-1146,0,0.0473864,"Missing"
Q15-1005,W04-2406,0,0.109124,"external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the giv"
Q15-1005,J98-1004,0,0.845845,"Missing"
Q15-1005,P10-1040,0,0.00846388,"al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment)"
Q15-1005,H05-1097,0,0.0351615,"inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on a"
Q15-1005,W11-1102,0,0.240639,"Missing"
Q15-1025,P05-1074,0,0.0343034,"Mohit Bansal† Kevin Gimpel† Karen Livescu† Dan Roth∗ ∗ University of Illinois at Urbana-Champaign, Urbana, IL, 61801, USA {wieting2,danr}@illinois.edu † Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA {mbansal,kgimpel,klivescu}@ttic.edu Abstract One component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning. The most recent work in this area is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora. The PPDB is a massive resource, containing 220 million paraphrase pairs. It captures many short paraphrases that would be difficult to obtain using any other resource. For example, the pair {we must do our utmost, we must make every effort} has little lexical overlap but is present in PPDB. The PPDB has recently been used for monolingual alignment (Yao et al., 2013), for predicting sentence similarity (Bjerva et al., 2014), and to improve the coverage of FrameNet (Rastogi and Van Durme, 2014). The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensi"
Q15-1025,P14-2131,1,0.218757,"46 our code and the trained models.3 2 Related Work There is a vast literature on representing words as vectors. The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddin"
Q15-1025,D10-1115,0,0.0117713,"pularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010). An alternative approach to composition, used by Socher et al. (2011), is to train a"
Q15-1025,P14-1133,0,0.033444,"over, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size"
Q15-1025,S14-2114,0,0.0312541,"Missing"
Q15-1025,D12-1050,0,0.0101664,"rds have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositiona"
Q15-1025,C04-1051,0,0.17504,"notatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases. Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). To our knowledge, there are no datasets that focus on the paraphrasability of short phrases. Thus, we created Annotated-PPDB so that researchers can focus on local compositional phenomena and measure the performance of models directly—avoiding the need to do so indirectly in a sentence-level task. Models that have strong performance on Annotated-PPDB can be used to provide more accurate confidence scores for the paraphrases in the PPDB as well as reduce the need for large paraphrase tables altogether. Annotated-PPDB was created in a multi-step process (outlined below) in"
Q15-1025,P13-1158,0,0.0201205,"well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of par"
Q15-1025,N15-1184,0,0.352035,"Missing"
Q15-1025,N13-1092,0,0.295969,"Missing"
Q15-1025,D14-1163,0,0.0687883,"els were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010). An alternative approach to composition, used by Socher et al. (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree. In particular, they trained their RNN as an unsupervised autoencoder. The RNN captures the latent structure of composition. Recent work has shown that this model struggles in tasks involving compositionality (Blacoe an"
Q15-1025,J15-4004,0,0.276726,"Missing"
Q15-1025,D14-1181,0,0.00646945,"est results reported to date. We also find that we can train low-dimensional word vectors that exceed the performance of much larger vectors. This is very useful as using large vectors can increase both time and memory consumption in NLP applications. To generate word vectors to use for downstream 10 Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results. 351 Sentiment Analysis As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis. We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating. We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821. The CNN uses m-gram filters, each of which is an m × n vector. The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called “max-pooling”). The score of the match is a single dimension in a feature vector"
Q15-1025,P14-5010,0,0.00474436,"rase bigrams, we consider the original bigram similarity task from Mitchell and Lapata (2010) as well as our newlyannotated version of it: ML-Paraphrase. 6.2 Evaluation and Baselines Extracting Training Data Training data for these tasks was extracted from the XL portion of PPDB. The bigram similarity task from Mitchell and Lapata (2010) contains three types of bigrams: adjectivenoun (JN), noun-noun (NN), and verb-noun (VN). We aimed to collect pairs from PPDB that mirrored these three types of bigrams. We found parsing to be unreliable on such short segments of text, so we used a POS tagger (Manning et al., 2014) to tag the tokens in each phrase. We then used the word alignments in PPDB to extract bigrams for training. For JN and NN, we extracted pairs containing aligned, adjacent tokens in the two phrases with the appropriate part-of-speech tag. Thus we extracted pairs like heasy job, simple taski for the JN section and htown meeting, town councili for the NN section. We used a different strategy for extracting training data for the VN subset: we took aligned VN tokens and took the closest noun after the verb. This was done to approximate the direct object that would have been ideally extracted with"
Q15-1025,D09-1040,0,0.011396,"Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size of the dataset used to build it. In practice, it can become unwieldy to work with as the size of th"
Q15-1025,P08-1028,0,0.0146968,"rs (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alterna"
Q15-1025,D14-1162,0,0.103607,"evaluating bigram paraphrases. We release the new datasets, complete with annotation instructions and raw annotations, as well as 346 our code and the trained models.3 2 Related Work There is a vast literature on representing words as vectors. The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition us"
Q15-1025,W04-3219,0,0.260201,"Missing"
Q15-1025,W14-2901,0,0.0392114,"Missing"
Q15-1025,W03-1604,0,0.100189,"Missing"
Q15-1025,D13-1170,0,0.00571035,"sional word vectors that exceed the performance of much larger vectors. This is very useful as using large vectors can increase both time and memory consumption in NLP applications. To generate word vectors to use for downstream 10 Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results. 351 Sentiment Analysis As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis. We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating. We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821. The CNN uses m-gram filters, each of which is an m × n vector. The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called “max-pooling”). The score of the match is a single dimension in a feature vector for the example, which is then associated with a weight in a linear classifier"
Q15-1025,Q14-1017,0,0.634426,". (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree. In particular, they trained their RNN as an unsupervised autoencoder. The RNN captures the latent structure of composition. Recent work has shown that this model struggles in tasks involving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).4 However, we found suc3 http://web.engr.illinois.edu/˜wieting2/ We also replicated this approach and found training to be time-consuming even using low-dimensional word vectors. 4 cess using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Data"
Q15-1025,D13-1056,0,0.0157861,"Missing"
Q15-1025,P14-2089,0,0.216257,"e also replicated this approach and found training to be time-consuming even using low-dimensional word vectors. 4 cess using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Datasets We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evalu"
Q15-1025,Q15-1017,0,0.0125763,"ed setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Datasets We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases. Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or ent"
Q15-1025,C10-1142,0,0.0109219,"Missing"
Q15-1025,N09-1003,0,\N,Missing
S16-1170,S12-1051,0,0.0526721,"ning entry in STS2015 when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ig"
S16-1170,S13-1004,0,0.0254217,"when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitiv"
S16-1170,S14-2010,0,0.0264574,"e STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitive interactions betwee"
S16-1170,S16-1081,0,0.0107772,"wieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outperforms the winning entry in STS2015 when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu"
S16-1170,P09-1053,0,0.0150928,"Missing"
S16-1170,N13-1092,0,0.0663307,"Missing"
S16-1170,P12-1091,0,0.0208662,"Missing"
S16-1170,N16-1108,1,0.380292,"Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outperforms the winning entry in STS2015 when eva"
S16-1170,D15-1181,1,0.645682,"ual Similarity Measurement Hua He1 , John Wieting2 , Kevin Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our"
S16-1170,S14-2072,0,0.0341736,"Missing"
S16-1170,S14-2078,0,0.0273443,"Missing"
S16-1170,N12-1019,0,0.0509841,"Missing"
S16-1170,P14-5010,0,0.00200919,"s yield sentence embeddings (via simple averaging) that perform well across STS tasks without task-specific tuning. Their performance is thought to be due in part to how the vectors for less important words have smaller norms than those for information-bearing words. 5 1st run 0.6607 0.7946 0.8154 0.8094 0.6187 0.7420 Pearson’s r 0.8040 0.7948 0.7622 0.7721 0.8015 Table 3: Ablation study on STS2015 test data. Experiments and Results Datasets. The test data of the SemEval-2016 English STS competition consists of five datasets from different domains. We tokenize all data using Stanford CoreNLP (Manning et al., 2014). Each pair has a similarity score ∈ [0, 5] which increases with similarity. We use training data from previous STS competitions (2012 to 2015). Table 1 provides a brief description. Experimental Settings. We largely follow the same experimental settings as He et al. (2015), e.g., we perform optimization with stochastic gradient descent using a fixed learning rate of 0.01. We use the 300-dimensional PARAGRAM - PHRASE XXL word embeddings (d = 300). Results on STS2016. We provide results of three runs in Table 2. The three runs are from the same system, but with models of different training epoc"
S16-1170,D14-1162,0,0.105816,"Missing"
S16-1170,D15-1044,0,0.0285488,"i words, each with a d-dimensional word embedding vector. S i [a] denotes the embedding vector of the a-th word in S i . We then define an attention matrix D ∈ R`0 ×`1 . Entry (a, b) in the matrix D represents the pairwise word similarity score between the a-th word embedding of S 0 and the b-th word embedding of S 1 . The similarity score uses cosine distance: X attenEmb i [a] = concat(S i [a], Ai [a] S i [a]) where represents element-wise multiplication. Our input interaction layer is inspired by recent work that incorporates attention mechanisms into neural networks (Bahdanau et al., 2014; Rush et al., 2015; Yin et al., 2015; Rockt¨aschel et al., 2016). Many of these add parameters and computational complexity to the model. However, our attentionbased input layer is simpler and more efficient. Moreover, we do not introduce any additional parameters, as we simply use cosine distance to create the attention weights. Nevertheless, adding this attention layer improves performance, as we show in Section 5. 4 Word Embeddings We compare several types of word embeddings to represent the initial sentence matrices (S i ). We use the PARAGRAM - SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM"
S16-1170,S12-1060,0,0.112547,"Missing"
S16-1170,S14-2039,0,0.0124857,"the model identify important input words for improved similarity measurement. We also use the strongly-performing PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) (Sec. 4) trained on phrase pairs from the Paraphrase Database (Ganitkevitch et al., 2013). These components comprise our submission to the SemEval-2016 STS competition (shown in Figure 1): an attention-based multi-perspective convolutional neural network augmented with PARAGRAM - PHRASE word embeddings. We provide details of each component in the following sections. Unlike much previous work in the SemEval ˇ c et al., 2012; Sultan et al., 2014), competitions (Sari´ we do not use sparse features, syntactic parsers, or external resources like WordNet. 1103 Proceedings of SemEval-2016, pages 1103–1108, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics from individual dimensions, while holistic filters can discover broader patterns of contextual information. We use both kinds of filters for a richer representation of the input. Output: Similarity Score Structured Similarity Measurement Layer Multi-Perspective Sentence Model Attention-Based Input Interaction Layer Cats Sit On The Mat bc bc bc Parag"
S16-1170,S15-2027,0,0.0490206,"rules of the STS2015 competition (Agirre et al., 2015). We remove or replace one component at a time from the full system and perform re-training and re-testing. 2 For in-domain evaluation, LSTMs outperformed averaging. 1106 We observe a significant drop when the attentionbased input interaction layer (Sec. 3) is removed. We also find that the PARAGRAM - PHRASE word embeddings are highly beneficial, outperforming both GloVe word embeddings (Pennington et al., 2014) and the PARAGRAM - SL 999 embeddings of Wieting et al. (2015). Our full system performs favorably compared to the winning system (Sultan et al., 2015) at the STS2015 SemEval competition. 6 Conclusion Our submission to the SemEval-2016 STS competition uses our multi-perspective convolutional neural network model as the base model. We develop an attention-based input interaction layer to guide the convolutional neural network to focus on the most important input words. We further improve performance by using the PARAGRAM - PHRASE word embeddings, yielding a result on the 2015 test data that surpasses that of the top system from STS2015. Acknowledgments This work was supported by NSF awards IIS1218043 and CNS-1405688. The views expressed here"
S16-1170,P15-1150,0,0.00822716,"asurement Hua He1 , John Wieting2 , Kevin Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outper"
S16-1170,U06-1019,0,0.0144564,"can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitive interactions between the input sentences. We address this problem by utilizing an attention mechanism (Bahdanau et al., 2014) to develop an attention-based input interaction layer (Sec. 3). It converts the two independent input sentences into an inter-related sentence pair, which can help the model identify important input words for improved similarit"
S16-1170,Q15-1025,1,0.582861,"ahdanau et al., 2014; Rush et al., 2015; Yin et al., 2015; Rockt¨aschel et al., 2016). Many of these add parameters and computational complexity to the model. However, our attentionbased input layer is simpler and more efficient. Moreover, we do not introduce any additional parameters, as we simply use cosine distance to create the attention weights. Nevertheless, adding this attention layer improves performance, as we show in Section 5. 4 Word Embeddings We compare several types of word embeddings to represent the initial sentence matrices (S i ). We use the PARAGRAM - SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM - PHRASE embeddings from Wieting et al. (2016). These were both constructed from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) by training on noisy paraphrase pairs using a hinge-based loss with negative sampling. However, they were trained on two different types of data. The PARAGRAM - SL 999 embeddings were trained on the lexical section of PPDB, which consists of word pairs only. The PARAGRAM - PHRASE embeddings were trained on the phrasal section of PPDB, which consists of phrase pairs. The representations for the phrases were created by simply averaging word"
S16-1170,S15-2045,0,\N,Missing
S16-1170,Q14-1034,0,\N,Missing
S16-1170,N15-1091,0,\N,Missing
S18-2024,W16-0202,0,0.241909,"cal analysis of stories (Propp, 1968; Schank and Abelson, 1975; Thorndyke, 1977; Wilensky, 1983). Later methods were based on various methods of planning from artificial intelligence (Theune et al., 2003; Oinonen et al., 2006; Riedl and Young, 2010) or commonsense knowledge resources (Liu and Singh, 2002; Winston, 2014). A detailed summary of this earlier work is beyond our scope; for surveys, please see Mani (2012), Gerv´as (2012), or Gatt and Krahmer (2017). More recent work in story generation has focused on data-driven methods (McIntyre and Lapata, 2009, 2010; McIntyre, 2011; Elson, 2012; Daza et al., 2016; Roemmele, 2016). The generation problem is often constrained via anchoring to some other input, such as a topic or list of keywords (McIntyre and Lapata, 2009), a sequence of images (Huang et al., 2016), a set of loosely3 Data Collection Our goal is to collect annotations of the quality of a sentence in a story given its preceding sentences. We use the term context to refer to the preceding sentences and continuation to refer to the next 1 However, since our scorer does not use a gold standard, it is possible to “game” the metric by directly optimizing the predicted score, so if used as an e"
S18-2024,W11-2123,0,0.0106284,"ntinuation PMI(u, v) Ncontext Ncontination where Ncontext and Ncontinuation are the numbers of tokens in the context and continuation. We create 6 versions of the above score, combining three window sizes (10, 25, and 50) with both standard PMI and positive PMI (PPMI). To compute PMI/PPMI, we use the Personal Story corpus.4 For efficiency and robustness, we only compute PMI/PPMI of a word pair if the pair appears more than 10 times in the corpus using the particular window size. • Language model: perplexity from a 4-gram language model with modified Kneser-Ney smoothing estimated using KenLM (Heafield, 2011) from the Personal Story corpus (Gordon and Swanson, 2009), which includes about 1.6 million personal stories from weblogs. • IDF: the average of the inverse document frequencies (IDFs) across all tokens in the continuation. The IDFs are computed using Wikipedia sentences as “documents”. 4.2.3 Entity Mention Features We compute several features to capture how relevant the continuation is to the input. In 4.2.2 PMI Features We use features based on pointwise mutual information (PMI) of word pairs in the context and continuation. We take inspiration from methods developed for the Choice of Plaus"
S18-2024,N16-1147,0,0.145428,"has a long history, beginning with rule-based systems in the 1970s (Klein et al., 1973; Meehan, 1977). Most story generation research has focused on modeling the plot, characters, and primary action of the story, using simplistic methods for producing the actual linguistic form of the stories (Turner, 1993; Riedl and Young, 2010). More recent work learns from data how to generate stories holistically without a clear separation between content selection and surface realization (McIntyre and Lapata, 2009), with a few recent methods based on recurrent neural networks (Roemmele and Gordon, 2015; Huang et al., 2016). We follow the latter style and focus on a setting in which a few sentences of a story are provided (the context) and the task is to generate the next sentence in the story (the continuation). Our goal 192 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 192–202 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics relevance, and interestingness. We collect multiple annotations for 4586 context/continuation pairs. These annotations permit us to compare methods for story generation and to study the relationships among the crit"
S18-2024,P14-5010,0,0.00311368,"criteria as seen in the final two rows of Table 5. This suggests that human annotators strongly preferred human generated stories over our models’ outputs. Some features may correlate with the annotated criteria if they separate human- and machinegenerated continuations (e.g., IDF). Table 5: Spearman correlations between features and annotations. The final two rows are “oracle” binary features that return 1 for continuations from those sets. order to compute these features we use the part-of-speech tagging, named entity recognition (NER), and coreference resolution tools in Stanford CoreNLP (Manning et al., 2014): • Has old mentions: a binary feature that returns 1 if the continuation has “old mentions,” i.e., mentions that are part of a coreference chain that began in the context. • Number of old mentions: the number of old mentions in the continuation. • Has new mentions: a binary feature that returns 1 if the continuation has “new mentions,” i.e., mentions that are not part of any coreference chain that began in the context. • Number of new mentions: the number of new mentions in the continuation. • Has new names: if the continuation has new mentions, this binary feature returns 1 if any of the new"
S18-2024,P09-1025,0,0.536733,"on in the climate of neural network natural language generation methods. Story generation (Mani, 2012; Gerv´as, 2012) has a long history, beginning with rule-based systems in the 1970s (Klein et al., 1973; Meehan, 1977). Most story generation research has focused on modeling the plot, characters, and primary action of the story, using simplistic methods for producing the actual linguistic form of the stories (Turner, 1993; Riedl and Young, 2010). More recent work learns from data how to generate stories holistically without a clear separation between content selection and surface realization (McIntyre and Lapata, 2009), with a few recent methods based on recurrent neural networks (Roemmele and Gordon, 2015; Huang et al., 2016). We follow the latter style and focus on a setting in which a few sentences of a story are provided (the context) and the task is to generate the next sentence in the story (the continuation). Our goal 192 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 192–202 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics relevance, and interestingness. We collect multiple annotations for 4586 context/continuation pairs. The"
S18-2024,D16-1032,0,0.0240055,"hnological Institute at Chicago, Chicago, IL, 60637, USA ∗ † manasvi@uchicago.edu, jwieting@cs.cmu.edu, {lifu,kgimpel}@ttic.edu Abstract is to produce continuations that are both interesting and relevant given the context. Neural networks are increasingly employed for natural language generation, most often with encoder-decoder architectures based on recurrent neural networks (Cho et al., 2014; Sutskever et al., 2014). However, while neural methods are effective for generation of individual sentences conditioned on some context, they struggle with coherence when used to generate longer texts (Kiddon et al., 2016). In addition, it is challenging to apply neural models in less constrained generation tasks with many valid solutions, such as open-domain dialogue and story continuation. The story continuation task is difficult to formulate and evaluate because there can be a wide variety of reasonable continuations for typical story contexts. This is also the case in open-domain dialogue systems, in which common evaluation metrics like BLEU (Papineni et al., 2002) are only weakly correlated with human judgments (Liu et al., 2016). Another problem with metrics like BLEU is the dependence on a gold standard."
S18-2024,P10-1158,0,0.293555,"Missing"
S18-2024,N16-1098,0,0.298453,"endence on a gold standard. In story generation and open-domain dialogue, there can be several equally good continuations for any given context which suggests that the quality of a continuation should be computable without reliance on a gold standard. In this paper, we study the question of identifying the characteristics of a good continuation for a given context. We begin by building several story generation systems that generate a continuation from a context. We develop simple systems based on recurrent neural networks and similarity-based retrieval and train them on the ROC story dataset (Mostafazadeh et al., 2016). We use crowdsourcing to collect annotations of the quality of the continuations without revealing the gold standard. We ask annotators to judge continuations along three distinct criteria: overall quality, We study the problem of measuring the quality of automatically-generated stories. We focus on the setting in which a few sentences of a story are provided and the task is to generate the next sentence (“continuation”) in the story. We seek to identify what makes a story continuation interesting, relevant, and have high overall quality. We crowdsource annotations along these three criteria"
S18-2024,D16-1230,0,0.039238,"Missing"
S18-2024,P02-1040,0,0.108988,"are effective for generation of individual sentences conditioned on some context, they struggle with coherence when used to generate longer texts (Kiddon et al., 2016). In addition, it is challenging to apply neural models in less constrained generation tasks with many valid solutions, such as open-domain dialogue and story continuation. The story continuation task is difficult to formulate and evaluate because there can be a wide variety of reasonable continuations for typical story contexts. This is also the case in open-domain dialogue systems, in which common evaluation metrics like BLEU (Papineni et al., 2002) are only weakly correlated with human judgments (Liu et al., 2016). Another problem with metrics like BLEU is the dependence on a gold standard. In story generation and open-domain dialogue, there can be several equally good continuations for any given context which suggests that the quality of a continuation should be computable without reliance on a gold standard. In this paper, we study the question of identifying the characteristics of a good continuation for a given context. We begin by building several story generation systems that generate a continuation from a context. We develop simp"
S18-2024,P17-1103,0,0.0168305,"ysis to evaluate story generation systems. They explore the various factors that affect the quality of a story by measuring feature values for different story generation systems, but they do not obtain any quality annotations as we do here. Since there is little work in automatic evaluation of story generation, we can turn to the related task of open-domain dialogue. Evaluation of dialogue systems often uses perplexity or metrics like BLEU (Papineni et al., 2002), but Liu et al. (2016) show that most common evaluation metrics for dialog systems are correlated very weakly with human judgments. Lowe et al. (2017) develop an automatic metric for dialog evaluation by training a model to predict crowdsourced quality judgments. While this idea is very similar to our work, one key difference is that their annotators were shown both system outputs and the gold standard for each context. We fear this can bias the annotations by turning them into a measure of similarity to the gold standard, so we do not show the gold standard to annotators. Wang et al. (2017) use crowdsourcing (upvotes on Quora) to obtain quality judgments for short stories and train models to predict them. One difference is that we obtain a"
S18-2024,D16-1157,1,0.816608,"s for brevity, but the PPMI features showed slightly higher correlations than PMI features. 198 which we denote vb and vc respectively. The first (“cont”) uses only the continuation embedding without any representation of the context or the similarity between the context and continuation: xcont = hvc i. The second (“sim+cont”) also contains the elementwise multiplication of the context and continuation embeddings concatenated with the absolute difference: xsim+cont = hvb vc , |vb − vc |, vc i. To compute representations v, we use the average of character n-gram embeddings (Huang et al., 2013; Wieting et al., 2016), fixing the output dimensionality to 300. We found this to outperform other methods. In particular, the next best method used gated recurrent averaging networks (GRANs; Wieting and Gimpel, 2017), followed by long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), and followed finally by word averaging. The input, whether xcont or xsim+cont , is fed to one fully-connected hidden layer with 300 units, followed by a rectified linear unit (ReLU) activation. Our manually computed features (Length, IDF, PMI, and Mention) are concatenated prior to this layer. The output layer foll"
S18-2024,P17-1190,1,0.845877,"without any representation of the context or the similarity between the context and continuation: xcont = hvc i. The second (“sim+cont”) also contains the elementwise multiplication of the context and continuation embeddings concatenated with the absolute difference: xsim+cont = hvb vc , |vb − vc |, vc i. To compute representations v, we use the average of character n-gram embeddings (Huang et al., 2013; Wieting et al., 2016), fixing the output dimensionality to 300. We found this to outperform other methods. In particular, the next best method used gated recurrent averaging networks (GRANs; Wieting and Gimpel, 2017), followed by long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), and followed finally by word averaging. The input, whether xcont or xsim+cont , is fed to one fully-connected hidden layer with 300 units, followed by a rectified linear unit (ReLU) activation. Our manually computed features (Length, IDF, PMI, and Mention) are concatenated prior to this layer. The output layer follows and uses a linear activation. We use mean absolute error as our loss function during training. We train to predict the three criteria jointly, so the loss is actually the sum of mean absolute"
W08-0302,W05-0909,0,0.0437062,"dering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p < 0.05).6 4 http://www.statmt.org/wmt08 METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6 Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu."
W08-0302,W04-3224,0,0.00884977,"ional challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3 An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). 10 tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also poi"
W08-0302,D07-1090,0,0.0218763,"seto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but ne"
W08-0302,J90-2002,0,0.404921,"scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU scor"
W08-0302,D07-1007,0,0.0344733,"luding feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese → English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the"
W08-0302,P07-1005,0,0.103979,"ble tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e ˆ that maximizes a linear combination of features and weights:1 We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase’s translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a"
W08-0302,P05-1033,0,0.0104973,"9 0.2051 5.957 0.2782 0.2003 5.889 0.2720 Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772 Table 5: English → German shared task system results using WMT-08 Europarl parallel data for training, dev06 for tuning, and three test sets, including the final 2008 test set. The row labeled “Context” uses the top-performing feature set {2 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline. models, including the lexicalized reordering model and the lexical translation model in the Moses MT system, or hierarchical or syntactic models (Chiang, 2005). Additional linguistic analysis (e.g., morphological disambiguation, named entity recognition, semantic role labeling) can be used to define new context features. 8 Conclusion We have described a straightforward, scalable method for improving phrase translation models by modeling features of a phrase’s source-side context. Our method allows incorporation of features from any kind of source-side annotation and barely affects the decoding algorithm. Experiments show performance rivaling or exceeding strong, state-of-the-art baselines on standard translation tasks. Automatic feature selection ca"
W08-0302,W01-0521,0,0.00620467,"ose a computational challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3 An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). 10 tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine transl"
W08-0302,W07-0719,0,0.18129,"Missing"
W08-0302,N03-1017,0,0.0123898,"on beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evalua"
W08-0302,P07-2045,0,0.048035,"e baseline system. 6 Experiments In this section we present experimental results using our context-endowed phrase translation model with a variety of different context features, on Chinese → English, German → English, and English → GerContext features None Lexical Shallow Lexical + Shallow Syntactic Positional Chinese → English (UN) BLEU NIST METEOR 0.3715 7.918 0.6486 0.4030 8.367 0.6716 0.3807 7.981 0.6523 0.4030 8.403 0.6703 0.3823 7.992 0.6531 0.3775 7.958 0.6510 man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f ) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke,"
W08-0302,W04-3250,0,0.0547816,"Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p < 0.05).6 4 http://www.statmt.org/wmt08 METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6 Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.edu/MT. 5 Table 2: Chinese → English experiments: training and testing on UN data. Boldface marks scores significantly higher than “None.” 13 Chinese → English Testing on UN Testing on News (NIST"
W08-0302,J06-4004,0,0.0340549,"Missing"
W08-0302,J03-1002,0,0.00179854,"an → English, and English → GerContext features None Lexical Shallow Lexical + Shallow Syntactic Positional Chinese → English (UN) BLEU NIST METEOR 0.3715 7.918 0.6486 0.4030 8.367 0.6716 0.3807 7.981 0.6523 0.4030 8.403 0.6703 0.3823 7.992 0.6531 0.3775 7.958 0.6510 man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f ) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained o"
W08-0302,P03-1021,0,0.337031,"ge monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(eji |f `k ) and p(f `k |eji ) for each phrase pair heji , f lk i in the candidate sentence pair.2 In this paper, we add and evaluate fea1 In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the features are actually probabilities, this"
W08-0302,P02-1040,0,0.10804,"enefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(eji |f `k ) and p(f `k |eji ) for each phrase pair heji , f lk i in the candidate sentence pair.2 In this paper, we add and evaluate fea1 In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the featu"
W08-0302,2007.tmi-papers.28,0,0.530763,"Missing"
W08-0302,H05-1097,0,0.130546,"mooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chines"
W08-0302,D08-1076,0,\N,Missing
W10-2925,J93-2004,0,0.0383684,"Missing"
W10-2925,J94-2001,0,0.150122,"Missing"
W10-2925,W03-0301,0,0.0211998,"Missing"
W10-2925,W03-0419,0,0.0128423,"Missing"
W10-2925,J93-2003,0,0.0351098,"rgence criterion is left unspecified here. Langford et al. (2009) present convergence rates via regret bounds, which are linear in D. The convergence rate √ of asynchronous stochastic gradient descent is O( T D), where T is the total number of updates made. In addition to the situation in which function components are chosen uniformly at random, Langford et al. provide results for several other scenarios, including the case in which an adversary supplies the training examples in whatever ordering he chooses. of training examples. For example, for simple word alignment models like IBM Model 1 (Brown et al., 1993), only parameters corresponding to words appearing in the particular subsample of sentence pairs are needed. The error introduced when making asynchronous updates should intuitively be less severe in these cases, where different mini-batches use small and mostly nonoverlapping subsets of θ. 5.1 Below we experiment with optimization of both convex and non-convex functions, using fixed step sizes and decreasing step size formulas, and consider several values of D. Even when exploring regions of the experimental space that are not yet supported by theoretical results, asynchronous algorithms perf"
W10-2925,D08-1024,0,0.0252101,"essors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the framework we discuss here and we leave the integration of the two ideas for future exploration. 4 (4) 5 Distributed Asynchronous Mini-Batch Optimization An asynchronous framework may use multiple processors more efficiently and minimize idle time (Nedic et al., 2001; Langford et al., 2009). In this setting, the master sends θ and a mini-batch Mk to e"
W10-2925,W08-0333,0,0.0135295,"-batch is shorter than the time for a full batch, mini-batch algorithms make far more updates and some processor cycles will be wasted in computing each one. Also, more mini-batches imply that more time will be lost due to per-mini-batch overhead (e.g., waiting for synchronization locks in sharedmemory systems, or sending data and θ to the processors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the fra"
W10-2925,P08-1109,0,0.312135,"nguage Technologies Institute Carnegie Mellon Univeristy Pittsburgh, PA 15213, USA {kgimpel,dipanjan,nasmith}@cs.cmu.edu Abstract Online algorithms offer fast convergence rates and scalability to large datasets, but distributed computing is a more natural fit for algorithms that require a lot of computation—e.g., processing a large batch of training examples—to be done between updates. Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms th"
W10-2925,D07-1031,0,0.0117089,"s,9 resulting in a total of 10 runs for each algorithm. We ran each for six hours, saving models every five minutes. After training completed, using each model we decoded the entire training data using posterior decoding and computed the log-likelihood. The results for 5 initial models and two example orderings are shown in Figure 6. We evaluated tagging performance using many-to-1 accuracy, which is obtained by mapping the HMM states to gold standard POS tags so as to maximize accuracy, where multiple states can be mapped to the same tag. This is the metric used by Liang and Klein (2009) and Johnson (2007), who report figures comparable to ours. The asynchronous algorithm converges much faster than the single-node algorithm, allowing a tagger to be trained from the Penn Treebank in less than two hours using a single machine. Furthermore, the 4-processor synchronous algorithm improves only marginally EM converges faster as m decreases. With large mini-batches, load-balancing becomes less important as there will be less variation in per-minibatch observed runtime. These results suggest that asynchronous mini-batch algorithms will be most useful for learning problems in which small minibatches wor"
W10-2925,D07-1033,0,0.0101298,"ingle−processor 86 0 2 4 6 Wall clock time (hours) 8 10 Figure 2: NER: (Top) Synchronous optimization improves very little when moving from 2 to 4 processors due to the need for load-balancing, leaving some processors idle for stretches of time. (Bottom) Asynchronous optimization does not require load balancing and therefore improves when moving from 2 to 4 processors because each processor is in nearconstant use. All curves use a mini-batch size of 4 and the “Single-processor” curve is identical in the two plots. Named Entity Recognition Our NER CRF used a standard set of features, following Kazama and Torisawa (2007), along with token shape features like those in Collins (2002) and simple gazetteer features; a feature was included if and only it occurred at least once in training data (total 1.3M). We used a diagonal Gaussian prior with a variance of 1.0 for each weight. We compared SGD on a single processor to distributed synchronous SGD and distributed asynchronous SGD. For all experiments, we used a fixed step size of 0.01 and chose each training example for each mini-batch uniformly at random from the full data set.3 We report performance by 3 88 plotting test-set accuracy against wall-time over 12 ho"
W10-2925,N03-1017,0,0.00503619,"k and NER is the size of the mini-batch used, so we experimented with several values for the mini-batch size m. Figure 5 shows the results. As m decreases, a larger fraction of time is spent updating parameters; this slows observed convergence time even when using the sparse update rule. It can be seen that, though synchronous and asynchronous stepwise EM converge at the same rate with a large minibatch size (m = 10000), asynchronous stepwise We trained IBM Model 1 in both directions. To align test data, we symmetrized both directional Viterbi alignments using the “grow-diag-final” heuristic (Koehn et al., 2003). We evaluated our models using alignment error rate (AER). Experiments on a Single Machine We followed Liang and Klein (2009) in using synchronous (mini-batch) stepwise EM on a single processor for this task. We used the same learning rate formula (η (t) = (t + 2)−q , with 0.5 &lt; q ≤ 1). We also used asynchronous stepwise EM by using the same update rule, but gathered sufficient statistics on 4 processors of a single machine in parallel, analogous to our asynchronous method from §5. Whenever a processor was done gathering the expected counts for its mini-batch, it updated the sufficient statis"
W10-2925,N09-1069,0,0.450305,"setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms that generalize those presented by Nedic et al. (2001) and Langford et al. (2009). In these algorithms, multiple mini-batches are processed simultaneously, each using potentially different and typically stale parameters. The key advantage of an asynchronous framework is that it allows processors to remain in near-constant use, preventing them from wasting cycles waiting for other processors to complete their portion of the current mini-batch. In this way, as"
W10-2925,W03-1013,0,\N,Missing
W10-2925,P02-1034,0,\N,Missing
W10-2925,P02-1062,0,\N,Missing
W11-2139,2007.mtsummit-papers.3,0,0.0309223,"e with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model about a translation hypoth"
W11-2139,P08-1024,0,0.69537,"we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion”"
W11-2139,J92-4003,0,0.294899,"Missing"
W11-2139,J93-2003,0,0.0182861,"ve use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Si"
W11-2139,W11-2103,0,0.0265339,"ogical analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manually annotated data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the paralle"
W11-2139,P96-1041,0,0.137601,"g data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a"
W11-2139,D08-1024,0,0.0449065,"ormation in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of t"
W11-2139,J07-2003,0,0.877323,"ments to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. 1 Introduction We describe the German-English translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation (WMT11) by the ARK research group at Carnegie Mellon University.1 The core translation system is a hierarchical phrase-based machine translation system (Chiang, 2007) that has been extended in several ways described in this paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2"
W11-2139,P10-1146,0,0.026468,"and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced"
W11-2139,P10-4002,1,0.856164,"ted data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the gr"
W11-2139,N09-1046,1,0.856464,"y higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sente"
W11-2139,W09-0439,0,0.105102,"the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337–343, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics annotations, morphological analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manu"
W11-2139,P06-1121,0,0.0545557,"segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features”"
W11-2139,D09-1023,1,0.847272,"4). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. Howe"
W11-2139,W11-2123,0,0.0277226,"development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used"
W11-2139,D11-1125,0,0.0365813,"that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of"
W11-2139,P02-1017,0,0.0707841,"nder the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of the span, and the length of the span. Importantly, the features do not rely on the target words being predicted, but only look at the structure of the translation derivation. As such, they can be understood as monolingual parse features.3 Table 1 lists the feature templates that were used. Template CTX :fi−1 , fj CTX :fi−1 , fj , x CTX :fi−1 , fj , x, (j − i) LU :fi−1 LB :fi"
W11-2139,N03-1017,0,0.162752,"ompounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many d"
W11-2139,N04-1022,0,0.0377714,") was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gi"
W11-2139,P09-1019,1,0.890376,"ds were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often po"
W11-2139,D09-1005,0,0.0209007,"ed to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary featu"
W11-2139,N09-1069,0,0.0173381,"a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through the training data, instantiating over 8M unique features. The regularization strength λ w"
W11-2139,P06-1096,0,0.14682,"Missing"
W11-2139,C08-1064,0,0.0343443,"e a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was cr"
W11-2139,P08-1114,0,0.022175,"ture modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we"
W11-2139,P02-1038,0,0.0739731,"that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Park"
W11-2139,P03-1021,0,0.537349,"version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (K"
W11-2139,P02-1040,0,0.0824535,"his paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proce"
W11-2139,W04-3201,0,0.116735,"training corpus, it is replaced by a special unknown token. The SMALLCAPS prefixes prevent accidental feature collisions. 3.1 Two-phase discriminative learning The parse features just introduced are numerous and sparse, which means that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm"
W11-2139,P09-1054,0,0.0541831,"− X log hf,ei∈T where pθ (e, d |f) = X pθ (e, d |f) d exp θ&gt; h(f, e, d) Z(f) , where d is a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through"
W11-2139,P10-1040,0,0.0101623,"prove long range reordering quality. To further support the modeling of larger spans, we incorporated a 7-gram class-based language model. Automatic word clusters are attractive because they can be learned for any language without supervised data, and, unlike part-of-speech annotations, each word is in only a single class, which simplifies inference. We performed Brown clustering (Brown et al., 1992) on 900k sentences from our language modeling data (including the news commentary corpus and a subset of Gigaword). We obtained 1,000 clusters using an implementation provided by Liang (2005),6 as Turian et al. (2010) found that relatively large numbers clusters gave better performance for information extraction tasks. We then replaced words with their clusters in our language modeling data and built a 7-gram LM with Witten-Bell smoothing (Witten and Bell, 1991).7 The last two rows of Ta6 http://www.cs.berkeley.edu/˜pliang/ software 7 The distributional assumptions made by the more commonly used Kneser-Ney estimator do not hold in the wordble 2 shows that in conjunction with the source parse features, a slight improvement comes from including the 7-gram LM. 4 Non-translating tokens When two languages share"
W11-2139,D07-1080,0,0.0804662,"algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model abou"
W11-2165,2007.mtsummit-papers.3,0,0.0302107,"previous work used heuristics or local statistical tests to extract patterns from corpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that buil"
W11-2165,P11-1131,0,0.0145795,"ibution over patterns: wCj ∼ ππππππππππππ( ) = either __ or Mult(µ). If the words wCj are not consistent Galley and Manning (2010) proposed ways of incorwith the color assignments, i.e., wrong number of porating phrase pairs with gaps into standard left-toit provides either too little or too much . words or gaps, gaps not in the correct locations, right decoding algorithms familiar to phrase-based it 's neither particularly complicated nor novel . repeat this step. and N -gram-based MT; both used heuristics to exnato must either say "" yes "" or "" no "" to the baltic states . tract phrase pairs. Bansal et al. (2011) presented a Thus, the probability of generating number of words good scientific ideas formulated in bad english either die or get repackaged . model and training procedure for word alignment n, words w1:n , color assignments c1:n , and number that uses phrase pairs with gaps. They use a semi- of colors m is Markov model with an enlarged dynamic programp(w1:n , c1:n , m |β, µ) ming state in order to represent alignment between    n Y m gappy phrases. Their model permits up to one gap 1 β n −β 1 1 = e pµ (π(Cj )) per phrase while our models permit an arbitrary Z n! n m j=1 number. (1) 3 M"
W11-2165,J92-4003,0,0.405129,"Missing"
W11-2165,W10-1703,0,0.127737,"ew one, and for an unaligned source word wi0 , c0i can be any source color, including a new one. The full equations for sampling can be easily derived using the equations from §3. 5 Evaluation We conducted evaluation to determine (1) what types of phenomena are captured by the most probable patterns discovered by our models, and (2) whether including the patterns as features can improve translation quality. 5.1 5.1.1 Qualitative Evaluation Monolingual Model Since inference is computationally expensive, we used the 126K-sentence English news commentary corpus provided for the WMT shared tasks (Callison-Burch et al., 2010). We ran Gibbs sampling for 600 iterations through the data, discarding the first 300 samples for burn-in and computing statistics of the patterns using the remaining 300 samples. Each iteration took approximately 3 minutes on a single 2.2GHz CPU. When looking primarily at the most frequent patterns, we found that this list did not vary much when only using half of the data instead. We set ν = 3 and α = 100; we found these hyperparameters to have only minor effects on the results. Since many frequent patterns include the period (.), we found it useful to constrain the model to treat this token"
W11-2165,W08-0336,0,0.0147491,"modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Language models were trained on the target side of the parallel corpus as well as the first 5 million additional sentences from the extra English monolingual newswire data provided for the shared tasks. We used news-test2008 for tuning and news-test2009 for testing. We also consider Chinese-English (ZH→EN) and followed a similar training procedure as above. We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. A trigram language model was estimated using modified KneserNey smoothing from the English side of the parallel corpus concatenated with 200M words of randomlyselected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used NIST MT03 for tuning and NIST MT05 for testing. For evaluation, we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a"
W11-2165,D08-1024,0,0.0390457,"Missing"
W11-2165,P05-1033,0,0.352732,"text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with th"
W11-2165,J07-2003,0,0.310873,"orpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily ma"
W11-2165,2009.eamt-1.10,0,0.0147609,"to a phrase-based MT system that scores new words based on all potential triggers from previous parts of the derivation. We are not aware of prior work that uses generative modeling and Bayesian nonparametrics to discover these same types of patterns automatically; doing so allows us to discover larger patterns with more words and gaps if they are warranted by the data. In addition to the gappy phrase-based (Simard et al., 2005) and hierarchical phrase-based (Chiang, 2005) models mentioned earlier, other researchers have explored the use of bilingual gappy structures for machine translation. Crego and Yvon (2009) and 512 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512–522, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics nato must either say "" yes "" or "" no "" to the baltic states . π( ) = nato π( ) = say π( ) = to the π( ) = must π( ) = "" __ "" __ "" __ "" π( ) = baltic states π( ) = either __ or π( ) = yes __ no π( ) = . Figure 1: A sentence from the news commentary corpus, along with color assignments for the words and the π function for each color. 1. Sample the number of words: n ∼ Poisson(β) 2. Sample the number of unique color"
W11-2165,N10-1140,0,0.0296996,"_ or π( ) = yes __ no π( ) = . Figure 1: A sentence from the news commentary corpus, along with color assignments for the words and the π function for each color. 1. Sample the number of words: n ∼ Poisson(β) 2. Sample the number of unique colors in the sentence given n: m ∼ Uniform(1, n) 3. For each word index i = 1 . . . n, sample the color of word i: ci ∼ Uniform(1, m). If any of the m colors has no words, repeat this step. 4. For each color j = 1 . . . m, sample from a multinomial distribution over patterns: wCj ∼ ππππππππππππ( ) = either __ or Mult(µ). If the words wCj are not consistent Galley and Manning (2010) proposed ways of incorwith the color assignments, i.e., wrong number of porating phrase pairs with gaps into standard left-toit provides either too little or too much . words or gaps, gaps not in the correct locations, right decoding algorithms familiar to phrase-based it 's neither particularly complicated nor novel . repeat this step. and N -gram-based MT; both used heuristics to exnato must either say "" yes "" or "" no "" to the baltic states . tract phrase pairs. Bansal et al. (2011) presented a Thus, the probability of generating number of words good scientific ideas formulated in bad engli"
W11-2165,P07-1019,0,0.0196628,"been shown to be effective for MT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). Though not shown in the algorithm, in practice we store the BLEU-best translation on each k-best list from all previous iterations and use it as e+ if it has a higher BLEU score than any on the k-best list on the current iteration. At decoding time, we follow a procedure similar to training: we generate lattices for each source sentence using Moses with its standard set of features and using weights λM . We rescore the lattices using λ∗ and use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to incorporate the gappy pattern features with weights θ ∗ . Cube pruning is necessary because the pattern features may match anywhere in the translation; thus they are non-local in the phrase lattice and require approximate inference. 5.3 ES→EN 25.64 25.85 Training Algorithm Comparison Before adding pattern features, we evaluate our training algorithm by comparing it to MERT using the same standard Moses features. As the ini520 5.4 Feature Preparation We chose monolingual and bilingual pattern features using the posterior samples obtained via the inference procedures described above. We rank"
W11-2165,N03-1017,0,0.028193,"patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with the success of phrase-based translation models (Koehn et al., 2003), a trend arose of modeling larger and increasingly complex structural units in translation. One thread of work has focused on the use of lexical patterns with gaps. Simard et al. (2005) proposed using phrase pairs with gaps in a phrase-based translation model, providing a heuristic method to extract gappy phrase pairs from wordaligned parallel corpora. The widely-used hierarchical phrase-based translation framework was introduced by Chiang (2005) and also relies on a simple heuristic for phrase pair extraction. On the monolingual side, researchers have taken inspiration from trigger-based lan"
W11-2165,P07-2045,0,0.015552,"e subject in English when translating from Spanish, as Spanish often drops the subject when it is clear from context, e.g., “we are(estamos)”. Also, one probable pattern for German-English was “the of the(des)” (des is aligned to the final the). The German determiner des is in the genitive case, so this pattern helps to encourage its object to also be in the genitive case when translated. 5.2 Quantitative Evaluation We consider the Spanish-to-English (ES→EN) translation task from the ACL-2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We trained a Moses system (Koehn et al., 2007) following the baseline training instructions for the shared task.7 In particular, we performed word alignment in each direction using GIZA++ (Och and Ney, 2003), used the “grow-diag-final-and” heuristic for symmetrization, and extracted phrase pairs up to a maximum length of seven. After filtering sentence pairs with one sentence longer than 50 words, we ended up with 1.45M sentence pairs of Europarl data and 91K sentence pairs of news commentary data. Language models (N = 5) were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Go"
W11-2165,P06-1096,0,0.0610772,"Missing"
W11-2165,J03-1002,0,0.0135678,"particular prepositions). There are also many probable patterns without gaps, shown at the bottom of Table 4. From these patterns we can see that our models can also be used to find collocations, but we note that these are discovered in the context of the gappy patterns. That is, due to the use of latent variables in our models (the color assignments), there is a natural trading-off effect whereby the gappy patterns encourage particular non-gappy patterns to be used, and vice versa. 5.1.2 Bilingual Model We use the news commentary corpus for each language and take the intersection of GIZA++ (Och and Ney, 2003) word alignments in each direction, thereby ensuring that they are 1-to-1 alignments. We ran Gibbs sampling for 300 iterations, averaging pattern counts from the last 200. We set α = 100, λ = 3, and γ = 0.5. We ran the model in 3 conditions: source words, target words; source clusters, target clusters; and source clusters, target words. We 6 We filter Brown cluster patterns in which every cluster is a singleton, since these patterns are typically already accounted for in the lexical patterns. Rank 2 6 28 178 239 8 12 21 23 43 46 149 172 180 5 9 19 40 45 50 56 68 98 131 1 15 30 47 62 72 73 113"
W11-2165,P03-1021,0,0.0604284,", we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a translation during decoding. We leave this problem for future work and instead simply add a feature for each of the most probable patterns discovered by our models. Each feature counts the number of occurrences of its pattern in the translation. We wish to add thousands of features to our model, but the standard training algorithm – minimum error rate training (MERT; Och, 2003) – cannot handle large numbers of features. So, we leverage recent work on feature-rich training for MT using online discriminative learning algorithms. Our training procedure is shown as Algorithm 1. We find it convenient to notationally distinguish feature weights for the standard Moses features (λ) from weights for our pattern features (θ). We use h(e) to denote the feature vector for translation e. The function Bi (t) returns the sentence BLEU score for translation t given reference ei (i.e., treating the sentence pair as a corpus).8 MERT is run to convergence on the tuning set to obtain w"
W11-2165,2001.mtsummit-papers.68,0,0.013434,"training procedure as above. We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. A trigram language model was estimated using modified KneserNey smoothing from the English side of the parallel corpus concatenated with 200M words of randomlyselected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used NIST MT03 for tuning and NIST MT05 for testing. For evaluation, we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a translation during decoding. We leave this problem for future work and instead simply add a feature for each of the most probable patterns discovered by our models. Each feature counts the number of occurrences of its pattern in the translation. We wish to add thousands of features to our model, but the standard training algorithm – minimum error rate training (MERT; Och, 2003) – cannot handle large numbers of features. So, w"
W11-2165,H05-1095,0,0.0483087,"Missing"
W11-2165,W02-1021,0,0.0431779,"work on feature-rich training for MT using online discriminative learning algorithms. Our training procedure is shown as Algorithm 1. We find it convenient to notationally distinguish feature weights for the standard Moses features (λ) from weights for our pattern features (θ). We use h(e) to denote the feature vector for translation e. The function Bi (t) returns the sentence BLEU score for translation t given reference ei (i.e., treating the sentence pair as a corpus).8 MERT is run to convergence on the tuning set to obtain weights for the standard Moses features (line 1). Phrase lattices (Ueffing et al., 2002) are generated for all source sentences in the tuning set using the trained weights λM (line 2). The lattices are used within a modified version of the margininfused relaxed algorithm (MIRA; Crammer et al., 2006) for structured max-margin learning (lines 515). A k-best list is extracted from the current lattice (line 7), then the translations on the k-best list with the highest and lowest sentence-level BLEU scores are found (lines 8 and 9). The step size is then computed using the standard MIRA formula (lines 1011) and the update is made (line 12). The returned weights are averaged over all u"
W11-2165,D07-1080,0,0.0178299,"ristics or local statistical tests to extract patterns from corpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy"
W11-2165,P11-1129,0,0.0733406,"t inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with the success of phrase-b"
W11-2165,P02-1040,0,\N,Missing
W11-2165,D08-1076,0,\N,Missing
W16-1612,W10-1408,0,0.0608533,"Missing"
W16-1612,D15-1041,0,0.0192778,"the mapper tends to be very quick because training examples are word types rather than word tokens. When we increase τt , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014"
W16-1612,P15-1001,0,0.0851014,"as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to"
W16-1612,P11-1070,1,0.750738,"nce we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems"
W16-1612,P14-2131,1,0.778269,"et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Model Parameters W Annotated Training Sentences Initial Embeddings eoi Parser Training Mapper Function Task-Trained Embeddings eti Mapper Function"
W16-1612,P14-1062,0,0.0219581,"Missing"
W16-1612,W09-3821,0,0.0868285,"Missing"
W16-1612,J03-3005,0,0.0248644,"examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong"
W16-1612,D14-1082,0,0.206154,"s. way, but for our purposes they can be any initial embeddings. Let T ⊆ V be the subset of words that appear in the annotated training data for some supervised task-specific training. We define unseen words as those in the set V  T . While our approach is general, for concreteness, we consider the task of dependency parsing, so the annotated data consists of sentences paired with dependency trees. We assume a dependency parser that learns task-specific word embeddings eti for word wi ∈ T , starting from the original embedding eoi . In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014). The goal of the mapper is as follows. We are given a training set of N pairs of  and task-trained  initial  embeddings D = eo1 , et1 , . . . , eoN , etN , and we want to learn a function G that maps each initial embedding eoi to be as close as possible to its corresponding output embedding eti . We denote the mapped embedding o m em i , i.e., ei = G (ei ). Figure 1a describes the training procedure of the mapper. We use a supervised parser which is trained on an annotated dataset and initialized with pre-trained word embeddings eoi . The parser uses back-propagation to update these embedd"
W16-1612,D14-1181,0,0.0133613,"l., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015). They induce embeddings for unseen words by combining the embeddings of the k nearest neighbors. In Sec. 4, we show that our approach outperforms theirs. Also related is the approach taken by Kiros et al. (2015). They learn a linear mapping of the initial embedding space via unregularize"
W16-1612,D15-1249,0,0.0203196,"and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015)"
W16-1612,D14-1108,0,0.051108,"Missing"
W16-1612,P15-1033,0,0.0139127,"s much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015)."
W16-1612,D15-1176,0,0.129017,"Missing"
W16-1612,W13-3512,0,0.0375429,"including those that learn a single 1 Note that the training of the mapper tends to be very quick because training examples are word types rather than word tokens. When we increase τt , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combinin"
W16-1612,D10-1069,0,0.0241754,"e test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors. We address this by learning a neural network to map from initial embeddings to the task-specific embedding space, via a multi-loss objective function. The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis. 1 Karen Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al.,"
W16-1612,P15-1002,0,0.0694421,"use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that"
W16-1612,K15-1009,0,0.0372302,"Missing"
W16-1612,W10-1410,0,0.0515923,"Missing"
W16-1612,J93-2004,0,0.0569575,"Missing"
W16-1612,W10-1402,0,0.0143993,"t , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies"
W16-1612,D13-1170,0,0.0124659,"Missing"
W16-1612,C12-2114,0,0.466507,"stract is a great deal of work on updating embeddings during supervised training to make them more task-specific (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014). These tasktrained embeddings have shown encouraging results but raise some concerns: (1) the updated embeddings of infrequent words are prone to overfitting, and (2) many words in the test data are not contained in the training data at all. In the latter case, at test time, systems either use a single, generic embedding for all unseen words or use their initial embeddings (typically derived from unlabelled data) (Søgaard and Johannsen, 2012; Collobert et al., 2011). Neither choice is ideal: A single unknown embedding conflates many words, while the initial embeddings may be in a space that is not comparable to the trained embedding space. In this paper, we address both concerns by learning to map from the initial embedding space to the task-trained space. We train a neural network mapping function that takes initial word embeddings and maps them to task-specific embeddings that are trained for the given task, via a multi-loss objective function. We tune the mapper’s hyperparameters to optimize performance on each domain of inter"
W16-1612,S15-1021,0,0.028312,"sentiment analysis. 1 Karen Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistic"
W16-1612,P15-1150,0,0.034653,"Missing"
W16-1612,D14-1162,0,0.086068,"he effect of the mapping of unseen words, showing statistically significant improvements on both parsing and a downstream task (sentiment analysis). cance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100K samples. 4 Web Treebank We expect our mapper to be most effective when parsing held-out data with many unseen words. This often happens when the held-out data is drawn from a different distribution than the training data. For example, when training a parser on newswire and testing on web data, 4.2 Pre-Trained Word Embeddings We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.2 4.3 Datasets We consider a number of datasets with varying rates of OOTV words. We define the OOTV rate (or, equivalently, the unseen rate) of a dataset as the percentage of the vocabulary (types) of words occurring in the set that were not seen in training. Wall Street Journal (WSJ) and OntoNotes-WSJ We conduct experiments on the Wall Street Journal portion of the English Penn Treebank dataset (Marcus et al., 1993). We follow the standard splits: sections 2-21 for training,"
W16-1612,P10-1040,0,0.0512462,"ren Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Model Parameters W"
W16-1612,D14-1122,0,0.0540584,"Missing"
W17-2604,P17-1055,0,0.248205,"gmax c X &lt; ht , hq &gt; &lt; ht , e(c) &gt; (1) t where e(c) is the vector embedding of the constant symbol (entity identifier) c. In practice the innerproduct &lt; ht , hq &gt; is normalized over t using a softmax to yield attention weights αt over t and Authors contributed equally. 26 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 26–36, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (1) becomes argmax &lt; e(c), c X αt ht &gt; . Reader (Kadlec et al., 2016), the Gated Attention Reader (Dhingra et al., 2017), and the Attentionover-Attention Reader (Cui et al., 2017). So far we have only considered anonymized datasets that require the handling of semanticsfree constant symbols. However, even for nonanonymized datasets such as Who-did-What, it is helpful to add features which indicate which positions in the passage are referring to which candidate answers. This indicates, not surprisingly, that reference is important in question answering. The fact that explicit reference features are needed in aggregation readers on non-anonymized data indicates that reference is not being solved by the aggregation readers. However, as reference seems to be important for"
W17-2604,P17-1168,0,0.413416,"q. They then select an answer c using a criterion similar to argmax c X &lt; ht , hq &gt; &lt; ht , e(c) &gt; (1) t where e(c) is the vector embedding of the constant symbol (entity identifier) c. In practice the innerproduct &lt; ht , hq &gt; is normalized over t using a softmax to yield attention weights αt over t and Authors contributed equally. 26 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 26–36, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (1) becomes argmax &lt; e(c), c X αt ht &gt; . Reader (Kadlec et al., 2016), the Gated Attention Reader (Dhingra et al., 2017), and the Attentionover-Attention Reader (Cui et al., 2017). So far we have only considered anonymized datasets that require the handling of semanticsfree constant symbols. However, even for nonanonymized datasets such as Who-did-What, it is helpful to add features which indicate which positions in the passage are referring to which candidate answers. This indicates, not surprisingly, that reference is important in question answering. The fact that explicit reference features are needed in aggregation readers on non-anonymized data indicates that reference is not being solved by the aggregatio"
W17-2604,D14-1162,0,0.0988832,"Missing"
W17-2604,P16-1086,0,0.541209,"passage and also an embedding hq of the question q. They then select an answer c using a criterion similar to argmax c X &lt; ht , hq &gt; &lt; ht , e(c) &gt; (1) t where e(c) is the vector embedding of the constant symbol (entity identifier) c. In practice the innerproduct &lt; ht , hq &gt; is normalized over t using a softmax to yield attention weights αt over t and Authors contributed equally. 26 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 26–36, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (1) becomes argmax &lt; e(c), c X αt ht &gt; . Reader (Kadlec et al., 2016), the Gated Attention Reader (Dhingra et al., 2017), and the Attentionover-Attention Reader (Cui et al., 2017). So far we have only considered anonymized datasets that require the handling of semanticsfree constant symbols. However, even for nonanonymized datasets such as Who-did-What, it is helpful to add features which indicate which positions in the passage are referring to which candidate answers. This indicates, not surprisingly, that reference is important in question answering. The fact that explicit reference features are needed in aggregation readers on non-anonymized data indicates t"
W17-2604,D16-1264,0,0.0248007,"machine comprehension we give a general formulation of the machine comprehension task. We take an instance of the task to be a four tuple (q, p, a, A), where q is a question given as a sequence of words containing a special token for a “blank” to be filled in, p is a document consisting of a sequence of 27 sourced workers. The dataset only contains 660 documents and is too small to train deep models. The bAbI dataset (Weston et al., 2016) is constructed automatically using synthetic text generation and can be perfectly answered by hand-written algorithms (Lee et al., 2016). The SQuAD dataset (Rajpurkar et al., 2016) consists of passage-question pairs where the passage is a Wikipedia article and the questions are written via crowdsourcing. The dataset contains over 100,000 problems, but the answer is often a word sequence which is difficult to handle with the reader models considered here. The Children’s Book Test (CBT) (Hill et al., 2016) takes any sequence of 21 consecutive sentences from a children’s book: the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence. The task complexity varies with the type of the omitted word (verb, preposition, named en"
W17-2604,D13-1020,0,0.0610026,"gation readers more specifically by equations (7) and (9) below. Explicit reference readers include the Attention-Sum Reader (Kadlec et al., 2016), the Gated-Attention Reader (Dhingra et al., 2017), and the Attention-over-Attention Reader (Cui et al., 2017). In this section we define explicit reference readers more specifically by equation (13) below. We first present the Stanford Reader as a paradigmatic aggregation reader and the Attention-Sum Reader as a paradigmatic explicit reference reader. Other Related Datasets. It is also worth mentioning several related datasets. The MCTest dataset (Richardson et al., 2013) consists of children’s stories and questions written by crowd28 3.1 It should be noted that (10) trains output vectors over the whole vocabulary rather than just those items occurring in the choice set A. This is empirically significant in non-anonymized datasets such as CBT and Who-did-What where choices at test time may never have occurred as choices in the training data. Attentive Reader. The Stanford Reader was derived from the Attentive Reader (Hermann et al., 2015). The Attentive Reader uses αt = softmaxt MLP([ht , hq ]) instead of (7). Here MLP(x) is the output of a multi layer percept"
W17-2604,P16-1223,0,\N,Missing
W17-2604,D16-1241,1,\N,Missing
W17-2604,E17-2009,1,\N,Missing
W17-2632,P14-2131,1,0.814463,"bed any word in its context. Introduction Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words. Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013). The majority of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embedWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing. We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our tok"
W17-2632,D14-1110,0,0.0277598,"Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling inform"
W17-2632,D15-1200,0,0.0250144,"ulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and those of its context word"
W17-2632,P15-1107,0,0.0345098,"r does not distinguish word j other than by centering the window at its position. It is left to the training objectives to place emphasis on word j as needed (see Section 3.3). Varying w0 will influence the phenomena captured by this encoder, with smaller windows capturing similarity in terms of local syntactic category (e.g., noun vs. verb) and larger windows helping to distinguish word senses or to identify properties of the discourse (e.g., topic or style). 266 3.2 train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (“seq2seq”) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015). That is, we use one LSTM as the encoder f and another LSTM for the decoder g, initializing g’s hidden state to the output of f . Since we use the same weighted reconstruction error described above, the decoder must output a single vector at each step rather than a distribution over word types. So we use an affine transformation on the LSTM decoder hidden vector at each step in order to generate the output vector for each step. Reconstruction error has efficiency advantages over log loss here in that it avoids the costly summation over the vocabulary. Recurrent Neural Netwo"
W17-2632,D15-1176,0,0.0344975,"eling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and those of its context words into a representation for the token. While multi-type embeddings require more data for training, parametric models require less. There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long shortterm memory (LSTM) networks (K˚ageb¨ack et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al., 2011), or other architectures. However, learning to represent tokens in supervised training can suffer from limited data. We instead focus on learning token embedding models on unlabeled data, then use them to produce features for downstream tasks. So we focus on efficient architectures and unsupervised learning criteria. The most closely related work consists of efforts to train LSTMs to represent tokens in context using unsupervised training objectives. Kawakami and Dyer (2015) use multilingual data to lear"
W17-2632,P11-2008,1,0.83856,"Missing"
W17-2632,J93-2004,0,0.0603514,"Missing"
W17-2632,C16-1256,0,0.0165663,"y parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to d"
W17-2632,K16-1006,0,0.0211522,"nomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and those of its context words into a representation for the token. While multi-type embeddings require more data for training, parametric models require less. There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long shortterm memory (LSTM) networks (K˚ageb¨ack et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al., 2011), or other architectures. However, learning to represent tokens in supervised training can suffer from limited data. We instead focus on learning token embedding models on unlabeled data, then use them to produce features for downstream tasks. So we focus on efficient architectures and unsupervised learning criteria. The most closely related work consists of efforts to train LSTMs to represent tokens in context using unsupervised training objectives. Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of"
W17-2632,C14-1048,0,0.0186892,"onal Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning mult"
W17-2632,D14-1113,0,0.0226952,"thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and t"
W17-2632,P12-1092,0,0.0413247,"s of the 2nd Workshop on Representation Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token emb"
W17-2632,N13-1039,1,0.916066,"Missing"
W17-2632,N15-1070,0,0.0190357,"t-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and those of its context words into a representation for the token. While multi-type embeddings require mo"
W17-2632,D14-1162,0,0.0890759,"s expressed in the context of a token. Unlike type embeddings, it is infeasible to precompute and store all possible (or even a significant fraction of) token embeddings. Instead, our token embedding models are parametric, so they can be applied on the fly to embed any word in its context. Introduction Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words. Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013). The majority of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embedWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downst"
W17-2632,W15-1504,0,0.0481548,"Missing"
W17-2632,P17-1161,0,0.0252544,"architectures. However, learning to represent tokens in supervised training can suffer from limited data. We instead focus on learning token embedding models on unlabeled data, then use them to produce features for downstream tasks. So we focus on efficient architectures and unsupervised learning criteria. The most closely related work consists of efforts to train LSTMs to represent tokens in context using unsupervised training objectives. Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al. (2017) use unsupervised learning with monolingual sentences. We experiment with LSTM token embedding models as well, though we focus on different tasks: POS tagging and de3 Token Embedding Models We assume access to pretrained type embeddings. Let W denote a vocabulary of word types. For each word type x ∈ W, we denote its type embedding by v x ∈ Rd . We define a word sequence x = hx1 , x2 , ..., x|x |i in which each entry xj is a word type, i.e., xj ∈ W. We define a word token as an element in a word sequence. We consider the class of functions f that take a word sequence x and index j of a particu"
W17-2632,R15-1061,0,0.0269009,"Missing"
W17-2632,D14-1108,0,0.364822,"ty of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embedWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing. We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our token embeddings to Tweeboparser (Kong et al., 2014), improving its performance and establishing a new state of the art for Twitter dependency parsing. 1 A word type is an entry in a vocabulary, while a word token is an instance of a word type in a corpus. 265 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-se"
W17-2632,N10-1013,0,0.0562442,"in a corpus. 265 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In"
W17-2632,N16-1160,0,0.0618676,"Missing"
W17-2632,C14-1016,0,0.02059,"p on Representation Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable"
W17-2632,P10-1040,0,0.0669986,"tric, so they can be applied on the fly to embed any word in its context. Introduction Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words. Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013). The majority of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embedWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing. We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS ta"
W17-2632,N16-1151,0,0.0143537,"stance of a word type in a corpus. 265 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 265–275, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work pendency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models. The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi˜na and Johansson, 2015; Wu and Giles, 2015). Some use bilingual inforˇ mation (Guo et al., 2014; Suster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014). These “multi-type” embeddings are restricted to modeling phenomena expressed by a single clustering"
W17-2632,D13-1141,0,0.0487331,"ontext. Introduction Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words. Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013). The majority of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embedWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing. We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our token embeddings to Tw"
